{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "\n",
    "### Supervised learning and classification tasks\n",
    "\n",
    "Let's start by reviewing some of the basics of machine learning. Supervised learning is the process of training a model using labelled data to predict the same values on unlabelled data. There are two types of supervised learning problems: classification and regression. The former is the process of predicting a label that can be categorized into one of two or more categories while the latter predicts a continuous variable.\n",
    "\n",
    "<img src=\"_images/supervised_learning.png\" width=\"600pix\">\n",
    "\n",
    "Text classification is a supervised learning task specifically. Natural language processing (NLP) models would require pre-labelled classification of \"documents\" (e.g., any collection of text: a PDF, an article, a tweet) and the potentially painstaking process of feature engineering to train a model to output the correct classifcation of unlabelled documents.\n",
    "\n",
    "LLMs can replace other machine learning tasks for classification of texts. Unlike NLP models that require pre-labeled training data or a pre-defined vocabulary of words or n-grams (i.e., feature engineering), LLMs allow for zero-shot or few-shot learning to label text-like data.\n",
    "\n",
    "Examples:\n",
    "1) sentiment analysis of Yelp reviews\n",
    "2) categorize customer support requests (refund, complaint, login issues, etc.)\n",
    "3) SPAM filter\n",
    "4) Hate speech or inappropriate speech detection on social media\n",
    "5) Topic identification of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code walkthrough: labeling sentiment of Tweets regarding a specific event or hashtag\n",
    "\n",
    "For this example, we are going to assume that the Tweets (or x-eets?) have already been mined from the API. The actual process of obtaining/purchacing an API key and interacting with the API for Twitter/X is beyond the scope of this example.\n",
    "\n",
    "Below are some tweets pulled from around the time the season finale of Game of Thrones ended on May 19, 2019 that contain the hashtags #GOT or #GameOfThrones. This eight year HBO series was the cultural zeitgeist of the last decade and despite having fantastic reviews the first seven seasons, had a very controversial ending.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "'''\n",
    "Can’t believe #GameOfThrones is coming to an end 😭. \n",
    "This season will never take away how much love I have for this show man\n",
    "''',\n",
    "'''\n",
    "Last #GameOfThrones episode tonight.  Nervous I’ll be disappointed. \n",
    "''',\n",
    "'''\n",
    "The more I ponder, the more I ADORE WITH A PASSION #LadyOlenna of House Tyrell.\n",
    "This BADASSERY WILL NEVER BE SEEN AGAIN ON TV. #GameOfThronesFinale #GameofThrones\n",
    "''',\n",
    "'''\n",
    "When you’re the only person at a GOT finale watch party \n",
    "that hasn’t seen one damn episode.  #me #GOT #sundaysareforwine\n",
    "''',\n",
    "'''\n",
    "It wouldnt be so bad if they didnt make us wait an extra year. \n",
    "But they did and they fed us 6 episodes of TBS original programming quality poop! #GoT\n",
    "''',\n",
    "'''\n",
    "Based on the uproar over the ending I'm glad I never watched #GameOfThrones\n",
    "''',\n",
    "'''\n",
    "Rewatching #Gameofthrones finale. \n",
    "Danny’s speech was so awesome. So badass. \n",
    "And the unsullied with the Uruk-hai spear chant. Dope.\n",
    "''',\n",
    "'''\n",
    "Bran=Dr Strange. Both knew what had to happen to save the world, \n",
    "but neither could interfere or it would disturb the timeline. \n",
    "They also used that knowledge to encourage certain situations to \n",
    "acquire the desired outcome. #GOT #AvengersEngame\n",
    "''',\n",
    "'''\n",
    "“'Game of Thrones' and star Peter Dinklage are big wins for portrayal of little people.\n",
    "\n",
    "Little people have always been stereotyped in movies and TV. \"Game of Thrones,\" \n",
    "and the character Tyrion, is a breakthrough”\n",
    "https://usatoday.com/story/life/2019/05/19/game-of-thrones-peter-dinklage-hero-little-people/3736538002/\n",
    "\n",
    "#GameOfThrones  #RepresentationMatters\n",
    "'''\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to organize these tweets into different categories? For example, we can ask if the person writing the tweet is a die-hard fan of the show or someone who has never watched it. We can also ask if the person has a positive, negative, or neutral reaction to the series finale.\n",
    "\n",
    "We'll use the [langchain](https://python.langchain.com/docs/get_started/introduction) software in Python to build our prompts. Langchain can interface with a number of different LLMs (including OpenAi, which we'll use in this example) and ways to chain together prompts to get the desired output.\n",
    "\n",
    "You can install the langchain module and its dependent openai module using pip.\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "You will also want to create an environment variable containing your OpenAI API token.\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY = YOUR-TOKEN-HERE\n",
    "```\n",
    "\n",
    "You can also do this in Python.\n",
    "```python\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR TOKEN HERE'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the .env file containing your API key\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Designing the right prompt\n",
    "\n",
    "We want to send a prompt to the AI that explains the task and the desired output. We may not need to be super precise when working with Chat GPT, but if we are working with code, we need to be more thoughtful about our design as we want a consistent output that won't trip up in automation. Here are some helpful tips to keep in mind.\n",
    "\n",
    "- Specify the output format: in this example, we want to classify the text into one of three categories of sentiments -- positive, negative, or neutral. We could tell GPT to output one of those texts, or more simply, an integer coded to one of those values. Also, a single integer is one token and therefore the cheapest output the API can give us (remember that both input and output token counts contribute to the overall cost of prompts)! You may also want a Python list, tuple, or maybe even a JSON, CSV, or HTML file for more complex information. Any of these are possible: just be specific! You can also specify a limit on the number of output tokens when initializing the LLM. If you are asking for output in a coding language, you might want to also specify the version (i.e., Python 2 vs 3).\n",
    "\n",
    "- Provide examples: Examples are the equivalent of providing a training set to supervised learning models; however, the list of examples does not nearly need to be as vast as a traditional training set. LLMs are pre-trained zero-shot learners, which means they can do a fairly good job at generating the desired output without the user needing to do any fine-tuning. However, providing a few examples of what the output should look like, or \"few-shot learning\", can make the model much more reliable. I gave a single example and was able to get consistent results; but feel free to provide more.\n",
    "\n",
    "##### Prompt template\n",
    "\n",
    "Langchain provides an easy way to design a prompt template we can use to enter multiple variables into the prompt so it can be used over and over again. In this case, the only variable we have is the tweet itself. We can enter that text into the prompt using f-strings in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      3\u001b[0m template \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mClassify the following tweet into one of the following categories:\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m'''\u001b[39m,\n\u001b[1;32m     22\u001b[0m input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m],output_parser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m## Note: langchain does allow for users to provide their own output parser class\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m## inherited from a base class offered in the library. Designing an output parser\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m## is beyond the scope of this work as it is more advanced Python coding. But those \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m## who are familiar with object oriented programming should give it a try!\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template='''\n",
    "Classify the following tweet into one of the following categories:\n",
    "\n",
    "1. Positive \n",
    "2. Negative\n",
    "3. Neutral\n",
    "\n",
    "Return the answer as a number 1, 2, or 3.\n",
    "\n",
    "===\n",
    "Example:\n",
    "Tweet: The ending of Game of Thrones was so bad. I can't believe they did that to us.\n",
    "\n",
    "Result: 2\n",
    "===\n",
    "\n",
    "Here is the Tweet:\n",
    "{tweet}\n",
    "''',\n",
    "input_variables=['tweet'],output_parser=None)\n",
    "\n",
    "## Note: langchain does allow for users to provide their own output parser class\n",
    "## inherited from a base class offered in the library. Designing an output parser\n",
    "## is beyond the scope of this work as it is more advanced Python coding. But those \n",
    "## who are familiar with object oriented programming should give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch a chat model from OpenAI using langchain as well. Let's use gpt-3.5-turbo with temperature=0 (no creativity) and max_tokens=1 (since we only want a single character output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,model_name='gpt-3.5-turbo',max_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll simply loop over each tweet, insert it into the prompt, and then call the API to predict the sentiment. If we were doing this for several hundred or more tweets, we might want need to worry about managing timing limits on the number of calls we are allowed to make per minute/day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for tweet in tweets:\n",
    "    prompt = template.format(tweet=tweet)\n",
    "    result = llm.predict(prompt)\n",
    "    sentiments.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask GPT to print out all of the positive (1) tweets and negative (2) tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These tweets are positive:\n",
      "\n",
      "Can’t believe #GameOfThrones is coming to an end 😭. \n",
      "This season will never take away how much love I have for this show man\n",
      "\n",
      "\n",
      "The more I ponder, the more I ADORE WITH A PASSION #LadyOlenna of House Tyrell.\n",
      "This BADASSERY WILL NEVER BE SEEN AGAIN ON TV. #GameOfThronesFinale #GameofThrones\n",
      "\n",
      "\n",
      "Rewatching #Gameofthrones finale. \n",
      "Danny’s speech was so awesome. So badass. \n",
      "And the unsullied with the Uruk-hai spear chant. Dope.\n",
      "\n",
      "=========================================\n",
      "These tweets are negative:\n",
      "\n",
      "Last #GameOfThrones episode tonight.  Nervous I’ll be disappointed. \n",
      "\n",
      "\n",
      "It wouldnt be so bad if they didnt make us wait an extra year. \n",
      "But they did and they fed us 6 episodes of TBS original programming quality poop! #GoT\n",
      "\n",
      "\n",
      "Based on the uproar over the ending I'm glad I never watched #GameOfThrones\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"These tweets are positive:\")\n",
    "\n",
    "for s,t, in zip(sentiments,tweets):\n",
    "    if s == '1':\n",
    "        print(t)\n",
    "        \n",
    "print(\"=========================================\")\n",
    "print(\"These tweets are negative:\")\n",
    "for s,t, in zip(sentiments,tweets):\n",
    "    if s == '2':\n",
    "        print(t)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can agree fairly well that these tweets do fall well into the sentiments predicted by GPT; however, we still don't know the root cause of the sentiment. For example, are they positive about a particular character? Is their negativity focused specifically on the finale? These are answers that we could still extract from GPT. We just have to write a more directed prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look ahead into the future of GPT: image classification\n",
    "\n",
    "While browsing Tweets, I noticed that many of them had most of the sentiment contained in the context of the image rather than the text itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_images/tyrion.png\" width=500pix>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, GPT 4 now has the ability to use static images (i.e., no videos or animated GIFs) as an input. Since this is such a new feature, we unfortunately do not have any examples to showcase the capabilities yet. Nonetheless, the ability to provide both text and images would greatly improve the predictability of AI model on sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}