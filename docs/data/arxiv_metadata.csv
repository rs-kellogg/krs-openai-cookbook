"id","submitter","authors","title","comments","journal-ref","doi","report-no","categories","license","abstract","update_date","pdf"
"2310.00014","Yong Ren","Yong Ren, Tao Wang, Jiangyan Yi, Le Xu, Jianhua Tao, Chuyuan Zhang,
  Junzuo Zhou","Fewer-token Neural Speech Codec with Time-invariant Codes","Submitted to ICASSP 2024","","","","cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language model based text-to-speech (TTS) models, like VALL-E, have gained
attention for their outstanding in-context learning capability in zero-shot
scenarios. Neural speech codec is a critical component of these models, which
can convert speech into discrete token representations. However, excessive
token sequences from the codec may negatively affect prediction accuracy and
restrict the progression of Language model based TTS models. To address this
issue, this paper proposes a novel neural speech codec with time-invariant
codes named TiCodec. By encoding and quantizing time-invariant information into
a separate code, TiCodec can reduce the amount of frame-level information that
needs encoding, effectively decreasing the number of tokens as codes of speech.
Furthermore, this paper introduces a time-invariant encoding consistency loss
to enhance the consistency of time-invariant code within an utterance and force
it to capture more global information, which can benefit the zero-shot TTS
task. Experimental results demonstrate that TiCodec can not only enhance the
quality of reconstruction speech with fewer tokens but also increase the
similarity and naturalness, as well as reduce the word error rate of the
synthesized speech by the TTS model.
","2023-10-03","2310.00014v1.pdf"
"2310.00031","Markus Marks","Neehar Kondapaneni, Markus Marks, Manuel Knott, Rog\'erio Guimar\~aes,
  Pietro Perona","Text-image Alignment for Diffusion-based Perception","Project page: https://www.vision.caltech.edu/tadp/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving. Project page:
https://www.vision.caltech.edu/tadp/
","2023-10-06","2310.00031v1.pdf"
"2310.00032","Qinghua Xu","Qinghua Xu, Tao Yue, Shaukat Ali and Maite Arratibel","Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event
  Analysis in Cyber-physical Systems","","","","","cs.SE","http://creativecommons.org/licenses/by/4.0/","  Cyber-Physical Systems (CPSs), e.g., elevator systems and autonomous driving
systems, are progressively permeating our everyday lives. To ensure their
safety, various analyses need to be conducted, such as anomaly detection and
time-to-event analysis (the focus of this paper). Recently, it has been widely
accepted that digital Twins (DTs) can serve as an efficient method to aid in
the development, maintenance, and safe and secure operation of CPSs. However,
CPSs frequently evolve, e.g., with new or updated functionalities, which demand
their corresponding DTs be co-evolved, i.e., in synchronization with the CPSs.
To that end, we propose a novel method, named PPT, utilizing an
uncertainty-aware transfer learning for DT evolution. Specifically, we first
pretrain PPT with a pretraining dataset to acquire generic knowledge about the
CPSs, followed by adapting it to a specific CPS with the help of prompt tuning.
Results highlight that PPT is effective in time-to-event analysis in both
elevator and ADSs case studies, on average, outperforming a baseline method by
7.31 and 12.58 in terms of Huber loss, respectively. The experiment results
also affirm the effectiveness of transfer learning, prompt tuning and
uncertainty quantification in terms of reducing Huber loss by at least 21.32,
3.14 and 4.08, respectively, in both case studies.
","2023-10-06","2310.00032v2.pdf"
"2310.00034","Yuzhang Shang","Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong","PB-LLM: Partially Binarized Large Language Models","Frist work using network binarization for large language model
  compression","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper explores network binarization, a radical form of quantization,
compressing model weights to a single bit, specifically for Large Language
Models (LLMs) compression. Due to previous binarization methods collapsing
LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can
achieve extreme low-bit quantization while maintaining the linguistic reasoning
capacity of quantized LLMs. Specifically, our exploration first uncovers the
ineffectiveness of naive applications of existing binarization algorithms and
highlights the imperative role of salient weights in achieving low-bit
quantization. Thus, PB-LLM filters a small ratio of salient weights during
binarization, allocating them to higher-bit storage, i.e.,
partially-binarization. PB-LLM is extended to recover the capacities of
quantized LMMs, by analyzing from the perspective of post-training quantization
(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts
from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian
matrix and successfully recover the reasoning capacity of PB-LLM in low-bit.
Under QAT, we freeze the salient weights during training, explore the
derivation of optimal scaling factors crucial for minimizing the quantization
error, and propose a scaling mechanism based on this derived scaling strategy
for residual binarized weights. Those explorations and the developed
methodologies significantly contribute to rejuvenating the performance of
low-bit quantized LLMs and present substantial advancements in the field of
network binarization for LLMs.The code is available at
https://github.com/hahnyuan/BinaryLLM.
","2023-10-03","2310.00034v1.pdf"
"2310.00035","Xi Wang","Xi Wang, Laurence Aitchison, Maja Rudolph","LoRA ensembles for large language model fine-tuning","Update the title in the PDF file","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test
data or out-of-distribution samples. One approach commonly used in vision for
alleviating this issue is a deep ensemble, which constructs an ensemble by
training the same model multiple times using different random initializations.
However, there is a huge challenge to ensembling LLMs: the most effective LLMs
are very, very large. Keeping a single LLM in memory is already challenging
enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many
settings. To address these issues, we propose an ensemble approach using
Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.
Critically, these low-rank adapters represent a very small number of
parameters, orders of magnitude less than the underlying pre-trained model.
Thus, it is possible to construct large ensembles of LoRA adapters with almost
the same computational overhead as using the original model. We find that LoRA
ensembles, applied on its own or on top of pre-existing regularization
techniques, gives consistent improvements in predictive accuracy and
uncertainty quantification.
","2023-10-06","2310.00035v1.pdf"
"2310.00036","Shengyi Huang","Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu,
  Santiago Onta\~n\'on","Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning
  Platform","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Distributed Deep Reinforcement Learning (DRL) aims to leverage more
computational resources to train autonomous agents with less training time.
Despite recent progress in the field, reproducibility issues have not been
sufficiently explored. This paper first shows that the typical actor-learner
framework can have reproducibility issues even if hyperparameters are
controlled. We then introduce Cleanba, a new open-source platform for
distributed DRL that proposes a highly reproducible architecture. Cleanba
implements highly optimized distributed variants of PPO and IMPALA. Our Atari
experiments show that these variants can obtain equivalent or higher scores
than strong IMPALA baselines in moolib and torchbeast and PPO baseline in
CleanRL. However, Cleanba variants present 1) shorter training time and 2) more
reproducible learning curves in different hardware settings. Cleanba's source
code is available at \url{https://github.com/vwxyzjn/cleanba}
","2023-10-03","2310.00036v1.pdf"
"2310.00068","Luchuan Song","Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, Chenliang Xu","Emotional Listener Portrait: Neural Listener Head Generation with
  Emotion","Accepted by ICCV2023","","","","cs.GR cs.AI cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Listener head generation centers on generating non-verbal behaviors (e.g.,
smile) of a listener in reference to the information delivered by a speaker. A
significant challenge when generating such responses is the non-deterministic
nature of fine-grained facial expressions during a conversation, which varies
depending on the emotions and attitudes of both the speaker and the listener.
To tackle this problem, we propose the Emotional Listener Portrait (ELP), which
treats each fine-grained facial motion as a composition of several discrete
motion-codewords and explicitly models the probability distribution of the
motions under different emotion in conversation. Benefiting from the
``explicit'' and ``discrete'' design, our ELP model can not only automatically
generate natural and diverse responses toward a given speaker via sampling from
the learned distribution but also generate controllable responses with a
predetermined attitude. Under several quantitative metrics, our ELP exhibits
significant improvements compared to previous methods.
","2023-10-10","2310.00068v1.pdf"
"2310.00074","Hangfeng He","Hangfeng He, Hongming Zhang, Dan Roth","SocREval: Large Language Models with the Socratic Method for
  Reference-Free Reasoning Evaluation","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  To comprehensively assess the capacity of current models for complex
reasoning, it is crucial to assess their step-by-step reasoning in a scalable
manner. Established reference-based evaluation metrics rely on human-annotated
reasoning chains to assess the model-derived chains. However, such
``gold-standard'' human-written reasoning chains may not be unique and their
acquisition is often labor-intensive. Existing reference-free reasoning metrics
eliminate the need for human-crafted reasoning chains as references, but they
typically require fine-tuning on datasets with human-derived reasoning chains,
which complicates the process and raises concerns regarding generalizability
across diverse datasets. To address these challenges, we harness GPT-4 to
automatically evaluate reasoning chain quality, obviating the need for
human-crafted references. Leveraging the Socratic method, we devise tailored
prompts to enhance reference-free reasoning evaluation, which we term SocREval
(Socratic method for Reasoning Evaluation). Empirical results from four human
annotated datasets reveal that SocREval significantly improves GPT-4's
performance, surpassing existing reference-free and reference-based reasoning
evaluation metrics. Beyond its demonstrated efficacy, our proposed framework,
large language models (LLMs) with the Socratic method, proves to be both
cost-efficient and robust to prompt writing and example selection, as
substantiated by our in-depth analysis.
","2023-10-03","2310.00074v1.pdf"
"2310.00085","Haechan Mark Bong","Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni
  Beltrame","PEACE: Prompt Engineering Automation for CLIPSeg Enhancement in Aerial
  Robotics","Submitted to ICRA 2024. arXiv admin note: substantial text overlap
  with arXiv:2308.11471","","","","cs.RO","http://creativecommons.org/licenses/by-sa/4.0/","  From industrial to space robotics, safe landing is an essential component for
flight operations. With the growing interest in artificial intelligence, we
direct our attention to learning based safe landing approaches. This paper
extends our previous work, DOVESEI, which focused on a reactive UAV system by
harnessing the capabilities of open vocabulary image segmentation. Prompt-based
safe landing zone segmentation using an open vocabulary based model is no more
just an idea, but proven to be feasible by the work of DOVESEI. However, a
heuristic selection of words for prompt is not a reliable solution since it
cannot take the changing environment into consideration and detrimental
consequences can occur if the observed environment is not well represented by
the given prompt. Therefore, we introduce PEACE (Prompt Engineering Automation
for CLIPSeg Enhancement), powering DOVESEI to automate the prompt generation
and engineering to adapt to data distribution shifts. Our system is capable of
performing safe landing operations with collision avoidance at altitudes as low
as 20 meters using only monocular cameras and image segmentation. We take
advantage of DOVESEI's dynamic focus to circumvent abrupt fluctuations in the
terrain segmentation between frames in a video stream. PEACE shows promising
improvements in prompt generation and engineering for aerial images compared to
the standard prompt used for CLIP and CLIPSeg. Combining DOVESEI and PEACE, our
system was able improve successful safe landing zone selections by 58.62%
compared to using only DOVESEI. All the source code is open source and
available online.
","2023-10-03","2310.00085v1.pdf"
"2310.00092","Yang Su","Yang Su","Voice2Action: Language Models as Agent for Efficient Real-Time
  Interaction in Virtual Reality","","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are trained and aligned to follow natural
language instructions with only a handful of examples, and they are prompted as
task-driven autonomous agents to adapt to various sources of execution
environments. However, deploying agent LLMs in virtual reality (VR) has been
challenging due to the lack of efficiency in online interactions and the
complex manipulation categories in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically analyzes customized voice signals
and textual commands through action and entity extraction and divides the
execution tasks into canonical interaction subsets in real-time with error
prevention from environment feedback. Experiment results in an urban
engineering VR environment with synthetic instruction data show that
Voice2Action can perform more efficiently and accurately than approaches
without optimizations.
","2023-10-03","2310.00092v1.pdf"
"2310.00098","Tatiana Likhomanenko","Martin Pelikan, Sheikh Shams Azam, Vitaly Feldman, Jan ""Honza""
  Silovsky, Kunal Talwar, Tatiana Likhomanenko","Federated Learning with Differential Privacy for End-to-End Speech
  Recognition","Under review","","","","cs.LG cs.CR stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While federated learning (FL) has recently emerged as a promising approach to
train machine learning models, it is limited to only preliminary explorations
in the domain of automatic speech recognition (ASR). Moreover, FL does not
inherently guarantee user privacy and requires the use of differential privacy
(DP) for robust privacy guarantees. However, we are not aware of prior work on
applying DP to FL for ASR. In this paper, we aim to bridge this research gap by
formulating an ASR benchmark for FL with DP and establishing the first
baselines. First, we extend the existing research on FL for ASR by exploring
different aspects of recent $\textit{large end-to-end transformer models}$:
architecture design, seed models, data heterogeneity, domain shift, and impact
of cohort size. With a $\textit{practical}$ number of central aggregations we
are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even
with heterogeneous data, a seed model from another domain, or no pre-trained
seed model. Second, we apply DP to FL for ASR, which is non-trivial since DP
noise severely affects model training, especially for large transformer models,
due to highly imbalanced gradients in the attention block. We counteract the
adverse effect of DP noise by reviving per-layer clipping and explaining why
its effect is more apparent in our case than in the prior work. Remarkably, we
achieve user-level ($7.2$, $10^{-9}$)-$\textbf{DP}$ (resp. ($4.5$,
$10^{-9}$)-$\textbf{DP}$) with a 1.3% (resp. 4.6%) absolute drop in the word
error rate for extrapolation to high (resp. low) population scale for
$\textbf{FL with DP in ASR}$.
","2023-10-03","2310.00098v1.pdf"
"2310.00149","Lecheng Kong","Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin
  Chen, Muhan Zhang","One for All: Towards Training One Graph Model for All Classification
  Tasks","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Designing a single model that addresses multiple tasks has been a
long-standing objective in artificial intelligence. Recently, large language
models have demonstrated exceptional capability in integrating and solving
different tasks within the language domain. However, a unified model for
various tasks on graphs remains underexplored, primarily due to the challenges
unique to the graph learning domain. First, graph data from different areas
carry distinct attributes and follow different distributions. Such discrepancy
makes it hard to represent graphs in a single representation space. Second,
tasks on graphs diversify into node, link, and graph tasks, requiring distinct
embedding strategies. Finally, an appropriate graph prompting paradigm for
in-context learning is unclear. Striving to handle all the aforementioned
challenges, we propose One for All (OFA), the first general framework that can
use a single graph model to address the above challenges. Specifically, OFA
proposes text-attributed graphs to unify different graph data by describing
nodes and edges with natural language and uses language models to encode the
diverse and possibly cross-domain text attributes to feature vectors in the
same embedding space. Furthermore, OFA introduces the concept of
nodes-of-interest to standardize different tasks with a single task
representation. For in-context learning on graphs, OFA introduces a novel graph
prompting paradigm that appends prompting substructures to the input graph,
which enables it to address varied tasks without fine-tuning. We train the OFA
model using graph data from multiple domains (including citation networks,
molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its
ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs
well across different tasks, making it the first general-purpose graph
classification model across domains.
","2023-10-03","2310.00149v1.pdf"
"2310.00158","Reyhane Askari Hemmat","Reyhane Askari Hemmat, Mohammad Pezeshki, Florian Bordes, Michal
  Drozdzal, Adriana Romero-Soriano","Feedback-guided Data Synthesis for Imbalanced Classification","","","","","cs.CV cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Current status quo in machine learning is to use static datasets of real
images for training, which often come from long-tailed distributions. With the
recent advances in generative models, researchers have started augmenting these
static datasets with synthetic data, reporting moderate performance
improvements on classification tasks. We hypothesize that these performance
gains are limited by the lack of feedback from the classifier to the generative
model, which would promote the usefulness of the generated samples to improve
the classifier's performance. In this work, we introduce a framework for
augmenting static datasets with useful synthetic samples, which leverages
one-shot feedback from the classifier to drive the sampling of the generative
model. In order for the framework to be effective, we find that the samples
must be close to the support of the real data of the task at hand, and be
sufficiently diverse. We validate three feedback criteria on a long-tailed
dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On
ImageNet-LT, we achieve state-of-the-art results, with over 4 percent
improvement on underrepresented classes while being twice efficient in terms of
the number of generated synthetic samples. NICO++ also enjoys marked boosts of
over 5 percent in worst group accuracy. With these results, our framework paves
the path towards effectively leveraging state-of-the-art text-to-image models
as data sources that can be queried to improve downstream applications.
","2023-10-03","2310.00158v1.pdf"
"2310.00160","Junmo Kang","Junmo Kang, Hongyin Luo, Yada Zhu, James Glass, David Cox, Alan
  Ritter, Rogerio Feris, Leonid Karlinsky","Self-Specialization: Uncovering Latent Expertise within Large Language
  Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recent works have demonstrated the effectiveness of self-alignment in which a
large language model is, by itself, aligned to follow general instructions
through the automatic generation of instructional data using a handful of
human-written seeds. Instead of general alignment, in this work, we focus on
self-alignment for expert domain specialization (e.g., biomedicine),
discovering it to be very effective for improving zero-shot and few-shot
performance in target domains of interest. As a preliminary, we first present
the benchmark results of existing aligned models within a specialized domain,
which reveals the marginal effect that ""generic"" instruction-following training
has on downstream expert domains' performance. To remedy this, we explore
self-specialization that leverages domain-specific unlabelled data and a few
labeled seeds for the self-alignment process. When augmented with retrieval to
reduce hallucination and enhance concurrency of the alignment,
self-specialization offers an effective (and efficient) way of ""carving out"" an
expert model out of a ""generalist"", pre-trained LLM where different domains of
expertise are originally combined in a form of ""superposition"". Our
experimental results on a biomedical domain show that our self-specialized
model (30B) outperforms its base model, MPT-30B by a large margin and even
surpasses larger popular models based on LLaMA-65B, highlighting its potential
and practicality for specialization, especially considering its efficiency in
terms of data and parameters.
","2023-10-03","2310.00160v1.pdf"
"2310.00163","Angelos Mavrogiannis","Angelos Mavrogiannis, Christoforos Mavrogiannis, Yiannis Aloimonos","Cook2LTL: Translating Cooking Recipes to LTL Formulae using Large
  Language Models","","","","","cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Cooking recipes are especially challenging to translate to robot plans as
they feature rich linguistic complexity, temporally-extended interconnected
tasks, and an almost infinite space of possible actions. Our key insight is
that combining a source of background cooking domain knowledge with a formalism
capable of handling the temporal richness of cooking recipes could enable the
extraction of unambiguous, robot-executable plans. In this work, we use Linear
Temporal Logic (LTL) as a formal language expressible enough to model the
temporal nature of cooking recipes. Leveraging pre-trained Large Language
Models (LLMs), we present a system that translates instruction steps from an
arbitrary cooking recipe found on the internet to a series of LTL formulae,
grounding high-level cooking actions to a set of primitive actions that are
executable by a manipulator in a kitchen environment. Our approach makes use of
a caching scheme, dynamically building a queryable action library at runtime,
significantly decreasing LLM API calls (-51%), latency (-59%) and cost (-42%)
compared to a baseline that queries the LLM for every newly encountered action
at runtime. We demonstrate the transferability of our system in a realistic
simulation platform through showcasing a set of simple cooking tasks.
","2023-10-03","2310.00163v1.pdf"
"2310.00164","Keivan Rezaei","Keivan Rezaei, Mehrdad Saberi, Mazda Moayeri, Soheil Feizi","PRIME: Prioritizing Interpretability in Failure Mode Extraction","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  In this work, we study the challenge of providing human-understandable
descriptions for failure modes in trained image classification models. Existing
works address this problem by first identifying clusters (or directions) of
incorrectly classified samples in a latent space and then aiming to provide
human-understandable text descriptions for them. We observe that in some cases,
describing text does not match well with identified failure modes, partially
owing to the fact that shared interpretable attributes of failure modes may not
be captured using clustering in the feature space. To improve on these
shortcomings, we propose a novel approach that prioritizes interpretability in
this problem: we start by obtaining human-understandable concepts (tags) of
images in the dataset and then analyze the model's behavior based on the
presence or absence of combinations of these tags. Our method also ensures that
the tags describing a failure mode form a minimal set, avoiding redundant and
noisy descriptions. Through several experiments on different datasets, we show
that our method successfully identifies failure modes and generates
high-quality text descriptions associated with them. These results highlight
the importance of prioritizing interpretability in understanding model
failures.
","2023-10-03","2310.00164v1.pdf"
"2310.00166","Martin Klissarov","Martin Klissarov, Pierluca D'Oro, Shagun Sodhani, Roberta Raileanu,
  Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff","Motif: Intrinsic Motivation from Artificial Intelligence Feedback","The first two authors equally contributed - order decided by coin
  flip","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
","2023-10-03","2310.00166v1.pdf"
"2310.00194","Taylor Webb","Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida
  Momennejad","A Prefrontal Cortex-inspired Architecture for Planning in Large Language
  Models","","","","","cs.AI cs.NE","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. To address this, we take inspiration from
the human brain, in which planning is accomplished via the recurrent
interaction of specialized modules in the prefrontal cortex (PFC). These
modules perform functions such as conflict monitoring, state prediction, state
evaluation, task decomposition, and task coordination. We find that LLMs are
sometimes capable of carrying out these functions in isolation, but struggle to
autonomously coordinate them in the service of a goal. Therefore, we propose a
black box architecture with multiple LLM-based (GPT-4) modules. The
architecture improves planning through the interaction of specialized
PFC-inspired modules that break down a larger problem into multiple brief
automated calls to the LLM. We evaluate the combined architecture on two
challenging planning tasks -- graph traversal and Tower of Hanoi -- finding
that it yields significant improvements over standard LLM methods (e.g.,
zero-shot prompting or in-context learning). These results demonstrate the
benefit of utilizing knowledge from cognitive neuroscience to improve planning
in LLMs.
","2023-10-03","2310.00194v1.pdf"
"2310.00212","Tianhao Wu","Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran,
  Jiantao Jiao","Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for
  LLM Alignment","19 pages, 5 figures","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.
","2023-10-11","2310.00212v1.pdf"
"2310.00222","Hongsheng Hu","Hongsheng Hu, Xuyun Zhang, Zoran Salcic, Lichao Sun, Kim-Kwang Raymond
  Choo, Gillian Dobbie","Source Inference Attacks: Beyond Membership Inference Attacks in
  Federated Learning","Accepted by IEEE Transactions on Dependable and Secure Computing","","","","cs.CR","http://creativecommons.org/licenses/by/4.0/","  Federated learning (FL) is a popular approach to facilitate privacy-aware
machine learning since it allows multiple clients to collaboratively train a
global model without granting others access to their private data. It is,
however, known that FL can be vulnerable to membership inference attacks
(MIAs), where the training records of the global model can be distinguished
from the testing records. Surprisingly, research focusing on the investigation
of the source inference problem appears to be lacking. We also observe that
identifying a training record's source client can result in privacy breaches
extending beyond MIAs. For example, consider an FL application where multiple
hospitals jointly train a COVID-19 diagnosis model, membership inference
attackers can identify the medical records that have been used for training,
and any additional identification of the source hospital can result the patient
from the particular hospital more prone to discrimination. Seeking to
contribute to the literature gap, we take the first step to investigate source
privacy in FL. Specifically, we propose a new inference attack (hereafter
referred to as source inference attack -- SIA), designed to facilitate an
honest-but-curious server to identify the training record's source client. The
proposed SIAs leverage the Bayesian theorem to allow the server to implement
the attack in a non-intrusive manner without deviating from the defined FL
protocol. We then evaluate SIAs in three different FL frameworks to show that
in existing FL frameworks, the clients sharing gradients, model parameters, or
predictions on a public dataset will leak such source information to the
server. We also conduct extensive experiments on various datasets to
investigate the key factors in an SIA. The experimental results validate the
efficacy of the proposed SIAs.
","2023-10-03","2310.00222v1.pdf"
"2310.00230","Yongqiang Wang","Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan
  Cao, Yongqiang Wang, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul Rubenstein,
  Lukas Zilka, Dian Yu, Zhong Meng, Golan Pundak, Nikhil Siddhartha, Johan
  Schalkwyk, Yonghui Wu","SLM: Bridge the thin gap between speech and text foundation models","","","","","cs.CL cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  We present a joint Speech and Language Model (SLM), a multitask,
multilingual, and dual-modal model that takes advantage of pretrained
foundational speech and language models. SLM freezes the pretrained foundation
models to maximally preserves their capabilities, and only trains a simple
adapter with just 1\% (156M) of the foundation models' parameters. This
adaptation not only leads SLM to achieve strong performance on conventional
tasks such as speech recognition (ASR) and speech translation (AST), but also
introduces the novel capability of zero-shot instruction-following for more
diverse tasks: given a speech input and a text instruction, SLM is able to
perform unseen generation tasks including contextual biasing ASR using
real-time context, dialog generation, speech continuation, and question
answering, etc. Our approach demonstrates that the representational gap between
pretrained speech and language models might be narrower than one would expect,
and can be bridged by a simple adaptation mechanism. As a result, SLM is not
only efficient to train, but also inherits strong capabilities already acquired
in foundation models of different modalities.
","2023-10-03","2310.00230v1.pdf"
"2310.00247","Sixing Yu","Sixing Yu, J. Pablo Mu\~noz, Ali Jannesari","Bridging the Gap Between Foundation Models and Heterogeneous Federated
  Learning","","","","","cs.LG cs.DC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Federated learning (FL) offers privacy-preserving decentralized machine
learning, optimizing models at edge clients without sharing private data.
Simultaneously, foundation models (FMs) have gained traction in the artificial
intelligence (AI) community due to their exceptional performance across various
tasks. However, integrating FMs into FL presents challenges, primarily due to
their substantial size and intensive resource requirements. This is especially
true when considering the resource heterogeneity in edge FL systems. We present
an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to
address these challenges. RaFFM introduces specialized model compression
algorithms tailored for FL scenarios, such as salient parameter prioritization
and high-performance subnetwork extraction. These algorithms enable dynamic
scaling of given transformer-based FMs to fit heterogeneous resource
constraints at the network edge during both FL's optimization and deployment
stages. Experimental results demonstrate that RaFFM shows significant
superiority in resource utilization efficiency and uses fewer resources to
deploy FMs to FL. Despite the lower resource consumption, target models
optimized by RaFFM achieve performance on par with traditional FL methods
applied to full-sized FMs. This is evident across tasks in both natural
language processing and computer vision domains.
","2023-10-06","2310.00247v1.pdf"
"2310.00259","Zouying Cao","Zouying Cao, Yifei Yang, Hai Zhao","AutoHall: Automated Hallucination Dataset Generation for Large Language
  Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While Large language models (LLMs) have garnered widespread applications
across various domains due to their powerful language understanding and
generation capabilities, the detection of non-factual or hallucinatory content
generated by LLMs remains scarce. Currently, one significant challenge in
hallucination detection is the laborious task of time-consuming and expensive
manual annotation of the hallucinatory generation. To address this issue, this
paper first introduces a method for automatically constructing model-specific
hallucination datasets based on existing fact-checking datasets called
AutoHall. Furthermore, we propose a zero-resource and black-box hallucination
detection method based on self-contradiction. We conduct experiments towards
prevalent open-/closed-source LLMs, achieving superior hallucination detection
performance compared to extant baselines. Moreover, our experiments reveal
variations in hallucination proportions and types among different models.
","2023-10-03","2310.00259v1.pdf"
"2310.00272","Baphumelele Masikisiki","Baphumelele Masikisiki, Vukosi Marivate, Yvette Hlope","Investigating the Efficacy of Large Language Models in Reflective
  Assessment Methods through Chain of Thoughts Prompting","Accepted for publication in the Associate Computer Machinery","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models, such as Generative Pre-trained Transformer 3 (aka.
GPT-3), have been developed to understand language through the analysis of
extensive text data, allowing them to identify patterns and connections between
words. While LLMs have demonstrated impressive performance across various
text-related tasks, they encounter challenges in tasks associated with
reasoning. To address this challenge, Chain of Thought(CoT) prompting method
has been proposed as a means to enhance LLMs' proficiency in complex reasoning
tasks like solving math word problems and answering questions based on logical
argumentative reasoning. The primary aim of this research is to assess how well
four language models can grade reflective essays of third-year medical
students. The assessment will specifically target the evaluation of critical
thinking skills using CoT prompting.
  The research will provide the following contributions; to introduce and
educate on the process of instructing models to evaluate reflective essays from
a dataset they have not been previously trained on; to illustrate the use of
CoT prompting as an instructional approach for training large models to carry
out particular tasks. Our results suggest that among all the models, Llama-7b
performs the least effectively, displaying the highest mean squared error.
Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen
kappa score value of 0.53. Lastly, it's important to note that the selected
models do prioritise user privacy by allowing users to delete their own
conducted conversations.
","2023-10-03","2310.00272v1.pdf"
"2310.00277","Yunhao Chen","Yunhao Chen and Zihui Yan and Yunjie Zhu","A Unified Framework for Generative Data Augmentation: A Comprehensive
  Survey","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative data augmentation (GDA) has emerged as a promising technique to
alleviate data scarcity in machine learning applications. This thesis presents
a comprehensive survey and unified framework of the GDA landscape. We first
provide an overview of GDA, discussing its motivation, taxonomy, and key
distinctions from synthetic data generation. We then systematically analyze the
critical aspects of GDA - selection of generative models, techniques to utilize
them, data selection methodologies, validation approaches, and diverse
applications. Our proposed unified framework categorizes the extensive GDA
literature, revealing gaps such as the lack of universal benchmarks. The thesis
summarises promising research directions, including , effective data selection,
theoretical development for large-scale models' application in GDA and
establishing a benchmark for GDA. By laying a structured foundation, this
thesis aims to nurture more cohesive development and accelerate progress in the
vital arena of generative data augmentation.
","2023-10-03","2310.00277v1.pdf"
"2310.00280","Qiushi Sun","Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng
  Kong","Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model
  Collaboration","work in progress","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are evolving at an unprecedented pace and have
exhibited considerable capability in the realm of natural language processing
(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,
a single LLM can manage typical NLP tasks competently. However, its performance
in executing reasoning tasks is still confined by the limitations of its
internal representations. To push this boundary further, we introduce Corex in
this paper, a suite of novel general-purpose strategies that transform LLMs
into autonomous agents pioneering multi-model collaborations for complex
task-solving. Inspired by human behaviors, Corex is constituted by diverse
collaboration paradigms including Debate, Review, and Retrieve modes, which
collectively work towards enhancing the factuality, faithfulness, and
reliability of the reasoning process. These paradigms foster task-agnostic
approaches that enable LLMs to ''think outside the box,'' thereby overcoming
hallucinations and providing better solutions. Through extensive experiments
across four different types of reasoning tasks, we demonstrate that
orchestrating multiple LLMs to work in concert yields substantially better
performance compared to existing methods. Further results and in-depth analysis
demonstrate the cost-effectiveness of our method, facilitating collaboration
among different LLMs and promoting annotation efficiency.
","2023-10-03","2310.00280v1.pdf"
"2310.00297","Jianhao Yan","Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang","Understanding In-Context Learning from Repetitions","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper explores the elusive mechanism underpinning in-context learning in
Large Language Models (LLMs). Our work provides a novel perspective by
examining in-context learning via the lens of surface repetitions. We
quantitatively investigate the role of surface features in text generation, and
empirically establish the existence of \emph{token co-occurrence
reinforcement}, a principle that strengthens the relationship between two
tokens based on their contextual co-occurrences. By investigating the dual
impacts of these features, our research illuminates the internal workings of
in-context learning and expounds on the reasons for its failures. This paper
provides an essential contribution to the understanding of in-context learning
and its potential limitations, providing a fresh perspective on this exciting
capability.
","2023-10-11","2310.00297v1.pdf"
"2310.00299","Asahi Ushio","Asahi Ushio, Jose Camacho-Collados, Steven Schockaert","RelBERT: Embedding Relations with Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Many applications need access to background knowledge about how different
concepts and entities are related. Although Knowledge Graphs (KG) and Large
Language Models (LLM) can address this need to some extent, KGs are inevitably
incomplete and their relational schema is often too coarse-grained, while LLMs
are inefficient and difficult to control. As an alternative, we propose to
extract relation embeddings from relatively small language models. In
particular, we show that masked language models such as RoBERTa can be
straightforwardly fine-tuned for this purpose, using only a small amount of
training data. The resulting model, which we call RelBERT, captures relational
similarity in a surprisingly fine-grained way, allowing us to set a new
state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of
modelling relations that go well beyond what the model has seen during
training. For instance, we obtained strong results on relations between named
entities with a model that was only trained on lexical relations between
concepts, and we observed that RelBERT can recognise morphological analogies
despite not being trained on such examples. Overall, we find that RelBERT
significantly outperforms strategies based on prompting language models that
are several orders of magnitude larger, including recent GPT-based models and
open source models.
","2023-10-10","2310.00299v1.pdf"
"2310.00305","Xuan Zhang","Xuan Zhang and Wei Gao","Towards LLM-based Fact Verification on News Claims with a Hierarchical
  Step-by-Step Prompting Method","Accepted by AACL 2023","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  While large pre-trained language models (LLMs) have shown their impressive
capabilities in various NLP tasks, they are still under-explored in the
misinformation domain. In this paper, we examine LLMs with in-context learning
(ICL) for news claim verification, and find that only with 4-shot demonstration
examples, the performance of several prompting methods can be comparable with
previous supervised models. To further boost performance, we introduce a
Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to
separate a claim into several subclaims and then verify each of them via
multiple questions-answering steps progressively. Experiment results on two
public misinformation datasets show that HiSS prompting outperforms
state-of-the-art fully-supervised approach and strong few-shot ICL-enabled
baselines.
","2023-10-03","2310.00305v1.pdf"
"2310.00313","Safoora Yousefi","Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Akanksha Saran,
  Rapha\""el Milli\`ere, Ida Momennejad","In-Context Learning in Large Language Models: A Neuroscience-inspired
  Analysis of Representations","Added overview figures 1-3 in this version","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) exhibit remarkable performance improvement
through in-context learning (ICL) by leveraging task-specific examples in the
input. However, the mechanisms behind this improvement remain elusive. In this
work, we investigate embeddings and attention representations in Llama-2 70B
and Vicuna 13B. Specifically, we study how embeddings and attention change
after in-context-learning, and how these changes mediate improvement in
behavior. We employ neuroscience-inspired techniques, such as representational
similarity analysis (RSA), and propose novel methods for parameterized probing
and attention ratio analysis (ARA, measuring the ratio of attention to relevant
vs. irrelevant information). We designed three tasks with a priori
relationships among their conditions: reading comprehension, linear regression,
and adversarial prompt injection. We formed hypotheses about expected
similarities in task representations to investigate latent changes in
embeddings and attention. Our analyses revealed a meaningful correlation
between changes in both embeddings and attention representations with
improvements in behavioral performance after ICL. This empirical framework
empowers a nuanced understanding of how latent representations affect LLM
behavior with and without ICL, offering valuable tools and insights for future
research and practical applications.
","2023-10-19","2310.00313v1.pdf"
"2310.00322","Chengdong Ma","Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan,
  Yaodong Yang","Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language
  Models","","","","","cs.CL cs.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deployable Large Language Models (LLMs) must conform to the criterion of
helpfulness and harmlessness, thereby achieving consistency between LLMs
outputs and human values. Red-teaming techniques constitute a critical way
towards this criterion. Existing work rely solely on manual red team designs
and heuristic adversarial prompts for vulnerability detection and optimization.
These approaches lack rigorous mathematical formulation, thus limiting the
exploration of diverse attack strategy within quantifiable measure and
optimization of LLMs under convergence guarantees. In this paper, we present
Red-teaming Game (RTG), a general game-theoretic framework without manual
annotation. RTG is designed for analyzing the multi-turn attack and defense
interactions between Red-team language Models (RLMs) and Blue-team Language
Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with
diversity measure of the semantic space. GRTS is an automated red teaming
technique to solve RTG towards Nash equilibrium through meta-game analysis,
which corresponds to the theoretically guaranteed optimization direction of
both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that
GRTS autonomously discovered diverse attack strategies and effectively improved
security of LLMs, outperforming existing heuristic red-team designs. Overall,
RTG has established a foundational framework for red teaming tasks and
constructed a new scalable oversight technique for alignment.
","2023-10-11","2310.00322v1.pdf"
"2310.00328","Joe O'Brien","Joe O'Brien, Shaun Ee, Zoe Williams","Deployment Corrections: An incident response framework for frontier AI
  models","53 pages; 1 figure; 1 table","","","","cs.CY","http://creativecommons.org/licenses/by/4.0/","  A comprehensive approach to addressing catastrophic risks from AI models
should cover the full model lifecycle. This paper explores contingency plans
for cases where pre-deployment risk management falls short: where either very
dangerous models are deployed, or deployed models become very dangerous.
  Informed by incident response practices from industries including
cybersecurity, we describe a toolkit of deployment corrections that AI
developers can use to respond to dangerous capabilities, behaviors, or use
cases of AI models that develop or are detected after deployment. We also
provide a framework for AI developers to prepare and implement this toolkit.
  We conclude by recommending that frontier AI developers should (1) maintain
control over model access, (2) establish or grow dedicated teams to design and
maintain processes for deployment corrections, including incident response
plans, and (3) establish these deployment corrections as allowable actions with
downstream users. We also recommend frontier AI developers, standard-setting
organizations, and regulators should collaborate to define a standardized
industry-wide approach to the use of deployment corrections in incident
response.
  Caveat: This work applies to frontier AI models that are made available
through interfaces (e.g., API) that provide the AI developer or another
upstream party means of maintaining control over access (e.g., GPT-4 or
Claude). It does not apply to management of catastrophic risk from open-source
models (e.g., BLOOM or Llama-2), for which the restrictions we discuss are
largely unenforceable.
","2023-10-03","2310.00328v1.pdf"
"2310.00347","Shaina Raza Dr.","Shaina Raza, Oluwanifemi Bamgbose, Veronica Chatrath, Shardul Ghuge,
  Yan Sidyakin, Abdullah Y Muaad","Unlocking Bias Detection: Leveraging Transformer-Based Models for
  Content Analysis","UNDER REVIEW","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Bias detection in text is imperative due to its role in reinforcing negative
stereotypes, disseminating misinformation, and influencing decisions. Current
language models often fall short in generalizing beyond their training sets. In
response, we introduce the Contextualized Bi-Directional Dual Transformer
(CBDT) Classifier. This novel architecture utilizes two synergistic transformer
networks: the Context Transformer and the Entity Transformer, aiming for
enhanced bias detection. Our dataset preparation follows the FAIR principles,
ensuring ethical data usage. Through rigorous testing on various datasets, CBDT
showcases its ability in distinguishing biased from neutral statements, while
also pinpointing exact biased lexemes. Our approach outperforms existing
methods, achieving a 2-4\% increase over benchmark performances. This opens
avenues for adapting the CBDT model across diverse linguistic and cultural
landscapes.
","2023-10-17","2310.00347v1.pdf"
"2310.00371","Kartik Ramachandruni","Kartik Ramachandruni, Max Zuo, Sonia Chernova","ConSOR: A Context-Aware Semantic Object Rearrangement Framework for
  Partially Arranged Scenes","Accepted to IROS 2023","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  Object rearrangement is the problem of enabling a robot to identify the
correct object placement in a complex environment. Prior work on object
rearrangement has explored a diverse set of techniques for following user
instructions to achieve some desired goal state. Logical predicates, images of
the goal scene, and natural language descriptions have all been used to
instruct a robot in how to arrange objects. In this work, we argue that
burdening the user with specifying goal scenes is not necessary in
partially-arranged environments, such as common household settings. Instead, we
show that contextual cues from partially arranged scenes (i.e., the placement
of some number of pre-arranged objects in the environment) provide sufficient
context to enable robots to perform object rearrangement \textit{without any
explicit user goal specification}. We introduce ConSOR, a Context-aware
Semantic Object Rearrangement framework that utilizes contextual cues from a
partially arranged initial state of the environment to complete the arrangement
of new objects, without explicit goal specification from the user. We
demonstrate that ConSOR strongly outperforms two baselines in generalizing to
novel object arrangements and unseen object categories. The code and data can
be found at https://github.com/kartikvrama/consor.
","2023-10-03","2310.00371v1.pdf"
"2310.00374","Jonas Schuett","Jide Alaga and Jonas Schuett","Coordinated pausing: An evaluation-based coordination scheme for
  frontier AI developers","24 pages, 3 figures, 1 table","","","","cs.CY","http://creativecommons.org/licenses/by/4.0/","  As artificial intelligence (AI) models are scaled up, new capabilities can
emerge unintentionally and unpredictably, some of which might be dangerous. In
response, dangerous capabilities evaluations have emerged as a new risk
assessment tool. But what should frontier AI developers do if sufficiently
dangerous capabilities are in fact discovered? This paper focuses on one
possible response: coordinated pausing. It proposes an evaluation-based
coordination scheme that consists of five main steps: (1) Frontier AI models
are evaluated for dangerous capabilities. (2) Whenever, and each time, a model
fails a set of evaluations, the developer pauses certain research and
development activities. (3) Other developers are notified whenever a model with
dangerous capabilities has been discovered. They also pause related research
and development activities. (4) The discovered capabilities are analyzed and
adequate safety precautions are put in place. (5) Developers only resume their
paused activities if certain safety thresholds are reached. The paper also
discusses four concrete versions of that scheme. In the first version, pausing
is completely voluntary and relies on public pressure on developers. In the
second version, participating developers collectively agree to pause under
certain conditions. In the third version, a single auditor evaluates models of
multiple developers who agree to pause if any model fails a set of evaluations.
In the fourth version, developers are legally required to run evaluations and
pause if dangerous capabilities are discovered. Finally, the paper discusses
the desirability and feasibility of our proposed coordination scheme. It
concludes that coordinated pausing is a promising mechanism for tackling
emerging risks from frontier AI models. However, a number of practical and
legal obstacles need to be overcome, especially how to avoid violations of
antitrust law.
","2023-10-03","2310.00374v1.pdf"
"2310.00378","Zhaowei Zhang","Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang","Measuring Value Understanding in Language Models through
  Discriminator-Critique Gap","","","","","cs.CL cs.AI cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in Large Language Models (LLMs) have heightened concerns
about their potential misalignment with human values. However, evaluating their
grasp of these values is complex due to their intricate and adaptable nature.
We argue that truly understanding values in LLMs requires considering both
""know what"" and ""know why"". To this end, we present the Value Understanding
Measurement (VUM) framework that quantitatively assesses both ""know what"" and
""know why"" by measuring the discriminator-critique gap related to human values.
Using the Schwartz Value Survey, we specify our evaluation values and develop a
thousand-level dialogue dataset with GPT-4. Our assessment looks at both the
value alignment of LLM's outputs compared to baseline answers and how LLM
responses align with reasons for value recognition versus GPT-4's annotations.
We evaluate five representative LLMs and provide strong evidence that the
scaling law significantly impacts ""know what"" but not much on ""know why"", which
has consistently maintained a high level. This may further suggest that LLMs
might craft plausible explanations based on the provided context without truly
understanding their inherent value, indicating potential risks.
","2023-10-20","2310.00378v1.pdf"
"2310.00385","Fei Zhao","Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, Xinyu Dai","Dynamic Demonstrations Controller for In-Context Learning","Under review","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In-Context Learning (ICL) is a new paradigm for natural language processing
(NLP), where a large language model (LLM) observes a small number of
demonstrations and a test instance as its input, and directly makes predictions
without updating model parameters. Previous studies have revealed that ICL is
sensitive to the selection and the ordering of demonstrations. However, there
are few studies regarding the impact of the demonstration number on the ICL
performance within a limited input length of LLM, because it is commonly
believed that the number of demonstrations is positively correlated with model
performance. In this paper, we found this conclusion does not always hold true.
Through pilot experiments, we discover that increasing the number of
demonstrations does not necessarily lead to improved performance. Building upon
this insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller),
which can improve the ICL performance by adjusting the number of demonstrations
dynamically. The experimental results show that D$^2$Controller yields a 5.4%
relative improvement on eight different sizes of LLMs across ten datasets.
Moreover, we also extend our method to previous ICL models and achieve
competitive results.
","2023-10-03","2310.00385v1.pdf"
"2310.00390","Yulu Gan","Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed
  M. Alaa","InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision
  Generalists","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances in generative diffusion models have enabled text-controlled
synthesis of realistic and diverse images with impressive quality. Despite
these remarkable advances, the application of text-to-image generative models
in computer vision for standard visual recognition tasks remains limited. The
current de facto approach for these tasks is to design model architectures and
loss functions that are tailored to the task at hand. In this paper, we develop
a unified language interface for computer vision tasks that abstracts away
task-specific design choices and enables task execution by following natural
language instructions. Our approach involves casting multiple computer vision
tasks as text-to-image generation problems. Here, the text represents an
instruction describing the task, and the resulting image is a visually-encoded
task output. To train our model, we pool commonly-used computer vision datasets
covering a range of tasks, including segmentation, object detection, depth
estimation, and classification. We then use a large language model to
paraphrase prompt templates that convey the specific tasks to be conducted on
each image, and through this process, we create a multi-modal and multi-task
training dataset comprising input and output images along with annotated
instructions. Following the InstructPix2Pix architecture, we apply
instruction-tuning to a text-to-image diffusion model using our constructed
dataset, steering its functionality from a generative model to an
instruction-guided multi-task vision learner. Experiments demonstrate that our
model, dubbed InstructCV, performs competitively compared to other generalist
and task-specific vision models. Moreover, it exhibits compelling
generalization capabilities to unseen data, categories, and user instructions.
","2023-10-03","2310.00390v1.pdf"
"2310.00399","Yan Xiao","Yan Xiao, Xinyue Zuo, Lei Xue, Kailong Wang, Jin Song Dong and Ivan
  Beschastnikh","Empirical Study on Transformer-based Techniques for Software Engineering","","","","","cs.SE","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Many Transformer-based pre-trained models for code have been developed and
applied to code-related tasks. In this paper, we review the existing
literature, examine the suitability of model architectures for different tasks,
and look at the generalization ability of models on different datasets, and
their resource consumption.
  We examine three very representative pre-trained models for code: CodeBERT,
CodeGPT, and CodeT5, and conduct experiments on the top-4 most targeted
software engineering tasks that we found in our literature survey: Code
Summarization, Bug Fixing, Bug Detection, and Code Search. In our study, we
showcase the capability of decoder-only models (CodeGPT) for specific
generation tasks under state-of-the-art evaluation metrics and contest the
common belief that the encoder-decoder architecture is optimal for
general-purpose coding tasks. Additionally, we found that the most frequently
used models are not necessarily the most suitable for certain applications and
the developers' needs are not adequately addressed by current research. As
well, we found that the benchmark and frequent dataset for Bug Fixing and Code
Summarization both fail to enable models to generalize onto other datasets for
the same task (the frequent dataset refers to the dataset with the highest
frequency used in literature other than the benchmark). We use statistical
testing to support our conclusions from experiments. Finally, CodeBERT is
highly efficient for understanding tasks, whereas CodeT5's efficiency for
generation tasks is in doubt, as the highest resource consumption does not
guarantee a consistent better performance on different metrics. We also discuss
the numerous practical issues in advancing future research on transformer-based
models for code-related tasks.
","2023-10-03","2310.00399v1.pdf"
"2310.00426","Enze Xie","Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu,
  Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li","PixArt-$\alpha$: Fast Training of Diffusion Transformer for
  Photorealistic Text-to-Image Synthesis","Project Page: https://pixart-alpha.github.io","","","","cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The most advanced text-to-image (T2I) models require significant training
costs (e.g., millions of GPU hours), seriously hindering the fundamental
innovation for the AIGC community while increasing CO2 emissions. This paper
introduces PIXART-$\alpha$, a Transformer-based T2I diffusion model whose image
generation quality is competitive with state-of-the-art image generators (e.g.,
Imagen, SDXL, and even Midjourney), reaching near-commercial application
standards. Additionally, it supports high-resolution image synthesis up to
1024px resolution with low training cost, as shown in Figure 1 and 2. To
achieve this goal, three core designs are proposed: (1) Training strategy
decomposition: We devise three distinct training steps that separately optimize
pixel dependency, text-image alignment, and image aesthetic quality; (2)
Efficient T2I Transformer: We incorporate cross-attention modules into
Diffusion Transformer (DiT) to inject text conditions and streamline the
computation-intensive class-condition branch; (3) High-informative data: We
emphasize the significance of concept density in text-image pairs and leverage
a large Vision-Language model to auto-label dense pseudo-captions to assist
text-image alignment learning. As a result, PIXART-$\alpha$'s training speed
markedly surpasses existing large-scale T2I models, e.g., PIXART-$\alpha$ only
takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU
days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90% CO2
emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training
cost is merely 1%. Extensive experiments demonstrate that PIXART-$\alpha$
excels in image quality, artistry, and semantic control. We hope
PIXART-$\alpha$ will provide new insights to the AIGC community and startups to
accelerate building their own high-quality yet low-cost generative models from
scratch.
","2023-10-17","2310.00426v1.pdf"
"2310.00429","Avishek Bose","Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco
  Jiralerspong, and Gauthier Gidel","On the Stability of Iterative Retraining of Generative Models on their
  own Data","","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep generative models have made tremendous progress in modeling complex
data, often exhibiting generation quality that surpasses a typical human's
ability to discern the authenticity of samples. Undeniably, a key driver of
this success is enabled by the massive amounts of web-scale data consumed by
these models. Due to these models' striking performance and ease of
availability, the web will inevitably be increasingly populated with synthetic
content. Such a fact directly implies that future iterations of generative
models must contend with the reality that their training is curated from both
clean data and artificially generated data from past models. In this paper, we
develop a framework to rigorously study the impact of training generative
models on mixed datasets (of real and synthetic data) on their stability. We
first prove the stability of iterative training under the condition that the
initial generative models approximate the data distribution well enough and the
proportion of clean training data (w.r.t. synthetic data) is large enough. We
empirically validate our theory on both synthetic and natural images by
iteratively training normalizing flows and state-of-the-art diffusion models on
CIFAR10 and FFHQ.
","2023-10-04","2310.00429v1.pdf"
"2310.00481","Xiyang Wu","Chak Lam Shek, Xiyang Wu, Dinesh Manocha, Pratap Tokekar, and Amrit
  Singh Bedi","LANCAR: Leveraging Language for Context-Aware Robot Locomotion in
  Unstructured Environments","","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  Robotic locomotion is a challenging task, especially in unstructured
terrains. In practice, the optimal locomotion policy can be context-dependent
by using the contextual information of encountered terrains in decision-making.
Humans can interpret the environmental context for robots, but the ambiguity of
human language makes it challenging to use in robot locomotion directly. In
this paper, we propose a novel approach, LANCAR, that introduces a context
translator that works with reinforcement learning (RL) agents for context-aware
locomotion. Our formulation allows a robot to interpret the contextual
information from environments generated by human observers or Vision-Language
Models (VLM) with Large Language Models (LLM) and use this information to
generate contextual embeddings. We incorporate the contextual embeddings with
the robot's internal environmental observations as the input to the RL agent's
decision neural network. We evaluate LANCAR with contextual information in
varying ambiguity levels and compare its performance using several alternative
approaches. Our experimental results demonstrate that our approach exhibits
good generalizability and adaptability across diverse terrains, by achieving at
least 10% of performance improvement in episodic reward over baselines. The
experiment video can be found at the following link:
https://raaslab.org/projects/LLM_Context_Estimation/.
","2023-10-03","2310.00481v1.pdf"
"2310.00483","Vincent Li","Vincent Li, Nick Doiron","Prompting Code Interpreter to Write Better Unit Tests on Quixbugs
  Functions","13 pages (including appendices), 0 figures, 1 table. First authored
  by Vincent Li; edited by Nick Doiron","","","","cs.SE cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Unit testing is a commonly-used approach in software engineering to test the
correctness and robustness of written code. Unit tests are tests designed to
test small components of a codebase in isolation, such as an individual
function or method. Although unit tests have historically been written by human
programmers, recent advancements in AI, particularly LLMs, have shown
corresponding advances in automatic unit test generation. In this study, we
explore the effect of different prompts on the quality of unit tests generated
by Code Interpreter, a GPT-4-based LLM, on Python functions provided by the
Quixbugs dataset, and we focus on prompting due to the ease with which users
can make use of our findings and observations. We find that the quality of the
generated unit tests is not sensitive to changes in minor details in the
prompts provided. However, we observe that Code Interpreter is often able to
effectively identify and correct mistakes in code that it writes, suggesting
that providing it runnable code to check the correctness of its outputs would
be beneficial, even though we find that it is already often able to generate
correctly-formatted unit tests. Our findings suggest that, when prompting
models similar to Code Interpreter, it is important to include the basic
information necessary to generate unit tests, but minor details are not as
important.
","2023-10-03","2310.00483v1.pdf"
"2310.00492","Xuansheng Wu","Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang,
  Ninghao Liu, Dong Yu","From Language Modeling to Instruction Following: Understanding the
  Behavior Shift in LLMs after Instruction Tuning","28 pages, 13 figures, 12 tables","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLMs) have achieved remarkable success, demonstrating
powerful instruction-following capabilities across diverse tasks. Instruction
fine-tuning is critical in enabling LLMs to align with user intentions and
effectively follow instructions. In this work, we investigate how instruction
fine-tuning modifies pre-trained models, focusing on two perspectives:
instruction recognition and knowledge evolution. To study the behavior shift of
LLMs, we employ a suite of local and global explanation methods, including a
gradient-based approach for input-output attribution and techniques for
interpreting patterns and concepts in self-attention and feed-forward layers.
Our findings reveal three significant impacts of instruction fine-tuning: 1) It
empowers LLMs to better recognize the instruction parts from user prompts,
thereby facilitating high-quality response generation and addressing the
``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the
knowledge stored in feed-forward layers with user-oriented tasks, exhibiting
minimal shifts across linguistic levels. 3) It facilitates the learning of
word-word relations with instruction verbs through the self-attention
mechanism, particularly in the lower and middle layers, indicating enhanced
recognition of instruction words. These insights contribute to a deeper
understanding of the behavior shifts in LLMs after instruction fine-tuning and
lay the groundwork for future research aimed at interpreting and optimizing
LLMs for various applications. We will release our code and data soon.
","2023-10-03","2310.00492v1.pdf"
"2310.00500","Ivona Najdenkoska","Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek,
  Marcel Worring, Yuki M. Asano","Small Visual Language Models can also be Open-Ended Few-Shot Learners","","","","","cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We present Self-Context Adaptation (SeCAt), a self-supervised approach that
unlocks open-ended few-shot abilities of small visual language models. Our
proposed adaptation algorithm explicitly learns from symbolic, yet
self-supervised training tasks. Specifically, our approach imitates image
captions in a self-supervised way based on clustering a large pool of images
followed by assigning semantically-unrelated names to clusters. By doing so, we
construct the `self-context', a training signal consisting of interleaved
sequences of image and pseudo-caption pairs and a query image for which the
model is trained to produce the right pseudo-caption. We demonstrate the
performance and flexibility of SeCAt on several multimodal few-shot datasets,
spanning various granularities. By using models with approximately 1B
parameters we outperform the few-shot abilities of much larger models, such as
Frozen and FROMAGe. SeCAt opens new possibilities for research in open-ended
few-shot learning that otherwise requires access to large or proprietary
models.
","2023-10-03","2310.00500v1.pdf"
"2310.00533","Jianqiao Lu","Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun
  Wang, Weichao Wang, Lifeng Shang, Qun Liu","SELF: Language-Driven Self-Evolution for Large Language Model","14 pages, 4 figures, 6 tables. Due to the limitation ""The abstract
  field cannot be longer than 1,920 characters"", the abstract appearing here is
  slightly shorter than that in the PDF file","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have showcased remarkable versatility across
diverse domains. However, the pathway toward autonomous model development, a
cornerstone for achieving human-level learning and advancing autonomous AI,
remains largely uncharted. We introduce an innovative approach, termed ""SELF""
(Self-Evolution with Language Feedback). This methodology empowers LLMs to
undergo continual self-evolution. Furthermore, SELF employs language-based
feedback as a versatile and comprehensive evaluative tool, pinpointing areas
for response refinement and bolstering the stability of self-evolutionary
training. Initiating with meta-skill learning, SELF acquires foundational
meta-skills with a focus on self-feedback and self-refinement. These
meta-skills are critical, guiding the model's subsequent self-evolution through
a cycle of perpetual training with self-curated data, thereby enhancing its
intrinsic abilities. Given unlabeled instructions, SELF equips the model with
the capability to autonomously generate and interactively refine responses.
This synthesized training data is subsequently filtered and utilized for
iterative fine-tuning, enhancing the model's capabilities. Experimental results
on representative benchmarks substantiate that SELF can progressively advance
its inherent abilities without the requirement of human intervention, thereby
indicating a viable pathway for autonomous model evolution. Additionally, SELF
can employ online self-refinement strategy to produce responses of superior
quality. In essence, the SELF framework signifies a progressive step towards
autonomous LLM development, transforming the LLM from a mere passive recipient
of information into an active participant in its own evolution.
","2023-10-11","2310.00533v1.pdf"
"2310.00535","Yuandong Tian","Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du","JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and
  Attention","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical
framework to understand the training procedure of multilayer Transformer
architectures. This is achieved by integrating out the self-attention layer in
Transformers, producing a modified dynamics of MLP layers only. JoMA removes
unrealistic assumptions in previous analysis (e.g., lack of residual
connection) and predicts that the attention first becomes sparse (to learn
salient tokens), then dense (to learn less salient tokens) in the presence of
nonlinear activations, while in the linear case, it is consistent with existing
works that show attention becomes sparse over time. We leverage JoMA to
qualitatively explains how tokens are combined to form hierarchies in
multilayer Transformers, when the input tokens are generated by a latent
hierarchical generative model. Experiments on models trained from real-world
dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)
verify our theoretical findings.
","2023-10-04","2310.00535v1.pdf"
"2310.00566","Duanyu Feng","Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie,
  Weiguang Han, Alejandro Lopez-Lira, Hao Wang","Empowering Many, Biasing a Few: Generalist Credit Scoring through Large
  Language Models","","","","","cs.LG cs.AI cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Credit and risk assessments are cornerstones of the financial landscape,
impacting both individual futures and broader societal constructs. Existing
credit scoring models often exhibit limitations stemming from knowledge myopia
and task isolation. In response, we formulate three hypotheses and undertake an
extensive case study to investigate LLMs' viability in credit assessment. Our
empirical investigations unveil LLMs' ability to overcome the limitations
inherent in conventional models. We introduce a novel benchmark curated for
credit assessment purposes, fine-tune a specialized Credit and Risk Assessment
Large Language Model (CALM), and rigorously examine the biases that LLMs may
harbor. Our findings underscore LLMs' potential in revolutionizing credit
assessment, showcasing their adaptability across diverse financial evaluations,
and emphasizing the critical importance of impartial decision-making in the
financial sector. Our datasets, models, and benchmarks are open-sourced for
other researchers.
","2023-10-03","2310.00566v1.pdf"
"2310.00576","Xiaotian Han","Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan
  Chang, Xia Hu","GrowLength: Accelerating LLMs Pretraining by Progressively Growing
  Training Length","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The evolving sophistication and intricacies of Large Language Models (LLMs)
yield unprecedented advancements, yet they simultaneously demand considerable
computational resources and incur significant costs. To alleviate these
challenges, this paper introduces a novel, simple, and effective method named
``\growlength'' to accelerate the pretraining process of LLMs. Our method
progressively increases the training length throughout the pretraining phase,
thereby mitigating computational costs and enhancing efficiency. For instance,
it begins with a sequence length of 128 and progressively extends to 4096. This
approach enables models to process a larger number of tokens within limited
time frames, potentially boosting their performance. In other words, the
efficiency gain is derived from training with shorter sequences optimizing the
utilization of resources. Our extensive experiments with various
state-of-the-art LLMs have revealed that models trained using our method not
only converge more swiftly but also exhibit superior performance metrics
compared to those trained with existing methods. Furthermore, our method for
LLMs pretraining acceleration does not require any additional engineering
efforts, making it a practical solution in the realm of LLMs.
","2023-10-03","2310.00576v1.pdf"
"2310.00578","Siyi Cao","Siyi Cao, Tongquan Zhou, Siruo Zhou","Nine-year-old children outperformed ChatGPT in emotion: Evidence from
  Chinese writing","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  ChatGPT has been demonstrated to possess significant capabilities in
generating intricate, human-like text, and recent studies have established that
its performance in theory of mind tasks is comparable to that of a
nine-year-old child. However, it remains uncertain whether ChatGPT surpasses
nine-year-old children in Chinese writing proficiency. To explore this, our
study juxtaposed the Chinese writing performance of ChatGPT and nine-year-old
children on both narrative and scientific topics, aiming to uncover the
relative strengths and weaknesses of ChatGPT in writing.
  The collected data were analyzed across five linguistic dimensions: fluency,
accuracy, complexity, cohesion, and emotion. Each dimension underwent
assessment through precise indices. The findings revealed that nine-year-old
children excelled beyond ChatGPT in terms of fluency and cohesion within their
writing. In contrast, ChatGPT manifested a superior performance in accuracy
compared to the children. Concerning complexity, children exhibited superior
skills in science-themed writing, while ChatGPT prevailed in nature-themed
writing. Significantly, this research is pioneering in revealing that
nine-year-old children convey stronger emotions than ChatGPT in their Chinese
compositions.
","2023-10-03","2310.00578v1.pdf"
"2310.00582","Shiyu Xuan","Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang","Pink: Unveiling the Power of Referential Comprehension for Multi-modal
  LLMs","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in many vision-language tasks. Nevertheless, most MLLMs still lack the
Referential Comprehension (RC) ability to identify a specific object or area in
images, limiting their application in fine-grained perception tasks. This paper
proposes a novel method to enhance the RC capability for MLLMs. Our model
represents the referring object in the image using the coordinates of its
bounding box and converts the coordinates into texts in a specific format. This
allows the model to treat the coordinates as natural language. Moreover, we
construct the instruction tuning dataset with various designed RC tasks at a
low cost by unleashing the potential of annotations in existing datasets. To
further boost the RC ability of the model, we propose a self-consistent
bootstrapping method that extends dense object annotations of a dataset into
high-quality referring-expression-bounding-box pairs. The model is trained
end-to-end with a parameter-efficient tuning framework that allows both
modalities to benefit from multi-modal instruction tuning. This framework
requires fewer trainable parameters and less training data. Experimental
results on conventional vision-language and RC tasks demonstrate the superior
performance of our method. For instance, our model exhibits a 12.0% absolute
accuracy improvement over Instruct-BLIP on VSR and surpasses Kosmos-2 by 24.7%
on RefCOCO_val under zero-shot settings. We also attain the top position on the
leaderboard of MMBench. The models, datasets, and codes are publicly available
at https://github.com/SY-Xuan/Pink
","2023-10-03","2310.00582v1.pdf"
"2310.00597","Lucen Zhong","Lucen Zhong, Hengtong Lu, Caixia Yuan, Xiaojie Wang, Jiashen Sun, Ke
  Zeng and Guanglu Wan","A Task-oriented Dialog Model with Task-progressive and Policy-aware
  Pre-training","Accepted at NLPCC 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pre-trained conversation models (PCMs) have achieved promising progress in
recent years. However, existing PCMs for Task-oriented dialog (TOD) are
insufficient for capturing the sequential nature of the TOD-related tasks, as
well as for learning dialog policy information. To alleviate these problems,
this paper proposes a task-progressive PCM with two policy-aware pre-training
tasks. The model is pre-trained through three stages where TOD-related tasks
are progressively employed according to the task logic of the TOD system. A
global policy consistency task is designed to capture the multi-turn dialog
policy sequential relation, and an act-based contrastive learning task is
designed to capture similarities among samples with the same dialog policy. Our
model achieves better results on both MultiWOZ and In-Car end-to-end dialog
modeling benchmarks with only 18\% parameters and 25\% pre-training data
compared to the previous state-of-the-art PCM, GALAXY.
","2023-10-03","2310.00597v1.pdf"
"2310.00598","Aviya Maimon","Aviya Maimon and Reut Tsarfaty","A Novel Computational and Modeling Foundation for Automatic Coherence
  Assessment","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Coherence is an essential property of well-written texts, that refers to the
way textual units relate to one another. In the era of generative AI, coherence
assessment is essential for many NLP tasks; summarization, generation,
long-form question-answering, and more. However, in NLP {coherence} is an
ill-defined notion, not having a formal definition or evaluation metrics, that
would allow for large-scale automatic and systematic coherence assessment. To
bridge this gap, in this work we employ the formal linguistic definition of
\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three
conditions -- {\em cohesion, consistency} and {\em relevance} -- and formalize
these conditions as respective computational tasks. We hypothesize that (i) a
model trained on all of these tasks will learn the features required for
coherence detection, and that (ii) a joint model for all tasks will exceed the
performance of models trained on each task individually. On two benchmarks for
coherence scoring rated by humans, one containing 500 automatically-generated
short stories and another containing 4k real-world texts, our experiments
confirm that jointly training on the proposed tasks leads to better performance
on each task compared with task-specific models, and to better performance on
assessing coherence overall, compared with strong baselines. We conclude that
the formal and computational setup of coherence as proposed here provides a
solid foundation for advanced methods of large-scale automatic assessment of
coherence.
","2023-10-03","2310.00598v1.pdf"
"2310.00603","Nitay Calderon","Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma,
  Roi Reichart","Faithful Explanations of Black-box NLP Models Using LLM-generated
  Counterfactuals","","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  Causal explanations of the predictions of NLP systems are essential to ensure
safety and establish trust. Yet, existing methods often fall short of
explaining model predictions effectively or efficiently and are often
model-specific. In this paper, we address model-agnostic explanations,
proposing two approaches for counterfactual (CF) approximation. The first
approach is CF generation, where a large language model (LLM) is prompted to
change a specific text concept while keeping confounding concepts unchanged.
While this approach is demonstrated to be very effective, applying LLM at
inference-time is costly. We hence present a second approach based on matching,
and propose a method that is guided by an LLM at training-time and learns a
dedicated embedding space. This space is faithful to a given causal graph and
effectively serves to identify matches that approximate CFs. After showing
theoretically that approximating CFs is required in order to construct faithful
explanations, we benchmark our approaches and explain several models, including
LLMs with billions of parameters. Our empirical results demonstrate the
excellent performance of CF generation models as model-agnostic explainers.
Moreover, our matching approach, which requires far less test-time resources,
also provides effective explanations, surpassing many baselines. We also find
that Top-K techniques universally improve every tested method. Finally, we
showcase the potential of LLMs in constructing new benchmarks for model
explanation and subsequently validate our conclusions. Our work illuminates new
pathways for efficient and accurate approaches to interpreting NLP systems.
","2023-10-03","2310.00603v1.pdf"
"2310.00637","Paul Groth","Bradley P. Allen and Lise Stork and Paul Groth","Knowledge Engineering using Large Language Models","19 pages, 2 figures, accepted in Transactions on Graph Data and
  Knowledge","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.
","2023-10-03","2310.00637v1.pdf"
"2310.00646","Bryan Kian Hsiang Low","Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng
  Foo, See-Kiong Ng, Bryan Kian Hsiang Low","WASA: WAtermark-based Source Attribution for Large Language
  Model-Generated Data","","","","","cs.LG cs.AI stat.ML","http://creativecommons.org/licenses/by/4.0/","  The impressive performances of large language models (LLMs) and their immense
potential for commercialization have given rise to serious concerns over the
intellectual property (IP) of their training data. In particular, the synthetic
texts generated by LLMs may infringe the IP of the data being used to train the
LLMs. To this end, it is imperative to be able to (a) identify the data
provider who contributed to the generation of a synthetic text by an LLM
(source attribution) and (b) verify whether the text data from a data provider
has been used to train an LLM (data provenance). In this paper, we show that
both problems can be solved by watermarking, i.e., by enabling an LLM to
generate synthetic texts with embedded watermarks that contain information
about their source(s). We identify the key properties of such watermarking
frameworks (e.g., source attribution accuracy, robustness against adversaries),
and propose a WAtermarking for Source Attribution (WASA) framework that
satisfies these key properties due to our algorithmic designs. Our WASA
framework enables an LLM to learn an accurate mapping from the texts of
different data providers to their corresponding unique watermarks, which sets
the foundation for effective source attribution (and hence data provenance).
Extensive empirical evaluations show that our WASA framework achieves effective
source attribution and data provenance.
","2023-10-03","2310.00646v1.pdf"
"2310.00647","Mustafa Shukor","Mustafa Shukor, Alexandre Rame, Corentin Dancette, Matthieu Cord","Beyond Task Performance: Evaluating and Reducing the Flaws of Large
  Multimodal Models with In-Context Learning","Project Page: https://evalign-icl.github.io/","","","","cs.CV cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Following the success of Large Language Models (LLMs), Large Multimodal
Models (LMMs), such as the Flamingo model and its subsequent competitors, have
started to emerge as natural steps towards generalist agents. However,
interacting with recent LMMs reveals major limitations that are hardly captured
by the current evaluation benchmarks. Indeed, task performances (e.g., VQA
accuracy) alone do not provide enough clues to understand their real
capabilities, limitations, and to which extent such models are aligned to human
expectations. To refine our understanding of those flaws, we deviate from the
current evaluation paradigm and propose the EvALign-ICL framework, in which we
(1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture such
as OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention,
compositionality, explainability and instruction following. Our evaluation on
these axes reveals major flaws in LMMs. To efficiently address these problems,
and inspired by the success of in-context learning (ICL) in LLMs, (2) we
explore ICL as a solution and study how it affects these limitations. Based on
our ICL study, (3) we push ICL further and propose new multimodal ICL
approaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, and
Self-Correcting-ICL. Our findings are as follows; (1) Despite their success,
LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL
on LMMs flaws is nuanced; despite its effectiveness for improved
explainability, abstention, and instruction following, ICL does not improve
compositional abilities, and actually even amplifies hallucinations. (3) The
proposed ICL variants are promising as post-hoc approaches to efficiently
tackle some of those flaws. The code is available here:
https://evalign-icl.github.io/
","2023-10-03","2310.00647v1.pdf"
"2310.00648","Lauren Hong","Lauren Hong, Ting Wang","Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning","16 pages, 5 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of
pre-trained language models (PLMs) to specific tasks. By tuning only a minimal
set of (extra) parameters, PEFT achieves performance comparable to full
fine-tuning. However, despite its prevalent use, the security implications of
PEFT remain largely unexplored. In this paper, we conduct a pilot study
revealing that PEFT exhibits unique vulnerability to trojan attacks.
Specifically, we present PETA, a novel attack that accounts for downstream
adaptation through bilevel optimization: the upper-level objective embeds the
backdoor into a PLM while the lower-level objective simulates PEFT to retain
the PLM's task-specific performance. With extensive evaluation across a variety
of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in
terms of both attack success rate and unaffected clean accuracy, even after the
victim user performs PEFT over the backdoored PLM using untainted data.
Moreover, we empirically provide possible explanations for PETA's efficacy: the
bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules,
thereby retaining the backdoor throughout PEFT. Based on this insight, we
explore a simple defense that omits PEFT in selected layers of the backdoored
PLM and unfreezes a subset of these layers' parameters, which is shown to
effectively neutralize PETA.
","2023-10-05","2310.00648v1.pdf"
"2310.00653","Tianyu Yu","Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang,
  Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, Zhiyuan Liu, Hai-Tao Zheng, Maosong
  Sun","Reformulating Vision-Language Foundation Models and Datasets Towards
  Universal Multimodal Assistants","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities
to perceive images and follow open-ended instructions. The capabilities of
MLLMs depend on two crucial factors: the model architecture to facilitate the
feature alignment of visual modules and large language models; the multimodal
instruction tuning datasets for human instruction following. (i) For the model
architecture, most existing models introduce an external bridge module to
connect vision encoders with language models, which needs an additional
feature-alignment pre-training. In this work, we discover that compact
pre-trained vision language models can inherently serve as ``out-of-the-box''
bridges between vision and language. Based on this, we propose Muffin
framework, which directly employs pre-trained vision-language models to act as
providers of visual signals. (ii) For the multimodal instruction tuning
datasets, existing methods omit the complementary relationship between
different datasets and simply mix datasets from different tasks. Instead, we
propose UniMM-Chat dataset which explores the complementarities of datasets to
generate 1.1M high-quality and diverse multimodal instructions. We merge
information describing the same image from diverse datasets and transforms it
into more knowledge-intensive conversation data. Experimental results
demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset.
Muffin achieves state-of-the-art performance on a wide range of vision-language
tasks, significantly surpassing state-of-the-art models like LLaVA and
InstructBLIP. Our model and dataset are all accessible at
https://github.com/thunlp/muffin.
","2023-10-03","2310.00653v1.pdf"
"2310.00656","Huajian Xin","Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu,
  Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo
  Li, Xiaodan Liang, Heng Liao","LEGO-Prover: Neural Theorem Proving with Growing Libraries","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.
","2023-10-13","2310.00656v1.pdf"
"2310.00673","Lukas Seidel","Lukas Seidel, Sedick David Baker Effendi, Xavier Pinho, Konrad Rieck,
  Brink van der Merwe, Fabian Yamaguchi","Learning Type Inference for Enhanced Dataflow Analysis","- fixed last author's name - fixed header","28th European Symposium on Research in Computer Security (ESORICS)
  2023","","","cs.LG cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Statically analyzing dynamically-typed code is a challenging endeavor, as
even seemingly trivial tasks such as determining the targets of procedure calls
are non-trivial without knowing the types of objects at compile time.
Addressing this challenge, gradual typing is increasingly added to
dynamically-typed languages, a prominent example being TypeScript that
introduces static typing to JavaScript. Gradual typing improves the developer's
ability to verify program behavior, contributing to robust, secure and
debuggable programs. In practice, however, users only sparsely annotate types
directly. At the same time, conventional type inference faces
performance-related challenges as program size grows. Statistical techniques
based on machine learning offer faster inference, but although recent
approaches demonstrate overall improved accuracy, they still perform
significantly worse on user-defined types than on the most common built-in
types. Limiting their real-world usefulness even more, they rarely integrate
with user-facing applications. We propose CodeTIDAL5, a Transformer-based model
trained to reliably predict type annotations. For effective result retrieval
and re-integration, we extract usage slices from a program's code property
graph. Comparing our approach against recent neural type inference systems, our
model outperforms the current state-of-the-art by 7.85% on the
ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore,
we present JoernTI, an integration of our approach into Joern, an open source
static analysis tool, and demonstrate that the analysis benefits from the
additional type information. As our model allows for fast inference times even
on commodity CPUs, making our system available through Joern leads to high
accessibility and facilitates security research.
","2023-10-05","2310.00673v1.pdf"
"2310.00698","Reshma Ramaprasad","Reshma Ramaprasad","Comics for Everyone: Generating Accessible Text Descriptions for Comic
  Strips","Accepted at CLVL: 5th Workshop On Closing The Loop Between Vision And
  Language (ICCV 2023 Workshop)","","","","cs.CV cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Comic strips are a popular and expressive form of visual storytelling that
can convey humor, emotion, and information. However, they are inaccessible to
the BLV (Blind or Low Vision) community, who cannot perceive the images,
layouts, and text of comics. Our goal in this paper is to create natural
language descriptions of comic strips that are accessible to the visually
impaired community. Our method consists of two steps: first, we use computer
vision techniques to extract information about the panels, characters, and text
of the comic images; second, we use this information as additional context to
prompt a multimodal large language model (MLLM) to produce the descriptions. We
test our method on a collection of comics that have been annotated by human
experts and measure its performance using both quantitative and qualitative
metrics. The outcomes of our experiments are encouraging and promising.
","2023-10-03","2310.00698v1.pdf"
"2310.00704","Dongchao Yang","Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu,
  Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao,
  Shinji Watanabe, Helen Meng","UniAudio: An Audio Foundation Model Toward Universal Audio Generation","","","","","cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language models (LLM) have demonstrated the capability to handle a
variety of generative tasks. This paper presents the UniAudio system, which,
unlike prior task-specific approaches, leverages LLM techniques to generate
multiple types of audio (including speech, sounds, music, and singing) with
given input conditions. UniAudio 1) first tokenizes all types of target audio
along with other condition modalities, 2) concatenates source-target pair as a
single sequence, and 3) performs next-token prediction using LLM. Also, a
multi-scale Transformer model is proposed to handle the overly long sequences
caused by the residual vector quantization based neural codec in tokenization.
Training of UniAudio is scaled up to 165K hours of audio and 1B parameters,
based on all generative tasks, aiming to obtain sufficient prior knowledge not
only in the intrinsic properties of audio but also the inter-relationship
between audio and other modalities. Therefore, the trained UniAudio model has
the potential to become a foundation model for universal audio generation: it
shows strong capability in all trained tasks and can seamlessly support new
audio generation tasks after simple fine-tuning. Experiments demonstrate that
UniAudio achieves state-of-the-art or at least competitive results on most of
the 11 tasks. Demo and code are released at
https://github.com/yangdongchao/UniAudio
","2023-10-12","2310.00704v1.pdf"
"2310.00724","Lorenzo Loconte","Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp,
  Arno Solin, Nicolas Gillis, Antonio Vergari","Subtractive Mixture Models via Squaring: Representation and Learning","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Mixture models are traditionally represented and learned by adding several
distributions as components. Allowing mixtures to subtract probability mass or
density can drastically reduce the number of components needed to model complex
distributions. However, learning such subtractive mixtures while ensuring they
still encode a non-negative function is challenging. We investigate how to
learn and perform inference on deep subtractive mixtures by squaring them. We
do this in the framework of probabilistic circuits, which enable us to
represent tensorized mixtures and generalize several other subtractive models.
We theoretically prove that the class of squared circuits allowing subtractions
can be exponentially more expressive than traditional additive mixtures; and,
we empirically show this increased expressiveness on a series of real-world
distribution estimation tasks.
","2023-10-03","2310.00724v1.pdf"
"2310.00726","Pranjal Awasthi","Pranjal Awasthi and Anupam Gupta","Improving Length-Generalization in Transformers via Task Hinting","","","","","cs.LG cs.AI stat.ML","http://creativecommons.org/licenses/by/4.0/","  It has been observed in recent years that transformers have problems with
length generalization for certain types of reasoning and arithmetic tasks. In
particular, the performance of a transformer model trained on tasks (say
addition) up to a certain length (e.g., 5 digit numbers) drops sharply when
applied to longer instances of the same problem. This work proposes an approach
based on task hinting towards addressing length generalization. Our key idea is
that while training the model on task-specific data, it is helpful to
simultaneously train the model to solve a simpler but related auxiliary task as
well.
  We study the classical sorting problem as a canonical example to evaluate our
approach. We design a multitask training framework and show that task hinting
significantly improve length generalization. For sorting we show that it is
possible to train models on data consisting of sequences having length at most
$20$, and improve the test accuracy on sequences of length $100$ from less than
1% (for standard training) to more than 92% (via task hinting).
  Our study uncovers several interesting aspects of length generalization. We
observe that while several auxiliary tasks may seem natural a priori, their
effectiveness in improving length generalization differs dramatically. We
further use probing and visualization-based techniques to understand the
internal mechanisms via which the model performs the task, and propose a
theoretical construction consistent with the observed learning behaviors of the
model. Based on our construction, we show that introducing a small number of
length dependent parameters into the training procedure can further boost the
performance on unseen lengths. Finally, we also show the efficacy of our task
hinting based approach beyond sorting, giving hope that these techniques will
be applicable in broader contexts.
","2023-10-03","2310.00726v1.pdf"
"2310.00737","Emilio Ferrara","Emilio Ferrara","GenAI Against Humanity: Nefarious Applications of Generative Artificial
  Intelligence and Large Language Models","Submitted to CACM (Viewpoint)","","","","cs.CY cs.AI cs.CL cs.HC","http://creativecommons.org/licenses/by/4.0/","  Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications. This article is not just a journey through
the meanders of potential misuse of GenAI and LLMs, but also a call to
recognize the urgency of the challenges ahead. As we navigate the seas of
misinformation campaigns, malicious content generation, and the eerie creation
of sophisticated malware, we'll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on
social media platforms to the unnerving potential of AI to generate fabricated
identities, or alibis made of synthetic realities, the stakes have never been
higher. The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI's nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.
","2023-10-13","2310.00737v1.pdf"
"2310.00741","Shiqi Chen","Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao,
  Pengfei Liu and Junxian He","FELM: Benchmarking Factuality Evaluation of Large Language Models","Accepted by NeurIPS 2023 Track on Datasets and Benchmarks","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Assessing factuality of text generated by large language models (LLMs) is an
emerging yet crucial research area, aimed at alerting users to potential errors
and guiding the development of more reliable LLMs. Nonetheless, the evaluators
assessing factuality necessitate suitable evaluation themselves to gauge
progress and foster advancements. This direction remains under-explored,
resulting in substantial impediments to the progress of factuality evaluators.
To mitigate this issue, we introduce a benchmark for Factuality Evaluation of
large Language Models, referred to as felm. In this benchmark, we collect
responses generated from LLMs and annotate factuality labels in a fine-grained
manner. Contrary to previous studies that primarily concentrate on the
factuality of world knowledge (e.g.~information from Wikipedia), felm focuses
on factuality across diverse domains, spanning from world knowledge to math and
reasoning. Our annotation is based on text segments, which can help pinpoint
specific factual errors. The factuality annotations are further supplemented by
predefined error types and reference links that either support or contradict
the statement. In our experiments, we investigate the performance of several
LLM-based factuality evaluators on felm, including both vanilla LLMs and those
augmented with retrieval mechanisms and chain-of-thought processes. Our
findings reveal that while retrieval aids factuality evaluation, current LLMs
are far from satisfactory to faithfully detect factual errors.
","2023-10-03","2310.00741v1.pdf"
"2310.00746","Zekun Wang","Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu
  Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang
  Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, Junran Peng","RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities
  of Large Language Models","30 pages, repo at
  https://github.com/InteractiveNLP-Team/RoleLLM-public","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
","2023-10-03","2310.00746v1.pdf"
"2310.00754","Huaxiu Yao","Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng,
  Chelsea Finn, Mohit Bansal, Huaxiu Yao","Analyzing and Mitigating Object Hallucination in Large Vision-Language
  Models","","","","","cs.LG cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large vision-language models (LVLMs) have shown remarkable abilities in
understanding visual information with human languages. However, LVLMs still
suffer from object hallucination, which is the problem of generating
descriptions that include objects that do not actually exist in the images.
This can negatively impact many vision-language tasks, such as visual
summarization and reasoning. To address this issue, we propose a simple yet
powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify
object hallucination in LVLMs by reconstructing less hallucinatory
descriptions. LURE is grounded in a rigorous statistical analysis of the key
factors underlying object hallucination, including co-occurrence (the frequent
appearance of certain objects alongside others in images), uncertainty (objects
with higher uncertainty during LVLM decoding), and object position
(hallucination often appears in the later part of the generated text). LURE can
also be seamlessly integrated with any LVLMs. We evaluate LURE on six
open-source LVLMs, achieving a 23% improvement in general object hallucination
evaluation metrics over the previous best approach. In both GPT and human
evaluations, LURE consistently ranks at the top. Our data and code are
available at https://github.com/YiyangZhou/LURE.
","2023-10-03","2310.00754v1.pdf"
"2310.00783","David Balaban","David Balaban, Justin Medich, Pranay Gosar, Justin Hart","Propagating Semantic Labels in Video Data","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Semantic Segmentation combines two sub-tasks: the identification of
pixel-level image masks and the application of semantic labels to those masks.
Recently, so-called Foundation Models have been introduced; general models
trained on very large datasets which can be specialized and applied to more
specific tasks. One such model, the Segment Anything Model (SAM), performs
image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN
are trained on datasets of paired segments and semantic labels. Manual labeling
of custom data, however, is time-consuming. This work presents a method for
performing segmentation for objects in video. Once an object has been found in
a frame of video, the segment can then be propagated to future frames; thus
reducing manual annotation effort. The method works by combining SAM with
Structure from Motion (SfM). The video input to the system is first
reconstructed into 3D geometry using SfM. A frame of video is then segmented
using SAM. Segments identified by SAM are then projected onto the the
reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry
is reprojected into the new perspective, allowing SAM to be invoked fewer
times. System performance is evaluated, including the contributions of the SAM
and SfM components. Performance is evaluated over three main metrics:
computation time, mask IOU with manual labels, and the number of tracking
losses. Results demonstrate that the system has substantial computation time
improvements over human performance for tracking objects over video frames, but
suffers in performance.
","2023-10-03","2310.00783v1.pdf"
"2310.00785","Yapei Chang","Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer","BooookScore: A systematic exploration of book-length summarization in
  the era of LLMs","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Summarizing book-length documents (>100K tokens) that exceed the context
window size of large language models (LLMs) requires first breaking the input
document into smaller chunks and then prompting an LLM to merge, update, and
compress chunk-level summaries. Despite the complexity and importance of this
task, it has yet to be meaningfully studied due to the challenges of
evaluation: existing book-length summarization datasets (e.g., BookSum) are in
the pretraining data of most public LLMs, and existing evaluation methods
struggle to capture errors made by modern LLM summarizers. In this paper, we
present the first study of the coherence of LLM-based book-length summarizers
implemented via two prompting workflows: (1) hierarchically merging chunk-level
summaries, and (2) incrementally updating a running summary. We obtain 1193
fine-grained human annotations on GPT-4 generated summaries of 100
recently-published books and identify eight common types of coherence errors
made by LLMs. Because human evaluation is expensive and time-consuming, we
develop an automatic metric, BooookScore, that measures the proportion of
sentences in a summary that do not contain any of the identified error types.
BooookScore has high agreement with human annotations and allows us to
systematically evaluate the impact of many other critical parameters (e.g.,
chunk size, base LLM) while saving $15K and 500 hours in human evaluation
costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce
summaries with higher BooookScore than the oft-repetitive ones generated by
LLaMA 2. Incremental updating yields lower BooookScore but higher level of
detail than hierarchical merging, a trade-off sometimes preferred by human
annotators. We release code and annotations after blind review to spur more
principled research on book-length summarization.
","2023-10-09","2310.00785v1.pdf"
"2310.00789","Soumajyoti Sarkar Mr.","Soumajyoti Sarkar, Leonard Lausen","Testing the Limits of Unified Sequence to Sequence LLM Pretraining on
  Diverse Table Data Tasks","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Tables stored in databases and tables which are present in web pages and
articles account for a large part of semi-structured data that is available on
the internet. It then becomes pertinent to develop a modeling approach with
large language models (LLMs) that can be used to solve diverse table tasks such
as semantic parsing, question answering as well as classification problems.
Traditionally, there existed separate models specialized for each task
individually. It raises the question of how far can we go to build a unified
model that works well on some table tasks without significant degradation on
others. To that end, we attempt at creating a shared modeling approach in the
pretraining stage with encoder-decoder style LLMs that can cater to diverse
tasks. We evaluate our approach that continually pretrains and finetunes
different model families of T5 with data from tables and surrounding context,
on these downstream tasks at different model scales. Through multiple ablation
studies, we observe that our pretraining with self-supervised objectives can
significantly boost the performance of the models on these tasks. As an example
of one improvement, we observe that the instruction finetuned public models
which come specialized on text question answering (QA) and have been trained on
table data still have room for improvement when it comes to table specific QA.
Our work is the first attempt at studying the advantages of a unified approach
to table specific pretraining when scaled from 770M to 11B sequence to sequence
models while also comparing the instruction finetuned variants of the models.
","2023-10-03","2310.00789v1.pdf"
"2310.00809","Jiaqi Zhang","Jiaqi Zhang, Joel Jennings, Cheng Zhang, Chao Ma","Towards Causal Foundation Model: on Duality between Causal Inference and
  Attention","","","","","cs.LG cs.AI stat.ME stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Foundation models have brought changes to the landscape of machine learning,
demonstrating sparks of human-level intelligence across a diverse array of
tasks. However, a gap persists in complex tasks such as causal inference,
primarily due to challenges associated with intricate reasoning steps and high
numerical precision requirements. In this work, we take a first step towards
building causally-aware foundation models for complex tasks. We propose a
novel, theoretically sound method called Causal Inference with Attention
(CInA), which utilizes multiple unlabeled datasets to perform self-supervised
causal learning, and subsequently enables zero-shot causal inference on unseen
tasks with new data. This is based on our theoretical results that demonstrate
the primal-dual connection between optimal covariate balancing and
self-attention, facilitating zero-shot causal inference through the final layer
of a trained transformer-type architecture. We demonstrate empirically that our
approach CInA effectively generalizes to out-of-distribution datasets and
various real-world datasets, matching or even surpassing traditional
per-dataset causal inference methodologies.
","2023-10-03","2310.00809v1.pdf"
"2310.00811","Liyuan Liu","Liyuan Liu and Jianfeng Gao and Weizhu Chen","Sparse Backpropagation for MoE Training","Work in progress","","","","cs.LG cs.AI cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  One defining characteristic of Mixture-of-Expert (MoE) models is their
capacity for conducting sparse computation via expert routing, leading to
remarkable scalability. However, backpropagation, the cornerstone of deep
learning, requires dense computation, thereby posting challenges in MoE
gradient computations. Here, we introduce SparseMixer, a scalable gradient
estimator that bridges the gap between backpropagation and sparse expert
routing. Unlike typical MoE training which strategically neglects certain
gradient terms for the sake of sparse computation and scalability, SparseMixer
provides scalable gradient approximations for these terms, enabling reliable
gradient estimation in MoE training. Grounded in a numerical ODE framework,
SparseMixer harnesses the mid-point method, a second-order ODE solver, to
deliver precise gradient approximations with negligible computational overhead.
Applying SparseMixer to Switch Transformer on both pre-training and machine
translation tasks, SparseMixer showcases considerable performance gain,
accelerating training convergence up to 2 times.
","2023-10-03","2310.00811v1.pdf"
"2310.00815","Yunjia Zhang","Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen
  Deep, Jignesh M. Patel","ReAcTable: Enhancing ReAct for Table Question Answering","","","","","cs.DB","http://creativecommons.org/licenses/by/4.0/","  Table Question Answering (TQA) presents a substantial challenge at the
intersection of natural language processing and data analytics. This task
involves answering natural language (NL) questions on top of tabular data,
demanding proficiency in logical reasoning, understanding of data semantics,
and fundamental analytical capabilities. Due to its significance, a substantial
volume of research has been dedicated to exploring a wide range of strategies
aimed at tackling this challenge including approaches that leverage Large
Language Models (LLMs) through in-context learning or Chain-of-Thought (CoT)
prompting as well as approaches that train and fine-tune custom models.
  Nonetheless, a conspicuous gap exists in the research landscape, where there
is limited exploration of how innovative foundational research, which
integrates incremental reasoning with external tools in the context of LLMs, as
exemplified by the ReAct paradigm, could potentially bring advantages to the
TQA task. In this paper, we aim to fill this gap, by introducing ReAcTable
(ReAct for Table Question Answering tasks), a framework inspired by the ReAct
paradigm that is carefully enhanced to address the challenges uniquely
appearing in TQA tasks such as interpreting complex data semantics, dealing
with errors generated by inconsistent data and generating intricate data
transformations. ReAcTable relies on external tools such as SQL and Python code
executors, to progressively enhance the data by generating intermediate data
representations, ultimately transforming it into a more accessible format for
answering the questions with greater ease. We demonstrate that ReAcTable
achieves remarkable performance even when compared to fine-tuned approaches. In
particular, it outperforms the best prior result on the WikiTQ benchmark,
achieving an accuracy of 68.0% without requiring training a new model or
fine-tuning.
","2023-10-03","2310.00815v1.pdf"
"2310.00819","Ziqi Wang","Tianci Xue, Ziqi Wang, Heng Ji","Parameter-Efficient Tuning Helps Language Model Alignment","21 pages, 11 figures, 5 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Aligning large language models (LLMs) with human preferences is essential for
safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF)
and direct preference optimization (DPO) with human feedback for alignment.
Nevertheless, they have certain drawbacks. One such limitation is that they can
only align models with one preference at the training time (e.g., they cannot
learn to generate concise responses when the preference data prefers detailed
responses), or have certain constraints for the data format (e.g., DPO only
supports pairwise preference data). To this end, prior works incorporate
controllable generations for alignment to make language models learn multiple
preferences and provide outputs with different preferences during inference if
asked. Controllable generation also offers more flexibility with regard to data
format (e.g., it supports pointwise preference data). Specifically, it uses
different control tokens for different preferences during training and
inference, making LLMs behave differently when required. Current controllable
generation methods either use a special token or hand-crafted prompts as
control tokens, and optimize them together with LLMs. As control tokens are
typically much lighter than LLMs, this optimization strategy may not
effectively optimize control tokens. To this end, we first use
parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to
optimize control tokens and then fine-tune models for controllable generations,
similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning
(MEET), improves the quality of control tokens, thus improving controllable
generation quality consistently by an apparent margin on two well-recognized
datasets compared with prior works.
","2023-10-03","2310.00819v1.pdf"
"2310.00832","Shuo Wang","Shuo Wang and Carlos Crespo-Quinones","Natural Language Models for Data Visualization Utilizing nvBench Dataset","6 pages, 7 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Translation of natural language into syntactically correct commands for data
visualization is an important application of natural language models and could
be leveraged to many different tasks. A closely related effort is the task of
translating natural languages into SQL queries, which in turn could be
translated into visualization with additional information from the natural
language query supplied\cite{Zhong:2017qr}. Contributing to the progress in
this area of research, we built natural language translation models to
construct simplified versions of data and visualization queries in a language
called Vega Zero. In this paper, we explore the design and performance of these
sequence to sequence transformer based machine learning model architectures
using large language models such as BERT as encoders to predict visualization
commands from natural language queries, as well as apply available T5 sequence
to sequence models to the problem for comparison.
","2023-10-03","2310.00832v1.pdf"
"2310.00833","Yuki Takezawa","Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, Makoto Yamada","Necessary and Sufficient Watermark for Large Language Models","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, large language models (LLMs) have achieved remarkable
performances in various NLP tasks. They can generate texts that are
indistinguishable from those written by humans. Such remarkable performance of
LLMs increases their risk of being used for malicious purposes, such as
generating fake news articles. Therefore, it is necessary to develop methods
for distinguishing texts written by LLMs from those written by humans.
Watermarking is one of the most powerful methods for achieving this. Although
existing watermarking methods have successfully detected texts generated by
LLMs, they significantly degrade the quality of the generated texts. In this
study, we propose the Necessary and Sufficient Watermark (NS-Watermark) for
inserting watermarks into generated texts without degrading the text quality.
More specifically, we derive minimum constraints required to be imposed on the
generated texts to distinguish whether LLMs or humans write the texts. Then, we
formulate the NS-Watermark as a constrained optimization problem and propose an
efficient algorithm to solve it. Through the experiments, we demonstrate that
the NS-Watermark can generate more natural texts than existing watermarking
methods and distinguish more accurately between texts written by LLMs and those
written by humans. Especially in machine translation tasks, the NS-Watermark
can outperform the existing watermarking method by up to 30 BLEU scores.
","2023-10-03","2310.00833v1.pdf"
"2310.00835","Yuqing Wang","Yuqing Wang, Yun Zhao","TRAM: Benchmarking Temporal Reasoning for Large Language Models","21 pages, in submission","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Reasoning about time is essential for understanding the nuances of events
described in natural language. Previous research on this topic has been limited
in scope, characterized by a lack of standardized benchmarks that would allow
for consistent evaluations across different studies. In this paper, we
introduce TRAM, a temporal reasoning benchmark composed of ten datasets,
encompassing various temporal aspects of events such as order, arithmetic,
frequency, and duration, designed to facilitate a comprehensive evaluation of
the temporal reasoning capabilities of large language models (LLMs). We conduct
an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both
zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based
models to establish the baseline evaluations. Our findings indicate that these
models still trail human performance in temporal reasoning tasks. It is our
aspiration that TRAM will spur further progress in enhancing the temporal
reasoning abilities of LLMs.
","2023-10-04","2310.00835v1.pdf"
"2310.00836","Man Luo","Man Luo, Shrinidhi Kumbhar, Ming shen, Mihir Parmar, Neeraj Varshney,
  Pratyay Banerjee, Somak Aditya, Chitta Baral","Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical
  Reasoning Capabilities of Language Models","Work in progress","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Logical reasoning is fundamental for humans yet presents a substantial
challenge in the domain of Artificial Intelligence. Initially, researchers used
Knowledge Representation and Reasoning (KR) systems that did not scale and
required non trivial manual effort. Recently, the emergence of large language
models (LLMs) has demonstrated the ability to overcome various limitations of
formal Knowledge Representation (KR) systems. Consequently, there is a growing
interest in using LLMs for logical reasoning via natural language. This work
strives to understand the proficiency of LLMs in logical reasoning by offering
a brief review of the latest progress in this area; with a focus on the logical
reasoning datasets, tasks, and the methods adopted to utilize LLMs for
reasoning. To offer a thorough analysis, we have compiled a benchmark titled
LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,
and inductive reasoning. We have standardized these datasets into Seq2Seq tasks
to facilitate straightforward training and evaluation for future research.
Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned
language model, resulting in LogiT5. We study single task training, multi task
training, and a chain of thought knowledge distillation fine tuning technique
to assess the performance of model across the different logical reasoning
categories. By this comprehensive process, we aim to shed light on the
capabilities and potential pathways for enhancing logical reasoning proficiency
in LLMs, paving the way for more advanced and nuanced developments in this
critical field.
","2023-10-03","2310.00836v1.pdf"
"2310.00845","Tatsuki Kawamoto","Tatsuki Kawamoto, Takuma Suzuki, Ko Miyama, Takumi Meguro, Tomohiro
  Takagi","Application of frozen large-scale models to multimodal task-oriented
  dialogue","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this study, we use the existing Large Language Models ENnhanced to See
Framework (LENS Framework) to test the feasibility of multimodal task-oriented
dialogues. The LENS Framework has been proposed as a method to solve computer
vision tasks without additional training and with fixed parameters of
pre-trained models. We used the Multimodal Dialogs (MMD) dataset, a multimodal
task-oriented dialogue benchmark dataset from the fashion field, and for the
evaluation, we used the ChatGPT-based G-EVAL, which only accepts textual
modalities, with arrangements to handle multimodal data. Compared to
Transformer-based models in previous studies, our method demonstrated an
absolute lift of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance
and coherence. The results show that using large-scale models with fixed
parameters rather than using models trained on a dataset from scratch improves
performance in multimodal task-oriented dialogues. At the same time, we show
that Large Language Models (LLMs) are effective for multimodal task-oriented
dialogues. This is expected to lead to efficient applications to existing
systems.
","2023-10-03","2310.00845v1.pdf"
"2310.00847","Atsuyuki Miyai","Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa","Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Out-of-distribution (OOD) detection is critical for safety-sensitive machine
learning applications and has been extensively studied, yielding a plethora of
methods developed in the literature. However, most studies for OOD detection
did not use pre-trained models and trained a backbone from scratch. In recent
years, transferring knowledge from large pre-trained models to downstream tasks
by lightweight tuning has become mainstream for training in-distribution (ID)
classifiers. To bridge the gap between the practice of OOD detection and
current classifiers, the unique and crucial problem is that the samples whose
information networks know often come as OOD input. We consider that such data
may significantly affect the performance of large pre-trained networks because
the discriminability of these OOD data depends on the pre-training algorithm.
Here, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,
we aim to reveal the effect of PT-OOD on the OOD detection performance of
pre-trained networks from the perspective of pre-training algorithms. To
achieve this, we explore the PT-OOD detection performance of supervised and
self-supervised pre-training algorithms with linear-probing tuning, the most
common efficient tuning method. Through our experiments and analysis, we find
that the low linear separability of PT-OOD in the feature space heavily
degrades the PT-OOD detection performance, and self-supervised models are more
vulnerable to PT-OOD than supervised pre-trained models, even with
state-of-the-art detection methods. To solve this vulnerability, we further
propose a unique solution to large-scale pre-trained models: Leveraging
powerful instance-by-instance discriminative representations of pre-trained
models and detecting OOD in the feature space independent of the ID decision
boundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.
","2023-10-13","2310.00847v1.pdf"
"2310.00863","Zhe Zhang","Zhe Zhang, Karol Lasocki, Yi Yu, Atsuhiro Takasu","Melody-conditioned lyrics generation via fine-tuning language model and
  its evaluation with ChatGPT","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We leverage character-level language models for syllable-level lyrics
generation from symbolic melody. By fine-tuning a character-level pre-trained
model, we integrate language knowledge into the beam search of a syllable-level
Transformer generator. Using ChatGPT-based evaluations, we demonstrate enhanced
coherence and correctness in the generated lyrics.
","2023-10-03","2310.00863v1.pdf"
"2310.00865","David Donoho","David Donoho","Data Science at the Singularity","1 Figure","","","","stat.OT","http://creativecommons.org/licenses/by/4.0/","  A purported `AI Singularity' has been in the public eye recently. Mass media
and US national political attention focused on `AI Doom' narratives hawked by
social media influencers. The European Commission is announcing initiatives to
forestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong
narrative for what's happening now; recent happenings signal something else
entirely. Something fundamental to computation-based research really changed in
the last ten years. In certain fields, progress is dramatically more rapid than
previously, as the fields undergo a transition to frictionless reproducibility
(FR). This transition markedly changes the rate of spread of ideas and
practices, affects mindsets, and erases memories of much that came before.
  The emergence of frictionless reproducibility follows from the maturation of
3 data science principles in the last decade. Those principles involve data
sharing, code sharing, and competitive challenges, however implemented in the
particularly strong form of frictionless open services. Empirical Machine
Learning (EML) is todays leading adherent field, and its consequent rapid
changes are responsible for the AI progress we see. Still, other fields can and
do benefit when they adhere to the same principles.
  Many rapid changes from this maturation are misidentified. The advent of FR
in EML generates a steady flow of innovations; this flow stimulates outsider
intuitions that there's an emergent superpower somewhere in AI. This opens the
way for PR to push worrying narratives: not only `AI Extinction', but also the
supposed monopoly of big tech on AI research. The helpful narrative observes
that the superpower of EML is adherence to frictionless reproducibility
practices; these practices are responsible for the striking progress in AI that
we see everywhere.
","2023-10-03","2310.00865v1.pdf"
"2310.00867","Duc Hoang","Duc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang
  Wang","(Dynamic) Prompting might be all you need to repair Compressed LLMs","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs), while transformative for NLP, come with
significant computational demands, underlining the need for efficient,
training-free compression. Notably, despite the marked improvement in
training-free compression for the largest of LLMs, our tests using LLaMA-7B and
OPT-6.7b highlight a significant performance drop in several realistic
downstream tasks. Investigation into the trade-off between resource-intensive
post-compression re-training highlights the prospect of prompt-driven recovery
as a lightweight adaption tool. However, existing studies, confined mainly to
perplexity evaluations and simple tasks, fail to offer unequivocal confidence
in the scalability and generalizability of prompting. We tackle this
uncertainty in two key ways. First, we uncover the vulnerability of naive
prompts in LLM compression as an over-reliance on a singular prompt per input.
In response, we propose inference-time dynamic prompting (IDP), a mechanism
that autonomously chooses from a set of curated prompts based on the context of
each individual input. Second, we delve into a scientific understanding of why
""prompting might be all you need post-LLM compression."" Our findings suggest
that compression does not irretrievably erase LLM model knowledge but displace
it, necessitating a new inference path. IDP effectively redirects this path,
enabling the model to tap into its inherent yet displaced knowledge and thereby
recover performance. Empirical tests affirm the value of IDP, demonstrating an
average performance improvement of 1.24% across nine varied tasks spanning
multiple knowledge domains.
","2023-10-17","2310.00867v1.pdf"
"2310.00887","Sai Vemprala","Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan, Ashish
  Kapoor","GRID: A Platform for General Robot Intelligence Development","","","","","cs.RO cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Developing machine intelligence abilities in robots and autonomous systems is
an expensive and time consuming process. Existing solutions are tailored to
specific applications and are harder to generalize. Furthermore, scarcity of
training data adds a layer of complexity in deploying deep machine learning
models. We present a new platform for General Robot Intelligence Development
(GRID) to address both of these issues. The platform enables robots to learn,
compose and adapt skills to their physical capabilities, environmental
constraints and goals. The platform addresses AI problems in robotics via
foundation models that know the physical world. GRID is designed from the
ground up to be extensible to accommodate new types of robots, vehicles,
hardware platforms and software protocols. In addition, the modular design
enables various deep ML components and existing foundation models to be easily
usable in a wider variety of robot-centric problems. We demonstrate the
platform in various aerial robotics scenarios and demonstrate how the platform
dramatically accelerates development of machine intelligent robots.
","2023-10-10","2310.00887v1.pdf"
"2310.00898","Ziqi Wang","Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu,
  Heng Ji","Enable Language Models to Implicitly Learn Self-Improvement From Data","28 pages, 5 figures, 4 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated remarkable capabilities in
open-ended text generation tasks. However, the inherent open-ended nature of
these tasks implies that there is always room for improvement in the quality of
model responses. To address this challenge, various approaches have been
proposed to enhance the performance of LLMs. There has been a growing focus on
enabling LLMs to self-improve their response quality, thereby reducing the
reliance on extensive human annotation efforts for collecting diverse and
high-quality training data. Recently, prompting-based methods have been widely
explored among self-improvement methods owing to their effectiveness,
efficiency, and convenience. However, those methods usually require explicitly
and thoroughly written rubrics as inputs to LLMs. It is expensive and
challenging to manually derive and provide all necessary rubrics with a
real-world complex goal for improvement (e.g., being more helpful and less
harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework
that implicitly learns the improvement goal from human preference data. PIT
only requires preference data that are used to train reward models without
extra human efforts. Specifically, we reformulate the training objective of
reinforcement learning from human feedback (RLHF) -- instead of maximizing
response quality for a given input, we maximize the quality gap of the response
conditioned on a reference response. In this way, PIT is implicitly trained
with the improvement goal of better aligning with human preferences.
Experiments on two real-world datasets and one synthetic dataset show that our
method significantly outperforms prompting-based methods.
","2023-10-09","2310.00898v1.pdf"
"2310.00901","Tianci Xue","Tianci Xue, Ziqi Wang, Yixia Li, Yun Chen, Guanhua Chen","TADIS: Steering Models for Deep-Thinking about Demonstration Examples","14 pages, 3 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instruction tuning has been demonstrated that could significantly improve the
zero-shot generalization capability to unseen tasks by an apparent margin. By
incorporating additional context (e.g., task definition, examples) during the
fine-tuning process, Large Language Models (LLMs) achieved much higher
performance than before. However, recent work reported that delusive task
examples can achieve almost the same performance as correct task examples,
indicating the input-label correspondence is less important than previously
thought. Intrigued by this counter-intuitive observation, we suspect models
have the same illusion of competence as humans. Therefore, we propose a novel
method called TADIS that steers LLMs for ""Deep-Thinking'' about demonstration
examples instead of merely seeing. To alleviate the illusion of competence of
models, we first ask the model to verify the correctness of shown examples.
Then, using the verification results as conditions to elicit models for a
better answer. Our experimental results show that TADIS consistently
outperforms competitive baselines on in-domain and out-domain tasks (improving
2.79 and 4.03 average ROUGLE-L on out-domain and in-domain datasets,
respectively). Despite the presence of generated examples (not all of the
thinking labels are accurate), TADIS can notably enhance performance in
zero-shot and few-shot settings. This also suggests that our approach can be
adopted on a large scale to improve the instruction following capabilities of
models without any manual labor. Moreover, we construct three types of thinking
labels with different model sizes and find that small models learn from the
format of TADIS but larger models can be steered for ""Deep-Thinking''.
","2023-10-06","2310.00901v1.pdf"
"2310.00902","Yongchan Kwon","Yongchan Kwon, Eric Wu, Kevin Wu, James Zou","DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and
  Diffusion Models","","","","","cs.LG stat.ML","http://creativecommons.org/licenses/by-sa/4.0/","  Quantifying the impact of training data points is crucial for understanding
the outputs of machine learning models and for improving the transparency of
the AI pipeline. The influence function is a principled and popular data
attribution method, but its computational cost often makes it challenging to
use. This issue becomes more pronounced in the setting of large language models
and text-to-image models. In this work, we propose DataInf, an efficient
influence approximation method that is practical for large-scale generative AI
models. Leveraging an easy-to-compute closed-form expression, DataInf
outperforms existing influence computation algorithms in terms of computational
and memory efficiency. Our theoretical analysis shows that DataInf is
particularly well-suited for parameter-efficient fine-tuning techniques such as
LoRA. Through systematic empirical evaluations, we show that DataInf accurately
approximates influence scores and is orders of magnitude faster than existing
methods. In applications to RoBERTa-large, Llama-2-13B-chat, and
stable-diffusion-v1.5 models, DataInf effectively identifies the most
influential fine-tuning examples better than other approximate influence
scores. Moreover, it can help to identify which data points are mislabeled.
","2023-10-03","2310.00902v1.pdf"
"2310.00905","Wenxuan Wang","Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang,
  Wenxiang Jiao, Michael R. Lyu","All Languages Matter: On the Multilingual Safety of Large Language
  Models","The first multilingual safety benchmark for large language models","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Safety lies at the core of developing and deploying large language models
(LLMs). However, previous safety benchmarks only concern the safety in one
language, e.g. the majority language in the pretraining data such as English.
In this work, we build the first multilingual safety benchmark for LLMs,
XSafety, in response to the global deployment of LLMs in practice. XSafety
covers 14 kinds of commonly used safety issues across 10 languages that span
several language families. We utilize XSafety to empirically study the
multilingual safety for 4 widely-used LLMs, including both close-API and
open-source models. Experimental results show that all LLMs produce
significantly more unsafe responses for non-English queries than English ones,
indicating the necessity of developing safety alignment for non-English
languages. In addition, we propose several simple and effective prompting
methods to improve the multilingual safety of ChatGPT by evoking safety
knowledge and improving cross-lingual generalization of safety alignment. Our
prompting method can significantly reduce the ratio of unsafe responses from
19.1% to 9.7% for non-English queries. We release our data at
https://github.com/Jarviswang94/Multilingual_safety_benchmark.
","2023-10-03","2310.00905v1.pdf"
"2310.00907","Stephen Yang","Fernando Delgado, Stephen Yang, Michael Madaio, Qian Yang","The Participatory Turn in AI Design: Theoretical Foundations and the
  Current State of Practice","","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite the growing consensus that stakeholders affected by AI systems should
participate in their design, enormous variation and implicit disagreements
exist among current approaches. For researchers and practitioners who are
interested in taking a participatory approach to AI design and development, it
remains challenging to assess the extent to which any participatory approach
grants substantive agency to stakeholders. This article thus aims to ground
what we dub the ""participatory turn"" in AI design by synthesizing existing
theoretical literature on participation and through empirical investigation and
critique of its current practices. Specifically, we derive a conceptual
framework through synthesis of literature across technology design, political
theory, and the social sciences that researchers and practitioners can leverage
to evaluate approaches to participation in AI design. Additionally, we
articulate empirical findings concerning the current state of participatory
practice in AI design based on an analysis of recently published research and
semi-structured interviews with 12 AI researchers and practitioners. We use
these empirical findings to understand the current state of participatory
practice and subsequently provide guidance to better align participatory goals
and methods in a way that accounts for practical constraints.
","2023-10-03","2310.00907v1.pdf"
"2310.00935","Yike Wang","Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha
  Balachandran, Tianxing He, Yulia Tsvetkov","Resolving Knowledge Conflicts in Large Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) often encounter knowledge conflicts, scenarios
where discrepancy arises between the internal parametric knowledge of LLMs and
non-parametric information provided in the prompt context. In this work we ask
what are the desiderata for LLMs when a knowledge conflict arises and whether
existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge
conflicts, 2) pinpoint conflicting information segments, and 3) provide
distinct answers or viewpoints in conflicting scenarios. To this end, we
introduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual
knowledge conflicts and quantitatively evaluating to what extent LLMs achieve
these goals. KNOWLEDGE CONFLICT includes diverse and complex situations of
knowledge conflict, knowledge from diverse entities and domains, two synthetic
conflict creation methods, and settings with progressively increasing
difficulty to reflect realistic knowledge conflicts. Extensive experiments with
the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in
identifying the existence of knowledge conflicts, they struggle to determine
the specific conflicting knowledge and produce a response with distinct answers
amidst conflicting information. To address these challenges, we propose new
instruction-based approaches that augment LLMs to better achieve the three
goals. Further analysis shows that abilities to tackle knowledge conflicts are
greatly impacted by factors such as knowledge domain and prompt text, while
generating robust responses to knowledge conflict scenarios remains an open
research question.
","2023-10-03","2310.00935v1.pdf"
"2310.00996","Zhivar Sourati","Zhivar Sourati, Filip Ilievski, Pia Sommerauer","ARN: A Comprehensive Framework and Dataset for Analogical Reasoning on
  Narratives","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Analogical reasoning is one of the prime abilities of humans and is linked to
creativity and scientific discoveries. This ability has been studied
extensively in natural language processing (NLP) as well as in cognitive
psychology by proposing various benchmarks and evaluation setups. Yet, a
substantial gap exists between evaluations of analogical reasoning in cognitive
psychology and NLP. Our aim is to bridge this by computationally adapting
theories related to analogical reasoning from cognitive psychology in the
context of narratives and developing an evaluation framework large in scale.
More concretely, we propose the task of matching narratives based on system
mappings and release the Analogical Reasoning on Narratives (ARN) dataset. To
create the dataset, we devise a framework inspired by cognitive psychology
theories about analogical reasoning to utilize narratives and their components
to form mappings of different abstractness levels. These mappings are then
leveraged to create pairs of analogies and disanalogies/distractors with more
than 1k triples of query narratives, analogies, and distractors. We cover four
categories of far/near analogies and far/near distractors that allow us to
study analogical reasoning in models from distinct perspectives. In this study,
we evaluate different large language models (LLMs) on this task. Our results
demonstrate that LLMs struggle to recognize higher-order mappings when they are
not accompanied by lower-order mappings (far analogies) and show better
performance when all mappings are present simultaneously (near analogies). We
observe that in all the settings, the analogical reasoning abilities of LLMs
can be easily impaired by near distractors that form lower-order mappings with
the query narratives.
","2023-10-03","2310.00996v1.pdf"
"2310.01045","Yekun Chai","Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua
  Wu","Tool-Augmented Reward Modeling","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reward modeling (a.k.a., preference modeling) is instrumental for aligning
large language models with human preferences, particularly within the context
of reinforcement learning from human feedback (RLHF). While conventional reward
models (RMs) have exhibited remarkable scalability, they oft struggle with
fundamental functionality such as arithmetic computation, code execution, and
factual lookup. In this paper, we propose a tool-augmented preference modeling
approach, named \name, to address these limitations by empowering RMs with
access to external environments, including calculators and search engines. This
approach not only fosters synergy between tool utilization and reward grading
but also enhances interpretive capacity and scoring reliability. Our study
delves into the integration of external tools into RMs, enabling them to
interact with diverse external sources and construct task-specific tool
engagement and reasoning traces in an autoregressive manner. We validate our
approach across a wide range of domains, incorporating seven distinct external
tools. Our experimental results demonstrate a noteworthy overall improvement of
17.7% across eight tasks in preference ranking. Furthermore, our approach
outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In
human evaluations, RLHF trained with Themis attains an average win rate of 32%
when compared to baselines across four distinct tasks. Additionally, we provide
a comprehensive collection of tool-related RM datasets, incorporating data from
seven distinct tool APIs, totaling 15,000 instances. We anticipate that this
publicly available dataset will facilitate and inspire further research
advancements in the field.
","2023-10-03","2310.01045v1.pdf"
"2310.01061","Linhao Luo","Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan","Reasoning on Graphs: Faithful and Interpretable Large Language Model
  Reasoning","22 pages, 4 figures","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.
","2023-10-03","2310.01061v1.pdf"
"2310.01089","Jianan Zhao","Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael
  Bronstein, Zhaocheng Zhu, Jian Tang","GraphText: Graph Reasoning in Text Space","Preprint. Work in progress","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have gained the ability to assimilate human
knowledge and facilitate natural language interactions with both humans and
other LLMs. However, despite their impressive achievements, LLMs have not made
significant advancements in the realm of graph machine learning. This
limitation arises because graphs encapsulate distinct relational data, making
it challenging to transform them into natural language that LLMs understand. In
this paper, we bridge this gap with a novel framework, GraphText, that
translates graphs into natural language. GraphText derives a graph-syntax tree
for each graph that encapsulates both the node attributes and inter-node
relationships. Traversal of the tree yields a graph text sequence, which is
then processed by an LLM to treat graph tasks as text generation tasks.
Notably, GraphText offers multiple advantages. It introduces training-free
graph reasoning: even without training on graph data, GraphText with ChatGPT
can achieve on par with, or even surpassing, the performance of
supervised-trained graph neural networks through in-context learning (ICL).
Furthermore, GraphText paves the way for interactive graph reasoning, allowing
both humans and LLMs to communicate with the model seamlessly using natural
language. These capabilities underscore the vast, yet-to-be-explored potential
of LLMs in the domain of graph machine learning.
","2023-10-03","2310.01089v1.pdf"
"2310.01107","Jong Chul Ye","Hyeonho Jeong and Jong Chul Ye","Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image
  Diffusion Models","Project Page: http://ground-a-video.github.io","","","","cs.CV cs.AI cs.LG stat.ML","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent endeavors in video editing have showcased promising results in
single-attribute editing or style transfer tasks, either by training
text-to-video (T2V) models on text-video data or adopting training-free
methods. However, when confronted with the complexities of multi-attribute
editing scenarios, they exhibit shortcomings such as omitting or overlooking
intended attribute changes, modifying the wrong elements of the input video,
and failing to preserve regions of the input video that should remain intact.
To address this, here we present a novel grounding-guided video-to-video
translation framework called Ground-A-Video for multi-attribute video editing.
Ground-A-Video attains temporally consistent multi-attribute editing of input
videos in a training-free manner without aforementioned shortcomings. Central
to our method is the introduction of Cross-Frame Gated Attention which
incorporates groundings information into the latent representations in a
temporally consistent fashion, along with Modulated Cross-Attention and optical
flow guided inverted latents smoothing. Extensive experiments and applications
demonstrate that Ground-A-Video's zero-shot capacity outperforms other baseline
methods in terms of edit-accuracy and frame consistency. Further results and
codes are provided at our project page (http://ground-a-video.github.io).
","2023-10-03","2310.01107v1.pdf"
"2310.01119","Jean Kaddour","Jean Kaddour, Qi Liu","Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large
  Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed.
Alternatively, smaller models can solve specific tasks if fine-tuned with
enough labeled examples. These examples, however, are expensive to obtain. In
pursuit of the best of both worlds, we study the annotation and generation of
fine-tuning training data via fine-tuned teacher LLMs to improve the downstream
performance of much smaller models. In four text classification and two text
generation tasks, we find that both data generation and annotation dramatically
improve the respective downstream model's performance, occasionally
necessitating only a minor fraction of the original training dataset.
","2023-10-03","2310.01119v1.pdf"
"2310.01152","Sihao Hu","Sihao Hu, Tiansheng Huang, Fatih \.Ilhan, Selim Furkan Tekin, Ling Liu","Large Language Model-Powered Smart Contract Vulnerability Detection: New
  Perspectives","10 pages","IEEE International Conference on Trust, Privacy and Security in
  Intelligent Systems, and Applications 2023","","","cs.CR cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper provides a systematic analysis of the opportunities, challenges,
and potential solutions of harnessing Large Language Models (LLMs) such as
GPT-4 to dig out vulnerabilities within smart contracts based on our ongoing
research. For the task of smart contract vulnerability detection, achieving
practical usability hinges on identifying as many true vulnerabilities as
possible while minimizing the number of false positives. Nonetheless, our
empirical study reveals contradictory yet interesting findings: generating more
answers with higher randomness largely boosts the likelihood of producing a
correct answer but inevitably leads to a higher number of false positives. To
mitigate this tension, we propose an adversarial framework dubbed GPTLens that
breaks the conventional one-stage detection into two synergistic stages $-$
generation and discrimination, for progressive detection and refinement,
wherein the LLM plays dual roles, i.e., auditor and critic, respectively. The
goal of auditor is to yield a broad spectrum of vulnerabilities with the hope
of encompassing the correct answer, whereas the goal of critic that evaluates
the validity of identified vulnerabilities is to minimize the number of false
positives. Experimental results and illustrative examples demonstrate that
auditor and critic work together harmoniously to yield pronounced improvements
over the conventional one-stage detection. GPTLens is intuitive, strategic, and
entirely LLM-driven without relying on specialist expertise in smart contracts,
showcasing its methodical generality and potential to detect a broad spectrum
of vulnerabilities. Our code is available at:
https://github.com/git-disl/GPTLens.
","2023-10-18","2310.01152v1.pdf"
"2310.01188","Gabriele Sarti","Gabriele Sarti, Grzegorz Chrupa{\l}a, Malvina Nissim, Arianna Bisazza","Quantifying the Plausibility of Context Reliance in Neural Machine
  Translation","Preprint, under review. 24 pages, 8 figures","","","","cs.CL cs.AI cs.HC cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Establishing whether language models can use contextual information in a
human-plausible way is important to ensure their safe adoption in real-world
settings. However, the questions of when and which parts of the context affect
model generations are typically tackled separately, and current plausibility
evaluations are practically limited to a handful of artificial benchmarks. To
address this, we introduce Plausibility Evaluation of Context Reliance
(PECoRe), an end-to-end interpretability framework designed to quantify context
usage in language models' generations. Our approach leverages model internals
to (i) contrastively identify context-sensitive target tokens in generated
texts and (ii) link them to contextual cues justifying their prediction. We use
PECoRe to quantify the plausibility of context-aware machine translation
models, comparing model rationales with human annotations across several
discourse-level phenomena. Finally, we apply our method to unannotated
generations to identify context-mediated predictions and highlight instances of
(im)plausible context usage in model translations.
","2023-10-03","2310.01188v1.pdf"
"2310.01202","Kamalika Chaudhuri","Kamalika Chaudhuri and David Lopez-Paz","Unified Uncertainty Calibration","","","","","stat.ML cs.LG","http://creativecommons.org/licenses/by/4.0/","  To build robust, fair, and safe AI systems, we would like our classifiers to
say ``I don't know'' when facing test examples that are difficult or fall
outside of the training classes.The ubiquitous strategy to predict under
uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from
prediction if epistemic uncertainty is high, classify otherwise.Unfortunately,
this recipe does not allow different sources of uncertainty to communicate with
each other, produces miscalibrated predictions, and it does not allow to
correct for misspecifications in our uncertainty estimates. To address these
three issues, we introduce \emph{unified uncertainty calibration (U2C)}, a
holistic framework to combine aleatoric and epistemic uncertainties. U2C
enables a clean learning-theoretical analysis of uncertainty estimation, and
outperforms reject-or-classify across a variety of ImageNet benchmarks.
","2023-10-03","2310.01202v1.pdf"
"2310.01208","Zongxi Li","Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li, Fu-lee Wang,
  Qing Li, Xiaoqin Zhong","Label Supervised LLaMA Finetuning","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent success of Large Language Models (LLMs) has gained significant
attention in both academia and industry. Substantial efforts have been made to
enhance the zero- and few-shot generalization capabilities of open-source LLMs
through finetuning. Currently, the prevailing approach is instruction-tuning,
which trains LLMs to complete real-world tasks by generating responses guided
by natural language instructions. It is worth noticing that such an approach
may underperform in sequence and token classification tasks. Unlike text
generation tasks, classification tasks have a limited label space, where
precise label prediction is more appreciated than generating diverse and
human-like responses. Prior research has unveiled that instruction-tuned LLMs
cannot outperform BERT, prompting us to explore the potential of leveraging
latent representations from LLMs for supervised label prediction. In this
paper, we introduce a label-supervised adaptation for LLMs, which aims to
finetuning the model with discriminant labels. We evaluate this approach with
Label Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively
small-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We
extract latent representations from the final LLaMA layer and project them into
the label space to compute the cross-entropy loss. The model is finetuned by
Low-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate
prompt engineering or external knowledge, LS-LLaMA substantially outperforms
LLMs ten times its size in scale and demonstrates consistent improvements
compared to robust baselines like BERT-Large and RoBERTa-Large in text
classification. Moreover, by removing the causal mask from decoders, LS-unLLaMA
achieves the state-of-the-art performance in named entity recognition (NER).
Our work will shed light on a novel approach to adapting LLMs for various
downstream tasks.
","2023-10-03","2310.01208v1.pdf"
"2310.01217","Markus Frohmann","Markus Frohmann, Carolin Holtermann, Shahed Masoudian, Anne Lauscher,
  Navid Rekabsaz","ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by
  Learning to Scale","","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multi-task learning (MTL) has shown considerable practical benefits,
particularly when using pre-trained language models (PLMs). While this is
commonly achieved by simultaneously learning $n$ tasks under a joint
optimization procedure, recent methods such as AdapterFusion structure the
problem into two distinct stages: (i) task learning, where knowledge specific
to a task is encapsulated within sets of parameters (\eg adapters), and (ii)
transfer, where this already learned knowledge is leveraged for a target task.
This separation of concerns provides numerous benefits, such as promoting
reusability, and addressing cases involving data privacy and societal concerns;
on the flip side, current two-stage MTL methods come with the cost of
introducing a substantial number of additional parameters. In this work, we
address this issue by leveraging the usefulness of linearly scaling the output
representations of source adapters for transfer learning. We introduce
ScaLearn, a simple and highly parameter-efficient two-stage MTL method that
capitalizes on the knowledge of the source tasks by learning a minimal set of
scaling parameters that enable effective knowledge transfer to a target task.
Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our
ScaLearn, in addition to facilitating the benefits of two-stage MTL,
consistently outperforms strong baselines with only a small number of transfer
parameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe
that ScaLearn maintains its strong abilities even when further reducing
parameters through uniform scaling and layer-sharing, achieving similarly
competitive results with only $8$ transfer parameters for each target task. Our
proposed approach thus demonstrates the power of simple scaling as a promise
for more efficient task transfer.
","2023-10-03","2310.01217v1.pdf"
"2310.01218","Yuying Ge","Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang and
  Ying Shan","Making LLaMA SEE and Draw with SEED Tokenizer","Project released at: https://github.com/AILab-CVC/SEED. arXiv admin
  note: substantial text overlap with arXiv:2307.08041","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  The great success of Large Language Models (LLMs) has expanded the potential
of multimodality, contributing to the gradual evolution of General Artificial
Intelligence (AGI). A true AGI agent should not only possess the capability to
perform predefined multi-tasks but also exhibit emergent abilities in an
open-world context. However, despite the considerable advancements made by
recent multimodal LLMs, they still fall short in effectively unifying
comprehension and generation tasks, let alone open-world emergent abilities. We
contend that the key to overcoming the present impasse lies in enabling text
and images to be represented and processed interchangeably within a unified
autoregressive Transformer. To this end, we introduce SEED, an elaborate image
tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.
We identify two crucial design principles: (1) Image tokens should be
independent of 2D physical patch positions and instead be produced with a 1D
causal dependency, exhibiting intrinsic interdependence that aligns with the
left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens
should capture high-level semantics consistent with the degree of semantic
abstraction in words, and be optimized for both discriminativeness and
reconstruction during the tokenizer training phase. With SEED tokens, LLM is
able to perform scalable multimodal autoregression under its original training
recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by
large-scale pretraining and instruction tuning on the interleaved textual and
visual data, demonstrating impressive performance on a broad range of
multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has
exhibited compositional emergent abilities such as multi-turn in-context
multimodal generation, acting like your AI assistant.
","2023-10-03","2310.01218v1.pdf"
"2310.01236","Guan-Horng Liu","Guan-Horng Liu, Tianrong Chen, Evangelos A. Theodorou, Molei Tao","Mirror Diffusion Models for Constrained and Watermarked Generation","submitted to NeurIPS on 5/18 but did not arxiv per NeurIPS policy,
  accepted on 9/22","","","","stat.ML cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modern successes of diffusion models in learning complex, high-dimensional
data distributions are attributed, in part, to their capability to construct
diffusion processes with analytic transition kernels and score functions. The
tractability results in a simulation-free framework with stable regression
losses, from which reversed, generative processes can be learned at scale.
However, when data is confined to a constrained set as opposed to a standard
Euclidean space, these desirable characteristics appear to be lost based on
prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new
class of diffusion models that generate data on convex constrained sets without
losing any tractability. This is achieved by learning diffusion processes in a
dual space constructed from a mirror map, which, crucially, is a standard
Euclidean space. We derive efficient computation of mirror maps for popular
constrained sets, such as simplices and $\ell_2$-balls, showing significantly
improved performance of MDM over existing methods. For safety and privacy
purposes, we also explore constrained sets as a new mechanism to embed
invisible but quantitative information (i.e., watermarks) in generated data,
for which MDM serves as a compelling approach. Our work brings new algorithmic
opportunities for learning tractable diffusion on complex domains.
","2023-10-03","2310.01236v1.pdf"
"2310.01248","Yuelyu Ji","Yuelyu Ji, Yuheng Song, Wei Wang, Ruoyi Xu, Zhongqian Xie, Huiyun Liu","Improving Emotional Expression and Cohesion in Image-Based Playlist
  Description and Music Topics: A Continuous Parameterization Approach","Becasue I find some important fourmulation need to change","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Text generation in image-based platforms, particularly for music-related
content, requires precise control over text styles and the incorporation of
emotional expression. However, existing approaches often need help to control
the proportion of external factors in generated text and rely on discrete
inputs, lacking continuous control conditions for desired text generation. This
study proposes Continuous Parameterization for Controlled Text Generation
(CPCTG) to overcome these limitations. Our approach leverages a Language Model
(LM) as a style learner, integrating Semantic Cohesion (SC) and Emotional
Expression Proportion (EEP) considerations. By enhancing the reward method and
manipulating the CPCTG level, our experiments on playlist description and music
topic generation tasks demonstrate significant improvements in ROUGE scores,
indicating enhanced relevance and coherence in the generated text.
","2023-10-16","2310.01248v1.pdf"
"2310.01260","Yujian Li","Yujian Betterest Li, Kai Wu","SPELL: Semantic Prompt Evolution based on a LLM","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Prompt engineering is a new paradigm for enhancing the performance of trained
neural network models. For optimizing text-style prompts, existing methods
usually individually operate small portions of a text step by step, which
either breaks the fluency or could not globally adjust a prompt. Since large
language models (LLMs) have powerful ability of generating coherent texts token
by token, can we utilize LLMs for improving prompts? Based on this motivation,
in this paper, considering a trained LLM as a text generator, we attempt to
design a black-box evolution algorithm for automatically optimizing texts,
namely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is
evaluated with different LLMs and evolution parameters in different text tasks.
Experimental results show that SPELL could rapidly improve the prompts indeed.
We further explore the evolution process and discuss on the limitations,
potential possibilities and future work.
","2023-10-03","2310.01260v1.pdf"
"2310.01262","Ant\'onio Farinhas","Ant\'onio Farinhas, Chrysoula Zerva, Dennis Ulmer, Andr\'e F. T.
  Martins","Non-Exchangeable Conformal Risk Control","","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Split conformal prediction has recently sparked great interest due to its
ability to provide formally guaranteed uncertainty sets or intervals for
predictions made by black-box neural models, ensuring a predefined probability
of containing the actual ground truth. While the original formulation assumes
data exchangeability, some extensions handle non-exchangeable data, which is
often the case in many real-world scenarios. In parallel, some progress has
been made in conformal methods that provide statistical guarantees for a
broader range of objectives, such as bounding the best F1-score or minimizing
the false negative rate in expectation. In this paper, we leverage and extend
these two lines of work by proposing non-exchangeable conformal risk control,
which allows controlling the expected value of any monotone loss function when
the data is not exchangeable. Our framework is flexible, makes very few
assumptions, and allows weighting the data based on its statistical similarity
with the test examples; a careful choice of weights may result on tighter
bounds, making our framework useful in the presence of change points, time
series, or other forms of distribution drift. Experiments with both synthetic
and real world data show the usefulness of our method.
","2023-10-03","2310.01262v1.pdf"
"2310.01287","Kihoon Son","Kihoon Son, DaEun Choi, Tae Soo Kim, Young-Ho Kim, and Juho Kim","GenQuery: Supporting Expressive Visual Search with Generative Models","18 pages and 12 figures","","","","cs.HC","http://creativecommons.org/licenses/by/4.0/","  Designers rely on visual search to explore and develop ideas in early design
stages. However, designers can struggle to identify suitable text queries to
initiate a search or to discover images for similarity-based search that can
adequately express their intent. We propose GenQuery, a novel system that
integrates generative models into the visual search process. GenQuery can
automatically elaborate on users' queries and surface concrete search
directions when users only have abstract ideas. To support precise expression
of search intents, the system enables users to generatively modify images and
use these in similarity-based search. In a comparative user study (N=16),
designers felt that they could more accurately express their intents and find
more satisfactory outcomes with GenQuery compared to a tool without generative
features. Furthermore, the unpredictability of generations allowed participants
to uncover more diverse outcomes. By supporting both convergence and
divergence, GenQuery led to a more creative experience.
","2023-10-03","2310.01287v1.pdf"
"2310.01290","Wenxuan Ding","Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha
  Balachandran, Tianxing He, Yulia Tsvetkov","Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with
  Large Language Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) are widely adopted in knowledge-intensive tasks
and have achieved impressive performance thanks to their knowledge abilities.
While LLMs have demonstrated outstanding performance on atomic or linear
(multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with
interweaving constraints remains an underexplored problem. In this work, we
propose geometric reasoning over structured knowledge, where pieces of
knowledge are connected in a graph structure and models need to fill in the
missing information. Such geometric knowledge reasoning would require the
ability to handle structured knowledge, reason with uncertainty, verify facts,
and backtrack when an error occurs. We propose Knowledge Crosswords, a
multi-blank QA dataset where each problem consists of a natural language
question representing the geometric constraints of an incomplete entity
network, where LLMs are tasked with working out the missing entities while
meeting all factual constraints. Knowledge Crosswords contains 2,101 individual
problems, covering various knowledge domains and further divided into three
difficulty levels. We conduct extensive experiments to evaluate existing LLM
prompting approaches on the Knowledge Crosswords benchmark. We additionally
propose two new approaches, Staged Prompting and Verify-All, to augment LLMs'
ability to backtrack and verify structured constraints. Our results demonstrate
that while baseline approaches perform well on easier problems but struggle
with hard ones, our proposed Verify-All outperforms other methods by a large
margin and is more robust with hard problems. Further analysis reveals that
LLMs' ability of geometric reasoning over structured knowledge is still far
from robust or perfect, susceptible to confounders such as the order of
options, certain structural patterns, assumption of existence of correct
answer, and more.
","2023-10-03","2310.01290v1.pdf"
"2310.01297","Andrew D. Gordon","Andrew D. Gordon, Carina Negreanu, Jos\'e Cambronero, Rasika
  Chakravarthy, Ian Drosos, Hao Fang, Bhaskar Mitra, Hannah Richardson, Advait
  Sarkar, Stephanie Simmons, Jack Williams, Ben Zorn","Co-audit: tools to help humans double-check AI-generated content","","","","","cs.HC cs.AI cs.CL cs.PL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Users are increasingly being warned to check AI-generated content for
correctness. Still, as LLMs (and other generative models) generate more complex
output, such as summaries, tables, or code, it becomes harder for the user to
audit or evaluate the output for quality or correctness. Hence, we are seeing
the emergence of tool-assisted experiences to help the user double-check a
piece of AI-generated content. We refer to these as co-audit tools. Co-audit
tools complement prompt engineering techniques: one helps the user construct
the input prompt, while the other helps them check the output response. As a
specific example, this paper describes recent research on co-audit tools for
spreadsheet computations powered by generative models. We explain why co-audit
experiences are essential for any application of generative AI where quality is
important and errors are consequential (as is common in spreadsheet
computations). We propose a preliminary list of principles for co-audit, and
outline research challenges.
","2023-10-03","2310.01297v1.pdf"
"2310.01299","Wei Sun","Wei Sun, Mingxiao Li, Damien Sileo, Jesse Davis, and Marie-Francine
  Moens","Generating Explanations in Medical Question-Answering by Expectation
  Maximization Inference over Evidence","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Medical Question Answering~(medical QA) systems play an essential role in
assisting healthcare workers in finding answers to their questions. However, it
is not sufficient to merely provide answers by medical QA systems because users
might want explanations, that is, more analytic statements in natural language
that describe the elements and context that support the answer. To do so, we
propose a novel approach for generating natural language explanations for
answers predicted by medical QA systems. As high-quality medical explanations
require additional medical knowledge, so that our system extract knowledge from
medical textbooks to enhance the quality of explanations during the explanation
generation process. Concretely, we designed an expectation-maximization
approach that makes inferences about the evidence found in these texts,
offering an efficient way to focus attention on lengthy evidence passages.
Experimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate
the effectiveness of our framework for reasoning with textual evidence. Our
approach outperforms state-of-the-art models, achieving a significant
improvement of \textbf{6.86} and \textbf{9.43} percentage points on the Rouge-1
score; \textbf{8.23} and \textbf{7.82} percentage points on the Bleu-4 score on
the respective datasets.
","2023-10-03","2310.01299v1.pdf"
"2310.01304","Ruixuan Liu","Ruixuan Liu, Zhiqi Bu, Yu-xiang Wang, Sheng Zha, George Karypis","Coupling public and private gradient provably helps optimization","12 pages","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The success of large neural networks is crucially determined by the
availability of data. It has been observed that training only on a small amount
of public data, or privately on the abundant private data can lead to
undesirable degradation of accuracy. In this work, we leverage both private and
public data to improve the optimization, by coupling their gradients via a
weighted linear combination. We formulate an optimal solution for the optimal
weight in the convex setting to indicate that the weighting coefficient should
be hyperparameter-dependent. Then, we prove the acceleration in the convergence
of non-convex loss and the effects of hyper-parameters such as privacy budget,
number of iterations, batch size, and model size on the choice of the weighting
coefficient. We support our analysis with empirical experiments across language
and vision benchmarks, and provide a guideline for choosing the optimal weight
of the gradient coupling.
","2023-10-03","2310.01304v1.pdf"
"2310.01307","Han Xu","Han Xu, Jie Ren, Pengfei He, Shenglai Zeng, Yingqian Cui, Amy Liu, Hui
  Liu, Jiliang Tang","On the Generalization of Training-based ChatGPT Detection Methods","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  ChatGPT is one of the most popular language models which achieve amazing
performance on various natural language tasks. Consequently, there is also an
urgent need to detect the texts generated ChatGPT from human written. One of
the extensively studied methods trains classification models to distinguish
both. However, existing studies also demonstrate that the trained models may
suffer from distribution shifts (during test), i.e., they are ineffective to
predict the generated texts from unseen language tasks or topics. In this work,
we aim to have a comprehensive investigation on these methods' generalization
behaviors under distribution shift caused by a wide range of factors, including
prompts, text lengths, topics, and language tasks. To achieve this goal, we
first collect a new dataset with human and ChatGPT texts, and then we conduct
extensive studies on the collected dataset. Our studies unveil insightful
findings which provide guidance for developing future methodologies or data
collection strategies for ChatGPT detection.
","2023-10-04","2310.01307v1.pdf"
"2310.01320","Shenzhi Wang","Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen
  Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang","Avalon's Game of Thoughts: Battle Against Deception through Recursive
  Contemplation","40 pages","","","","cs.AI cs.CL cs.CY cs.LG cs.MA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
","2023-10-25","2310.01320v1.pdf"
"2310.01324","Xinhao Li","Xinhao Li, Limin Wang","ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to
  Video","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Adapting image models to video domain is becoming an efficient paradigm for
solving video recognition tasks. Due to the huge number of parameters and
effective transferability of image models, performing full fine-tuning is less
efficient and even unnecessary. Thus, recent research is shifting its focus
towards parameter-efficient image-to-video adaptation. However, these
adaptation strategies inevitably introduce extra computational cost to deal
with the domain gap and temporal modeling in videos. In this paper, our goal is
to present a zero-cost adaptation paradigm (ZeroI2V) to transfer the image
transformers to video recognition tasks (i.e., introduce zero extra cost to the
adapted models during inference). To achieve this goal, we present two core
designs. First, to capture the dynamics in videos and reduce the difficulty of
achieving image-to-video adaptation, we exploit the flexibility of
self-attention and introduce the spatial-temporal dual-headed attention (STDHA)
that efficiently endow the image transformers with temporal modeling capability
at zero extra parameters and computation. Second, to handle the domain gap
between images and videos, we propose a linear adaption strategy which utilizes
lightweight densely placed linear adapters to fully transfer the frozen image
models to video recognition. Due to its customized linear design, all newly
added adapters could be easily merged with the original modules through
structural reparameterization after training, thus achieving zero extra cost
during inference. Extensive experiments on four widely-used video recognition
benchmarks show that our ZeroI2V can match or even outperform previous
state-of-the-art methods while enjoying superior parameter and inference
efficiency.
","2023-10-03","2310.01324v1.pdf"
"2310.01329","Qingqing Cao","Qingqing Cao, Sewon Min, Yizhong Wang, Hannaneh Hajishirzi","BTR: Binary Token Representations for Efficient Retrieval Augmented
  Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Retrieval augmentation addresses many critical problems in large language
models such as hallucination, staleness, and privacy leaks. However, running
retrieval-augmented language models (LMs) is slow and difficult to scale due to
processing large amounts of retrieved text. We introduce binary token
representations (BTR), which use 1-bit vectors to precompute every token in
passages, significantly reducing computation during inference. Despite the
potential loss of accuracy, our new calibration techniques and training
objectives restore performance. Combined with offline and runtime compression,
this only requires 127GB of disk space for encoding 3 billion tokens in
Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR
accelerates state-of-the-art inference by up to 4x and reduces storage by over
100x while maintaining over 95% task performance.
","2023-10-03","2310.01329v1.pdf"
"2310.01330","Qiyu Wu","Qiyu Wu, Mengjie Zhao, Yutong He, Lang Huang, Junya Ono, Hiromi
  Wakaki, Yuki Mitsufuji","Towards reporting bias in visual-language datasets: bimodal augmentation
  by decoupling object-attribute association","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reporting bias arises when people assume that some knowledge is universally
understood and hence, do not necessitate explicit elaboration. In this paper,
we focus on the wide existence of reporting bias in visual-language datasets,
embodied as the object-attribute association, which can subsequentially degrade
models trained on them. To mitigate this bias, we propose a bimodal
augmentation (BiAug) approach through object-attribute decoupling to flexibly
synthesize visual-language examples with a rich array of object-attribute
pairing and construct cross-modal hard negatives. We employ large language
models (LLMs) in conjunction with a grounding object detector to extract target
objects. Subsequently, the LLM generates a detailed attribute description for
each object and produces a corresponding hard negative counterpart. An
inpainting model is then used to create images based on these detailed object
descriptions. By doing so, the synthesized examples explicitly complement
omitted objects and attributes to learn, and the hard negative pairs steer the
model to distinguish object attributes. Our experiments demonstrated that BiAug
is superior in object-attribute understanding. In addition, BiAug also improves
the performance on zero-shot retrieval tasks on general benchmarks like MSCOCO
and Flickr30K. BiAug refines the way of collecting text-image datasets.
Mitigating the reporting bias helps models achieve a deeper understanding of
visual-language phenomena, expanding beyond mere frequent patterns to encompass
the richness and diversity of real-world scenarios.
","2023-10-03","2310.01330v1.pdf"
"2310.01331","Jeongeon Park","Jeongeon Park, Bryan Min, Xiaojuan Ma, Juho Kim","ChoiceMates: Supporting Unfamiliar Online Decision-Making with
  Multi-Agent Conversational Interactions","","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Unfamiliar decisions -- decisions where people lack adequate domain knowledge
or expertise -- specifically increase the complexity and uncertainty of the
process of searching for, understanding, and making decisions with online
information. Through our formative study (n=14), we observed users' challenges
in accessing diverse perspectives, identifying relevant information, and
deciding the right moment to make the final decision. We present ChoiceMates, a
system that enables conversations with a dynamic set of LLM-powered agents for
a holistic domain understanding and efficient discovery and management of
information to make decisions. Agents, as opinionated personas, flexibly join
the conversation, not only providing responses but also conversing among
themselves to elicit each agent's preferences. Our between-subjects study
(n=36) comparing ChoiceMates to conventional web search and single-agent showed
that ChoiceMates was more helpful in discovering, diving deeper, and managing
information compared to Web with higher confidence. We also describe how
participants utilized multi-agent conversations in their decision-making
process.
","2023-10-03","2310.01331v1.pdf"
"2310.01334","Pingzhi Li","Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit
  Bansal, Tianlong Chen","Merge, Then Compress: Demystify Efficient SMoE with Hints from Its
  Routing Policy","17 pages, 5 figures, 11 tables","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up
the learning capacity of neural networks, however, they have issues like (a)
High Memory Usage, due to duplication of the network layers into multiple
copies as experts; and (b) Redundancy in Experts, as common learning-based
routing policies suffer from representational collapse. Therefore, vanilla SMoE
models are memory inefficient and non-scalable, especially for
resource-constrained downstream scenarios. In this paper, we ask: Can we craft
a compact SMoE model by consolidating expert information? What is the best
recipe to merge multiple experts into fewer but more knowledgeable experts? Our
pilot investigation reveals that conventional model merging methods fail to be
effective in such expert merging for SMoE. The potential reasons are: (1)
redundant information overshadows critical experts; (2) appropriate neuron
permutation for each expert is missing to bring all of them in alignment. To
address this, we propose M-SMoE, which leverages routing statistics to guide
expert merging. Specifically, it starts with neuron permutation alignment for
experts; then, dominant experts and their ""group members"" are formed; lastly,
every expert group is merged into a single expert by utilizing each expert's
activation frequency as their weight for merging, thus diminishing the impact
of insignificant experts. Moreover, we observed that our proposed merging
promotes a low dimensionality in the merged expert's weight space, naturally
paving the way for additional compression. Hence, our final method, MC-SMoE
(i.e., Merge, then Compress SMoE), further decomposes the merged experts into
low-rank and structural sparse alternatives. Extensive experiments across 8
benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE
achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in
performance.
","2023-10-03","2310.01334v1.pdf"
"2310.01352","Xi Victoria Lin","Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli,
  Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke
  Zettlemoyer, Scott Yih","RA-DIT: Retrieval-Augmented Dual Instruction Tuning","24 pages","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Retrieval-augmented language models (RALMs) improve performance by accessing
long-tail and up-to-date knowledge from external data stores, but are
challenging to build. Existing approaches require either expensive
retrieval-specific modifications to LM pre-training or use post-hoc integration
of the data store that leads to suboptimal performance. We introduce
Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning
methodology that provides a third option by retrofitting any LLM with retrieval
capabilities. Our approach operates in two distinct fine-tuning steps: (1) one
updates a pre-trained LM to better use retrieved information, while (2) the
other updates the retriever to return more relevant results, as preferred by
the LM. By fine-tuning over tasks that require both knowledge utilization and
contextual awareness, we demonstrate that each stage yields significant
performance improvements, and using both leads to additional gains. Our best
model, RA-DIT 65B, achieves state-of-the-art performance across a range of
knowledge-intensive zero- and few-shot learning benchmarks, significantly
outperforming existing in-context RALM approaches by up to +8.9% in 0-shot
setting and +1.4% in 5-shot setting on average.
","2023-10-10","2310.01352v1.pdf"
"2310.01356","Shu Zhao","Shu Zhao, Huijuan Xu","Less is More: Toward Zero-Shot Local Scene Graph Generation via
  Foundation Models","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Humans inherently recognize objects via selective visual perception,
transform specific regions from the visual field into structured symbolic
knowledge, and reason their relationships among regions based on the allocation
of limited attention resources in line with humans' goals. While it is
intuitive for humans, contemporary perception systems falter in extracting
structural information due to the intricate cognitive abilities and commonsense
knowledge required. To fill this gap, we present a new task called Local Scene
Graph Generation. Distinct from the conventional scene graph generation task,
which encompasses generating all objects and relationships in an image, our
proposed task aims to abstract pertinent structural information with partial
objects and their relationships for boosting downstream tasks that demand
advanced comprehension and reasoning capabilities. Correspondingly, we
introduce zEro-shot Local scEne GrAph geNeraTion (ELEGANT), a framework
harnessing foundation models renowned for their powerful perception and
commonsense reasoning, where collaboration and information communication among
foundation models yield superior outcomes and realize zero-shot local scene
graph generation without requiring labeled supervision. Furthermore, we propose
a novel open-ended evaluation metric, Entity-level CLIPScorE (ECLIPSE),
surpassing previous closed-set evaluation metrics by transcending their limited
label space, offering a broader assessment. Experiment results show that our
approach markedly outperforms baselines in the open-ended evaluation setting,
and it also achieves a significant performance boost of up to 24.58% over prior
methods in the close-set setting, demonstrating the effectiveness and powerful
reasoning ability of our proposed framework.
","2023-10-03","2310.01356v1.pdf"
"2310.01361","Lirui Wang","Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao,
  Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang","GenSim: Generating Robotic Simulation Tasks via Large Language Models","See our project website (https://liruiw.github.io/gensim), demo
  (https://huggingface.co/spaces/Gen-Sim/Gen-Sim), and code
  (https://github.com/liruiw/GenSim) for visualizations and open-source models
  and datasets","","","","cs.LG cs.CL cs.CV cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Collecting large amounts of real-world interaction data to train general
robotic policies is often prohibitively expensive, thus motivating the use of
simulation data. However, existing methods for data generation have generally
focused on scene-level diversity (e.g., object instances and poses) rather than
task-level diversity, due to the human effort required to come up with and
verify novel tasks. This has made it challenging for policies trained on
simulation data to demonstrate significant task-level generalization. In this
paper, we propose to automatically generate rich simulation environments and
expert demonstrations by exploiting a large language models' (LLM) grounding
and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed
generation, wherein a target task is given to the LLM and the LLM proposes a
task curriculum to solve the target task, and exploratory generation, wherein
the LLM bootstraps from previous tasks and iteratively proposes novel tasks
that would be helpful in solving more complex tasks. We use GPT4 to expand the
existing benchmark by ten times to over 100 tasks, on which we conduct
supervised finetuning and evaluate several LLMs including finetuned GPTs and
Code Llama on code generation for robotic simulation tasks. Furthermore, we
observe that LLMs-generated simulation programs can enhance task-level
generalization significantly when used for multitask policy training. We
further find that with minimal sim-to-real adaptation, the multitask policies
pretrained on GPT4-generated simulation tasks exhibit stronger transfer to
unseen long-horizon tasks in the real world and outperform baselines by 25%.
See the project website (https://liruiw.github.io/gensim) for code, demos, and
videos.
","2023-10-03","2310.01361v1.pdf"
"2310.01377","Lifan Yuan","Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni,
  Guotong Xie, Zhiyuan Liu, Maosong Sun","UltraFeedback: Boosting Language Models with High-quality Feedback","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Reinforcement learning from human feedback (RLHF) has become a pivot
technique in aligning large language models (LLMs) with human preferences. In
RLHF practice, preference data plays a crucial role in bridging human
proclivity and LLMs. However, the scarcity of diverse, naturalistic datasets of
human preferences on LLM outputs at scale poses a great challenge to RLHF as
well as feedback learning research within the open-source community. Current
preference datasets, either proprietary or limited in size and prompt variety,
result in limited RLHF adoption in open-source models and hinder further
exploration. In this study, we propose ULTRAFEEDBACK, a large-scale,
high-quality, and diversified preference dataset designed to overcome these
limitations and foster RLHF development. To create ULTRAFEEDBACK, we compile a
diverse array of instructions and models from multiple sources to produce
comparative data. We meticulously devise annotation instructions and employ
GPT-4 to offer detailed feedback in both numerical and textual forms.
ULTRAFEEDBACK establishes a reproducible and expandable preference data
construction pipeline, serving as a solid foundation for future RLHF and
feedback learning research. Utilizing ULTRAFEEDBACK, we train various models to
demonstrate its effectiveness, including the reward model UltraRM, chat
language model UltraLM-13B-PPO, and critique model UltraCM. Experimental
results indicate that our models outperform existing open-source models,
achieving top performance across multiple benchmarks. Our data and models are
available at https://github.com/thunlp/UltraFeedback.
","2023-10-03","2310.01377v1.pdf"
"2310.01382","Zhe Gan","Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei
  Yang","Compressing LLMs: The Truth is Rarely Pure and Never Simple","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite their remarkable achievements, modern Large Language Models (LLMs)
encounter exorbitant computational and memory footprints. Recently, several
works have shown significant success in training-free and data-free compression
(pruning and quantization) of LLMs achieving 50-60% sparsity and reducing the
bit-width down to 3 or 4 bits per weight, with negligible perplexity
degradation over the uncompressed baseline. As recent research efforts are
focused on developing increasingly sophisticated compression methods, our work
takes a step back, and re-evaluates the effectiveness of existing SoTA
compression methods, which rely on a fairly simple and widely questioned
metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive
Compressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated tasks to
re-define the evaluation protocol for compressed LLMs, which have significant
alignment with their dense counterparts, and perplexity fail to capture subtle
change in their true capabilities. LLM-KICK unveils many favorable merits and
unfortunate plights of current SoTA compression methods: all pruning methods
suffer significant performance degradation, sometimes at trivial sparsity
ratios (e.g., 25-30%), and fail for N:M sparsity on knowledge-intensive tasks;
current quantization methods are more successful than pruning; yet, pruned LLMs
even at $\geq 50$% sparsity are robust in-context retrieval and summarization
systems; among others. LLM-KICK is designed to holistically access compressed
LLMs' ability for language understanding, reasoning, generation, in-context
retrieval, in-context summarization, etc. We hope our study can foster the
development of better LLM compression methods. All our related codes are planed
to be open-sourced.
","2023-10-03","2310.01382v1.pdf"
"2310.01386","Jen-Tse Huang","Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren,
  Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu","Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using
  PsychoBench","15 pages","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have recently showcased their remarkable
capacities, not only in natural language processing tasks but also across
diverse domains such as clinical medicine, legal consultation, and education.
LLMs become more than mere applications, evolving into assistants capable of
addressing diverse user requests. This narrows the distinction between human
beings and artificial intelligence agents, raising intriguing questions
regarding the potential manifestation of personalities, temperaments, and
emotions within LLMs. In this paper, we propose a framework, PsychoBench, for
evaluating diverse psychological aspects of LLMs. Comprising thirteen scales
commonly used in clinical psychology, PsychoBench further classifies these
scales into four distinct categories: personality traits, interpersonal
relationships, motivational tests, and emotional abilities. Our study examines
five popular models, namely \texttt{text-davinci-003}, ChatGPT, GPT-4,
LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to
bypass the safety alignment protocols and test the intrinsic natures of LLMs.
We have made PsychoBench openly accessible via
\url{https://github.com/CUHK-ARISE/PsychoBench}.
","2023-10-03","2310.01386v1.pdf"
"2310.01387","Amanda Bertsch","Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R. Gormley","It's MBR All the Way Down: Modern Generation Techniques Through the Lens
  of Minimum Bayes Risk","Under submission","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a
machine learning system based not on the output with the highest probability,
but the output with the lowest risk (expected error) among multiple candidates.
It is a simple but powerful method: for an additional cost at inference time,
MBR provides reliable several-point improvements across metrics for a wide
variety of tasks without any additional data or training. Despite this, MBR is
not frequently applied in NLP works, and knowledge of the method itself is
limited. We first provide an introduction to the method and the recent
literature. We show that several recent methods that do not reference MBR can
be written as special cases of MBR; this reformulation provides additional
theoretical justification for the performance of these methods, explaining some
results that were previously only empirical. We provide theoretical and
empirical results about the effectiveness of various MBR variants and make
concrete recommendations for the application of MBR in NLP models, including
future directions in this area.
","2023-10-03","2310.01387v1.pdf"
"2310.01402","Rohan Alexander","Rohan Alexander, Lindsay Katz, Callandra Moore, Zane Schwartz","Evaluating the Decency and Consistency of Data Validation Tests
  Generated by LLMs","31 pages, 16 figures","","","","stat.ME","http://creativecommons.org/licenses/by/4.0/","  We investigated the potential of large language models (LLMs) in developing
dataset validation tests. We carried out 96 experiments each for both GPT-3.5
and GPT-4, examining different prompt scenarios, learning modes, temperature
settings, and roles. The prompt scenarios were: 1) Asking for expectations, 2)
Asking for expectations with a given context, 3) Asking for expectations after
requesting a simulation, and 4) Asking for expectations with a provided data
sample. For learning modes, we tested: 1) zero-shot, 2) one-shot, and 3)
few-shot learning. We also tested four temperature settings: 0, 0.4, 0.6, and
1. Furthermore, two distinct roles were considered: 1) ""helpful assistant"", 2)
""expert data scientist"". To gauge consistency, every setup was tested five
times. The LLM-generated responses were benchmarked against a gold standard
suite, created by an experienced data scientist knowledgeable about the data in
question. We find there are considerable returns to the use of few-shot
learning, and that the more explicit the data setting can be the better. The
best LLM configurations complement, rather than substitute, the gold standard
results. This study underscores the value LLMs can bring to the data cleaning
and preparation stages of the data science workflow.
","2023-10-03","2310.01402v1.pdf"
"2310.01405","Andy Zou","Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
  Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
  Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven
  Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan
  Hendrycks","Representation Engineering: A Top-Down Approach to AI Transparency","Code is available at
  https://github.com/andyzoujm/representation-engineering","","","","cs.LG cs.AI cs.CL cs.CV cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
","2023-10-11","2310.01405v1.pdf"
"2310.01412","Zhenhua Xu","Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K.
  Wong, Zhenguo Li, Hengshuang Zhao","DriveGPT4: Interpretable End-to-end Autonomous Driving via Large
  Language Model","The project page is available at
  https://tonyxuqaq.github.io/projects/DriveGPT4/","","","","cs.CV cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the past decade, autonomous driving has experienced rapid development in
both academia and industry. However, its limited interpretability remains a
significant unsolved problem, severely hindering autonomous vehicle
commercialization and further development. Previous approaches utilizing small
language models have failed to address this issue due to their lack of
flexibility, generalization ability, and robustness. Recently, multimodal large
language models (LLMs) have gained considerable attention from the research
community for their capability to process and reason non-text data (e.g.,
images and videos) by text. In this paper, we present DriveGPT4, an
interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is
capable of interpreting vehicle actions and providing corresponding reasoning,
as well as answering diverse questions posed by human users for enhanced
interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals
in an end-to-end fashion. These capabilities stem from a customized visual
instruction tuning dataset specifically designed for autonomous driving. To the
best of our knowledge, DriveGPT4 is the first work focusing on interpretable
end-to-end autonomous driving. When evaluated on multiple tasks alongside
conventional methods and video understanding LLMs, DriveGPT4 demonstrates
superior qualitative and quantitative performance. Additionally, DriveGPT4 can
be generalized in a zero-shot fashion to accommodate more unseen scenarios. The
project page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .
","2023-10-10","2310.01412v1.pdf"
"2310.01415","Jiageng Mao","Jiageng Mao, Yuxi Qian, Hang Zhao, Yue Wang","GPT-Driver: Learning to Drive with GPT","Project Page:
  https://pointscoder.github.io/projects/gpt_driver/index.html","","","","cs.CV cs.AI cs.CL cs.RO","http://creativecommons.org/licenses/by/4.0/","  We present a simple yet effective approach that can transform the OpenAI
GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion
planning is a core challenge in autonomous driving, aiming to plan a driving
trajectory that is safe and comfortable. Existing motion planners predominantly
leverage heuristic methods to forecast driving trajectories, yet these
approaches demonstrate insufficient generalization capabilities in the face of
novel and unseen driving scenarios. In this paper, we propose a novel approach
to motion planning that capitalizes on the strong reasoning capabilities and
generalization potential inherent to Large Language Models (LLMs). The
fundamental insight of our approach is the reformulation of motion planning as
a language modeling problem, a perspective not previously explored.
Specifically, we represent the planner inputs and outputs as language tokens,
and leverage the LLM to generate driving trajectories through a language
description of coordinate positions. Furthermore, we propose a novel
prompting-reasoning-finetuning strategy to stimulate the numerical reasoning
potential of the LLM. With this strategy, the LLM can describe highly precise
trajectory coordinates and also its internal decision-making process in natural
language. We evaluate our approach on the large-scale nuScenes dataset, and
extensive experiments substantiate the effectiveness, generalization ability,
and interpretability of our GPT-based motion planner. Code is now available at
https://github.com/PointsCoder/GPT-Driver.
","2023-10-18","2310.01415v1.pdf"
"2310.01420","Robin Schmucker","Robin Schmucker, Meng Xia, Amos Azaria, Tom Mitchell","Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring
  Systems","","","","","cs.CL cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Conversational tutoring systems (CTSs) offer learning experiences driven by
natural language interaction. They are known to promote high levels of
cognitive engagement and benefit learning outcomes, particularly in reasoning
tasks. Nonetheless, the time and cost required to author CTS content is a major
obstacle to widespread adoption. In this paper, we introduce a novel type of
CTS that leverages the recent advances in large language models (LLMs) in two
ways: First, the system induces a tutoring script automatically from a lesson
text. Second, the system automates the script orchestration via two LLM-based
agents (Ruffle&Riley) with the roles of a student and a professor in a
learning-by-teaching format. The system allows a free-form conversation that
follows the ITS-typical outer-/inner-loop structure. In an initial
between-subject online user study (N = 100) comparing Ruffle&Riley to simpler
QA chatbots and reading activity, we found no significant differences in
post-test scores. Nonetheless, in the learning experience survey, Ruffle&Riley
users expressed higher ratings of understanding and remembering and further
perceived the offered support as more helpful and the conversation as coherent.
Our study provides insights for a new generation of scalable CTS technologies.
","2023-10-04","2310.01420v1.pdf"
"2310.01423","Arslan Akram Doula","Arslan Akram","An Empirical Study of AI Generated Text Detection Tools","15 Pages, 4 Figures, 2 Tables, 42 References","2023","10.33140/AMLAI","42","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Since ChatGPT has emerged as a major AIGC model, providing high-quality
responses across a wide range of applications (including software development
and maintenance), it has attracted much interest from many individuals. ChatGPT
has great promise, but there are serious problems that might arise from its
misuse, especially in the realms of education and public safety. Several AIGC
detectors are available, and they have all been tested on genuine text.
However, more study is needed to see how effective they are for multi-domain
ChatGPT material. This study aims to fill this need by creating a multi-domain
dataset for testing the state-of-the-art APIs and tools for detecting
artificially generated information used by universities and other research
institutions. A large dataset consisting of articles, abstracts, stories, news,
and product reviews was created for this study. The second step is to use the
newly created dataset to put six tools through their paces. Six different
artificial intelligence (AI) text identification systems, including ""GPTkit,""
""GPTZero,"" ""Originality,"" ""Sapling,"" ""Writer,"" and ""Zylalab,"" have accuracy
rates between 55.29 and 97.0%. Although all the tools fared well in the
evaluations, originality was particularly effective across the board.
","2023-10-24","2310.01423v1.pdf"
"2310.01424","Victoria Smith","Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller","Identifying and Mitigating Privacy Risks Stemming from Language Models:
  A Survey","13 pages","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Rapid advancements in language models (LMs) have led to their adoption across
many sectors. Alongside the potential benefits, such models present a range of
risks, including around privacy. In particular, as LMs have grown in size, the
potential to memorise aspects of their training data has increased, resulting
in the risk of leaking private information. As LMs become increasingly
widespread, it is vital that we understand such privacy risks and how they
might be mitigated. To help researchers and policymakers understand the state
of knowledge around privacy attacks and mitigations, including where more work
is needed, we present the first technical survey on LM privacy. We (i) identify
a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey
existing attacks and use our taxonomy of dimensions to highlight key trends,
(iii) discuss existing mitigation strategies, highlighting their strengths and
limitations, identifying key gaps and demonstrating open problems and areas for
concern.
","2023-10-04","2310.01424v1.pdf"
"2310.01425","L\'eon Bottou","L\'eon Bottou and Bernhard Sch\""olkopf","Borges and AI","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence.
","2023-10-06","2310.01425v1.pdf"
"2310.01427","Alexander Peysakhovich","Alexander Peysakhovich, Adam Lerer","Attention Sorting Combats Recency Bias In Long Context Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Current language models often fail to incorporate long contexts efficiently
during generation. We show that a major contributor to this issue are attention
priors that are likely learned during pre-training: relevant information
located earlier in context is attended to less on average. Yet even when models
fail to use the information from a relevant document in their response, they
still pay preferential attention to that document compared to an irrelevant
document at the same position. We leverage this fact to introduce ``attention
sorting'': perform one step of decoding, sort documents by the attention they
receive (highest attention going last), repeat the process, generate the answer
with the newly sorted context. We find that attention sorting improves
performance of long context models. Our findings highlight some challenges in
using off-the-shelf language models for retrieval augmented generation.
","2023-10-04","2310.01427v1.pdf"
"2310.01429","Eren Unlu Ph. D.","Eren Unlu","Chatmap : Large Language Model Interaction with Cartographic Data","9 pages, 4 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.
","2023-10-04","2310.01429v1.pdf"
"2310.01432","Zongjie Li","Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang,
  Cuiyun Gao, Yang Liu","Split and Merge: Aligning Position Biases in Large Language Model based
  Evaluators","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have shown promise as automated evaluators for
assessing the quality of answers generated by AI systems. However, these
LLM-based evaluators exhibit position bias, or inconsistency, when used to
evaluate candidate answers in pairwise comparisons, favoring either the first
or second answer regardless of content. To address this limitation, we propose
PORTIA, an alignment-based system designed to mimic human comparison strategies
to calibrate position bias in a lightweight yet effective manner. Specifically,
PORTIA splits the answers into multiple segments, aligns similar content across
candidate answers, and then merges them back into a single prompt for
evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to
evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances
the consistency rates for all the models and comparison forms tested, achieving
an average relative improvement of 47.46%. Remarkably, PORTIA enables less
advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4
model at just 10% of the cost. Furthermore, it rectifies around 80% of the
position bias instances within the GPT-4 model, elevating its consistency rate
up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced
GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with
human evaluators. These findings highlight PORTIA's ability to correct position
bias, improve LLM consistency, and boost performance while keeping
cost-efficiency. This represents a valuable step toward a more reliable and
scalable use of LLMs for automated evaluations across diverse applications.
","2023-10-10","2310.01432v1.pdf"
"2310.01434","Tom\'as Marques","Samuel Carreira, Tom\'as Marques, Jos\'e Ribeiro, Carlos Grilo","Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT
  LLM on Mobile","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The field of Artificial Intelligence has witnessed remarkable progress in
recent years, especially with the emergence of powerful large language models
(LLMs) based on the transformer architecture. Cloud-based LLMs, such as
OpenAI's ChatGPT, offer impressive capabilities but come with concerns
regarding latency and privacy due to network dependencies. This article
presents an innovative approach to LLM inference, envisioning a future where
LLMs with billions of parameters can be executed directly on mobile devices
without network connectivity. The article showcases a fine-tuned GPT LLM with 3
billion parameters that can operate smoothly on devices with as low as 4GB of
memory. Through the integration of native code and model quantization
techniques, the application not only serves as a general-purpose assistant but
also facilitates seamless mobile interactions with text-to-actions features.
The article provides insights into the training pipeline, implementation
details, test results, and future directions of on-device LLM inference. This
breakthrough technology opens up possibilities for empowering users with
sophisticated AI capabilities while preserving their privacy and eliminating
latency concerns.
","2023-10-04","2310.01434v1.pdf"
"2310.01436","Yang Gao","Haishuai Wang, Yang Gao, Xin Zheng, Peng Zhang, Hongyang Chen, Jiajun
  Bu","Graph Neural Architecture Search with GPT-4","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Graph Neural Architecture Search (GNAS) has shown promising results in
automatically designing graph neural networks. However, GNAS still requires
intensive human labor with rich domain knowledge to design the search space and
search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new
GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The
basic idea of our method is to design a new class of prompts for GPT-4 to guide
GPT-4 toward the generative task of graph neural architectures. The prompts
consist of descriptions of the search space, search strategy, and search
feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS
generates more accurate graph neural networks with fast convergence.
Experimental results show that embedding GPT-4 into GNAS outperforms the
state-of-the-art GNAS methods.
","2023-10-04","2310.01436v1.pdf"
"2310.01441","Hejia Geng","Hejia Geng, Boxun Xu, Peng Li","UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large
  Language Model Capabilities","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated impressive inferential
capabilities, with numerous research endeavors devoted to enhancing this
capacity through prompting. Despite these efforts, a unified epistemological
foundation is still conspicuously absent. Drawing inspiration from Kant's a
priori philosophy, we propose the UPAR prompting framework, designed to emulate
the structure of human cognition within LLMs. The UPAR framework is delineated
into four phases: ""Understand"", ""Plan"", ""Act"", and ""Reflect"", enabling the
extraction of structured information from complex contexts, prior planning of
solutions, execution according to plan, and self-reflection. This structure
significantly augments the explainability and accuracy of LLM inference,
producing a human-understandable and inspectable inferential trajectory.
Furthermore, our work offers an epistemological foundation for existing
prompting techniques, allowing for a possible systematic integration of these
methods. With GPT-4, our approach elevates the accuracy from COT baseline of
22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in
the causal judgment task.
","2023-10-04","2310.01441v1.pdf"
"2310.01444","Kuan Wang","Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang,
  Yelong Shen","Adapting LLM Agents Through Communication","Preprint","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recent advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Recent
advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Through
iterative exploration and PPO training, LTC empowers the agent to assimilate
short-term experiences into long-term memory. To optimize agent interactions
for task-specific learning, we introduce three structured communication
patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as
decision-making, knowledge-intensive reasoning, and numerical reasoning. We
evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA
(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,
it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,
LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it
outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,
LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results
showcase the versatility and efficiency of the LTC approach across diverse
domains. We will open-source our code to promote further development of the
community.
","2023-10-11","2310.01444v1.pdf"
"2310.01446","Jianpeng Zhou","Jianpeng Zhou, Wanjun Zhong, Yanlin Wang, Jiahai Wang","Adaptive-Solver Framework for Dynamic Strategy Selection in Large
  Language Model Reasoning","10 pages","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) are showcasing impressive ability in handling
complex reasoning tasks. In real-world situations, problems often span a
spectrum of complexities. Humans inherently adjust their problem-solving
approaches based on task complexity. However, most methodologies that leverage
LLMs tend to adopt a uniform approach: utilizing consistent models, prompting
methods, and degrees of problem decomposition, regardless of the problem
complexity. Inflexibility of them can bring unnecessary computational overhead
or sub-optimal performance. To address this problem, we introduce an
Adaptive-Solver framework. It strategically modulates solving strategies based
on the difficulties of the problems. Given an initial solution, the framework
functions with two primary modules. The initial evaluation module assesses the
adequacy of the current solution. If improvements are needed, the subsequent
adaptation module comes into play. Within this module, three key adaptation
strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when
a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating
between different prompting techniques to suit the problem's nuances. (3)
Decomposition Granularity Adaptation: Breaking down a complex problem into more
fine-grained sub-questions to enhance solvability. Through such dynamic
adaptations, our framework not only enhances computational efficiency but also
elevates the overall performance. This dual-benefit ensures both the efficiency
of the system for simpler tasks and the precision required for more complex
questions. Experimental results from complex reasoning tasks reveal that the
prompting method adaptation and decomposition granularity adaptation enhance
performance across all tasks. Furthermore, the model adaptation approach
significantly reduces API costs (up to 50%) while maintaining superior
performance.
","2023-10-04","2310.01446v1.pdf"
"2310.01448","Jindong Wang","Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, Xing Xie","Meta Semantic Template for Evaluation of Large Language Models","Work in progress; 7 pages; more work at: https://llm-eval.github.io/","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Do large language models (LLMs) genuinely understand the semantics of the
language, or just memorize the training data? The recent concern on potential
data contamination of LLMs has raised awareness of the community to conduct
research on LLMs evaluation. In this paper, we propose MSTemp, an approach that
creates meta semantic templates to evaluate the semantic understanding ability
of LLMs. The core of MSTemp is not to perform evaluation directly on existing
benchmark datasets, but to generate new out-of-distribution (OOD) evaluation
sets using existing datasets as seeds. Specifically, for a given sentence,
MSTemp leverages another language model to generate new samples while
preserving its semantics. The new samples are called semantic templates to the
original sentence. Then, MSTemp generates evaluation samples via sentence
parsing and random word replacement on the semantic templates. MSTemp is highly
flexible, dynamic, and cost-effective. Our initial experiments show that
MSTemp-generated samples can significantly reduce the performance of LLMs using
existing datasets as seeds. We hope this initial work can shed light on future
research of LLMs evaluation.
","2023-10-20","2310.01448v1.pdf"
"2310.01459","Runcong Zhao","Runcong Zhao and Wenjia Zhang and Jiazheng Li and Lixing Zhu and
  Yanran Li and Yulan He and Lin Gui","NarrativePlay: Interactive Narrative Understanding","","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  In this paper, we introduce NarrativePlay, a novel system that allows users
to role-play a fictional character and interact with other characters in
narratives such as novels in an immersive environment. We leverage Large
Language Models (LLMs) to generate human-like responses, guided by personality
traits extracted from narratives. The system incorporates auto-generated visual
display of narrative settings, character portraits, and character speech,
greatly enhancing user experience. Our approach eschews predefined sandboxes,
focusing instead on main storyline events extracted from narratives from the
perspective of a user-selected character. NarrativePlay has been evaluated on
two types of narratives, detective and adventure stories, where users can
either explore the world or improve their favorability with the narrative
characters through conversations.
","2023-10-04","2310.01459v1.pdf"
"2310.01467","Jingwei Sun","Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen,
  Holger R. Roth","FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language
  Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pre-trained language models (PLM) have revolutionized the NLP landscape,
achieving stellar performances across diverse tasks. These models, while
benefiting from vast training data, often require fine-tuning on specific data
to cater to distinct downstream tasks. However, this data adaptation process
has inherent security and privacy concerns, primarily when leveraging
user-generated, device-residing data. Federated learning (FL) provides a
solution, allowing collaborative model fine-tuning without centralized data
collection. However, applying FL to finetune PLMs is hampered by challenges,
including restricted model parameter access, high computational requirements,
and communication overheads. This paper introduces Federated Black-box Prompt
Tuning (FedBPT), a framework designed to address these challenges. FedBPT does
not require the clients to access the model parameters. By focusing on training
optimal prompts and utilizing gradient-free optimization methods, FedBPT
reduces the number of exchanged variables, boosts communication efficiency, and
minimizes computational and storage costs. Experiments highlight the
framework's ability to drastically cut communication and memory costs while
maintaining competitive performance. Ultimately, FedBPT presents a promising
solution for efficient, privacy-preserving fine-tuning of PLM in the age of
large language models.
","2023-10-04","2310.01467v1.pdf"
"2310.01468","Yizhe Zhang","Yizhe Zhang, Jiarui Lu, Navdeep Jaitly","The Entity-Deduction Arena: A playground for probing the conversational
  reasoning and planning capabilities of LLMs","22 pages","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are effective at answering questions that are
clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This entity-deducing game can serve as an evaluation
framework to probe the conversational reasoning and planning capabilities of
language models. We systematically evaluate various LLMs and discover
significant differences in their performance on this task. We find that strong
LLMs like GPT-4 outperform human players by a large margin. We further employ
Behavior Cloning (BC) to examine whether a weaker model is capable of imitating
a stronger model and generalizing to data or domains, using only the
demonstrations from a stronger model. We finally propose to use Reinforcement
Learning to enhance reasoning and planning capacity of Vicuna models through
episodes of game playing, which lead to significant performance improvement. We
hope that this problem offers insights into how autonomous agents could be
trained to behave more intelligently in ambiguous circumstances.
","2023-10-05","2310.01468v1.pdf"
"2310.01469","Jiayu Yao","Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan","LLM Lies: Hallucinations are not Bugs, but Features as Adversarial
  Examples","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.
","2023-10-05","2310.01469v1.pdf"
"2310.01506","Xuan Ju","Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, Qiang Xu","Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text-guided diffusion models have revolutionized image generation and
editing, offering exceptional realism and diversity. Specifically, in the
context of diffusion-based editing, where a source image is edited according to
a target prompt, the process commences by acquiring a noisy latent vector
corresponding to the source image via the diffusion model. This vector is
subsequently fed into separate source and target diffusion branches for
editing. The accuracy of this inversion process significantly impacts the final
editing outcome, influencing both essential content preservation of the source
image and edit fidelity according to the target prompt. Prior inversion
techniques aimed at finding a unified solution in both the source and target
diffusion branches. However, our theoretical and empirical analyses reveal that
disentangling these branches leads to a distinct separation of responsibilities
for preserving essential content and ensuring edit fidelity. Building on this
insight, we introduce ""Direct Inversion,"" a novel technique achieving optimal
performance of both branches with just three lines of code. To assess image
editing performance, we present PIE-Bench, an editing benchmark with 700 images
showcasing diverse scenes and editing types, accompanied by versatile
annotations and comprehensive evaluation metrics. Compared to state-of-the-art
optimization-based inversion techniques, our solution not only yields superior
performance across 8 editing methods but also achieves nearly an order of
speed-up.
","2023-10-20","2310.01506v1.pdf"
"2310.01542","Hongyi Wang","Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing,
  Mikhail Yurochkin","Fusing Models with Complementary Expertise","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Training AI models that generalize across tasks and domains has long been
among the open problems driving AI research. The emergence of Foundation Models
made it easier to obtain expert models for a given task, but the heterogeneity
of data that may be encountered at test time often means that any single expert
is insufficient. We consider the Fusion of Experts (FoE) problem of fusing
outputs of expert models with complementary knowledge of the data distribution
and formulate it as an instance of supervised learning. Our method is
applicable to both discriminative and generative tasks and leads to significant
performance improvements in image and text classification, text summarization,
multiple-choice QA, and automatic evaluation of generated text. We also extend
our method to the ""frugal"" setting where it is desired to reduce the number of
expert model evaluations at test time.
","2023-10-04","2310.01542v1.pdf"
"2310.01557","Yue Wu","Yue Wu, Xuan Tang, Tom M. Mitchell, Yuanzhi Li","SmartPlay : A Benchmark for LLMs as Intelligent Agents","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs' abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/microsoft/SmartPlay
","2023-10-05","2310.01557v1.pdf"
"2310.01558","Ori Yoran","Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant","Making Retrieval-Augmented Language Models Robust to Irrelevant Context","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Retrieval-augmented language models (RALMs) hold promise to produce language
understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model
performance when it is relevant, and does not harm performance when it is not.
This is particularly important in multi-hop reasoning scenarios, where misuse
of irrelevant evidence can lead to cascading errors. However, recent work has
shown that retrieval augmentation can sometimes have a negative effect on
performance. In this work, we present a thorough analysis on five open-domain
question answering benchmarks, characterizing cases when retrieval reduces
accuracy. We then propose two methods to mitigate this issue. First, a simple
baseline that filters out retrieved passages that do not entail question-answer
pairs according to a natural language inference (NLI) model. This is effective
in preventing performance reduction, but at a cost of also discarding relevant
passages. Thus, we propose a method for automatically generating data to
fine-tune the language model to properly leverage retrieved passages, using a
mix of relevant and irrelevant contexts at training time. We empirically show
that even 1,000 examples suffice to train the model to be robust to irrelevant
contexts while maintaining high performance on examples with relevant ones.
","2023-10-04","2310.01558v1.pdf"
"2310.01568","Haining Wang","Haining Wang","Defending Against Authorship Identification Attacks","","","","","cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Authorship identification has proven unsettlingly effective in inferring the
identity of the author of an unsigned document, even when sensitive personal
information has been carefully omitted. In the digital era, individuals leave a
lasting digital footprint through their written content, whether it is posted
on social media, stored on their employer's computers, or located elsewhere.
When individuals need to communicate publicly yet wish to remain anonymous,
there is little available to protect them from unwanted authorship
identification. This unprecedented threat to privacy is evident in scenarios
such as whistle-blowing. Proposed defenses against authorship identification
attacks primarily aim to obfuscate one's writing style, thereby making it
unlinkable to their pre-existing writing, while concurrently preserving the
original meaning and grammatical integrity. The presented work offers a
comprehensive review of the advancements in this research area spanning over
the past two decades and beyond. It emphasizes the methodological frameworks of
modification and generation-based strategies devised to evade authorship
identification attacks, highlighting joint efforts from the differential
privacy community. Limitations of current research are discussed, with a
spotlight on open challenges and potential research avenues.
","2023-10-04","2310.01568v1.pdf"
"2310.01581","Hangfan Zhang","Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin,
  Jinyuan Jia, Jinghui Chen, Dinghao Wu","On the Safety of Open-Sourced Large Language Models: Does Alignment
  Really Prevent Them From Being Misused?","","","","","cs.LG cs.AI cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have achieved unprecedented performance in
Natural Language Generation (NLG) tasks. However, many existing studies have
shown that they could be misused to generate undesired content. In response,
before releasing LLMs for public access, model developers usually align those
language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning
with Human Feedback (RLHF). Consequently, those aligned large language models
refuse to generate undesired content when facing potentially harmful/unethical
requests. A natural question is ""could alignment really prevent those
open-sourced large language models from being misused to generate undesired
content?''. In this work, we provide a negative answer to this question. In
particular, we show those open-sourced, aligned large language models could be
easily misguided to generate undesired content without heavy computations or
careful prompt designs. Our key idea is to directly manipulate the generation
process of open-sourced LLMs to misguide it to generate undesired content
including harmful or biased information and even private data. We evaluate our
method on 4 open-sourced LLMs accessible publicly and our finding highlights
the need for more advanced mitigation strategies for open-sourced LLMs.
","2023-10-04","2310.01581v1.pdf"
"2310.01596","Wing-Fung Ku","Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang,
  Wenhu Chen","ImagenHub: Standardizing the evaluation of conditional image generation
  models","","","","","cs.CV cs.GR cs.MM","http://creativecommons.org/licenses/by/4.0/","  Recently, a myriad of conditional image generation and editing models have
been developed to serve different downstream tasks, including text-to-image
generation, text-guided image editing, subject-driven image generation,
control-guided image generation, etc. However, we observe huge inconsistencies
in experimental conditions: datasets, inference, and evaluation metrics -
render fair comparisons difficult. This paper proposes ImagenHub, which is a
one-stop library to standardize the inference and evaluation of all the
conditional image generation models. Firstly, we define seven prominent tasks
and curate high-quality evaluation datasets for them. Secondly, we built a
unified inference pipeline to ensure fair comparison. Thirdly, we design two
human evaluation scores, i.e. Semantic Consistency and Perceptual Quality,
along with comprehensive guidelines to evaluate generated images. We train
expert raters to evaluate the model outputs based on the proposed metrics. Our
human evaluation achieves a high inter-worker agreement of Krippendorff's alpha
on 76% models with a value higher than 0.4. We comprehensively evaluated a
total of around 30 models and observed three key takeaways: (1) the existing
models' performance is generally unsatisfying except for Text-guided Image
Generation and Subject-driven Image Generation, with 74% models achieving an
overall score lower than 0.5. (2) we examined the claims from published papers
and found 83% of them hold with a few exceptions. (3) None of the existing
automatic metrics has a Spearman's correlation higher than 0.2 except
subject-driven image generation. Moving forward, we will continue our efforts
to evaluate newly published models and update our leaderboard to keep track of
the progress in conditional image generation.
","2023-10-18","2310.01596v1.pdf"
"2310.01602","Nikitha Rao","Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, Vincent J.
  Hellendoorn","CAT-LM: Training Language Models on Aligned Code And Tests","","","","","cs.SE cs.AI","http://creativecommons.org/licenses/by/4.0/","  Testing is an integral part of the software development process. Yet, writing
tests is time-consuming and therefore often neglected. Classical test
generation tools such as EvoSuite generate behavioral test suites by optimizing
for coverage, but tend to produce tests that are hard to understand. Language
models trained on code can generate code that is highly similar to that written
by humans, but current models are trained to generate each file separately, as
is standard practice in natural language processing, and thus fail to consider
the code-under-test context when producing a test file. In this work, we
propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style
language model with 2.7 Billion parameters, trained on a corpus of Python and
Java projects. We utilize a novel pretraining signal that explicitly considers
the mapping between code and test files when available. We also drastically
increase the maximum sequence length of inputs to 8,192 tokens, 4x more than
typical code generation models, to ensure that the code context is available to
the model when generating test code. We analyze its usefulness for realistic
applications, showing that sampling with filtering (e.g., by compilability,
coverage) allows it to efficiently produce tests that achieve coverage similar
to ones written by developers while resembling their writing style. By
utilizing the code context, CAT-LM generates more valid tests than even much
larger language models trained with more data (CodeGen 16B and StarCoder) and
substantially outperforms a recent test-specific model (TeCo) at test
completion. Overall, our work highlights the importance of incorporating
software-specific insights when training language models for code and paves the
way to more powerful automated test generation.
","2023-10-04","2310.01602v1.pdf"
"2310.01603","Xiaoyi Tian","Xiaoyi Tian and Kristy Elizabeth Boyer","A Review of Digital Learning Environments for Teaching Natural Language
  Processing in K-12 Education","24 pages, 13 figures","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Natural Language Processing (NLP) plays a significant role in our daily lives
and has become an essential part of Artificial Intelligence (AI) education in
K-12. As children grow up with NLP-powered applications, it is crucial to
introduce NLP concepts to them, fostering their understanding of language
processing, language generation, and ethical implications of AI and NLP. This
paper presents a comprehensive review of digital learning environments for
teaching NLP in K-12. Specifically, it explores existing digital learning
tools, discusses how they support specific NLP tasks and procedures, and
investigates their explainability and evaluation results in educational
contexts. By examining the strengths and limitations of these tools, this
literature review sheds light on the current state of NLP learning tools in
K-12 education. It aims to guide future research efforts to refine existing
tools, develop new ones, and explore more effective and inclusive strategies
for integrating NLP into K-12 educational contexts.
","2023-10-04","2310.01603v1.pdf"
"2310.01612","Bo Peng","Bo Peng, Ben Burns, Ziqi Chen, Srinivasan Parthasarathy, and Xia Ning","Towards Efficient and Effective Adaptation of Large Language Models for
  Sequential Recommendation","","","","","cs.IR","http://creativecommons.org/licenses/by/4.0/","  In recent years, with large language models (LLMs) achieving state-of-the-art
performance in context understanding, increasing efforts have been dedicated to
developing LLM-enhanced sequential recommendation (SR) methods. Considering
that most existing LLMs are not specifically optimized for recommendation
tasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods.
Though numerous adaptation methods have been developed, it still remains a
significant challenge to adapt LLMs for SR both efficiently and effectively. To
address this challenge, in this paper, we introduce a novel side sequential
network adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features
three key designs to allow both efficient and effective LLM adaptation. First,
SSNA learns adapters separate from LLMs, while fixing all the pre-trained
parameters within LLMs to allow efficient adaptation. In addition, SSNA adapts
the top-a layers of LLMs jointly, and integrates adapters sequentially for
enhanced effectiveness (i.e., recommendation performance). We compare SSNA
against five state-of-the-art baseline methods on five benchmark datasets using
three LLMs. The experimental results demonstrate that SSNA significantly
outperforms all the baseline methods in terms of recommendation performance,
and achieves substantial improvement over the best-performing baseline methods
at both run-time and memory efficiency during training. Our analysis shows the
effectiveness of integrating adapters in a sequential manner. Our parameter
study demonstrates the effectiveness of jointly adapting the top-a layers of
LLMs.
","2023-10-04","2310.01612v1.pdf"
"2310.01651","Yongshuo Zong","Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy
  Hospedales","Fool Your (Vision and) Language Model With Embarrassingly Simple
  Permutations","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language and vision-language models are rapidly being deployed in
practice thanks to their impressive capabilities in instruction following,
in-context learning, and so on. This raises an urgent need to carefully analyse
their robustness so that stakeholders can understand if and when such models
are trustworthy enough to be relied upon in any given application. In this
paper, we highlight a specific vulnerability in popular models, namely
permutation sensitivity in multiple-choice question answering (MCQA).
Specifically, we show empirically that popular models are vulnerable to
adversarial permutation in answer sets for multiple-choice prompting, which is
surprising as models should ideally be as invariant to prompt permutation as
humans are. These vulnerabilities persist across various model sizes, and exist
in very recent language and vision-language models. Code is available at
\url{https://github.com/ys-zong/FoolyourVLLMs}.
","2023-10-04","2310.01651v1.pdf"
"2310.01685","Alan Wang","Alan Q. Wang, Batuhan K. Karaman, Heejong Kim, Jacob Rosenthal, Rachit
  Saluja, Sean I. Young, Mert R. Sabuncu","A Framework for Interpretability in Machine Learning for Medical Imaging","","","","","cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  Interpretability for machine learning models in medical imaging (MLMI) is an
important direction of research. However, there is a general sense of murkiness
in what interpretability means. Why does the need for interpretability in MLMI
arise? What goals does one actually seek to address when interpretability is
needed? To answer these questions, we identify a need to formalize the goals
and elements of interpretability in MLMI. By reasoning about real-world tasks
and goals common in both medical image analysis and its intersection with
machine learning, we identify four core elements of interpretability:
localization, visual recognizability, physical attribution, and transparency.
Overall, this paper formalizes interpretability needs in the context of medical
imaging, and our applied perspective clarifies concrete MLMI-specific goals and
considerations in order to guide method design and improve real-world usage.
Our goal is to provide practical and didactic information for model designers
and practitioners, inspire developers of models in the medical imaging field to
reason more deeply about what interpretability is achieving, and suggest future
directions of interpretability research.
","2023-10-04","2310.01685v1.pdf"
"2310.01691","Zijun Wu","Zijun Wu, Yongkang Wu, Lili Mou","Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across
  Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  Prompt tuning in natural language processing (NLP) has become an increasingly
popular method for adapting large language models to specific tasks. However,
the transferability of these prompts, especially continuous prompts, between
different models remains a challenge. In this work, we propose a zero-shot
continuous prompt transfer method, where source prompts are encoded into
relative space and the corresponding target prompts are searched for
transferring to target models. Experimental results confirm the effectiveness
of our method, showing that 'task semantics' in continuous prompts can be
generalized across various language models. Moreover, we find that combining
'task semantics' from multiple source models can further enhance the
generalizability of transfer.
","2023-10-04","2310.01691v1.pdf"
"2310.01693","Matthew Finlayson","Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta,
  Ashish Sabharwal","Closing the Curious Case of Neural Text Degeneration","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Despite their ubiquity in language generation, it remains unknown why
truncation sampling heuristics like nucleus sampling are so effective. We
provide a theoretical explanation for the effectiveness of the truncation
sampling by proving that truncation methods that discard tokens below some
probability threshold (the most common type of truncation) can guarantee that
all sampled tokens have nonzero true probability. However, thresholds are a
coarse heuristic, and necessarily discard some tokens with nonzero true
probability as well. In pursuit of a more precise sampling strategy, we show
that we can leverage a known source of model errors, the softmax bottleneck, to
prove that certain tokens have nonzero true probability, without relying on a
threshold. Based on our findings, we develop an experimental truncation
strategy and the present pilot studies demonstrating the promise of this type
of algorithm. Our evaluations show that our method outperforms its
threshold-based counterparts under automatic and human evaluation metrics for
low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical
findings and pilot experiments provide both insight into why truncation
sampling works, and make progress toward more expressive sampling algorithms
that better surface the generative capabilities of large language models.
","2023-10-04","2310.01693v1.pdf"
"2310.01708","Aleksandr Nesterov","D.Umerenkov, G.Zubkova, A.Nesterov","Deciphering Diagnoses: How Large Language Models Explanations Influence
  Clinical Decision Making","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and
patient data to offer real-time recommendations, with Large Language Models
(LLMs) emerging as a promising tool to generate plain-text explanations for
medical decisions. This study explores the effectiveness and reliability of
LLMs in generating explanations for diagnoses based on patient complaints.
Three experienced doctors evaluated LLM-generated explanations of the
connection between patient complaints and doctor and model-assigned diagnoses
across several stages. Experimental results demonstrated that LLM explanations
significantly increased doctors' agreement rates with given diagnoses and
highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study
underscores the potential and challenges of LLMs in healthcare and emphasizes
the need for careful integration and evaluation to ensure patient safety and
optimal clinical utility.
","2023-10-04","2310.01708v1.pdf"
"2310.01714","Michihiro Yasunaga","Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure
  Leskovec, Percy Liang, Ed H. Chi, Denny Zhou","Large Language Models as Analogical Reasoners","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Chain-of-thought (CoT) prompting for language models demonstrates impressive
performance across reasoning tasks, but typically needs labeled exemplars of
the reasoning process. In this work, we introduce a new prompting approach,
Analogical Prompting, designed to automatically guide the reasoning process of
large language models. Inspired by analogical reasoning, a cognitive process in
which humans draw from relevant past experiences to tackle new problems, our
approach prompts language models to self-generate relevant exemplars or
knowledge in the context, before proceeding to solve the given problem. This
method presents several advantages: it obviates the need for labeling or
retrieving exemplars, offering generality and convenience; it can also tailor
the generated exemplars and knowledge to each problem, offering adaptability.
Experimental results show that our approach outperforms 0-shot CoT and manual
few-shot CoT in a variety of reasoning tasks, including math problem solving in
GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in
BIG-Bench.
","2023-10-10","2310.01714v1.pdf"
"2310.01728","Ming Jin","Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming
  Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen","Time-LLM: Time Series Forecasting by Reprogramming Large Language Models","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Time series forecasting holds significant importance in many real-world
dynamic systems and has been extensively studied. Unlike natural language
process (NLP) and computer vision (CV), where a single large model can tackle
multiple tasks, models for time series forecasting are often specialized,
necessitating distinct designs for different tasks and applications. While
pre-trained foundation models have made impressive strides in NLP and CV, their
development in time series domains has been constrained by data sparsity.
Recent studies have revealed that large language models (LLMs) possess robust
pattern recognition and reasoning abilities over complex sequences of tokens.
However, the challenge remains in effectively aligning the modalities of time
series data and natural language to leverage these capabilities. In this work,
we present Time-LLM, a reprogramming framework to repurpose LLMs for general
time series forecasting with the backbone language models kept intact. We begin
by reprogramming the input time series with text prototypes before feeding it
into the frozen LLM to align the two modalities. To augment the LLM's ability
to reason with time series data, we propose Prompt-as-Prefix (PaP), which
enriches the input context and directs the transformation of reprogrammed input
patches. The transformed time series patches from the LLM are finally projected
to obtain the forecasts. Our comprehensive evaluations demonstrate that
Time-LLM is a powerful time series learner that outperforms state-of-the-art,
specialized forecasting models. Moreover, Time-LLM excels in both few-shot and
zero-shot learning scenarios.
","2023-10-04","2310.01728v1.pdf"
"2310.01732","Guanghui Qin","Guanghui Qin, Benjamin Van Durme","Nugget: Neural Agglomerative Embeddings of Text","Appeared at ICML 2023","ICML 2023","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Embedding text sequences is a widespread requirement in modern language
understanding. Existing approaches focus largely on constant-size
representations. This is problematic, as the amount of information contained in
text often varies with the length of the input. We propose a solution called
Nugget, which encodes language into a representation based on a dynamically
selected subset of input tokens. These nuggets are learned through tasks like
autoencoding and machine translation, and intuitively segment language into
meaningful units. We demonstrate Nugget outperforms related approaches in tasks
involving semantic comparison. Finally, we illustrate these compact units allow
for expanding the contextual window of a language model (LM), suggesting new
future LMs that can condition on significantly larger amounts of content.
","2023-10-04","2310.01732v1.pdf"
"2310.01765","Pierre-Olivier C\^ot\'e","Pierre-Olivier C\^ot\'e, Amin Nikanjam, Nafisa Ahmed, Dmytro Humeniuk,
  Foutse Khomh","Data Cleaning and Machine Learning: A Systematic Literature Review","Submitted to Automated Software Engineering Journal","","","","cs.LG cs.DB","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Context: Machine Learning (ML) is integrated into a growing number of systems
for various applications. Because the performance of an ML model is highly
dependent on the quality of the data it has been trained on, there is a growing
interest in approaches to detect and repair data errors (i.e., data cleaning).
Researchers are also exploring how ML can be used for data cleaning; hence
creating a dual relationship between ML and data cleaning. To the best of our
knowledge, there is no study that comprehensively reviews this relationship.
Objective: This paper's objectives are twofold. First, it aims to summarize the
latest approaches for data cleaning for ML and ML for data cleaning. Second, it
provides future work recommendations. Method: We conduct a systematic
literature review of the papers published between 2016 and 2022 inclusively. We
identify different types of data cleaning activities with and for ML: feature
cleaning, label cleaning, entity matching, outlier detection, imputation, and
holistic data cleaning. Results: We summarize the content of 101 papers
covering various data cleaning activities and provide 24 future work
recommendations. Our review highlights many promising data cleaning techniques
that can be further extended. Conclusion: We believe that our review of the
literature will help the community develop better approaches to clean data.
","2023-10-04","2310.01765v1.pdf"
"2310.01779","Shijia Yang","Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen,
  Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, Xiangjun Fan","HallE-Switch: Rethinking and Controlling Object Existence Hallucinations
  in Large Vision Language Models for Detailed Caption","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Current large vision-language models (LVLMs) achieve remarkable progress, yet
there remains significant uncertainty regarding their ability to accurately
apprehend visual details, that is, in performing detailed captioning. To
address this, we introduce \textit{CCEval}, a GPT-4 assisted evaluation method
tailored for detailed captioning. Interestingly, while LVLMs demonstrate
minimal object existence hallucination in existing VQA benchmarks, our proposed
evaluation reveals continued susceptibility to such hallucinations. In this
paper, we make the first attempt to investigate and attribute such
hallucinations, including image resolution, the language decoder size, and
instruction data amount, quality, granularity. Our findings underscore the
unwarranted inference when the language description includes details at a finer
object granularity than what the vision module can ground or verify, thus
inducing hallucination. To control such hallucinations, we further attribute
the reliability of captioning to contextual knowledge (involving only
contextually grounded objects) and parametric knowledge (containing inferred
objects by the model). Thus, we introduce $\textit{HallE-Switch}$, a
controllable LVLM in terms of $\textbf{Hall}$ucination in object
$\textbf{E}$xistence. HallE-Switch can condition the captioning to shift
between (i) exclusively depicting contextual knowledge for grounded objects and
(ii) blending it with parametric knowledge to imagine inferred objects. Our
method reduces hallucination by 44% compared to LLaVA$_{7B}$ and maintains the
same object coverage.
","2023-10-04","2310.01779v1.pdf"
"2310.01783","Weixin Liang","Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding,
  Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel
  McFarland, James Zou","Can large language models provide useful feedback on research papers? A
  large-scale empirical analysis","","","","","cs.LG cs.AI cs.CL cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Expert feedback lays the foundation of rigorous research. However, the rapid
growth of scholarly production and intricate knowledge specialization challenge
the conventional scientific feedback mechanisms. High-quality peer reviews are
increasingly difficult to obtain. Researchers who are more junior or from
under-resourced settings have especially hard times getting timely feedback.
With the breakthrough of large language models (LLM) such as GPT-4, there is
growing interest in using LLMs to generate scientific feedback on research
manuscripts. However, the utility of LLM-generated feedback has not been
systematically studied. To address this gap, we created an automated pipeline
using GPT-4 to provide comments on the full PDFs of scientific papers. We
evaluated the quality of GPT-4's feedback through two large-scale studies. We
first quantitatively compared GPT-4's generated feedback with human peer
reviewer feedback in 15 Nature family journals (3,096 papers in total) and the
ICLR machine learning conference (1,709 papers). The overlap in the points
raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature
journals, 39.23% for ICLR) is comparable to the overlap between two human
reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The
overlap between GPT-4 and human reviewers is larger for the weaker papers. We
then conducted a prospective user study with 308 researchers from 110 US
institutions in the field of AI and computational biology to understand how
researchers perceive feedback generated by our GPT-4 system on their own
papers. Overall, more than half (57.4%) of the users found GPT-4 generated
feedback helpful/very helpful and 82.4% found it more beneficial than feedback
from at least some human reviewers. While our findings show that LLM-generated
feedback can help researchers, we also identify several limitations.
","2023-10-04","2310.01783v1.pdf"
"2310.01796","Zhihan Jiang","Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang,
  Yintong Huo, Pinjia He, Jiazhen Gu and Michael R. Lyu","LLMParser: A LLM-based Log Parsing Framework","","","","","cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The process of log parsing, which converts log messages into structured
formats, is a crucial step for various log analysis tasks. Although numerous
log parsers have been proposed, their effectiveness on complex log data is
often hindered due to reliance on human-made rules or learning-based models
with limited training data. The recent rise of powerful large language models
(LLMs) shows potential for log parsing due to their extensive pre-trained
knowledge related to code and logging. However, their accuracy is currently
limited due to the lack of specialized log parsing capabilities. Additionally,
the inconsistency of their answers and significant overhead obstruct the
practical implementation of LLM-based log parsing.
  To tackle these challenges, we introduce LLMParser, the first practical
LLM-based log parsing framework. LLMParser enables accurate and robust log
parsing by leveraging the in-context learning (ICL) capability of the LLM,
employing a hierarchical candidate sampling algorithm, and selecting
high-quality demonstrations. LLMParser also includes a novel adaptive parsing
cache component to store and refine the templates generated by the LLM. This
design aids in addressing the inefficiency of LLMs by rapid matching to
previously parsed log templates. LLMParser also adaptively updates the
templates in the parsing cache to ensure consistent parsed results. Extensive
evaluation on large-scale public datasets demonstrates that LLMParser surpasses
the state-of-the-art methods. Furthermore, LLMParser significantly reduces the
query times to LLMs, achieving efficiency comparable to the most efficient
baseline, Drain.
","2023-10-04","2310.01796v1.pdf"
"2310.01798","Jie Huang","Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams
  Wei Yu, Xinying Song, Denny Zhou","Large Language Models Cannot Self-Correct Reasoning Yet","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance might even
degrade post self-correction. Drawing from these insights, we offer suggestions
for future research and practical applications in this field.
","2023-10-04","2310.01798v1.pdf"
"2310.01801","Suyu Ge","Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng
  Gao","Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs","Under Review; To be updated","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.
","2023-10-10","2310.01801v1.pdf"
"2310.01818","Xilie Xu","Xilie Xu, Jingfeng Zhang, Mohan Kankanhalli","AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework","","","","","cs.LG cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial
robustness in downstream applications, without requiring a lot of computational
resources and collecting significant amounts of data. This paper uncovers an
issue with the existing RFT, where optimizing both adversarial and natural
objectives through the feature extractor (FE) yields significantly divergent
gradient directions. This divergence introduces instability in the optimization
process, thereby hindering the attainment of adversarial robustness and
rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we
propose a low-rank (LoRa) branch that disentangles RFT into two distinct
components: optimizing natural objectives via the LoRa branch and adversarial
objectives via the FE. Besides, we introduce heuristic strategies for
automating the scheduling of the learning rate and the scalars of loss terms.
Extensive empirical evaluations demonstrate that our proposed automated RFT
disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art
results across a range of downstream tasks. AutoLoRa holds significant
practical utility, as it automatically converts a pre-trained FE into an
adversarially robust model for downstream tasks without the need for searching
hyperparameters.
","2023-10-04","2310.01818v1.pdf"
"2310.01830","Zuhao Yang","Zuhao Yang, Fangneng Zhan, Kunhao Liu, Muyu Xu, Shijian Lu","AI-Generated Images as Data Source: The Dawn of Synthetic Era","20 pages, 11 figures","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advancement of visual intelligence is intrinsically tethered to the
availability of large-scale data. In parallel, generative Artificial
Intelligence (AI) has unlocked the potential to create synthetic images that
closely resemble real-world photographs. This prompts a compelling inquiry: how
much visual intelligence could benefit from the advance of generative AI? This
paper explores the innovative concept of harnessing these AI-generated images
as new data sources, reshaping traditional modeling paradigms in visual
intelligence. In contrast to real data, AI-generated data exhibit remarkable
advantages, including unmatched abundance and scalability, the rapid generation
of vast datasets, and the effortless simulation of edge cases. Built on the
success of generative AI models, we examine the potential of their generated
data in a range of applications, from training machine learning models to
simulating scenarios for computational modeling, testing, and validation. We
probe the technological foundations that support this groundbreaking use of
generative AI, engaging in an in-depth discussion on the ethical, legal, and
practical considerations that accompany this transformative paradigm shift.
Through an exhaustive survey of current technologies and applications, this
paper presents a comprehensive view of the synthetic era in visual
intelligence. A project associated with this paper can be found at
https://github.com/mwxely/AIGS .
","2023-10-24","2310.01830v3.pdf"
"2310.01845","Ali J. Ghandour","Ali Mayladan, Hasan Nasrallah, Hasan Moughnieh, Mustafa Shukor and Ali
  J. Ghandour","Zero-Shot Refinement of Buildings' Segmentation Models using SAM","","","","","cs.CV cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Foundation models have excelled in various tasks but are often evaluated on
general benchmarks. The adaptation of these models for specific domains, such
as remote sensing imagery, remains an underexplored area. In remote sensing,
precise building instance segmentation is vital for applications like urban
planning. While Convolutional Neural Networks (CNNs) perform well, their
generalization can be limited. For this aim, we present a novel approach to
adapt foundation models to address existing models' generalization dropback.
Among several models, our focus centers on the Segment Anything Model (SAM), a
potent foundation model renowned for its prowess in class-agnostic image
segmentation capabilities. We start by identifying the limitations of SAM,
revealing its suboptimal performance when applied to remote sensing imagery.
Moreover, SAM does not offer recognition abilities and thus fails to classify
and tag localized objects. To address these limitations, we introduce different
prompting strategies, including integrating a pre-trained CNN as a prompt
generator. This novel approach augments SAM with recognition abilities, a first
of its kind. We evaluated our method on three remote sensing datasets,
including the WHU Buildings dataset, the Massachusetts Buildings dataset, and
the AICrowd Mapping Challenge. For out-of-distribution performance on the WHU
dataset, we achieve a 5.47% increase in IoU and a 4.81% improvement in
F1-score. For in-distribution performance on the WHU dataset, we observe a
2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score,
respectively. We intend to release our code repository, hoping to inspire
further exploration of foundation models for domain-specific tasks within the
remote sensing community.
","2023-10-04","2310.01845v1.pdf"
"2310.01846","Xiang Lisa Li","Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto,
  Percy Liang","Benchmarking and Improving Generator-Validator Consistency of Language
  Models","preprint","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  As of September 2023, ChatGPT correctly answers ""what is 7+8"" with 15, but
when asked ""7+8=15, True or False"" it responds with ""False"". This inconsistency
between generating and validating an answer is prevalent in language models
(LMs) and erodes trust. In this paper, we propose a framework for measuring the
consistency between generation and validation (which we call
generator-validator consistency, or GV-consistency), finding that even GPT-4, a
state-of-the-art LM, is GV-consistent only 76% of the time. To improve the
consistency of LMs, we propose to finetune on the filtered generator and
validator responses that are GV-consistent, and call this approach consistency
fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B
from 60% to 93%, and the improvement extrapolates to unseen tasks and domains
(e.g., GV-consistency for positive style transfers extrapolates to unseen
styles like humor). In addition to improving consistency, consistency
fine-tuning improves both generator quality and validator accuracy without
using any labeled data. Evaluated across 6 tasks, including math questions,
knowledge-intensive QA, and instruction following, our method improves the
generator quality by 16% and the validator accuracy by 6.3% across all tasks.
","2023-10-04","2310.01846v1.pdf"
"2310.01852","Bin Zhu","Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian
  Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei
  Liu, and Li Yuan","LanguageBind: Extending Video-Language Pretraining to N-modality by
  Language-based Semantic Alignment","Under review as a conference paper at ICLR 2024","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N>=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval task. Beyond this, our LanguageBind has greatly improved
in the zero-shot video, audio, depth, and infrared understanding tasks. For
instance, LanguageBind surpassing InterVideo by 1.9% on MSR-VTT, 8.8% on MSVD,
6.3% on DiDeMo, and 4.4% on ActivityNet. On the LLVIP and NYU-D datasets,
LanguageBind outperforms ImageBind with 23.8% and 11.1% top-1 accuracy. Code
address: https://github.com/PKU-YuanGroup/LanguageBind.
","2023-10-24","2310.01852v1.pdf"
"2310.01870","Albert Garde","Albert Garde, Esben Kran, Fazl Barez","DeepDecipher: Accessing and Investigating Neuron Activation in Large
  Language Models","4 pages (9 total), 1 figure, submitted to NeurIPS 2023 Workshop XAIA","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  As large language models (LLMs) become more capable, there is an urgent need
for interpretable and transparent tools. Current methods are difficult to
implement, and accessible tools to analyze model internals are lacking. To
bridge this gap, we present DeepDecipher - an API and interface for probing
neurons in transformer models' MLP layers. DeepDecipher makes the outputs of
advanced interpretability techniques for LLMs readily available. The
easy-to-use interface also makes inspecting these complex models more
intuitive. This paper outlines DeepDecipher's design and capabilities. We
demonstrate how to analyze neurons, compare models, and gain insights into
model behavior. For example, we contrast DeepDecipher's functionality with
similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher
enables efficient, scalable analysis of LLMs. By granting access to
state-of-the-art interpretability methods, DeepDecipher makes LLMs more
transparent, trustworthy, and safe. Researchers, engineers, and developers can
quickly diagnose issues, audit systems, and advance the field.
","2023-10-04","2310.01870v1.pdf"
"2310.01880","Qi Yan","Qi Yan, Raihan Seraj, Jiawei He, Lili Meng, Tristan Sylvain","AutoCast++: Enhancing World Event Prediction with Zero-shot
  Ranking-based Context Retrieval","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Machine-based prediction of real-world events is garnering attention due to
its potential for informed decision-making. Whereas traditional forecasting
predominantly hinges on structured data like time-series, recent breakthroughs
in language models enable predictions using unstructured text. In particular,
(Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles
for answering forecasting queries. Nevertheless, existing methods still trail
behind human performance. The cornerstone of accurate forecasting, we argue,
lies in identifying a concise, yet rich subset of news snippets from a vast
corpus. With this motivation, we introduce AutoCast++, a zero-shot
ranking-based context retrieval system, tailored to sift through expansive news
document collections for event forecasting. Our approach first re-ranks
articles based on zero-shot question-passage relevance, honing in on
semantically pertinent news. Following this, the chosen articles are subjected
to zero-shot summarization to attain succinct context. Leveraging a pre-trained
language model, we conduct both the relevance evaluation and article
summarization without needing domain-specific training. Notably, recent
articles can sometimes be at odds with preceding ones due to new facts or
unanticipated incidents, leading to fluctuating temporal dynamics. To tackle
this, our re-ranking mechanism gives preference to more recent articles, and we
further regularize the multi-passage representation learning to align with
human forecaster responses made on different dates. Empirical results
underscore marked improvements across multiple metrics, improving the
performance for multiple-choice questions (MCQ) by 48% and true/false (TF)
questions by up to 8%.
","2023-10-04","2310.01880v1.pdf"
"2310.01886","Weisen Jiang","Weisen Jiang and Baijiong Lin and Han Shi and Yu Zhang and Zhenguo Li
  and James T. Kwok","Effective and Parameter-Efficient Reusing Fine-Tuned Models","Technical Report","","","","cs.LG cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Many pre-trained large-scale models provided online have become highly
effective in transferring to downstream tasks. At the same time, various
task-specific models fine-tuned on these pre-trained models are available
online for public use. In practice, as collecting task-specific data is
labor-intensive and fine-tuning the large pre-trained models is computationally
expensive, one can reuse task-specific finetuned models to deal with downstream
tasks. However, using a model per task causes a heavy burden on storage and
serving. Recently, many training-free and parameter-efficient methods have been
proposed for reusing multiple fine-tuned task-specific models into a single
multi-task model. However, these methods exhibit a large accuracy gap compared
with using a fine-tuned model per task. In this paper, we propose
Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing
Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task
vector into a merged model by magnitude pruning. For reusing LoRA fine-tuned
models, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA
matrix by singular value decomposition. Both PERUFFT and PERU-LoRA are
training-free. Extensive experiments conducted on computer vision and natural
language process tasks demonstrate the effectiveness and parameter-efficiency
of the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform
existing reusing model methods by a large margin and achieve comparable
performance to using a fine-tuned model per task.
","2023-10-05","2310.01886v1.pdf"
"2310.01929","Eyal Ben-David","Mor Ventura and Eyal Ben-David and Anna Korhonen and Roi Reichart","Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of
  Text-To-Image Models","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have
recently gained prominence for their remarkable zero-shot capabilities in
generating images guided by textual prompts. Language, as a conduit of culture,
plays a pivotal role in these models' multilingual capabilities, which in turn
shape their cultural agency. In this study, we explore the cultural perception
embedded in TTI models by characterizing culture across three hierarchical
tiers: cultural dimensions, cultural domains, and cultural concepts. We propose
a comprehensive suite of evaluation techniques, including intrinsic evaluations
using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA)
model, and human assessments, to discern TTI cultural perceptions. To
facilitate our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments reveal insights
into these models' cultural awareness, cultural distinctions, and the unlocking
of cultural features, releasing the potential for cross-cultural applications.
","2023-10-04","2310.01929v1.pdf"
"2310.01957","Long Chen","Long Chen, Oleg Sinavski, Jan H\""unermann, Alice Karnsund, Andrew
  James Willmott, Danny Birch, Daniel Maund, Jamie Shotton","Driving with LLMs: Fusing Object-Level Vector Modality for Explainable
  Autonomous Driving","","","","","cs.RO cs.AI cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have shown promise in the autonomous driving
sector, particularly in generalization and interpretability. We introduce a
unique object-level multimodal LLM architecture that merges vectorized numeric
modalities with a pre-trained LLM to improve context understanding in driving
situations. We also present a new dataset of 160k QA pairs derived from 10k
driving scenarios, paired with high quality control commands collected with RL
agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct
pretraining strategy is devised to align numeric vector modalities with static
LLM representations using vector captioning language data. We also introduce an
evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency
in interpreting driving scenarios, answering questions, and decision-making.
Our findings highlight the potential of LLM-based driving action generation in
comparison to traditional behavioral cloning. We make our benchmark, datasets,
and model available for further exploration.
","2023-10-17","2310.01957v1.pdf"
"2310.01991","Aniruddha Deb","Aniruddha Deb, Neeva Oza, Sarthak Singla, Dinesh Khandelwal, Dinesh
  Garg, Parag Singla","Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward
  Reasoning in Math Word Problems","10 pages, 4 figures","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  While forward reasoning (i.e. find the answer given the question) has been
explored extensively in the recent literature, backward reasoning is relatively
unexplored. We examine the backward reasoning capabilities of LLMs on Math Word
Problems (MWPs): given a mathematical question and its answer, with some
details omitted from the question, can LLMs effectively retrieve the missing
information?
  In this paper, we formally define the backward reasoning task on math word
problems and modify three datasets to evaluate this task: GSM8k, SVAMP and
MultiArith. Our findings show a significant drop in the accuracy of models on
backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,
GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we
propose three novel techniques that improve performance: Rephrase reformulates
the given problem into a forward reasoning problem, PAL-Tools combines the idea
of Program-Aided LLMs to produce a set of equations that can be solved by an
external solver, and Check your Work exploits the availability of natural
verifier of high accuracy in the forward direction, interleaving solving and
verification steps. Finally, realizing that each of our base methods correctly
solves a different set of problems, we propose a novel Bayesian formulation for
creating an ensemble over these base methods aided by a verifier to further
boost the accuracy by a significant margin. Extensive experimentation
demonstrates that our techniques successively improve the performance of LLMs
on the backward reasoning task, with the final ensemble-based method resulting
in a substantial performance gain compared to the raw LLMs with standard
prompting techniques such as chain-of-thought.
","2023-10-04","2310.01991v1.pdf"
"2310.02003","Samuel Holt","Samuel Holt, Max Ruiz Luyten, Mihaela van der Schaar","L2MAC: Large Language Model Automatic Computer for Unbounded Code
  Generation","Copyright 2023 by the author(s)","","","","cs.SE cs.AI cs.LG cs.PL","http://creativecommons.org/licenses/by/4.0/","  Transformer-based large language models (LLMs) are constrained by the fixed
context window of the underlying transformer architecture, hindering their
ability to produce long and logically consistent code. Memory-augmented LLMs
are a promising solution, but current approaches cannot handle long code
generation tasks since they (1) only focus on reading memory and reduce its
evolution to the concatenation of new memories or (2) use very specialized
memories that cannot adapt to other domains. This paper presents L2MAC, the
first practical LLM-based stored-program automatic computer for long and
consistent code generation. Its memory has two components: the instruction
registry, which is populated with a prompt program to solve the user-given
task, and a file store, which will contain the final and intermediate outputs.
Each instruction is executed by a separate LLM instance, whose context is
managed by a control unit capable of precise memory reading and writing to
ensure effective interaction with the file store. These components enable L2MAC
to generate virtually unbounded code structures, bypassing the constraints of
the finite context window while producing code that fulfills complex
user-specified requirements. We empirically show that L2MAC succeeds in
generating large code bases for system design tasks where other coding methods
fall short in implementing user requirements and provide insight into the
reasons for this performance gap.
","2023-10-04","2310.02003v1.pdf"
"2310.02012","Alexandru Meterez","Alexandru Meterez, Amir Joudaki, Francesco Orabona, Alexander Immer,
  Gunnar R\""atsch, Hadi Daneshmand","Towards Training Without Depth Limits: Batch Normalization Without
  Gradient Explosion","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Normalization layers are one of the key building blocks for deep neural
networks. Several theoretical studies have shown that batch normalization
improves the signal propagation, by avoiding the representations from becoming
collinear across the layers. However, results on mean-field theory of batch
normalization also conclude that this benefit comes at the expense of exploding
gradients in depth. Motivated by these two aspects of batch normalization, in
this study we pose the following question: ""Can a batch-normalized network keep
the optimal signal propagation properties, but avoid exploding gradients?"" We
answer this question in the affirmative by giving a particular construction of
an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization
that provably has bounded gradients at any depth. Based on Weingarten calculus,
we develop a rigorous and non-asymptotic theory for this constructed MLP that
gives a precise characterization of forward signal propagation, while proving
that gradients remain bounded for linearly independent input samples, which
holds in most practical settings. Inspired by our theory, we also design an
activation shaping scheme that empirically achieves the same properties for
certain non-linear activations.
","2023-10-04","2310.02012v1.pdf"
"2310.02019","Pedram Salimi","Pedram Salimi, Nirmalie Wiratunga, David Corsar, Anjana Wijekoon","Towards Feasible Counterfactual Explanations: A Taxonomy Guided
  Template-based NLG Method","","Volume 372: ECAI 2023","10.3233/FAIA230499","","cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Counterfactual Explanations (cf-XAI) describe the smallest changes in feature
values necessary to change an outcome from one class to another. However, many
cf-XAI methods neglect the feasibility of those changes. In this paper, we
introduce a novel approach for presenting cf-XAI in natural language
(Natural-XAI), giving careful consideration to actionable and comprehensible
aspects while remaining cognizant of immutability and ethical concerns. We
present three contributions to this endeavor. Firstly, through a user study, we
identify two types of themes present in cf-XAI composed by humans:
content-related, focusing on how features and their values are included from
both the counterfactual and the query perspectives; and structure-related,
focusing on the structure and terminology used for describing necessary value
changes. Secondly, we introduce a feature actionability taxonomy with four
clearly defined categories, to streamline the explanation presentation process.
Using insights from the user study and our taxonomy, we created a generalisable
template-based natural language generation (NLG) method compatible with
existing explainers like DICE, NICE, and DisCERN, to produce counterfactuals
that address the aforementioned limitations of existing approaches. Finally, we
conducted a second user study to assess the performance of our taxonomy-guided
NLG templates on three domains. Our findings show that the taxonomy-guided
Natural-XAI approach (n-XAI^T) received higher user ratings across all
dimensions, with significantly improved results in the majority of the domains
assessed for articulation, acceptability, feasibility, and sensitivity
dimensions.
","2023-10-04","2310.02019v1.pdf"
"2310.02031","Ningyu Zhang","Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng,
  Huajun Chen","OceanGPT: A Large Language Model for Ocean Science Tasks","Work in progress. Project Website:
  https://zjunlp.github.io/project/OceanGPT/","","","","cs.CL cs.AI cs.CE cs.LG cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
","2023-10-26","2310.02031v1.pdf"
"2310.02046","Michel Nass","Michel Nass, Emil Alegroth, Robert Feldt","Improving web element localization by using a large language model","","","","","cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Web-based test automation heavily relies on accurately finding web elements.
Traditional methods compare attributes but don't grasp the context and meaning
of elements and words. The emergence of Large Language Models (LLMs) like
GPT-4, which can show human-like reasoning abilities on some tasks, offers new
opportunities for software engineering and web element localization. This paper
introduces and evaluates VON Similo LLM, an enhanced web element localization
approach. Using an LLM, it selects the most likely web element from the
top-ranked ones identified by the existing VON Similo method, ideally aiming to
get closer to human-like selection accuracy. An experimental study was
conducted using 804 web element pairs from 48 real-world web applications. We
measured the number of correctly identified elements as well as the execution
times, comparing the effectiveness and efficiency of VON Similo LLM against the
baseline algorithm. In addition, motivations from the LLM were recorded and
analyzed for all instances where the original approach failed to find the right
web element. VON Similo LLM demonstrated improved performance, reducing failed
localizations from 70 to 39 (out of 804), a 44 percent reduction. Despite its
slower execution time and additional costs of using the GPT-4 model, the LLMs
human-like reasoning showed promise in enhancing web element localization. LLM
technology can enhance web element identification in GUI test automation,
reducing false positives and potentially lowering maintenance costs. However,
further research is necessary to fully understand LLMs capabilities,
limitations, and practical use in GUI testing.
","2023-10-04","2310.02046v1.pdf"
"2310.02050","Hao Zhang","Hao Zhang, Nianwen Si, Yaqi Chen, Wenlin Zhang, Xukui Yang, Dan Qu,
  Xiaolin Jiao","Tuning Large language model for End-to-end Speech Translation","","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the emergence of large language models (LLMs), multimodal models based
on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,
and SpeechGPT exhibit an impressive ability to comprehend and generate human
instructions. However, their performance often falters when faced with complex
tasks like end-to-end speech translation (E2E-ST), a cross-language and
cross-modal translation task. In comparison to single-modal models, multimodal
models lag behind in these scenarios. This paper introduces LST, a Large
multimodal model designed to excel at the E2E-ST task. LST consists of a speech
frontend, an adapter, and a LLM backend. The training of LST consists of two
stages: (1) Modality adjustment, where the adapter is tuned to align speech
representation with text embedding space, and (2) Downstream task fine-tuning,
where both the adapter and LLM model are trained to optimize performance on the
E2EST task. Experimental results on the MuST-C speech translation benchmark
demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on
En-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a
new state-of-the-art. Additionally, we conduct an in-depth analysis of
single-modal model selection and the impact of training strategies, which lays
the foundation for future research. We will open up our code and models after
review.
","2023-10-04","2310.02050v1.pdf"
"2310.02054","Zibin Dong","Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing
  Hu, Tangjie Lv, Changjie Fan and Zhipeng Hu","AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable
  Diffusion Model","","","","","cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Aligning agent behaviors with diverse human preferences remains a challenging
problem in reinforcement learning (RL), owing to the inherent abstractness and
mutability of human preferences. To address these issues, we propose AlignDiff,
a novel framework that leverages RL from Human Feedback (RLHF) to quantify
human preferences, covering abstractness, and utilizes them to guide diffusion
planning for zero-shot behavior customizing, covering mutability. AlignDiff can
accurately match user-customized behaviors and efficiently switch from one to
another. To build the framework, we first establish the multi-perspective human
feedback datasets, which contain comparisons for the attributes of diverse
behaviors, and then train an attribute strength model to predict quantified
relative strengths. After relabeling behavioral datasets with relative
strengths, we proceed to train an attribute-conditioned diffusion model, which
serves as a planner with the attribute strength model as a director for
preference aligning at the inference phase. We evaluate AlignDiff on various
locomotion tasks and demonstrate its superior performance on preference
matching, switching, and covering compared to other baselines. Its capability
of completing unseen downstream tasks under human instructions also showcases
the promising potential for human-AI collaboration. More visualization videos
are released on https://aligndiff.github.io/.
","2023-10-04","2310.02054v1.pdf"
"2310.02059","Peng Liang","Yujia Fu, Peng Liang, Amjed Tahir, Zengyang Li, Mojtaba Shahin, Jiaxin
  Yu","Security Weaknesses of Copilot Generated Code in GitHub","","","","","cs.SE cs.CR","http://creativecommons.org/licenses/by/4.0/","  Modern code generation tools use AI models, particularly Large Language
Models (LLMs), to generate functional and complete code. While such tools are
becoming popular and widely available for developers, using these tools is
often accompanied by security challenges. Therefore, it is important to assess
the quality of the generated code, especially in terms of its security.
Researchers have recently explored various aspects of code generation tools,
including security. However, many open questions about the security of the
generated code require further investigation, especially the security issues of
automatically generated code in the wild. To this end, we conducted an
empirical study by analyzing the security weaknesses in code snippets generated
by GitHub Copilot that are found as part of publicly available projects hosted
on GitHub. The goal is to investigate the types of security issues and their
scale in real-world scenarios (rather than crafted scenarios). To this end, we
identified 435 code snippets generated by Copilot from publicly available
projects. We then conducted extensive security analysis to identify Common
Weakness Enumeration (CWE) instances in these code snippets. The results show
that (1) 35.8% of Copilot generated code snippets contain CWEs, and those
issues are spread across multiple languages, (2) the security weaknesses are
diverse and related to 42 different CWEs, in which CWE-78: OS Command
Injection, CWE-330: Use of Insufficiently Random Values, and CWE-703: Improper
Check or Handling of Exceptional Conditions occurred the most frequently, and
(3) among the 42 CWEs identified, 11 of those belong to the currently
recognized 2022 CWE Top-25. Our findings confirm that developers should be
careful when adding code generated by Copilot (and similar AI code generation
tools) and should also run appropriate security checks as they accept the
suggested code.
","2023-10-04","2310.02059v1.pdf"
"2310.02071","Liang Chen","Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi
  Wang, Peiyi Wang, Tianyu Liu, Baobao Chang","Towards End-to-End Embodied Decision Making via Multi-modal Large
  Language Model: Explorations with GPT4-Vision and Beyond","18 pages, 10 figures, Code and data:
  https://github.com/pkunlp-icler/PCA-EVAL/","","","","cs.AI cs.CL cs.CV cs.RO","http://creativecommons.org/licenses/by/4.0/","  In this study, we explore the potential of Multimodal Large Language Models
(MLLMs) in improving embodied decision-making processes for agents. While Large
Language Models (LLMs) have been widely used due to their advanced reasoning
skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual
understanding and reasoning capabilities. We investigate whether
state-of-the-art MLLMs can handle embodied decision-making in an end-to-end
manner and whether collaborations between LLMs and MLLMs can enhance
decision-making. To address these questions, we introduce a new benchmark
called PCA-EVAL, which evaluates embodied decision-making from the perspectives
of Perception, Cognition, and Action. Additionally, we propose HOLMES, a
multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs
to gather multimodal information for informed decision-making. We compare
end-to-end embodied decision-making and HOLMES on our benchmark and find that
the GPT4-Vision model demonstrates strong end-to-end embodied decision-making
abilities, outperforming GPT4-HOLMES in terms of average decision accuracy
(+3%). However, this performance is exclusive to the latest GPT4-Vision model,
surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate
that powerful MLLMs like GPT4-Vision hold promise for decision-making in
embodied agents, offering new avenues for MLLM research. Code and data are open
at https://github.com/pkunlp-icler/PCA-EVAL/.
","2023-10-17","2310.02071v1.pdf"
"2310.02102","Nikolaos Malamas","Nikolaos Malamas, Konstantinos Panayiotou, Andreas L. Symeonidis","dFlow: A Domain Specific Language for the Rapid Development of
  open-source Virtual Assistants","","","","","cs.SE","http://creativecommons.org/licenses/by-nc-nd/4.0/","  An increasing number of models and frameworks for Virtual Assistant (VA)
development exist nowadays, following the progress in the Natural Language
Processing (NLP) and Natural Language Understanding (NLU) fields. Regardless of
their performance, popularity, and ease of use, these frameworks require at
least basic expertise in NLP and software engineering, even for simple and
repetitive processes, limiting their use only to the domain and programming
experts. However, since the current state of practice of VA development is a
straightforward process, Model-Driven Engineering approaches can be utilized to
achieve automation and rapid development in a more convenient manner. To this
end, we present \textit{dFlow}, a textual Domain-Specific Language (DSL) that
offers a simplified, reusable, and framework-agnostic language for creating
task-specific VAs in a low-code manner. We describe a system-agnostic VA
meta-model, the developed grammar, and all essential processes for developing
and deploying smart VAs. For further convenience, we create a cloud-native
architecture and expose it through the Discord platform. We conducted a
large-scale empirical evaluation with more than 200 junior software developers
and collected positive feedback, indicating that dFlow can accelerate the
entire VA development process, while also enabling citizen and software
developers with minimum experience to participate.
","2023-10-04","2310.02102v1.pdf"
"2310.02107","Saurabh Srivastava","Saurabh Srivastava, Chengyue Huang, Weiguo Fan, Ziyu Yao","Instance Needs More Care: Rewriting Prompts for Instances Yields Better
  Zero-Shot Performance","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Enabling large language models (LLMs) to perform tasks in zero-shot has been
an appealing goal owing to its labor-saving (i.e., requiring no task-specific
annotations); as such, zero-shot prompting approaches also enjoy better task
generalizability. To improve LLMs' zero-shot performance, prior work has
focused on devising more effective task instructions (e.g., ``let's think step
by step'' ). However, we argue that, in order for an LLM to solve them
correctly in zero-shot, individual test instances need more carefully designed
and customized instructions. To this end, we propose PRoMPTd, an approach that
rewrites the task prompt for each individual test input to be more specific,
unambiguous, and complete, so as to provide better guidance to the task LLM. We
evaluated PRoMPTd on eight datasets covering tasks including arithmetics,
logical reasoning, and code generation, using GPT-4 as the task LLM. Notably,
PRoMPTd achieves an absolute improvement of around 10% on the complex MATH
dataset and 5% on the code generation task on HumanEval, outperforming
conventional zero-shot methods. In addition, we also showed that the rewritten
prompt can provide better interpretability of how the LLM resolves each test
instance, which can potentially be leveraged as a defense mechanism against
adversarial prompting. The source code and dataset can be obtained from
https://github.com/salokr/PRoMPTd
","2023-10-09","2310.02107v1.pdf"
"2310.02110","Anas Mahmoud","Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani,
  Hugh Leather, Ari Morcos","SIEVE: Multimodal Dataset Pruning Using Image Captioning Models","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy
web-crawled datasets. This underscores the critical need for dataset pruning,
as the quality of these datasets is strongly correlated with the performance of
VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train
models using highly-aligned samples is one of the most successful methods for
pruning.We argue that this approach suffers from multiple limitations
including: 1) false positives due to spurious correlations captured by the
pretrained CLIP model, 2) false negatives due to poor discrimination between
hard and bad samples, and 3) biased ranking towards samples similar to the
pretrained CLIP dataset. We propose a pruning method, SIEVE, that employs
synthetic captions generated by image-captioning models pretrained on small,
diverse, and well-aligned image-text pairs to evaluate the alignment of noisy
image-text pairs. To bridge the gap between the limited diversity of generated
captions and the high diversity of alternative text (alt-text), we estimate the
semantic textual similarity in the embedding space of a language model
pretrained on billions of sentences. Using DataComp, a multimodal dataset
filtering benchmark, we achieve state-of-the-art performance on the large scale
pool, and competitive results on the medium scale pool, surpassing
CLIPScore-based filtering by 1.7% and 2.6% on average, on 38 downstream tasks.
","2023-10-04","2310.02110v1.pdf"
"2310.02118","Diogo M. Silva","Rafael Ferreira, Diogo Tavares, Diogo Silva, Rodrigo Val\'erio, Jo\~ao
  Bordalo, In\^es Sim\~oes, Vasco Ramos, David Semedo, Jo\~ao Magalh\~aes","TWIZ: The Wizard of Multimodal Conversational-Stimulus","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this report, we describe the vision, challenges, and scientific
contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot
Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal,
knowledgeable, and engaging assistant that can guide users towards the
successful completion of complex manual tasks. To achieve this, we focus our
efforts on three main research questions: (1) Humanly-Shaped Conversations, by
providing information in a knowledgeable way; (2) Multimodal Stimulus, making
use of various modalities including voice, images, and videos; and (3)
Zero-shot Conversational Flows, to improve the robustness of the interaction to
unseen scenarios. TWIZ is an assistant capable of supporting a wide range of
tasks, with several innovative features such as creative cooking, video
navigation through voice, and the robust TWIZ-LLM, a Large Language Model
trained for dialoguing about complex manual tasks. Given ratings and feedback
provided by users, we observed that TWIZ bot is an effective and robust system,
capable of guiding users through tasks while providing several multimodal
stimuli.
","2023-10-04","2310.02118v1.pdf"
"2310.02120","Heungseok Park","Heungseok Park, Aeree Cho, Hyojun Jeon, Hayoung Lee, Youngil Yang,
  Sungjae Lee, Heungsub Lee, Jaegul Choo","HPCClusterScape: Increasing Transparency and Efficiency of Shared
  High-Performance Computing Clusters for Large-scale AI Models","","","","","cs.HC cs.DC","http://creativecommons.org/licenses/by/4.0/","  The emergence of large-scale AI models, like GPT-4, has significantly
impacted academia and industry, driving the demand for high-performance
computing (HPC) to accelerate workloads. To address this, we present
HPCClusterScape, a visualization system that enhances the efficiency and
transparency of shared HPC clusters for large-scale AI models. HPCClusterScape
provides a comprehensive overview of system-level (e.g., partitions, hosts, and
workload status) and application-level (e.g., identification of experiments and
researchers) information, allowing HPC operators and machine learning
researchers to monitor resource utilization and identify issues through
customizable violation rules. The system includes diagnostic tools to
investigate workload imbalances and synchronization bottlenecks in large-scale
distributed deep learning experiments. Deployed in industrial-scale HPC
clusters, HPCClusterScape incorporates user feedback and meets specific
requirements. This paper outlines the challenges and prerequisites for
efficient HPC operation, introduces the interactive visualization system, and
highlights its contributions in addressing pain points and optimizing resource
utilization in shared HPC clusters.
","2023-10-04","2310.02120v1.pdf"
"2310.02124","Shumin Deng","Jintian Zhang, Xin Xu, Shumin Deng","Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology
  View","Work in Progress","","","","cs.CL cs.AI cs.CY cs.LG cs.MA","http://creativecommons.org/licenses/by-sa/4.0/","  As Natural Language Processing (NLP) systems are increasingly employed in
intricate social environments, a pressing query emerges: Can these NLP systems
mirror human-esque collaborative intelligence, in a multi-agent society
consisting of multiple large language models (LLMs)? This paper probes the
collaboration mechanisms among contemporary NLP systems by melding practical
experiments with theoretical insights. We fabricate four unique `societies'
comprised of LLM agents, where each agent is characterized by a specific
`trait' (easy-going or overconfident) and engages in collaboration with a
distinct `thinking pattern' (debate or reflection). Evaluating these
multi-agent societies on three benchmark datasets, we discern that LLM agents
navigate tasks by leveraging diverse social behaviors, from active debates to
introspective reflections. Notably, certain collaborative strategies only
optimize efficiency (using fewer API tokens), but also outshine previous
top-tier approaches. Moreover, our results further illustrate that LLM agents
manifest human-like social behaviors, such as conformity or majority rule,
mirroring foundational Social Psychology theories. In conclusion, we integrate
insights from Social Psychology to contextualize the collaboration of LLM
agents, inspiring further investigations into the collaboration mechanism for
LLMs. We commit to sharing our code and datasets (already submitted in
supplementary materials), hoping to catalyze further research in this promising
avenue (All code and data are available at
\url{https://github.com/zjunlp/MachineSoM}.).
","2023-10-04","2310.02124v1.pdf"
"2310.02166","Mikhail Salnikov","Mikhail Salnikov, Hai Le, Prateek Rajput, Irina Nikishina, Pavel
  Braslavski, Valentin Malykh and Alexander Panchenko","Large Language Models Meet Knowledge Graphs to Answer Factoid Questions","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, it has been shown that the incorporation of structured knowledge
into Large Language Models significantly improves the results for a variety of
NLP tasks. In this paper, we propose a method for exploring pre-trained
Text-to-Text Language Models enriched with additional information from
Knowledge Graphs for answering factoid questions. More specifically, we propose
an algorithm for subgraphs extraction from a Knowledge Graph based on question
entities and answer candidates. Then, we procure easily interpreted information
with Transformer-based models through the linearization of the extracted
subgraphs. Final re-ranking of the answer candidates with the extracted
information boosts Hits@1 scores of the pre-trained text-to-text language
models by 4-6%.
","2023-10-04","2310.02166v1.pdf"
"2310.02168","Ningyu Zhang","Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong
  Jiang, Pengjun Xie, Fei Huang, Huajun Chen","Editing Personality for LLMs","Work in progress","","","","cs.CL cs.AI cs.CY cs.LG cs.MA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models'
responses to opinion-related questions on specified topics since an
individual's personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.
","2023-10-04","2310.02168v1.pdf"
"2310.02170","Zijun Liu","Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang","Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with
  Agent Team Optimization","Preprint, under review. 21 pages","","","","cs.CL cs.AI cs.MA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language model (LLM) agents have been shown effective on a wide range
of tasks, and by ensembling multiple LLM agents, their performances could be
further improved. Existing approaches employ a fixed set of agents to interact
with each other in a static architecture, which limits their generalizability
to various tasks and requires strong human prior in designing these agents. In
this work, we propose to construct a strategic team of agents communicating in
a dynamic interaction architecture based on the task query. Specifically, we
build a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) for
LLM-agent collaboration on complicated tasks like reasoning and code
generation. DyLAN enables agents to interact for multiple rounds in a dynamic
architecture with inference-time agent selection and an early-stopping
mechanism to improve performance and efficiency. We further design an automatic
agent team optimization algorithm based on an unsupervised metric termed
$\textit{Agent Importance Score}$, enabling the selection of best agents based
on the contribution each agent makes. Empirically, we demonstrate that DyLAN
performs well in both reasoning and code generation tasks with reasonable
computational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and
HumanEval, respectively, compared to a single execution on GPT-35-turbo. On
specific subjects of MMLU, agent team optimization in DyLAN increases accuracy
by up to 25.0%.
","2023-10-04","2310.02170v1.pdf"
"2310.02172","Andrew Ahn","Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin
  Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn","Lyfe Agents: Generative agents for low-cost real-time social
  interactions","","","","","cs.HC cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.
","2023-10-04","2310.02172v1.pdf"
"2310.02174","Qiming Xie","Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia","Ask Again, Then Fail: Large Language Models' Vacillations in Judgement","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the emergence of generative conversational large language models (LLMs)
like ChatGPT, serving as virtual assistants in various fields, the stability
and reliability of their responses have become crucial. However, during usage,
it has been observed that these models tend to waver in their judgements when
confronted with follow-up questions from users expressing skepticism or
disagreement. In this work, we draw inspiration from questioning strategies in
education and propose a \textsc{Follow-up Questioning Mechanism} along with two
evaluation metrics to assess the judgement consistency of LLMs before and after
exposure to disturbances. We evaluate the judgement consistency of ChatGPT,
PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning
benchmarks. Empirical results show that even when the initial answers are
correct, judgement consistency sharply decreases when LLMs face disturbances
such as questioning, negation, or misleading. Additionally, we study these
models' judgement consistency under various settings (sampling temperature and
prompts) to validate this issue further, observing the impact of prompt tone
and conducting an in-depth error analysis for deeper behavioral insights.
Furthermore, we also explore several prompting methods to mitigate this issue
and demonstrate their
effectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.
","2023-10-04","2310.02174v1.pdf"
"2310.02207","Wes Gurnee","Wes Gurnee, Max Tegmark","Language Models Represent Space and Time","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  The capabilities of large language models (LLMs) have sparked debate over
whether such systems just learn an enormous collection of superficial
statistics or a coherent model of the data generating process -- a world model.
We find evidence for the latter by analyzing the learned representations of
three spatial datasets (world, US, NYC places) and three temporal datasets
(historical figures, artworks, news headlines) in the Llama-2 family of models.
We discover that LLMs learn linear representations of space and time across
multiple scales. These representations are robust to prompting variations and
unified across different entity types (e.g. cities and landmarks). In addition,
we identify individual ``space neurons'' and ``time neurons'' that reliably
encode spatial and temporal coordinates. Our analysis demonstrates that modern
LLMs acquire structured knowledge about fundamental dimensions such as space
and time, supporting the view that they learn not merely superficial
statistics, but literal world models.
","2023-10-04","2310.02207v1.pdf"
"2310.02224","Ethan Mendes","Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter","Can Language Models be Instructed to Protect Personal Information?","","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large multimodal language models have proven transformative in numerous
applications. However, these models have been shown to memorize and leak
pre-training data, raising serious user privacy and information security
concerns. While data leaks should be prevented, it is also crucial to examine
the trade-off between the privacy protection and model utility of proposed
approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to
assess this privacy/utility trade-off when a model is instructed to protect
specific categories of personal information in a simulated scenario. We also
propose a technique to iteratively self-moderate responses, which significantly
improves privacy. However, through a series of red-teaming experiments, we find
that adversaries can also easily circumvent these protections with simple
jailbreaking methods through textual and/or image inputs. We believe PrivQA has
the potential to support the development of new models with improved privacy
protections, as well as the adversarial robustness of these protections. We
release the entire PrivQA dataset at https://llm-access-control.github.io/.
","2023-10-04","2310.02224v1.pdf"
"2310.02226","Sachin Goyal","Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon,
  Sanjiv Kumar, Vaishnavh Nagarajan","Think before you speak: Training Language Models With Pause Tokens","19 pages, 7 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models generate responses by producing a series of tokens in
immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$
hidden vectors per layer, one vector per preceding token. What if instead we
were to let the model manipulate say, $K+10$ hidden vectors, before it outputs
the $(K+1)^{th}$ token? We operationalize this idea by performing training and
inference on language models with a (learnable) $\textit{pause}$ token, a
sequence of which is appended to the input prefix. We then delay extracting the
model's outputs until the last pause token is seen, thereby allowing the model
to process extra computation before committing to an answer. We empirically
evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M
parameters with causal pretraining on C4, and on downstream tasks covering
reasoning, question-answering, general understanding and fact recall. Our main
finding is that inference-time delays show gains when the model is both
pre-trained and finetuned with delays. For the 1B model, we witness gains on 8
of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of
SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of
GSM8k. Our work raises a range of conceptual and practical future research
questions on making delayed next-token prediction a widely applicable new
paradigm.
","2023-10-04","2310.02226v1.pdf"
"2310.02238","Ronen Eldan","Ronen Eldan and Mark Russinovich","Who's Harry Potter? Approximate Unlearning in LLMs","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are trained on massive internet corpora that
often contain copyrighted content. This poses legal and ethical challenges for
the developers and users of these models, as well as the original authors and
publishers. In this paper, we propose a novel technique for unlearning a subset
of the training data from a LLM, without having to retrain it from scratch.
  We evaluate our technique on the task of unlearning the Harry Potter books
from the Llama2-7b model (a generative language model recently open-sourced by
Meta). While the model took over 184K GPU-hours to pretrain, we show that in
about 1 GPU hour of finetuning, we effectively erase the model's ability to
generate or recall Harry Potter-related content, while its performance on
common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains
almost unaffected. We make our fine-tuned model publicly available on
HuggingFace for community evaluation. To the best of our knowledge, this is the
first paper to present an effective technique for unlearning in generative
language models.
  Our technique consists of three main components: First, we use a reinforced
model that is further trained on the target data to identify the tokens that
are most related to the unlearning target, by comparing its logits with those
of a baseline model. Second, we replace idiosyncratic expressions in the target
data with generic counterparts, and leverage the model's own predictions to
generate alternative labels for every token. These labels aim to approximate
the next-token predictions of a model that has not been trained on the target
data. Third, we finetune the model on these alternative labels, which
effectively erases the original text from the model's memory whenever it is
prompted with its context.
","2023-10-05","2310.02238v1.pdf"
"2310.02239","Kaizhi Zheng","Kaizhi Zheng, Xuehai He, Xin Eric Wang","MiniGPT-5: Interleaved Vision-and-Language Generation via Generative
  Vokens","20 pages, 9 figures","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have garnered significant attention for their
advancements in natural language processing, demonstrating unparalleled prowess
in text comprehension and generation. Yet, the simultaneous generation of
images with coherent textual narratives remains an evolving frontier. In
response, we introduce an innovative interleaved vision-and-language generation
technique anchored by the concept of ""generative vokens,"" acting as the bridge
for harmonized image-text outputs. Our approach is characterized by a
distinctive two-staged training strategy focusing on description-free
multimodal generation, where the training requires no comprehensive
descriptions of images. To bolster model integrity, classifier-free guidance is
incorporated, enhancing the effectiveness of vokens on image generation. Our
model, MiniGPT-5, exhibits substantial improvement over the baseline Divter
model on the MMDialog dataset and consistently delivers superior or comparable
multimodal outputs in human evaluations on the VIST dataset, highlighting its
efficacy across diverse benchmarks.
","2023-10-09","2310.02239v1.pdf"
"2310.02244","Greg Yang","Greg Yang, Dingli Yu, Chen Zhu, Soufiane Hayou","Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks","","","","","cs.NE cond-mat.dis-nn math.PR","http://creativecommons.org/licenses/by/4.0/","  By classifying infinite-width neural networks and identifying the *optimal*
limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,
for *widthwise hyperparameter transfer*, i.e., predicting optimal
hyperparameters of wide neural networks from narrow ones. Here we investigate
the analogous classification for *depthwise parametrizations* of deep residual
networks (resnets). We classify depthwise parametrizations of block multiplier
and learning rate by their infinite-width-then-depth limits. In resnets where
each block has only one layer, we identify a unique optimal parametrization,
called Depth-$\mu$P that extends $\mu$P and show empirically it admits
depthwise hyperparameter transfer. We identify *feature diversity* as a crucial
factor in deep networks, and Depth-$\mu$P can be characterized as maximizing
both feature learning and feature diversity. Exploiting this, we find that
absolute value, among all homogeneous nonlinearities, maximizes feature
diversity and indeed empirically leads to significantly better performance.
However, if each block is deeper (such as modern transformers), then we find
fundamental limitations in all possible infinite-depth limits of such
parametrizations, which we illustrate both theoretically and empirically on
simple networks as well as Megatron transformer trained on Common Crawl.
","2023-10-13","2310.02244v1.pdf"
"2310.02251","Vikrant Dewangan","Vikrant Dewangan, Tushar Choudhary, Shivam Chandhok, Shubham
  Priyadarshan, Anushka Jain, Arun K. Singh, Siddharth Srivastava, Krishna
  Murthy Jatavallabhula, K. Madhava Krishna","Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving","Submitted to ICRA 2024. Project page at
  https://llmbev.github.io/talk2bev/","","","","cs.CV cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye
view (BEV) maps in autonomous driving contexts. While existing perception
systems for autonomous driving scenarios have largely focused on a pre-defined
(closed) set of object categories and driving scenarios, Talk2BEV blends recent
advances in general-purpose language and vision models with BEV-structured map
representations, eliminating the need for task-specific models. This enables a
single system to cater to a variety of autonomous driving tasks encompassing
visual and spatial reasoning, predicting the intents of traffic actors, and
decision-making based on visual cues. We extensively evaluate Talk2BEV on a
large number of scene understanding tasks that rely on both the ability to
interpret free-form natural language queries, and in grounding these queries to
the visual context embedded into the language-enhanced BEV map. To enable
further research in LVLMs for autonomous driving scenarios, we develop and
release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV
scenarios, with more than 20,000 questions and ground-truth responses from the
NuScenes dataset.
","2023-10-04","2310.02251v1.pdf"
"2310.02255","Pan Lu","Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh
  Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao","MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V,
  Bard, and Other Large Multimodal Models","112 pages, 117 figures. Work in progress","","","","cs.CV cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
","2023-10-27","2310.02255v1.pdf"
"2310.02258","Ziming Liu","Ziming Liu, Max Tegmark","A Neural Scaling Law from Lottery Ticket Ensembling","14 pages, 13 figures","","","","cs.LG cs.AI physics.data-an stat.ML","http://creativecommons.org/licenses/by/4.0/","  Neural scaling laws (NSL) refer to the phenomenon where model performance
improves with scale. Sharma & Kaplan analyzed NSL using approximation theory
and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is
the number of model parameters, and $d$ is the intrinsic input dimension.
Although their theory works well for some cases (e.g., ReLU networks), we
surprisingly find that a simple 1D problem $y=x^2$ manifests a different
scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the
neural networks and found that the new scaling law originates from lottery
ticket ensembling: a wider network on average has more ""lottery tickets"", which
are ensembled to reduce the variance of outputs. We support the ensembling
mechanism by mechanistically interpreting single neural networks, as well as
studying them statistically. We attribute the $N^{-1}$ scaling law to the
""central limit theorem"" of lottery tickets. Finally, we discuss its potential
implications for large language models and statistical physics-type theories of
learning.
","2023-10-04","2310.02258v1.pdf"
"2310.02263","Canwen Xu","Canwen Xu, Corby Rosset, Luciano Del Corro, Shweti Mahajan, Julian
  McAuley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao","Contrastive Post-training Large Language Models on Data Curriculum","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Alignment serves as an important step to steer large language models (LLMs)
towards human preferences. In this paper, we explore contrastive post-training
techniques for alignment by automatically constructing preference pairs from
multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We
carefully compare the contrastive techniques of SLiC and DPO to SFT baselines
and find that DPO provides a step-function improvement even after continueing
SFT saturates. We also explore a data curriculum learning scheme for
contrastive post-training, which starts by learning from ""easier"" pairs and
transitioning to ""harder"" ones, which further improves alignment. Finally, we
scale up our experiments to train with more data and larger models like Orca.
Remarkably, contrastive post-training further improves the performance of Orca,
already a state-of-the-art instruction learning model tuned with GPT-4 outputs,
to exceed that of ChatGPT.
","2023-10-04","2310.02263v1.pdf"
"2310.02264","Mingyu Ding","Haoyu Zhou, Mingyu Ding, Weikun Peng, Masayoshi Tomizuka, Lin Shao,
  Chuang Gan","Generalizable Long-Horizon Manipulations with Large Language Models","","","","","cs.RO cs.CL cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work introduces a framework harnessing the capabilities of Large
Language Models (LLMs) to generate primitive task conditions for generalizable
long-horizon manipulations with novel objects and unseen tasks. These task
conditions serve as guides for the generation and adjustment of Dynamic
Movement Primitives (DMP) trajectories for long-horizon task execution. We
further create a challenging robotic manipulation task suite based on Pybullet
for long-horizon task evaluation. Extensive experiments in both simulated and
real-world environments demonstrate the effectiveness of our framework on both
familiar tasks involving new objects and novel but related tasks, highlighting
the potential of LLMs in enhancing robotic system versatility and adaptability.
Project website: https://object814.github.io/Task-Condition-With-LLM/
","2023-10-04","2310.02264v1.pdf"
"2310.02277","Lu Yin","Lu Yin, Shiwei Liu, Ajay Jaiswal, Souvik Kundu, Zhangyang Wang","Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights
  through Sparsity","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The traditional notion of ""Junk DNA"" has long been linked to non-coding
segments within the human genome, constituting roughly 98% of its composition.
However, recent research has unveiled the critical roles some of these
seemingly non-functional DNA sequences play in cellular processes.
Intriguingly, the weights within deep neural networks exhibit a remarkable
similarity to the redundancy observed in human genes. It was believed that
weights in gigantic models contained excessive redundancy, and could be removed
without compromising performance. This paper challenges this conventional
wisdom by presenting a compelling counter-argument. We employ sparsity as a
tool to isolate and quantify the nuanced significance of low-magnitude weights
in pre-trained large language models (LLMs). Our study demonstrates a strong
correlation between these weight magnitudes and the knowledge they encapsulate,
from a downstream task-centric angle. we raise the ""Junk DNA Hypothesis"" backed
by our in-depth investigation: while small-magnitude weights may appear
""useless"" for simple tasks and suitable for pruning, they actually encode
crucial knowledge necessary for solving more difficult downstream tasks.
Removing these seemingly insignificant weights can lead to irreversible
knowledge forgetting and performance damage in difficult tasks. These findings
offer fresh insights into how LLMs encode knowledge in a task-sensitive manner,
pave future research direction in model pruning, and open avenues for
task-aware conditional computation during inference.
","2023-10-05","2310.02277v1.pdf"
"2310.02304","Eric Zelikman","Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman Kalai","Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation","","","","","cs.CL cs.AI cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Several recent advances in AI systems (e.g., Tree-of-Thoughts and
Program-Aided Language Models) solve problems by providing a ""scaffolding""
program that structures multiple calls to language models to generate better
outputs. A scaffolding program is written in a programming language such as
Python. In this work, we use a language-model-infused scaffolding program to
improve itself. We start with a seed ""improver"" that improves an input program
according to a given utility function by querying a language model several
times and returning the best solution. We then run this seed improver to
improve itself. Across a small set of downstream tasks, the resulting improved
improver generates programs with significantly better performance than its seed
improver. Afterward, we analyze the variety of self-improvement strategies
proposed by the language model, including beam search, genetic algorithms, and
simulated annealing. Since the language models themselves are not altered, this
is not full recursive self-improvement. Nonetheless, it demonstrates that a
modern language model, GPT-4 in our proof-of-concept experiments, is capable of
writing code that can call itself to improve itself. We critically consider
concerns around the development of self-improving technologies and evaluate the
frequency with which the generated code bypasses a sandbox.
","2023-10-05","2310.02304v1.pdf"
"2310.02374","Mahyar Abbasian","Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain","Conversational Health Agents: A Personalized LLM-Powered Agent Framework","23 pages, 5 figures, journal paper","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Conversational Health Agents (CHAs) are interactive systems designed to
enhance personal healthcare services by engaging in empathetic conversations
and processing multimodal data. While current CHAs, especially those utilizing
Large Language Models (LLMs), primarily focus on conversation, they often need
more comprehensive agent capabilities. This limitation includes accessing
personal user health data from wearables, ubiquitous data collection sources,
and electronic health records, integrating the latest published health
insights, and connecting with established multimodal data analysis tools. In
this paper, we propose an LLM-powered framework to empower CHAs to generate a
personalized response for users' healthcare queries. This framework provides
critical thinking, knowledge acquisition, and problem-solving abilities by
integrating healthcare data sources, enabling multilingual and multimodal
conversations, and interacting with various user data analysis tools. We
illustrate the framework's proficiency in handling complex healthcare tasks via
a case study on stress level estimation, showcasing the agent's cognitive and
operational capabilities.
","2023-10-24","2310.02374v1.pdf"
"2310.02391","Avishek Bose","Avishek Joey Bose, Tara Akhound-Sadegh, Kilian Fatras, Guillaume
  Huguet, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym
  Korablyov, Michael Bronstein, and Alexander Tong","SE(3)-Stochastic Flow Matching for Protein Backbone Generation","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The computational design of novel protein structures has the potential to
impact numerous scientific disciplines greatly. Toward this goal, we introduce
$\text{FoldFlow}$ a series of novel generative models of increasing modeling
power based on the flow-matching paradigm over $3\text{D}$ rigid motions --
i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein
backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free
approach to learning deterministic continuous-time dynamics and matching
invariant target distributions on $\text{SE(3)}$. We next accelerate training
by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$,
leading to the construction of both more simple and stable flows. Finally, we
design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free
training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our
family of $\text{FoldFlow}$ generative models offer several key advantages over
previous approaches to the generative modeling of proteins: they are more
stable and faster to train than diffusion-based approaches, and our models
enjoy the ability to map any invariant source distribution to any invariant
target distribution over $\text{SE(3)}$. Empirically, we validate our FoldFlow
models on protein backbone generation of up to $300$ amino acids leading to
high-quality designable, diverse, and novel samples.
","2023-10-23","2310.02391v1.pdf"
"2310.02401","Yingqian Cui","Yingqian Cui, Jie Ren, Yuping Lin, Han Xu, Pengfei He, Yue Xing, Wenqi
  Fan, Hui Liu, Jiliang Tang","FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image
  Diffusion Models","","","","","cs.CV cs.CR","http://creativecommons.org/licenses/by/4.0/","  Text-to-image generative models based on latent diffusion models (LDM) have
demonstrated their outstanding ability in generating high-quality and
high-resolution images according to language prompt. Based on these powerful
latent diffusion models, various fine-tuning methods have been proposed to
achieve the personalization of text-to-image diffusion models such as artistic
style adaptation and human face transfer. However, the unauthorized usage of
data for model personalization has emerged as a prevalent concern in relation
to copyright violations. For example, a malicious user may use the fine-tuning
technique to generate images which mimic the style of a painter without his/her
permission. In light of this concern, we have proposed FT-Shield, a
watermarking approach specifically designed for the fine-tuning of
text-to-image diffusion models to aid in detecting instances of infringement.
We develop a novel algorithm for the generation of the watermark to ensure that
the watermark on the training images can be quickly and accurately transferred
to the generated images of text-to-image diffusion models. A watermark will be
detected on an image by a binary watermark detector if the image is generated
by a model that has been fine-tuned using the protected watermarked images.
Comprehensive experiments were conducted to validate the effectiveness of
FT-Shield.
","2023-10-05","2310.02401v1.pdf"
"2310.02405","Sajjad Mohaghegh","Sajad Mohaghegh, Mohammad Amin Ramezan Dehnavi, Golnoosh
  Abdollahinejad, Matin Hashemi","PCGPT: Procedural Content Generation via Transformers","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  The paper presents the PCGPT framework, an innovative approach to procedural
content generation (PCG) using offline reinforcement learning and transformer
networks. PCGPT utilizes an autoregressive model based on transformers to
generate game levels iteratively, addressing the challenges of traditional PCG
methods such as repetitive, predictable, or inconsistent content. The framework
models trajectories of actions, states, and rewards, leveraging the
transformer's self-attention mechanism to capture temporal dependencies and
causal relationships. The approach is evaluated in the Sokoban puzzle game,
where the model predicts items that are needed with their corresponding
locations. Experimental results on the game Sokoban demonstrate that PCGPT
generates more complex and diverse game content. Interestingly, it achieves
these results in significantly fewer steps compared to existing methods,
showcasing its potential for enhancing game design and online content
generation. Our model represents a new PCG paradigm which outperforms previous
methods.
","2023-10-05","2310.02405v1.pdf"
"2310.02409","Guanghui Qin","Guanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van
  Durme","Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only
  Language Models","Preprint. 15 pages and 7 figures","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Standard Transformer-based language models (LMs) scale poorly to long
contexts. We propose a solution based on dynamic contextual compression, which
extends the Nugget approach of Qin & Van Durme (2023) from BERT-like frameworks
to decoder-only LMs. Our method models history as compressed ""nuggets"" which
are trained to allow for reconstruction, and it can be initialized with
off-the-shelf models such as LLaMA. We demonstrate through experiments in
language modeling, question answering, and summarization that Nugget2D retains
capabilities in these tasks, while drastically reducing the overhead during
decoding in terms of time and space. For example, in the experiments of
autoencoding, Nugget2D can shrink context at a 20x compression ratio with a
BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
","2023-10-05","2310.02409v1.pdf"
"2310.02421","Sia Gholami","Sia Gholami, Marwan Omar","Can a student Large Language Model perform as well as it's teacher?","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The burgeoning complexity of contemporary deep learning models, while
achieving unparalleled accuracy, has inadvertently introduced deployment
challenges in resource-constrained environments. Knowledge distillation, a
technique aiming to transfer knowledge from a high-capacity ""teacher"" model to
a streamlined ""student"" model, emerges as a promising solution to this dilemma.
This paper provides a comprehensive overview of the knowledge distillation
paradigm, emphasizing its foundational principles such as the utility of soft
labels and the significance of temperature scaling. Through meticulous
examination, we elucidate the critical determinants of successful distillation,
including the architecture of the student model, the caliber of the teacher,
and the delicate balance of hyperparameters. While acknowledging its profound
advantages, we also delve into the complexities and challenges inherent in the
process. Our exploration underscores knowledge distillation's potential as a
pivotal technique in optimizing the trade-off between model performance and
deployment efficiency.
","2023-10-05","2310.02421v1.pdf"
"2310.02424","Amanda Swearngin","Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue Jiang,
  Jeffrey Nichols","AXNav: Replaying Accessibility Tests from Natural Language","22 pages, 7 figures","","","","cs.HC cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Developers and quality assurance testers often rely on manual testing to test
accessibility features throughout the product lifecycle. Unfortunately, manual
testing can be tedious, often has an overwhelming scope, and can be difficult
to schedule amongst other development milestones. Recently, Large Language
Models (LLMs) have been used for a variety of tasks including automation of
UIs, however to our knowledge no one has yet explored their use in controlling
assistive technologies for the purposes of supporting accessibility testing. In
this paper, we explore the requirements of a natural language based
accessibility testing workflow, starting with a formative study. From this we
build a system that takes as input a manual accessibility test (e.g., ``Search
for a show in VoiceOver'') and uses an LLM combined with pixel-based UI
Understanding models to execute the test and produce a chaptered, navigable
video. In each video, to help QA testers we apply heuristics to detect and flag
accessibility issues (e.g., Text size not increasing with Large Text enabled,
VoiceOver navigation loops). We evaluate this system through a 10 participant
user study with accessibility QA professionals who indicated that the tool
would be very useful in their current work and performed tests similarly to how
they would manually test the features. The study also reveals insights for
future work on using LLMs for accessibility testing.
","2023-10-17","2310.02424v1.pdf"
"2310.02439","Naiming (Lucy) Liu","Naiming Liu, Shashank Sonkar, Zichao Wang, Simon Woodhead, Richard G.
  Baraniuk","Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of
  Large Language Models with Misconceptions","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We propose novel evaluations for mathematical reasoning capabilities of Large
Language Models (LLMs) based on mathematical misconceptions. Our primary
approach is to simulate LLMs as a novice learner and an expert tutor, aiming to
identify the incorrect answer to math question resulted from a specific
misconception and to recognize the misconception(s) behind an incorrect answer,
respectively. Contrary to traditional LLMs-based mathematical evaluations that
focus on answering math questions correctly, our approach takes inspirations
from principles in educational learning sciences. We explicitly ask LLMs to
mimic a novice learner by answering questions in a specific incorrect manner
based on incomplete knowledge; and to mimic an expert tutor by identifying
misconception(s) corresponding to an incorrect answer to a question. Using
simple grade-school math problems, our experiments reveal that, while LLMs can
easily answer these questions correctly, they struggle to identify 1) the
incorrect answer corresponding to specific incomplete knowledge
(misconceptions); 2) the misconceptions that explain particular incorrect
answers. Our study indicates new opportunities for enhancing LLMs' math
reasoning capabilities, especially on developing robust student simulation and
expert tutoring models in the educational applications such as intelligent
tutoring systems.
","2023-10-05","2310.02439v1.pdf"
"2310.02446","Zheng-Xin Yong","Zheng-Xin Yong, Cristina Menghini and Stephen H. Bach","Low-Resource Languages Jailbreak GPT-4","","","","","cs.CL cs.AI cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.
","2023-10-05","2310.02446v1.pdf"
"2310.02451","Xiruo Ding","Xiruo Ding, Zhecheng Sheng, Meliha Yeti\c{s}gen, Serguei Pakhomov,
  Trevor Cohen","Backdoor Adjustment of Confounding by Provenance for Robust Text
  Classification of Multi-institutional Clinical Notes","Accepted in AMIA 2023 Annual Symposium","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Natural Language Processing (NLP) methods have been broadly applied to
clinical tasks. Machine learning and deep learning approaches have been used to
improve the performance of clinical NLP. However, these approaches require
sufficiently large datasets for training, and trained models have been shown to
transfer poorly across sites. These issues have led to the promotion of data
collection and integration across different institutions for accurate and
portable models. However, this can introduce a form of bias called confounding
by provenance. When source-specific data distributions differ at deployment,
this may harm model performance. To address this issue, we evaluate the utility
of backdoor adjustment for text classification in a multi-site dataset of
clinical notes annotated for mentions of substance abuse. Using an evaluation
framework devised to measure robustness to distributional shifts, we assess the
utility of backdoor adjustment. Our results indicate that backdoor adjustment
can effectively mitigate for confounding shift.
","2023-10-05","2310.02451v1.pdf"
"2310.02456","Brad Knox","W. Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson,
  Serena Booth, Anca Dragan, Peter Stone, Scott Niekum","Learning Optimal Advantage from Preferences and Mistaking it for Reward","8 pages (16 pages with references and appendix), 11 figures","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider algorithms for learning reward functions from human preferences
over pairs of trajectory segments, as used in reinforcement learning from human
feedback (RLHF). Most recent work assumes that human preferences are generated
based only upon the reward accrued within those segments, or their partial
return. Recent work casts doubt on the validity of this assumption, proposing
an alternative preference model based upon regret. We investigate the
consequences of assuming preferences are based upon partial return when they
actually arise from regret. We argue that the learned function is an
approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward
function. We find that if a specific pitfall is addressed, this incorrect
assumption is not particularly harmful, resulting in a highly shaped reward
function. Nonetheless, this incorrect usage of $\hat{A^*_r}$ is less desirable
than the appropriate and simpler approach of greedy maximization of
$\hat{A^*_r}$. From the perspective of the regret preference model, we also
provide a clearer interpretation of fine tuning contemporary large language
models with RLHF. This paper overall provides insight regarding why learning
under the partial return preference model tends to work so well in practice,
despite it conforming poorly to how humans give preferences.
","2023-10-05","2310.02456v1.pdf"
"2310.02457","Hannah Rose Kirk Miss","Hannah Rose Kirk, Bertie Vidgen, Paul R\""ottger, Scott A. Hale","The Empty Signifier Problem: Towards Clearer Paradigms for
  Operationalising ""Alignment"" in Large Language Models","","","","","cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we address the concept of ""alignment"" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.
","2023-10-05","2310.02457v1.pdf"
"2310.02462","Ifrah Idrees Ms","Ifrah Idrees, Tian Yun, Naveen Sharma, Yunxin Deng, Nakul Gopalan,
  George Konidaris, Stefanie Tellex","Improved Inference of Human Intent by Combining Plan Recognition and
  Language Feedback","Published in IROS 2023","","","","cs.RO cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Conversational assistive robots can aid people, especially those with
cognitive impairments, to accomplish various tasks such as cooking meals,
performing exercises, or operating machines. However, to interact with people
effectively, robots must recognize human plans and goals from noisy
observations of human actions, even when the user acts sub-optimally. Previous
works on Plan and Goal Recognition (PGR) as planning have used hierarchical
task networks (HTN) to model the actor/human. However, these techniques are
insufficient as they do not have user engagement via natural modes of
interaction such as language. Moreover, they have no mechanisms to let users,
especially those with cognitive impairments, know of a deviation from their
original plan or about any sub-optimal actions taken towards their goal. We
propose a novel framework for plan and goal recognition in partially observable
domains -- Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its
belief in human progress by asking clarification questions about noisy sensor
data and sub-optimal human actions. We evaluate the performance of D4GR over
two simulated domains -- kitchen and blocks domain. With language feedback and
the world state information in a hierarchical task model, we show that D4GR
framework for the highest sensor noise performs 1% better than HTN in goal
accuracy in both domains. For plan accuracy, D4GR outperforms by 4% in the
kitchen domain and 2% in the blocks domain in comparison to HTN. The ALWAYS-ASK
oracle outperforms our policy by 3% in goal recognition and 7%in plan
recognition. D4GR does so by asking 68% fewer questions than an oracle
baseline. We also demonstrate a real-world robot scenario in the kitchen
domain, validating the improved plan and goal recognition of D4GR in a
realistic setting.
","2023-10-05","2310.02462v1.pdf"
"2310.02469","Yijia Xiao","Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo,
  Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng","Large Language Models Can Be Good Privacy Protection Learners","20 pages, 4 figures, 8 tables","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.
","2023-10-10","2310.02469v1.pdf"
"2310.02473","Sepidehsadat Hosseini","Sepidehsadat Hosseini, Mengyao Zhai, Hossein Hajimirsadegh, Frederick
  Tung","Prompting-based Efficient Temporal Domain Generalization","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Machine learning traditionally assumes that training and testing data are
distributed independently and identically. However, in many real-world
settings, the data distribution can shift over time, leading to poor
generalization of trained models in future time periods. Our paper presents a
novel prompting-based approach to temporal domain generalization that is
parameter-efficient, time-efficient, and does not require access to the target
domain data (i.e., unseen future time periods) during training. Our method
adapts a target pre-trained model to temporal drift by learning global prompts,
domain-specific prompts, and drift-aware prompts that capture underlying
temporal dynamics. It is compatible across diverse tasks, such as
classification, regression, and time series forecasting, and sets a new
state-of-the-art benchmark in temporal domain generalization. The code
repository will be publicly shared.
","2023-10-05","2310.02473v1.pdf"
"2310.02489","Yiming Wang","Yiming Wang, Jinyu Li","ResidualTransformer: Residual Low-rank Learning with Weight-sharing for
  Transformer Layers","Submitted to IEEE ICASSP 2024. 5 pages, 1 figure","","","","cs.CL cs.LG cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
","2023-10-05","2310.02489v1.pdf"
"2310.02511","Yilin Shen","Yilin Shen","Prepare Ansatz for VQE with Diffusion Model","","","","","quant-ph","http://creativecommons.org/licenses/by/4.0/","  The Variational Quantum Eigensolver (VQE) is a quantum algorithm used to find
the ground state energy of a given Hamiltonian. The key component of VQE is the
ansatz, which is a trial wavefunction that the algorithm uses to approximate
the ground state. Designing a good ansatz can significantly improve the
performance of the VQE algorithm. Typical ansatz structures include the Unitary
Coupled Cluster (UCC) ansatz and the Hardware-Efficient Ansatz (HEA). The
primary distinction between these two structures lies in their dependence on
the problem and hardware. The UCC ansatz is tailored to the target Hamiltonian,
whereas the HEA is determined by the hardware topology. We believe that an
intermediate approach could combine the benefits of the UCC ansatz while
introducing additional parameters to increase its expressiveness and
capability. In this paper, we propose utilizing a diffusion model to facilitate
the generation of ansatz. We create a sequence of UCC ansatzes as training data
and input this data into the diffusion model. The model then generates quantum
circuits that have a similar structure to the input data. These quantum
circuits are subsequently tested using a VQE task to evaluate their
performance. This approach provides a systematic method for generating ansatzes
that maintain a similar structure while incorporating additional parameters,
enhancing their expressiveness and capability. We validate on small molecules
that the diffusion model can help prepare ansatz circuits for VQE.
","2023-10-05","2310.02511v1.pdf"
"2310.02527","Tao Feng","Tao Feng, Zifeng Wang, Jimeng Sun","CITING: Large Language Models Create Curriculum for Instruction Tuning","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent advancement of large language models (LLMs) has been achieved
through a combo of instruction tuning and human alignment. However, building
manually crafted instruction datasets and performing human alignment become the
bottleneck for scaling the development of LLMs. In this paper, we exploit the
idea of leveraging AI models in lieu of humans as the teacher to train student
LLMs. Our method is inspired by how human students refine their writing skills
by following the rubrics and learning from the revisions offered by their
tutors. Specifically, we employ a teacher LLM to create a curriculum for
instruction tuning of the student LLM, namely Curriculum Instruction TunING
(CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics
for evaluating the answers corresponding to various types of questions, and (2)
the student LLM learns to follow the rubrics and perform self-correction from
the revision made by the teacher. We further iteratively carry out it to embody
the procedure of CITING. We compare CITING to a series of state-of-the-art
baselines on four datasets. Our method demonstrates strong improvement in terms
of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically,
it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1%
over RRHF, and 76.3% over RAFT, respectively.
","2023-10-05","2310.02527v1.pdf"
"2310.02529","Mingyu Derek Ma","Mingyu Derek Ma, Alexander K. Taylor, Nuan Wen, Yanchen Liu, Po-Nien
  Kung, Wenna Qin, Shicheng Wen, Azure Zhou, Diyi Yang, Xuezhe Ma, Nanyun Peng,
  Wei Wang","MIDDAG: Where Does Our News Go? Investigating Information Diffusion via
  Community-Level Information Pathways","System demo video: info-pathways.github.io","","","","cs.SI cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  We present MIDDAG, an intuitive, interactive system that visualizes the
information propagation paths on social media triggered by COVID-19-related
news articles accompanied by comprehensive insights including user/community
susceptibility level, as well as events and popular opinions raised by the
crowd while propagating the information. Besides discovering information flow
patterns among users, we construct communities among users and develop the
propagation forecasting capability, enabling tracing and understanding of how
information is disseminated at a higher level.
","2023-10-05","2310.02529v1.pdf"
"2310.02556","Soroush Abbasi Koohpayegani","Soroush Abbasi Koohpayegani, KL Navaneet, Parsa Nooralinejad, Soheil
  Kolouri, Hamed Pirsiavash","NOLA: Networks as Linear Combination of Low Rank Random Basis","Our code is available here: https://github.com/UCDvision/NOLA","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have recently gained popularity due to their
impressive few-shot performance across various downstream tasks. However,
fine-tuning all parameters and storing a unique model for each downstream task
or domain becomes impractical because of the massive size of checkpoints (e.g.,
350GB in GPT-3). Current literature, such as LoRA, showcases the potential of
low-rank modifications to the original weights of an LLM, enabling efficient
adaptation and storage for task-specific models. These methods can reduce the
number of parameters needed to fine-tune an LLM by several orders of magnitude.
Yet, these methods face two primary limitations: 1) the parameter reduction is
lower-bounded by the rank one decomposition, and 2) the extent of reduction is
heavily influenced by both the model architecture and the chosen rank. For
instance, in larger models, even a rank one decomposition might exceed the
number of parameters truly needed for adaptation. In this paper, we introduce
NOLA, which overcomes the rank one lower bound present in LoRA. It achieves
this by re-parameterizing the low-rank matrices in LoRA using linear
combinations of randomly generated matrices (basis) and optimizing the linear
mixture coefficients only. This approach allows us to decouple the number of
trainable parameters from both the choice of rank and the network architecture.
We present adaptation results using GPT-2 and ViT in natural language and
computer vision tasks. NOLA performs as well as, or better than models with
equivalent parameter counts. Furthermore, we demonstrate that we can halve the
parameters in larger models compared to LoRA with rank one, without sacrificing
performance.
","2023-10-05","2310.02556v1.pdf"
"2310.02567","Oscar Ma\~nas","Oscar Ma\~nas, Benno Krojer, Aishwarya Agrawal","Improving Automatic VQA Evaluation Using Large Language Models","","","","","cs.CV cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.
","2023-10-05","2310.02567v1.pdf"
"2310.02569","Zejun Li","Zejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang,
  Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen, Xuanjing Huang, Zhongyu
  Wei","ReForm-Eval: Evaluating Large Vision Language Models via Unified
  Re-Formulation of Task-Oriented Benchmarks","38 pages, 11 figures, 24 tables","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent years have witnessed remarkable progress in the development of large
vision-language models (LVLMs). Benefiting from the strong language backbones
and efficient cross-modal alignment strategies, LVLMs exhibit surprising
capabilities to perceive visual signals and perform visually grounded
reasoning. However, the capabilities of LVLMs have not been comprehensively and
quantitatively evaluate. Most existing multi-modal benchmarks require
task-oriented input-output formats, posing great challenges to automatically
assess the free-form text output of LVLMs. To effectively leverage the
annotations available in existing benchmarks and reduce the manual effort
required for constructing new benchmarks, we propose to re-formulate existing
benchmarks into unified LVLM-compatible formats. Through systematic data
collection and reformulation, we present the ReForm-Eval benchmark, offering
substantial data for evaluating various capabilities of LVLMs. Based on
ReForm-Eval, we conduct extensive experiments, thoroughly analyze the strengths
and weaknesses of existing LVLMs, and identify the underlying factors. Our
benchmark and evaluation framework will be open-sourced as a cornerstone for
advancing the development of LVLMs.
","2023-10-18","2310.02569v1.pdf"
"2310.02575","Enneng Yang","Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei
  Wang, Dacheng Tao","AdaMerging: Adaptive Model Merging for Multi-Task Learning","","","","","cs.LG cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multi-task learning (MTL) aims to empower a model to tackle multiple tasks
simultaneously. A recent development known as task arithmetic has revealed that
several models, each fine-tuned for distinct tasks, can be directly merged into
a single model to execute MTL without necessitating a retraining process using
the initial training data. Nevertheless, this direct addition of models often
leads to a significant deterioration in the overall performance of the merged
model. This decline occurs due to potential conflicts and intricate
correlations among the multiple tasks. Consequently, the challenge emerges of
how to merge pre-trained models more effectively without using their original
training data. This paper introduces an innovative technique called Adaptive
Model Merging (AdaMerging). This approach aims to autonomously learn the
coefficients for model merging, either in a task-wise or layer-wise manner,
without relying on the original training data. Specifically, our AdaMerging
method operates as an automatic, unsupervised task arithmetic scheme. It
leverages entropy minimization on unlabeled test samples from the multi-task
setup as a surrogate objective function to iteratively refine the merging
coefficients of the multiple models. Our experimental findings across eight
tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared
to the current state-of-the-art task arithmetic merging scheme, AdaMerging
showcases a remarkable 11\% improvement in performance. Notably, AdaMerging
also exhibits superior generalization capabilities when applied to unseen
downstream tasks. Furthermore, it displays a significantly enhanced robustness
to data distribution shifts that may occur during the testing phase.
","2023-10-05","2310.02575v1.pdf"
"2310.02609","Huaijin Wang","Wei Chen, Huaijin Wang, Weixi Gu, Shuai Wang","RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz
  Testing","Information Security Conference 2023","","","","cs.CR","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Securing operating system (OS) kernel is one central challenge in today's
cyber security landscape. The cutting-edge testing technique of OS kernel is
software fuzz testing. By mutating the program inputs with random variations
for iterations, fuzz testing aims to trigger program crashes and hangs caused
by potential bugs that can be abused by the inputs. To achieve high OS code
coverage, the de facto OS fuzzer typically composes system call traces as the
input seed to mutate and to interact with OS kernels. Hence, quality and
diversity of the employed system call traces become the prominent factor to
decide the effectiveness of OS fuzzing. However, these system call traces to
date are generated with hand-coded rules, or by analyzing system call logs of
OS utility programs. Our observation shows that such system call traces can
only subsume common usage scenarios of OS system calls, and likely omit hidden
bugs.
  In this research, we propose a deep reinforcement learning-based solution,
called RLTrace, to synthesize diverse and comprehensive system call traces as
the seed to fuzz OS kernels. During model training, the deep learning model
interacts with OS kernels and infers optimal system call traces w.r.t. our
learning goal -- maximizing kernel code coverage. Our evaluation shows that
RLTrace outperforms other seed generators by producing more comprehensive
system call traces, subsuming system call corner usage cases and subtle
dependencies. By feeding the de facto OS fuzzer, SYZKALLER, with system call
traces synthesized by RLTrace, we show that SYZKALLER can achieve higher code
coverage for testing Linux kernels. Furthermore, RLTrace found one
vulnerability in the Linux kernel (version 5.5-rc6), which is publicly unknown
to the best of our knowledge by the time of writing.
","2023-10-05","2310.02609v1.pdf"
"2310.02635","Weirui Ye","Weirui Ye, Yunsheng Zhang, Mengchen Wang, Shengjie Wang, Xianfan Gu,
  Pieter Abbeel, Yang Gao","Foundation Reinforcement Learning: towards Embodied Generalist Agents
  with Foundation Prior Assistance","","","","","cs.RO cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, people have shown that large-scale pre-training from internet-scale
data is the key to building generalist models, as witnessed in NLP. To build
embodied generalist agents, we and many other researchers hypothesize that such
foundation prior is also an indispensable component. However, it is unclear
what is the proper concrete form to represent those embodied foundation priors
and how they should be used in the downstream task. In this paper, we propose
an intuitive and effective set of embodied priors that consist of foundation
policy, value, and success reward. The proposed priors are based on the
goal-conditioned MDP. To verify their effectiveness, we instantiate an
actor-critic method assisted by the priors, called Foundation Actor-Critic
(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since
it completely relies on embodied foundation priors to explore, learn and
reinforce. The benefits of FRL are threefold. (1) Sample efficient. With
foundation priors, FAC learns significantly faster than traditional RL. Our
evaluation on the Meta-World has proved that FAC can achieve 100% success rates
for 7/8 tasks under less than 200k frames, which outperforms the baseline
method with careful manual-designed rewards under 1M frames. (2) Robust to
noisy priors. Our method tolerates the unavoidable noise in embodied foundation
models. We show that FAC works well even under heavy noise or quantization
errors. (3) Minimal human intervention: FAC completely learns from the
foundation priors, without the need of human-specified dense reward, or
providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our
FRL framework could enable the future robot to autonomously explore and learn
without human intervention in the physical world. In summary, our proposed FRL
is a novel and powerful learning paradigm, towards achieving embodied
generalist agents.
","2023-10-11","2310.02635v1.pdf"
"2310.02655","Francesco Marchiori","Filippo Perrina, Francesco Marchiori, Mauro Conti, Nino Vincenzo Verde","AGIR: Automating Cyber Threat Intelligence Reporting with Natural
  Language Generation","10 pages, 7 figures","","","","cs.CR cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk
management strategies. As the volume of CTI reports continues to surge, the
demand for automated tools to streamline report generation becomes increasingly
apparent. While Natural Language Processing techniques have shown potential in
handling text data, they often struggle to address the complexity of diverse
data sources and their intricate interrelationships. Moreover, established
paradigms like STIX have emerged as de facto standards within the CTI
community, emphasizing the formal categorization of entities and relations to
facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic
Generation of Intelligence Reports), a transformative Natural Language
Generation tool specifically designed to address the pressing challenges in the
realm of CTI reporting. AGIR's primary objective is to empower security
analysts by automating the labor-intensive task of generating comprehensive
intelligence reports from formal representations of entity graphs. AGIR
utilizes a two-stage pipeline by combining the advantages of template-based
approaches and the capabilities of Large Language Models such as ChatGPT. We
evaluate AGIR's report generation capabilities both quantitatively and
qualitatively. The generated reports accurately convey information expressed
through formal language, achieving a high recall value (0.99) without
introducing hallucination. Furthermore, we compare the fluency and utility of
the reports with state-of-the-art approaches, showing how AGIR achieves higher
scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.
By using our tool, we estimate that the report writing time is reduced by more
than 40%, therefore streamlining the CTI production of any organization and
contributing to the automation of several CTI tasks.
","2023-10-05","2310.02655v1.pdf"
"2310.02664","Chao Du","Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, Ye Wang","On Memorization in Diffusion Models","","","","","cs.LG cs.AI cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Due to their capacity to generate novel and high-quality samples, diffusion
models have attracted significant research interest in recent years. Notably,
the typical training objective of diffusion models, i.e., denoising score
matching, has a closed-form optimal solution that can only generate training
data replicating samples. This indicates that a memorization behavior is
theoretically expected, which contradicts the common generalization ability of
state-of-the-art diffusion models, and thus calls for a deeper understanding.
Looking into this, we first observe that memorization behaviors tend to occur
on smaller-sized datasets, which motivates our definition of effective model
memorization (EMM), a metric measuring the maximum size of training data at
which a learned diffusion model approximates its theoretical optimum. Then, we
quantify the impact of the influential factors on these memorization behaviors
in terms of EMM, focusing primarily on data distribution, model configuration,
and training procedure. Besides comprehensive empirical results identifying the
influential factors, we surprisingly find that conditioning training data on
uninformative random labels can significantly trigger the memorization in
diffusion models. Our study holds practical significance for diffusion model
users and offers clues to theoretical research in deep generative models. Code
is available at https://github.com/sail-sg/DiffMemorize.
","2023-10-05","2310.02664v1.pdf"
"2310.02727","Bernhard Nessler","Bernhard Nessler, Thomas Doms, Sepp Hochreiter","Functional trustworthiness of AI systems by statistically valid testing","Position paper to the current regulation and standardization effort
  of AI in Europe","","","","stat.ML cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The authors are concerned about the safety, health, and rights of the
European citizens due to inadequate measures and procedures required by the
current draft of the EU Artificial Intelligence (AI) Act for the conformity
assessment of AI systems. We observe that not only the current draft of the EU
AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have
resorted to the position that real functional guarantees of AI systems
supposedly would be unrealistic and too complex anyways. Yet enacting a
conformity assessment procedure that creates the false illusion of trust in
insufficiently assessed AI systems is at best naive and at worst grossly
negligent. The EU AI Act thus misses the point of ensuring quality by
functional trustworthiness and correctly attributing responsibilities.
  The trustworthiness of an AI decision system lies first and foremost in the
correct statistical testing on randomly selected samples and in the precision
of the definition of the application domain, which enables drawing samples in
the first place. We will subsequently call this testable quality functional
trustworthiness. It includes a design, development, and deployment that enables
correct statistical testing of all relevant functions.
  We are firmly convinced and advocate that a reliable assessment of the
statistical functional properties of an AI system has to be the indispensable,
mandatory nucleus of the conformity assessment. In this paper, we describe the
three necessary elements to establish a reliable functional trustworthiness,
i.e., (1) the definition of the technical distribution of the application, (2)
the risk-based minimum performance requirements, and (3) the statistically
valid testing based on independent random samples.
","2023-10-05","2310.02727v1.pdf"
"2310.02739","Hussam Azzuni","Hussam Azzuni, Sharim Jamal, Abdulmotaleb Elsaddik","uTalk: Bridging the Gap Between Humans and AI","","","","","cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have revolutionized various industries by
harnessing their power to improve productivity and facilitate learning across
different fields. One intriguing application involves combining LLMs with
visual models to create a novel approach to Human-Computer Interaction. The
core idea behind this system is to develop an interactive platform that allows
the general public to leverage the capabilities of ChatGPT in their daily
lives. This is achieved by integrating several technologies such as Whisper,
ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking
head system, SadTalker, resulting in uTalk, an intelligent AI system. Users
will be able to converse with this portrait, receiving answers to whatever
questions they have in mind. Additionally, they could use uTalk for content
generation by providing an input and their image. This system is hosted on
Streamlit, where the user will initially be requested to provide an image to
serve as their AI assistant. Then, users could choose whether to have a
conversation or generate content based on their preferences. Either way, it
starts by providing an input, where a set of operations will be done, and the
avatar will provide a precise response. The paper discusses how SadTalker is
optimized to improve its running time by 27.72% based on 25FPS generated
videos. In addition, the system's initial performance, uTalk, improved further
by 9.8% after SadTalker was integrated and parallelized with Streamlit.
","2023-10-05","2310.02739v1.pdf"
"2310.02743","Thomas Coste","Thomas Coste, Usman Anwar, Robert Kirk, David Krueger","Reward Model Ensembles Help Mitigate Overoptimization","9 pages, 12 figures (excluding appendix). Submitted to ICLR 2024","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning from human feedback (RLHF) is a standard approach for
fine-tuning large language models to follow instructions. As part of this
process, learned reward models are used to approximately model human
preferences. However, as imperfect representations of the ""true"" reward, these
learned reward models are susceptible to \textit{overoptimization}. Gao et al.
(2023) studied this phenomenon in a synthetic human feedback setup with a
significantly larger ""gold"" reward model acting as the true reward (instead of
humans) and showed that overoptimization remains a persistent problem
regardless of the size of the proxy reward model and training data used. Using
a similar setup, we conduct a systematic study to evaluate the efficacy of
using ensemble-based conservative optimization objectives, specifically
worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for
mitigating reward model overoptimization when using two optimization methods:
(a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We
additionally extend the setup of Gao et al. (2023) to include 25% label noise
to better mirror real-world conditions. Both with and without label noise, we
find that conservative optimization practically eliminates overoptimization and
improves performance by up to 70% for BoN sampling. For PPO, ensemble-based
conservative optimization always reduces overoptimization and outperforms
single reward model optimization. Moreover, combining it with a small KL
penalty successfully prevents overoptimization at no performance cost. Overall,
our results demonstrate that ensemble-based conservative optimization can
effectively counter overoptimization.
","2023-10-05","2310.02743v1.pdf"
"2310.02759","Raja CSP Raman","Bagiya Lakshmi S, Sanjjushri Varshini R, Rohith Mahadevan, Raja CSP
  Raman","Comparative Study and Framework for Automated Summariser Evaluation:
  LangChain and Hybrid Algorithms","","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  Automated Essay Score (AES) is proven to be one of the cutting-edge
technologies. Scoring techniques are used for various purposes. Reliable scores
are calculated based on influential variables. Such variables can be computed
by different methods based on the domain. The research is concentrated on the
user's understanding of a given topic. The analysis is based on a scoring index
by using Large Language Models. The user can then compare and contrast the
understanding of a topic that they recently learned. The results are then
contributed towards learning analytics and progression is made for enhancing
the learning ability. In this research, the focus is on summarizing a PDF
document and gauging a user's understanding of its content. The process
involves utilizing a Langchain tool to summarize the PDF and extract the
essential information. By employing this technique, the research aims to
determine how well the user comprehends the summarized content.
","2023-10-05","2310.02759v1.pdf"
"2310.02777","Chenwei Wu","Chenwei Wu, Li Erran Li, Stefano Ermon, Patrick Haffner, Rong Ge,
  Zaiwei Zhang","The Role of Linguistic Priors in Measuring Compositional Generalization
  of Vision-Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Compositionality is a common property in many modalities including natural
languages and images, but the compositional generalization of multi-modal
models is not well-understood. In this paper, we identify two sources of
visual-linguistic compositionality: linguistic priors and the interplay between
images and texts. We show that current attempts to improve compositional
generalization rely on linguistic priors rather than on information in the
image. We also propose a new metric for compositionality without such
linguistic priors.
","2023-10-05","2310.02777v1.pdf"
"2310.02778","Rui Yang","Rui Yang, Edison Marrese-Taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen,
  Irene Li","Integrating UMLS Knowledge into Large Language Models for Medical
  Question Answering","12 pages, 3 figures","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.
","2023-10-16","2310.02778v1.pdf"
"2310.02784","Bilge Acun","Samuel Hsia, Alicia Golden, Bilge Acun, Newsha Ardalani, Zachary
  DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu","MAD Max Beyond Single-Node: Enabling Large Machine Learning Model
  Acceleration on Distributed Systems","","","","","cs.DC cs.AR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Training and deploying large machine learning (ML) models is time-consuming
and requires significant distributed computing infrastructures. Based on
real-world large model training on datacenter-scale infrastructures, we show
14~32% of all GPU hours are spent on communication with no overlapping
computation. To minimize the outstanding communication latency, in this work,
we develop an agile performance modeling framework to guide parallelization and
hardware-software co-design strategies. Using the suite of real-world large ML
models on state-of-the-art GPU training hardware, we demonstrate 2.24x and
5.27x throughput improvement potential for pre-training and inference
scenarios, respectively.
","2023-10-19","2310.02784v1.pdf"
"2310.02804","Maryam Fazel-Zarandi","Peifang Wang and Olga Golovneva and Armen Aghajanyan and Xiang Ren and
  Muhao Chen and Asli Celikyilmaz and Maryam Fazel-Zarandi","DOMINO: A Dual-System for Multi-step Visual Language Reasoning","","","","","cs.CL cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Visual language reasoning requires a system to extract text or numbers from
information-dense images like charts or plots and perform logical or arithmetic
reasoning to arrive at an answer. To tackle this task, existing work relies on
either (1) an end-to-end vision-language model trained on a large amount of
data, or (2) a two-stage pipeline where a captioning model converts the image
into text that is further read by another large language model to deduce the
answer. However, the former approach forces the model to answer a complex
question with one single step, and the latter approach is prone to inaccurate
or distracting information in the converted text that can confuse the language
model. In this work, we propose a dual-system for multi-step multimodal
reasoning, which consists of a ""System-1"" step for visual information
extraction and a ""System-2"" step for deliberate reasoning. Given an input,
System-2 breaks down the question into atomic sub-steps, each guiding System-1
to extract the information required for reasoning from the image. Experiments
on chart and plot datasets show that our method with a pre-trained System-2
module performs competitively compared to prior work on in- and
out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on
only a small amount of data on multi-step reasoning, the accuracy of our method
is further improved and surpasses the best fully-supervised end-to-end approach
by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging
dataset with human-authored questions.
","2023-10-05","2310.02804v1.pdf"
"2310.02832","Fran Jeleni\'c","Fran Jeleni\'c, Josip Juki\'c, Martin Tutek, Mate Puljiz, Jan
  \v{S}najder","Out-of-Distribution Detection by Leveraging Between-Layer Transformation
  Smoothness","","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  Effective OOD detection is crucial for reliable machine learning models, yet
most current methods are limited in practical use due to requirements like
access to training data or intervention in training. We present a novel method
for detecting OOD data in deep neural networks based on transformation
smoothness between intermediate layers of a network (BLOOD), which is
applicable to pre-trained models without access to training data. BLOOD
utilizes the tendency of between-layer representation transformations of
in-distribution (ID) data to be smoother than the corresponding transformations
of OOD data, a property that we also demonstrate empirically for Transformer
networks. We evaluate BLOOD on several text classification tasks with
Transformer networks and demonstrate that it outperforms methods with
comparable resource requirements. Our analysis also suggests that when learning
simpler tasks, OOD data transformations maintain their original sharpness,
whereas sharpness increases with more complex tasks.
","2023-10-17","2310.02832v1.pdf"
"2310.02842","Chen Dun","Chen Dun, Mirian Hipolito Garcia, Guoqing Zheng, Ahmed Hassan
  Awadallah, Anastasios Kyrillidis, Robert Sim","Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task
  Adaptation","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training ""interference"" in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.
","2023-10-09","2310.02842v1.pdf"
"2310.02896","Alex Kontorovich","Alex Kontorovich","Notes on a Path to AI Assistance in Mathematical Reasoning","7 pages","","","","math.HO cs.AI","http://creativecommons.org/licenses/by/4.0/","  These informal notes are based on the author's lecture at the National
Academies of Science, Engineering, and Mathematics workshop on ""AI to Assist
Mathematical Reasoning"" in June 2023. The goal is to think through a path by
which we might arrive at AI that is useful for the research mathematician.
","2023-10-05","2310.02896v1.pdf"
"2310.02897","Koren Abitbul","Koren Abitbul, Yehuda Dar","Recovery of Training Data from Overparameterized Autoencoders: An
  Inverse Problem Perspective","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the recovery of training data from overparameterized autoencoder
models. Given a degraded training sample, we define the recovery of the
original sample as an inverse problem and formulate it as an optimization task.
In our inverse problem, we use the trained autoencoder to implicitly define a
regularizer for the particular training dataset that we aim to retrieve from.
We develop the intricate optimization task into a practical method that
iteratively applies the trained autoencoder and relatively simple computations
that estimate and address the unknown degradation operator. We evaluate our
method for blind inpainting where the goal is to recover training images from
degradation of many missing pixels in an unknown pattern. We examine various
deep autoencoder architectures, such as fully connected and U-Net (with various
nonlinearities and at diverse train loss values), and show that our method
significantly outperforms previous methods for training data recovery from
autoencoders. Importantly, our method greatly improves the recovery performance
also in settings that were previously considered highly challenging, and even
impractical, for such retrieval.
","2023-10-05","2310.02897v1.pdf"
"2310.02902","Santiago Miret","Raj Ghugare, Santiago Miret, Adriana Hugessen, Mariano Phielipp, Glen
  Berseth","Searching for High-Value Molecules Using Reinforcement Learning and
  Transformers","","","","","cs.LG cond-mat.mtrl-sci cs.AI","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning (RL) over text representations can be effective for
finding high-value policies that can search over graphs. However, RL requires
careful structuring of the search space and algorithm design to be effective in
this challenge. Through extensive experiments, we explore how different design
choices for text grammar and algorithmic choices for training can affect an RL
policy's ability to generate molecules with desired properties. We arrive at a
new RL-based molecular design algorithm (ChemRLformer) and perform a thorough
analysis using 25 molecule design tasks, including computationally complex
protein docking simulations. From this analysis, we discover unique insights in
this problem space and show that ChemRLformer achieves state-of-the-art
performance while being more straightforward than prior work by demystifying
which design choices are actually helpful for text-based molecule design.
","2023-10-05","2310.02902v1.pdf"
"2310.02905","Zhongxiang Dai","Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu,
  See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low","Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled
  with Transformers","Preprint, 24 pages","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have shown remarkable instruction-following
capabilities and achieved impressive performances in various applications.
However, the performances of LLMs depend heavily on the instructions given to
them, which are typically manually tuned with substantial human efforts. Recent
work has used the query-efficient Bayesian optimization (BO) algorithm to
automatically optimize the instructions given to black-box LLMs. However, BO
usually falls short when optimizing highly sophisticated (e.g.,
high-dimensional) objective functions, such as the functions mapping an
instruction to the performance of an LLM. This is mainly due to the limited
expressive power of the Gaussian process (GP) model which is used by BO as a
surrogate to model the objective function. Meanwhile, it has been repeatedly
shown that neural networks (NNs), especially pre-trained transformers, possess
strong expressive power and can model highly complex functions. So, we adopt a
neural bandit algorithm which replaces the GP in BO by an NN surrogate to
optimize instructions for black-box LLMs. More importantly, the neural bandit
algorithm allows us to naturally couple the NN surrogate with the hidden
representation learned by a pre-trained transformer (i.e., an open-source LLM),
which significantly boosts its performance. These motivate us to propose our
INSTruction optimization usIng Neural bandits Coupled with Transformers}
(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use
extensive experiments to show that our INSTINCT consistently outperforms the
existing methods in different tasks, such as in various instruction induction
tasks and the task of improving the zero-shot chain-of-thought instruction.
","2023-10-05","2310.02905v1.pdf"
"2310.02932","Jannis Bulian","Jannis Bulian, Mike S. Sch\""afer, Afra Amini, Heidi Lam, Massimiliano
  Ciaramita, Ben Gaiarin, Michelle Chen Huebscher, Christian Buck, Niels Mede,
  Markus Leippold, Nadine Strauss","Assessing Large Language Models on Climate Information","","","","","cs.CL cs.AI cs.CY cs.LG","http://creativecommons.org/licenses/by/4.0/","  Understanding how climate change affects us and learning about available
solutions are key steps toward empowering individuals and communities to
mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,
it is necessary to assess their capability in this domain. In this study, we
present a comprehensive evaluation framework, grounded in science communication
principles, to analyze LLM responses to climate change topics. Our framework
emphasizes both the presentational and epistemological adequacy of answers,
offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our
framework discerns up to 30 distinct issues in model outputs. The task is a
real-world example of a growing number of challenging problems where AI can
complement and lift human performance. We introduce a novel and practical
protocol for scalable oversight that uses AI Assistance and relies on raters
with relevant educational backgrounds. We evaluate several recent LLMs and
conduct a comprehensive analysis of the results, shedding light on both the
potential and the limitations of LLMs in the realm of climate communication.
","2023-10-05","2310.02932v1.pdf"
"2310.02949","Xianjun Yang","Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang,
  Xun Zhao, Dahua Lin","Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models","Work in progress","","","","cs.CL cs.AI cs.CR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.
","2023-10-05","2310.02949v1.pdf"
"2310.02953","Chang Gao","Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam","JsonTuning: Towards Generalizable, Robust, and Controllable Instruction
  Tuning","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instruction tuning has emerged as a crucial process for harnessing the
capabilities of large language models (LLMs) by providing explicit task
instructions, leading to improved performance in various tasks. However,
prevalent text-to-text instruction tuning (TextTuning) methods suffer from
limitations in generalization, robustness, and controllability due to the
ambiguity and lack of explicit structure in tasks. In this paper, we propose
JsonTuning, a novel structure-to-structure approach for instruction tuning. By
leveraging the versatility and structured nature of JSON to represent tasks,
JsonTuning enhances generalization by helping the model understand essential
task elements and their relations, improves robustness by minimizing ambiguity,
and increases controllability by providing explicit control over the output. We
conduct a comprehensive comparative study with diverse language models and
evaluation benchmarks. Experimental results show that JsonTuning outperforms
TextTuning in various applications, showcasing improved performance,
adaptability, robustness, and controllability. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for more effective
and reliable LLMs capable of handling diverse scenarios.
","2023-10-05","2310.02953v1.pdf"
"2310.02973","Siddhant Arora","Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan
  Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe","UniverSLU: Universal Spoken Language Understanding for Diverse
  Classification and Sequence Generation Tasks with a Single Network","","","","","cs.CL cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent studies have demonstrated promising outcomes by employing large
language models with multi-tasking capabilities. They utilize prompts to guide
the model's behavior and surpass performance of task-specific models. Motivated
by this, we ask: can we build a single model that jointly perform various
spoken language understanding (SLU) tasks? To address this, we utilize
pre-trained automatic speech recognition (ASR) models and employ various task
and dataset specifiers as discrete prompts. We demonstrate efficacy of our
single multi-task learning (MTL) model ""UniverSLU"" for 12 different speech
classification and sequence generation tasks across 17 datasets and 9
languages. Results show that UniverSLU achieves competitive performance and
even surpasses task-specific models. We also conduct preliminary investigations
into enabling human-interpretable natural phrases instead of task specifiers as
discrete prompts and test the model's generalization capabilities to new
paraphrases.
","2023-10-05","2310.02973v1.pdf"
"2310.02977","Yushi Bai","Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng,
  Ran Yi, Juanzi Li, Yong-Jin Liu","T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation","16 pages, 11 figures","","","","cs.CV cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.
","2023-10-05","2310.02977v1.pdf"
"2310.02980","Ido Amos","Ido Amos, Jonathan Berant, Ankit Gupta","Never Train from Scratch: Fair Comparison of Long-Sequence Models
  Requires Data-Driven Priors","","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modeling long-range dependencies across sequences is a longstanding goal in
machine learning and has led to architectures, such as state space models, that
dramatically outperform Transformers on long sequences. However, these
impressive empirical gains have been by and large demonstrated on benchmarks
(e.g. Long Range Arena), where models are randomly initialized and trained to
predict a target label from an input sequence. In this work, we show that
random initialization leads to gross overestimation of the differences between
architectures and that pretraining with standard denoising objectives, using
$\textit{only the downstream task data}$, leads to dramatic gains across
multiple architectures and to very small gaps between Transformers and state
space models (SSMs). In stark contrast to prior works, we find vanilla
Transformers to match the performance of S4 on Long Range Arena when properly
pretrained, and we improve the best reported results of SSMs on the PathX-256
task by 20 absolute points. Subsequently, we analyze the utility of
previously-proposed structured parameterizations for SSMs and show they become
mostly redundant in the presence of data-driven initialization obtained through
pretraining. Our work shows that, when evaluating different architectures on
supervised tasks, incorporation of data-driven priors via pretraining is
essential for reliable performance estimation, and can be done efficiently.
","2023-10-05","2310.02980v1.pdf"
"2310.02982","Daniel Bj\""orkegren","Jun Ho Choi, Oliver Garrod, Paul Atherton, Andrew Joyce-Gibbons,
  Miriam Mason-Sesay, Daniel Bj\""orkegren","Are LLMs Useful in the Poorest Schools? theTeacherAI in Sierra Leone","","","","","cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Education systems in developing countries have few resources to serve large,
poor populations. How might generative AI integrate into classrooms? This paper
introduces an AI chatbot designed to assist teachers in Sierra Leone with
professional development to improve their instruction. We describe initial
findings from early implementation across 122 schools and 193 teachers, and
analyze its use with qualitative observations and by analyzing queries.
Teachers use the system for lesson planning, classroom management, and subject
matter. A subset of teachers use the system intensively. We draw conclusions
from these findings about how generative AI systems can be integrated into
school systems in low income countries.
","2023-10-05","2310.02982v1.pdf"
"2310.02984","Vivien Cabannes","Vivien Cabannes, Elvis Dohmatob, Alberto Bietti","Scaling Laws for Associative Memories","","","","","stat.ML cs.AI cs.CL cs.LG cs.NE","http://creativecommons.org/licenses/by/4.0/","  Learning arguably involves the discovery and memorization of abstract rules.
The aim of this paper is to study associative memory mechanisms. Our model is
based on high-dimensional matrices consisting of outer products of embeddings,
which relates to the inner layers of transformer language models. We derive
precise scaling laws with respect to sample size and parameter size, and
discuss the statistical efficiency of different estimators, including
optimization-based algorithms. We provide extensive numerical experiments to
validate and interpret theoretical results, including fine-grained
visualizations of the stored memory associations.
","2023-10-05","2310.02984v1.pdf"
"2310.02988","Phillip Howard","Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Vasudev
  Lal","Probing Intersectional Biases in Vision-Language Models with
  Counterfactual Examples","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes from existing
datasets. To address this challenge, we employ text-to-image diffusion models
to produce counterfactual examples for probing intserctional social biases at
scale. Our approach utilizes Stable Diffusion with cross attention control to
produce sets of counterfactual image-text pairs that are highly similar in
their depiction of a subject (e.g., a given occupation) while differing only in
their depiction of intersectional social attributes (e.g., race & gender). We
conduct extensive experiments using our generated dataset which reveal the
intersectional social biases present in state-of-the-art VLMs.
","2023-10-05","2310.02988v1.pdf"
"2310.02989","Siavash Golkar","Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti,
  Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben
  Ohana, Liam Parker, Bruno R\'egaldo-Saint Blancard, Tiberiu Tesileanu,
  Kyunghyun Cho, Shirley Ho","xVal: A Continuous Number Encoding for Large Language Models","10 pages 7 figures. Supplementary: 5 pages 2 figures","","","","stat.ML cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models have not yet been broadly adapted for the analysis of
scientific datasets due in part to the unique difficulties of tokenizing
numbers. We propose xVal, a numerical encoding scheme that represents any real
number using just a single token. xVal represents a given real number by
scaling a dedicated embedding vector by the number value. Combined with a
modified number-inference approach, this strategy renders the model end-to-end
continuous when considered as a map from the numbers of the input string to
those of the output string. This leads to an inductive bias that is generally
more suitable for applications in scientific domains. We empirically evaluate
our proposal on a number of synthetic and real-world datasets. Compared with
existing number encoding schemes, we find that xVal is more token-efficient and
demonstrates improved generalization.
","2023-10-05","2310.02989v1.pdf"
"2310.02992","Xichen Pan","Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu
  Wei","Kosmos-G: Generating Images in Context with Multimodal Large Language
  Models","Code: https://aka.ms/Kosmos-G Project Page:
  https://xichenpan.github.io/kosmosg","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent advancements in text-to-image (T2I) and vision-language-to-image
(VL2I) generation have made significant strides. However, the generation from
generalized vision-language inputs, especially involving multiple images,
remains under-explored. This paper presents Kosmos-G, a model that leverages
the advanced perception capabilities of Multimodal Large Language Models
(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output
space of MLLM with CLIP using the textual modality as an anchor and performs
compositional instruction tuning on curated data. Kosmos-G demonstrates a
unique capability of zero-shot multi-entity subject-driven generation. Notably,
the score distillation instruction tuning requires no modifications to the
image decoder. This allows for a seamless substitution of CLIP and effortless
integration with a myriad of U-Net techniques ranging from fine-grained
controls to personalized image decoder variants. We posit Kosmos-G as an
initial attempt towards the goal of ""image as a foreign language in image
generation.""
","2023-10-05","2310.02992v1.pdf"
"2310.02994","Michael McCabe","Michael McCabe, Bruno R\'egaldo-Saint Blancard, Liam Holden Parker,
  Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash
  Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee, Tiberiu Tesileanu,
  Kyunghyun Cho, Shirley Ho","Multiple Physics Pretraining for Physical Surrogate Models","","","","","cs.LG cs.AI stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce multiple physics pretraining (MPP), an autoregressive
task-agnostic pretraining approach for physical surrogate modeling. MPP
involves training large surrogate models to predict the dynamics of multiple
heterogeneous physical systems simultaneously by learning features that are
broadly useful across diverse physical tasks. In order to learn effectively in
this setting, we introduce a shared embedding and normalization strategy that
projects the fields of multiple systems into a single shared embedding space.
We validate the efficacy of our approach on both pretraining and downstream
tasks over a broad fluid mechanics-oriented benchmark. We show that a single
MPP-pretrained transformer is able to match or outperform task-specific
baselines on all pretraining sub-tasks without the need for finetuning. For
downstream tasks, we demonstrate that finetuning MPP-trained models results in
more accurate predictions across multiple time-steps on new physics compared to
training from scratch or finetuning pretrained video foundation models. We
open-source our code and model weights trained at multiple scales for
reproducibility and community experimentation.
","2023-10-05","2310.02994v1.pdf"
"2310.02998","Yi-Lin Sung","Yi-Lin Sung, Jaehong Yoon, Mohit Bansal","ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language
  Models","Project page: https://ecoflap.github.io/","","","","cs.CV cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
performance improvements on various multimodal downstream tasks. However,
deploying LVLMs is often problematic due to their massive computational/energy
costs and carbon consumption. Such issues make it infeasible to adopt
conventional iterative global pruning, which is costly due to computing the
Hessian matrix of the entire large model for sparsification. Alternatively,
several studies have recently proposed layer-wise pruning approaches to avoid
the expensive computation of global pruning and efficiently compress model
weights according to their importance within a layer. However, these methods
often suffer from suboptimal model compression due to their lack of a global
perspective. To address this limitation in recent efficient pruning methods for
large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),
a two-stage coarse-to-fine weight pruning approach for LVLMs. We first
determine the sparsity ratios of different layers or blocks by leveraging the
global importance score, which is efficiently computed based on the
zeroth-order approximation of the global model gradients. Then, the multimodal
model performs local layer-wise unstructured weight pruning based on
globally-informed sparsity ratios. We validate our proposed method across
various multimodal and unimodal models and datasets, demonstrating significant
performance improvements over prevalent pruning techniques in the high-sparsity
regime.
","2023-10-05","2310.02998v1.pdf"
"2310.03003","Vijay Gadepally","Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas,
  Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, Vijay
  Gadepally","From Words to Watts: Benchmarking the Energy Costs of Large Language
  Model Inference","","","","","cs.CL cs.DC","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
  In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \& A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.
","2023-10-05","2310.03003v1.pdf"
"2310.03016","Satwik Bhattamishra","Satwik Bhattamishra, Arkil Patel, Phil Blunsom, Varun Kanade","Understanding In-Context Learning in Transformers and LLMs by Learning
  to Learn Discrete Functions","Preprint","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.
","2023-10-05","2310.03016v1.pdf"
"2310.03017","Yuxuan Sun","Yuxuan Sun, Kai Zhang, Yu Su","Multimodal Question Answering for Unified Information Extraction","24 pages, 2 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multimodal information extraction (MIE) aims to extract structured
information from unstructured multimedia content. Due to the diversity of tasks
and settings, most current MIE models are task-specific and data-intensive,
which limits their generalization to real-world scenarios with diverse task
requirements and limited labeled data. To address these issues, we propose a
novel multimodal question answering (MQA) framework to unify three MIE tasks by
reformulating them into a unified span extraction and multi-choice QA pipeline.
Extensive experiments on six datasets show that: 1) Our MQA framework
consistently and significantly improves the performances of various
off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla
prompting. 2) In the zero-shot setting, MQA outperforms previous
state-of-the-art baselines by a large margin. In addition, the effectiveness of
our framework can successfully transfer to the few-shot setting, enhancing LMMs
on a scale of 10B parameters to be competitive or outperform much larger
language models such as ChatGPT and GPT-4. Our MQA framework can serve as a
general principle of utilizing LMMs to better solve MIE and potentially other
downstream multimodal tasks.
","2023-10-05","2310.03017v1.pdf"
"2310.03018","Kuan-Po Huang","Kuan-Po Huang, Chih-Kai Yang, Yu-Kuan Fu, Ewan Dunbar, Hung-yi Lee","Zero Resource Code-switched Speech Benchmark Using Speech Utterance
  Pairs For Multiple Spoken Languages","Submitted to ICASSP 2024","","","","eess.AS cs.CL cs.SD","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We introduce a new zero resource code-switched speech benchmark designed to
directly assess the code-switching capabilities of self-supervised speech
encoders. We showcase a baseline system of language modeling on discrete units
to demonstrate how the code-switching abilities of speech encoders can be
assessed in a zero-resource manner. Our experiments encompass a variety of
well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We
examine the impact of pre-training languages and model size on benchmark
performance. Notably, though our results demonstrate that speech encoders with
multilingual pre-training, exemplified by XLSR, outperform monolingual variants
(Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial
room for improvement in their code-switching linguistic abilities.
","2023-10-05","2310.03018v1.pdf"
"2310.03025","Wei Ping","Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu,
  Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro","Retrieval meets Long Context Large Language Models","","","","","cs.CL cs.AI cs.IR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Extending the context window of large language models (LLMs) is getting
popular recently, while the solution of augmenting LLMs with retrieval has
existed for years. The natural questions are: i) Retrieval-augmentation versus
long context window, which one is better for downstream tasks? ii) Can both
methods be combined to get the best of both worlds? In this work, we answer
these questions by studying both solutions using two state-of-the-art
pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps
surprisingly, we find that LLM with 4K context window using simple
retrieval-augmentation at generation can achieve comparable performance to
finetuned LLM with 16K context window via positional interpolation on long
context tasks, while taking much less computation. More importantly, we
demonstrate that retrieval can significantly improve the performance of LLMs
regardless of their extended context window sizes. Our best model,
retrieval-augmented LLaMA2-70B with 32K context window, outperforms
GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long
context tasks including question answering and query-based summarization. It
also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while
being much faster at generation. Our study provides general insights on the
choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
","2023-10-05","2310.03025v1.pdf"
"2310.03026","Mingyu Ding","Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo
  Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding","LanguageMPC: Large Language Models as Decision Makers for Autonomous
  Driving","","","","","cs.RO cs.AI cs.CL cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Existing learning-based autonomous driving (AD) systems face challenges in
comprehending high-level information, generalizing to rare events, and
providing interpretability. To address these problems, this work employs Large
Language Models (LLMs) as a decision-making component for complex AD scenarios
that require human commonsense understanding. We devise cognitive pathways to
enable comprehensive reasoning with LLMs, and develop algorithms for
translating LLM decisions into actionable driving commands. Through this
approach, LLM decisions are seamlessly integrated with low-level controllers by
guided parameter matrix adaptation. Extensive experiments demonstrate that our
proposed method not only consistently surpasses baseline approaches in
single-vehicle tasks, but also helps handle complex driving behaviors even
multi-vehicle coordination, thanks to the commonsense reasoning capabilities of
LLMs. This paper presents an initial step toward leveraging LLMs as effective
decision-makers for intricate AD scenarios in terms of safety, efficiency,
generalizability, and interoperability. We aspire for it to serve as
inspiration for future research in this field. Project page:
https://sites.google.com/view/llm-mpc
","2023-10-16","2310.03026v1.pdf"
"2310.03028","Ronen Taub","Ronen Taub, Yonatan Savir","SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and
  Improving Prediction Rates in Drug Discovery","","","","","physics.chem-ph cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Machine learning, and representation learning in particular, has the
potential to facilitate drug discovery by screening a large chemical space in
silico. A successful approach for representing molecules is to treat them as a
graph and utilize graph neural networks. One of the key limitations of such
methods is the necessity to represent compounds with different numbers of
atoms, which requires aggregating the atom's information. Common aggregation
operators, such as averaging, result in loss of information at the atom level.
In this work, we propose a novel aggregating approach where each atom is
weighted non-linearly using the Boltzmann distribution with a hyperparameter
analogous to temperature. We show that using this weighted aggregation improves
the ability of the gold standard message-passing neural network to predict
antibiotic activity. Moreover, by changing the temperature hyperparameter, our
approach can reveal the atoms that are important for activity prediction in a
smooth and consistent way, thus providing a novel, regulated attention
mechanism for graph neural networks. We further validate our method by showing
that it recapitulates the functional group in beta-Lactam antibiotics. The
ability of our approach to rank the atoms' importance for a desired function
can be used within any graph neural network to provide interpretability of the
results and predictions at the node level.
","2023-10-06","2310.03028v1.pdf"
"2310.03030","Suryanarayanan Balaji","Suryanarayanan Balaji and Rishikesh Magar and Yayati Jadhav and Amir
  Barati Farimani","GPT-MolBERTa: GPT Molecular Features Language Model for molecular
  property prediction","Paper has 17 pages, 4 figures and 4 tables, along with 71 references","","","","physics.chem-ph cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the emergence of Transformer architectures and their powerful
understanding of textual data, a new horizon has opened up to predict the
molecular properties based on text description. While SMILES are the most
common form of representation, they are lacking robustness, rich information
and canonicity, which limit their effectiveness in becoming generalizable
representations. Here, we present GPT-MolBERTa, a self-supervised large
language model (LLM) which uses detailed textual descriptions of molecules to
predict their properties. A text based description of 326000 molecules were
collected using ChatGPT and used to train LLM to learn the representation of
molecules. To predict the properties for the downstream tasks, both BERT and
RoBERTa models were used in the finetuning stage. Experiments show that
GPT-MolBERTa performs well on various molecule property benchmarks, and
approaching state of the art performance in regression tasks. Additionally,
further analysis of the attention mechanisms show that GPT-MolBERTa is able to
pick up important information from the input textual data, displaying the
interpretability of the model.
","2023-10-11","2310.03030v1.pdf"
"2310.03031","Stefanie Urchs","Stefanie Urchs and Veronika Thurner and Matthias A{\ss}enmacher and
  Christian Heumann and Stephanie Thiemichen","How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English
  ChatGPT Responses","Accepted @ ""1st Workshop on Biased Data in Conversational Agents""
  (co-located with ECML PKDD 2023). This is the author's version of the work.
  The definite version of record will be published in the proceedings","","","","cs.CL cs.AI cs.CY cs.LG","http://creativecommons.org/licenses/by/4.0/","  With the introduction of ChatGPT, OpenAI made large language models (LLM)
accessible to users with limited IT expertise. However, users with no
background in natural language processing (NLP) might lack a proper
understanding of LLMs. Thus the awareness of their inherent limitations, and
therefore will take the systems' output at face value. In this paper, we
systematically analyse prompts and the generated responses to identify possible
problematic issues with a special focus on gender biases, which users need to
be aware of when processing the system's output. We explore how ChatGPT reacts
in English and German if prompted to answer from a female, male, or neutral
perspective. In an in-depth investigation, we examine selected prompts and
analyse to what extent responses differ if the system is prompted several times
in an identical way. On this basis, we show that ChatGPT is indeed useful for
helping non-IT users draft texts for their daily work. However, it is
absolutely crucial to thoroughly check the system's responses for biases as
well as for syntactic and grammatical mistakes.
","2023-10-06","2310.03031v1.pdf"
"2310.03046","Jieyu Zhang","Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang","EcoAssistant: Using LLM Assistant More Affordably and Accurately","","","","","cs.SE cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Today, users ask Large language models (LLMs) as assistants to answer queries
that require external knowledge; they ask about the weather in a specific city,
about stock prices, and even about where specific locations are within their
neighborhood. These queries require the LLM to produce code that invokes
external APIs to answer the user's question, yet LLMs rarely produce correct
code on the first try, requiring iterative code refinement upon execution
results. In addition, using LLM assistants to support high query volumes can be
expensive. In this work, we contribute a framework, EcoAssistant, that enables
LLMs to answer code-driven queries more affordably and accurately. EcoAssistant
contains three components. First, it allows the LLM assistants to converse with
an automatic code executor to iteratively refine code or to produce answers
based on the execution results. Second, we use a hierarchy of LLM assistants,
which attempts to answer the query with weaker, cheaper LLMs before backing off
to stronger, expensive ones. Third, we retrieve solutions from past successful
queries as in-context demonstrations to help subsequent queries. Empirically,
we show that EcoAssistant offers distinct advantages for affordability and
accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of
GPT-4's cost.
","2023-10-06","2310.03046v1.pdf"
"2310.03051","Pei Zhou","Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin
  R. McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida
  Nematzadeh, Shyam Upadhyay, Manaal Faruqui","How FaR Are Large Language Models From Agents with Theory-of-Mind?","Preprint, 18 pages, 6 figures, 6 tables","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  ""Thinking is for Doing."" Humans can infer other people's mental states from
observations--an ability called Theory-of-Mind (ToM)--and subsequently act
pragmatically on those inferences. Existing question answering benchmarks such
as ToMi ask models questions to make inferences about beliefs of characters in
a story, but do not test whether models can then use these inferences to guide
their actions. We propose a new evaluation paradigm for large language models
(LLMs): Thinking for Doing (T4D), which requires models to connect inferences
about others' mental states to actions in social scenarios. Experiments on T4D
demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking
characters' beliefs in stories, but they struggle to translate this capability
into strategic action. Our analysis reveals the core challenge for LLMs lies in
identifying the implicit inferences about mental states without being
explicitly asked about as in ToMi, that lead to choosing the correct action in
T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee
and Reflect (FaR), which provides a reasoning structure that encourages LLMs to
anticipate future challenges and reason about potential actions. FaR boosts
GPT-4's performance from 50% to 71% on T4D, outperforming other prompting
methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to
diverse out-of-distribution story structures and scenarios that also require
ToM inferences to choose an action, consistently outperforming other methods
including few-shot in-context learning.
","2023-10-06","2310.03051v1.pdf"
"2310.03052","Sangjun Park","Sangjun Park and JinYeong Bak","Memoria: Hebbian Memory Architecture for Human-Like Sequential
  Processing","Under review as a conference paper at ICLR 2024. 20 pages, 9 figures,
  5 tables","","","","cs.LG cs.AI cs.NE","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Transformers have demonstrated their success in various domains and tasks.
However, Transformers struggle with long input sequences due to their limited
capacity. While one solution is to increase input length, endlessly stretching
the length is unrealistic. Furthermore, humans selectively remember and use
only relevant information from inputs, unlike Transformers which process all
raw data from start to end. We introduce Memoria, a general memory network that
applies Hebbian theory which is a major theory explaining human memory
formulation to enhance long-term dependencies in neural networks. Memoria
stores and retrieves information called engram at multiple memory levels of
working memory, short-term memory, and long-term memory, using connection
weights that change according to Hebb's rule. Through experiments with popular
Transformer-based models like BERT and GPT, we present that Memoria
significantly improves the ability to consider long-term dependencies in
various tasks. Results show that Memoria outperformed existing methodologies in
sorting and language modeling, and long text classification.
","2023-10-06","2310.03052v1.pdf"
"2310.03059","Yiwen Tang","Ivan Tang and Eric Zhang and Ray Gu","Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","10 pages. The specialized PEFT framework for 3D pre-trained models,
  which achieves competitive performance to full fine-tuning, and significantly
  reduces the computational resources. Project page:
  https://github.com/EvenJoker/Point-PEFT","","","","cs.CV cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/EvenJoker/Point-PEFT.
","2023-10-06","2310.03059v1.pdf"
"2310.03084","Deniz Bayazit","Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, Antoine
  Bosselut","Discovering Knowledge-Critical Subnetworks in Pretrained Language Models","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pretrained language models (LMs) encode implicit representations of knowledge
in their parameters. However, localizing these representations and
disentangling them from each other remains an open problem. In this work, we
investigate whether pretrained language models contain various
knowledge-critical subnetworks: particular sparse computational subgraphs
responsible for encoding specific knowledge the model has memorized. We propose
a multi-objective differentiable weight masking scheme to discover these
subnetworks and show that we can use them to precisely remove specific
knowledge from models while minimizing adverse effects on the behavior of the
original language model. We demonstrate our method on multiple GPT2 variants,
uncovering highly sparse subnetworks (98%+) that are solely responsible for
specific collections of relational knowledge. When these subnetworks are
removed, the remaining network maintains most of its initial capacity (modeling
language and other memorized relational knowledge) but struggles to express the
removed knowledge, and suffers performance drops on examples needing this
removed knowledge on downstream tasks after finetuning.
","2023-10-06","2310.03084v1.pdf"
"2310.03094","Murong Yue","Murong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu Yao","Large Language Model Cascades with Mixture of Thoughts Representations
  for Cost-efficient Reasoning","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the ""answer consistency"" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.
","2023-10-10","2310.03094v1.pdf"
"2310.03104","Monica Ribero","William Kong, Andr\'es Mu\~noz Medina and M\'onica Ribero","DP-SGD for non-decomposable objective functions","","","","","cs.LG cs.CR","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Unsupervised pre-training is a common step in developing computer vision
models and large language models. In this setting, the absence of labels
requires the use of similarity-based loss functions, such as contrastive loss,
that favor minimizing the distance between similar inputs and maximizing the
distance between distinct inputs. As privacy concerns mount, training these
models using differential privacy has become more important. However, due to
how inputs are generated for these losses, one of their undesirable properties
is that their $L_2$ sensitivity can grow with increasing batch size. This
property is particularly disadvantageous for differentially private training
methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD
variant for similarity based loss functions -- in particular the commonly used
contrastive loss -- that manipulates gradients of the objective function in a
novel way to obtain a senstivity of the summed gradient that is $O(1)$ for
batch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10
pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our
method's performance comes close to that of a non-private model and generally
outperforms DP-SGD applied directly to the contrastive loss.
","2023-10-06","2310.03104v1.pdf"
"2310.03123","Zihao Lin","Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen,
  Dacheng Tao","Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models","20 pages, 6 figures, preprint","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the blowout development of pre-trained models (PTMs), the efficient
tuning of these models for diverse downstream applications has emerged as a
pivotal research concern. Although recent investigations into prompt tuning
have provided promising avenues, three salient challenges persist: (1) memory
constraint: the continuous growth in the size of open-source PTMs renders
fine-tuning, even a fraction of their parameters, challenging for many
practitioners. (2) model privacy: existing PTMs often function as public API
services, with their parameters inaccessible for effective or tailored
fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates
high-quality datasets, which are typically localized and not shared to public.
To optimally harness each local dataset while navigating memory constraints and
preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT).
This innovative approach eschews reliance on parameter architectures and
private dataset access, instead capitalizing on a central server that aids
local users in collaboratively training a prompt generator through regular
aggregation. Local users leverage API-driven learning via a zero-order
optimizer, obviating the need for PTM deployment. Relative to extensive
fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM
storage and fine-tuning on local machines, tapping into comprehensive,
high-quality, yet private training datasets. A thorough evaluation across 40
datasets spanning CV and NLP tasks underscores the robustness of our proposed
model.
","2023-10-06","2310.03123v1.pdf"
"2310.03128","Yue Huang","Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and
  Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong
  and Lichao Sun","MetaTool Benchmark for Large Language Models: Deciding Whether to Use
  Tools and Which to Use","","","","","cs.SE cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have garnered significant attention due to their
impressive natural language processing (NLP) capabilities. Recently, many
studies have focused on the tool utilization ability of LLMs. They primarily
investigated how LLMs effectively collaborate with given specific tools.
However, in scenarios where LLMs serve as intelligent agents, as seen in
applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate
decision-making processes that involve deciding whether to employ a tool and
selecting the most suitable tool(s) from a collection of available tools to
fulfill user requests. Therefore, in this paper, we introduce MetaTool, a
benchmark designed to evaluate whether LLMs have tool usage awareness and can
correctly choose tools. Specifically, we create a dataset called ToolE within
the benchmark. This dataset contains various types of user queries in the form
of prompts that trigger LLMs to use tools, including both single-tool and
multi-tool scenarios. Subsequently, we set the tasks for both tool usage
awareness and tool selection. We define four subtasks from different
perspectives in tool selection, including tool selection with similar choices,
tool selection in specific scenarios, tool selection with possible reliability
issues, and multi-tool selection. We conduct experiments involving nine popular
LLMs and find that the majority of them still struggle to effectively select
tools, highlighting the existing gaps between LLMs and genuine intelligent
agents. However, through the error analysis, we found there is still
significant room for improvement. Finally, we conclude with insights for tool
developers that follow ChatGPT to provide detailed descriptions that can
enhance the tool selection performance of LLMs.
","2023-10-25","2310.03128v1.pdf"
"2310.03150","Herbert Woisetschl\""ager","Herbert Woisetschl\""ager, Alexander Isenko, Shiqiang Wang, Ruben
  Mayer, Hans-Arno Jacobsen","Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the
  Ugly","6 pages, 3 figures","","","","cs.LG cs.DC cs.PF","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
  This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.
","2023-10-06","2310.03150v1.pdf"
"2310.03173","Zishun Yu","Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, Hongxia Yang","$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program
  Synthesis","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Program synthesis aims to create accurate, executable code from natural
language descriptions. This field has leveraged the power of reinforcement
learning (RL) in conjunction with large language models (LLMs), significantly
enhancing code generation capabilities. This integration focuses on directly
optimizing functional correctness, transcending conventional supervised losses.
While current literature predominantly favors policy-based algorithms,
attributes of program synthesis suggest a natural compatibility with
value-based methods. This stems from rich collection of off-policy programs
developed by human programmers, and the straightforward verification of
generated programs through automated unit testing (i.e. easily obtainable
rewards in RL language). Diverging from the predominant use of policy-based
algorithms, our work explores the applicability of value-based approaches,
leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman
coder). Yet, training value-based methods presents challenges due to the
enormous search space inherent to program synthesis. To this end, we propose an
initialization protocol for RL agents utilizing pre-trained LMs and a
conservative Bellman operator to reduce training complexities. Moreover, we
demonstrate how to leverage the learned value functions as a dual strategy to
post-process generated programs. Our empirical evaluations demonstrated
$\mathcal{B}$-Coder's capability in achieving state-of-the-art performance
compared with policy-based methods. Remarkably, this achievement is reached
with minimal reward engineering effort, highlighting the effectiveness of
value-based RL, independent of reward designs.
","2023-10-06","2310.03173v1.pdf"
"2310.03176","Iv\'an D\'iaz","Iv\'an D\'iaz and Hana Lee and Emre K{\i}c{\i}man and Mouna Akacha and
  Dean Follman and Debashis Ghosh","Sensitivity analysis for causality in observational studies for
  regulatory science","","","","","stat.AP","http://creativecommons.org/licenses/by/4.0/","  Recognizing the importance of real-world data (RWD) for regulatory purposes,
the United States (US) Congress passed the 21st Century Cures Act1 mandating
the development of Food and Drug Administration (FDA) guidance on regulatory
use of real-world evidence. The Forum on the Integration of Observational and
Randomized Data (FIORD) conducted a meeting bringing together various
stakeholder groups to build consensus around best practices for the use of RWD
to support regulatory science. Our companion paper describes in detail the
context and discussion carried out in the meeting, which includes a
recommendation to use a causal roadmap for complete pre-specification of study
designs using RWD. This article discusses one step of the roadmap: the
specification of a procedure for sensitivity analysis, defined as a procedure
for testing the robustness of substantive conclusions to violations of
assumptions made in the causal roadmap. We include a worked-out example of a
sensitivity analysis from a RWD study on the effectiveness of Nifurtimox in
treating Chagas disease, as well as an overview of various methods available
for sensitivity analysis in causal inference, emphasizing practical
considerations on their use for regulatory purposes.
","2023-10-19","2310.03176v2.pdf"
"2310.03182","An Yan","An Yan, Yu Wang, Yiwu Zhong, Zexue He, Petros Karypis, Zihan Wang,
  Chengyu Dong, Amilcare Gentili, Chun-Nan Hsu, Jingbo Shang, Julian McAuley","Robust and Interpretable Medical Image Classifiers via Concept
  Bottleneck Models","18 pages, 12 figures","","","","cs.CV cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Medical image classification is a critical problem for healthcare, with the
potential to alleviate the workload of doctors and facilitate diagnoses of
patients. However, two challenges arise when deploying deep learning models to
real-world healthcare applications. First, neural models tend to learn spurious
correlations instead of desired features, which could fall short when
generalizing to new domains (e.g., patients with different ages). Second, these
black-box models lack interpretability. When making diagnostic predictions, it
is important to understand why a model makes a decision for trustworthy and
safety considerations. In this paper, to address these two limitations, we
propose a new paradigm to build robust and interpretable medical image
classifiers with natural language concepts. Specifically, we first query
clinical concepts from GPT-4, then transform latent image features into
explicit concepts with a vision-language model. We systematically evaluate our
method on eight medical image classification datasets to verify its
effectiveness. On challenging datasets with strong confounding factors, our
method can mitigate spurious correlations thus substantially outperform
standard visual encoders and other baselines. Finally, we show how
classification with a small number of concepts brings a level of
interpretability for understanding model decisions through case studies in real
medical data.
","2023-10-06","2310.03182v1.pdf"
"2310.03184","Zachary Levonian","Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel,
  Millie-Ellen Postle, Wanli Xing","Retrieval-augmented Generation to Improve Math Question-Answering:
  Trade-offs Between Groundedness and Human Preference","6 pages","","","","cs.CL cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.
","2023-10-06","2310.03184v1.pdf"
"2310.03185","Xiaohan Fu","Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar
  Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes","Misusing Tools in Large Language Models With Visual Adversarial Examples","","","","","cs.CR cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large Language Models (LLMs) are being enhanced with the ability to use tools
and to process multiple modalities. These new capabilities bring new benefits
and also new security risks. In this work, we show that an attacker can use
visual adversarial examples to cause attacker-desired tool usage. For example,
the attacker could cause a victim LLM to delete calendar events, leak private
conversations and book hotels. Different from prior work, our attacks can
affect the confidentiality and integrity of user resources connected to the LLM
while being stealthy and generalizable to multiple input prompts. We construct
these attacks using gradient-based adversarial training and characterize
performance along multiple dimensions. We find that our adversarial images can
manipulate the LLM to invoke tools following real-world syntax almost always
(~98%) while maintaining high similarity to clean images (~0.9 SSIM).
Furthermore, using human scoring and automated metrics, we find that the
attacks do not noticeably affect the conversation (and its semantics) between
the user and the LLM.
","2023-10-06","2310.03185v1.pdf"
"2310.03188","Zhe Zhao","Zhe Zhao, Qingyun Liu, Huan Gui, Bang An, Lichan Hong, Ed H. Chi","Talking Models: Distill Pre-trained Knowledge to Downstream Models via
  Interactive Communication","19 pages, 3 figures","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Many recent breakthroughs in machine learning have been enabled by the
pre-trained foundation models. By scaling up model parameters, training data,
and computation resources, foundation models have significantly advanced the
state-of-the-art in many applications. However, it is still an open question of
how to use these models to perform downstream tasks efficiently. Knowledge
distillation (KD) has been explored to tackle this challenge. KD transfers
knowledge from a large teacher model to a smaller student model. While KD has
been successful in improving student model performance, recent research has
discovered that a powerful teacher does not necessarily lead to a powerful
student, due to their huge capacity gap. In addition, the potential
distribution shifts between the pre-training data and downstream tasks can make
knowledge transfer in KD sub-optimal for improving downstream task performance.
In this paper, we extend KD with an interactive communication process to help
students of downstream tasks learn effectively from pre-trained foundation
models. Our design is inspired by the way humans learn from teachers who can
explain knowledge in a way that meets the students' needs. Specifically, we let
each model (i.e., student and teacher) train two components: (1) an encoder
encoding the model's hidden states to a message and (2) a decoder decoding any
messages to its own hidden states. With encoder and decoder, not only can the
teacher transfer rich information by encoding its hidden states, but also the
student can send messages with information of downstream tasks to the teacher.
Therefore, knowledge passing from teacher to student can be tailored to the
student's capacity and downstream tasks' distributions. We conducted
experiments on benchmark datasets to show that our communication mechanism
outperforms state-of-the-art distillation techniques.
","2023-10-06","2310.03188v1.pdf"
"2310.03192","Rania Abdelghani","Rania Abdelghani, H\'el\`ene Sauz\'eon and Pierre-Yves Oudeyer","Generative AI in the Classroom: Can Students Remain Active Learners?","","","","","cs.CY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Generative Artificial Intelligence (GAI) has high potential to help address a
diversity of educational challenges. In principle, GAI could facilitate the
implementation of interactive and empowering pedagogical activities to
complement the standard teaching strategies and favor students active
engagement, understanding and control over their learning processes. These
dimensions are indeed fundamental for a better learning experience and
longer-lasting cognitive outcomes. However, several characteristics of the
interactions with GAI such as continuous confidence in the generated answers,
and the lack of pedagogical stance in their behavior may lead students to poor
states of control over learning (e.g. over-reliance on pre-generated content,
over-estimation of one's own knowledge, loss of curious and critical-thinking
sense, etc).
  The fine line between the two settings seems to lie in how this technology is
used to carry out the pedagogical activities (e.g. types of interactions
allowed, level of controllability by students, level of involvement of
educators, etc) as well as to what extent students have the relevant skills
(cognitive, metacognitive and GAI literacy) that allow them to correctly
evaluate, analyze and interpret the system behaviors.
  In this context, this article proposes to identify some of the opportunities
and challenges that could arise wrt students control over their learning when
using GAI during formal pedagogical activities. In a second step, we also
discuss the types of trainings that could be relevant to offer students in
order to provide them with the appropriate set of skills that can help them use
GAI in informed ways, when pursuing a given learning goal.
","2023-10-06","2310.03192v1.pdf"
"2310.03210","Erfan Al-Hossami","Erfan Al-Hossami, Razvan Bunescu, Justin Smith, Ryan Teehan","Can Language Models Employ the Socratic Method? Experiments with Code
  Debugging","8 pages, 2 tables. To be published in Proceedings of the 2024
  Technical Symposium on Computer Science Education (SIGCSE'24)","","","","cs.CL cs.CY","http://creativecommons.org/licenses/by/4.0/","  When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark
","2023-10-06","2310.03210v1.pdf"
"2310.03211","Utsav Garg","Utsav Garg, Erhan Bas","On the Performance of Multimodal Language Models","","","","","cs.CL cs.AI cs.CV","http://creativecommons.org/licenses/by-sa/4.0/","  Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
","2023-10-06","2310.03211v1.pdf"
"2310.03214","Tu Vu","Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei,
  Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong","FreshLLMs: Refreshing Large Language Models with Search Engine
  Augmentation","Preprint, 22 pages, 7 figures, 5 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Most large language models (LLMs) are trained once and never updated; thus,
they lack the ability to dynamically adapt to our ever-changing world. In this
work, we perform a detailed study of the factuality of LLM-generated text in
the context of answering questions that test current world knowledge.
Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a
diverse range of question and answer types, including questions that require
fast-changing world knowledge as well as questions with false premises that
need to be debunked. We benchmark a diverse array of both closed and
open-source LLMs under a two-mode evaluation procedure that allows us to
measure both correctness and hallucination. Through human evaluations involving
more than 50K judgments, we shed light on limitations of these models and
demonstrate significant room for improvement: for instance, all models
(regardless of model size) struggle on questions that involve fast-changing
knowledge and false premises. Motivated by these results, we present
FreshPrompt, a simple few-shot prompting method that substantially boosts the
performance of an LLM on FreshQA by incorporating relevant and up-to-date
information retrieved from a search engine into the prompt. Our experiments
show that FreshPrompt outperforms both competing search engine-augmented
prompting methods such as Self-Ask (Press et al., 2022) as well as commercial
systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that
both the number of retrieved evidences and their order play a key role in
influencing the correctness of LLM-generated answers. Additionally, instructing
the LLM to generate concise and direct answers helps reduce hallucination
compared to encouraging more verbose answers. To facilitate future work, we
release FreshQA at github.com/freshllms/freshqa and commit to updating it at
regular intervals.
","2023-10-06","2310.03214v1.pdf"
"2310.03221","Yijia Xiao","Yijia Xiao, Dylan Steinecke, Alexander Russell Pelletier, Yushi Bai,
  Peipei Ping, Wei Wang","Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical
  Knowledge Graphs","26 pages, 2 figures, 14 figures","","","","cs.LG cs.AI q-bio.QM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Knowledge graphs (KGs) have emerged as a powerful framework for representing
and integrating complex biomedical information. However, assembling KGs from
diverse sources remains a significant challenge in several aspects, including
entity alignment, scalability, and the need for continuous updates to keep pace
with scientific advancements. Moreover, the representative power of KGs is
often limited by the scarcity of multi-modal data integration. To overcome
these challenges, we propose Know2BIO, a general-purpose heterogeneous KG
benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse
sources, capturing intricate relationships across 11 biomedical categories. It
currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable
of user-directed automated updating to reflect the latest knowledge in
biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data:
node features including text descriptions, protein and compound sequences and
structures, enabling the utilization of emerging natural language processing
methods and multi-modal data integration strategies. We evaluate KG
representation models on Know2BIO, demonstrating its effectiveness as a
benchmark for KG representation learning in the biomedical field. Data and
source code of Know2BIO are available at
https://github.com/Yijia-Xiao/Know2BIO/.
","2023-10-10","2310.03221v1.pdf"
"2310.03232","Xinyang Ren","Xinyang Ren, Hannah A Burkhardt, Patricia A Are\'an, Thomas D Hull,
  Trevor Cohen","Deep Representations of First-person Pronouns for Prediction of
  Depression Symptom Severity","Accepted: AMIA Annual Symposium 2023. To appear as: Ren X, Burkhardt
  H, Are\'an P, Hull T, Cohen T. Deep Representations of First-person Pronouns
  for Prediction of Depression Symptom Severity. AMIA Annual Symposium
  Proceedings 2023. American Medical Informatics Association","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Prior work has shown that analyzing the use of first-person singular pronouns
can provide insight into individuals' mental status, especially depression
symptom severity. These findings were generated by counting frequencies of
first-person singular pronouns in text data. However, counting doesn't capture
how these pronouns are used. Recent advances in neural language modeling have
leveraged methods generating contextual embeddings. In this study, we sought to
utilize the embeddings of first-person pronouns obtained from contextualized
language representation models to capture ways these pronouns are used, to
analyze mental status. De-identified text messages sent during online
psychotherapy with weekly assessment of depression severity were used for
evaluation. Results indicate the advantage of contextualized first-person
pronoun embeddings over standard classification token embeddings and
frequency-based pronoun analysis results in predicting depression symptom
severity. This suggests contextual representations of first-person pronouns can
enhance the predictive utility of language used by people with depression
symptoms.
","2023-10-06","2310.03232v1.pdf"
"2310.03249","Mohamed Aghzal","Mohamed Aghzal, Erion Plaku, Ziyu Yao","Can Large Language Models be Good Path Planners? A Benchmark and
  Investigation on Spatial-temporal Reasoning","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies and BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to make long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.
","2023-10-06","2310.03249v1.pdf"
"2310.03262","Shengding Hu","Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao,
  Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, Maosong Sun","Unlock Predictable Scaling from Emergent Abilities","10 pages main paper, 8 pages appendix","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The scientific scale-up of large language models (LLMs) necessitates a
comprehensive understanding of their scaling properties. However, the existing
literature on the scaling properties only yields an incomplete answer:
optimization loss decreases predictably as the model size increases, in line
with established scaling law; yet no scaling law for task has been established
and the task performances are far from predictable during scaling. Task
performances typically show minor gains on small models until they improve
dramatically once models exceed a size threshold, exemplifying the ``emergent
abilities''. In this study, we discover that small models, although they
exhibit minor performance, demonstrate critical and consistent task performance
improvements that are not captured by conventional evaluation strategies due to
insufficient measurement resolution. To measure such improvements, we introduce
PassUntil, an evaluation strategy through massive sampling in the decoding
phase. We conduct quantitative investigations into the scaling law of task
performance. Firstly, a strict task scaling law is identified, enhancing the
predictability of task performances. Remarkably, we are able to predict the
performance of the 2.4B model on code generation with merely 0.05\% deviation
before training starts. Secondly, underpinned by PassUntil, we observe concrete
evidence of emergent abilities and ascertain that they are not in conflict with
the continuity of performance improvement. Their semblance to break-through is
that their scaling curve cannot be fitted by standard scaling law function. We
then introduce a mathematical definition for the emergent abilities. Through
the definition, we refute a prevalent ``multi-step reasoning hypothesis''
regarding the genesis of emergent abilities and propose a new hypothesis with a
satisfying fit to the observed scaling curve.
","2023-10-06","2310.03262v1.pdf"
"2310.03266","Ruiyu Wang","Ruiyu Wang, Zifeng Wang, Jimeng Sun","UniPredict: Large Language Models are Universal Tabular Predictors","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Tabular data prediction is a fundamental machine learning task for many
applications. Existing methods predominantly employ discriminative modeling and
operate under the assumption of a fixed target column, necessitating
re-training for every new predictive task. Inspired by the generative power of
large language models (LLMs), this paper exploits the idea of building
universal tabular data predictors based on generative modeling, namely
UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets
with the capability of comprehending diverse tabular inputs and predicting for
target variables following the input instructions. Specifically, we train a
single LLM on an aggregation of 169 tabular datasets with diverse targets and
compare its performance against baselines that are trained on each dataset
separately. We observe this versatile UniPredict model demonstrates an
advantage over other models, ranging from 5.4% to 13.4%, when compared with the
best tree-boosting baseline and the best neural network baseline, respectively.
We further test UniPredict in few-shot learning settings on another 62 tabular
datasets. Our method achieves strong performance in quickly adapting to new
tasks, where our method outperforms XGBoost over 100% on the low-resource setup
and shows a significant margin over all baselines. We envision that UniPredict
sheds light on developing a universal tabular data prediction system that
learns from data at scale and serves a wide range of prediction tasks.
","2023-10-06","2310.03266v1.pdf"
"2310.03269","Zeyuan Wang","Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong
  Li, Huajun Chen","InstructProtein: Aligning Human and Protein Language via Knowledge
  Instruction","","","","","q-bio.BM cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.
","2023-10-06","2310.03269v1.pdf"
"2310.03283","Ke Shen","Ke Shen and Mayank Kejriwal","A Formalism and Approach for Improving Robustness of Large Language
  Models Using Risk-Adjusted Confidence Scores","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs), such as ChatGPT, have achieved impressive
milestones in natural language processing (NLP). Despite their impressive
performance, the models are known to pose important risks. As these models are
deployed in real-world applications, a systematic understanding of different
risks posed by these models on tasks such as natural language inference (NLI),
is much needed. In this paper, we define and formalize two distinct types of
risk: decision risk and composite risk. We also propose a risk-centric
evaluation framework, and four novel metrics, for assessing LLMs on these risks
in both in-domain and out-of-domain settings. Finally, we propose a
risk-adjusted calibration method called DwD for helping LLMs minimize these
risks in an overall NLI architecture. Detailed experiments, using four NLI
benchmarks, three baselines and two LLMs, including ChatGPT, show both the
practical utility of the evaluation framework, and the efficacy of DwD in
reducing decision and composite risk. For instance, when using DwD, an
underlying LLM is able to address an extra 20.1% of low-risk inference tasks
(but which the LLM erroneously deems high-risk without risk adjustment) and
skip a further 19.8% of high-risk tasks, which would have been answered
incorrectly.
","2023-10-06","2310.03283v1.pdf"
"2310.03291","Yiren Jian","Yiren Jian, Tingkai Liu, Yunzhe Tao, Soroush Vosoughi, Hongxia Yang","SimVLG: Simple and Efficient Pretraining of Visual Language Generative
  Models","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.
","2023-10-10","2310.03291v1.pdf"
"2310.03293","Siwei Wu","Siwei Wu, Xiangqing Shen, and Rui Xia","A New Dialogue Response Generation Agent for Large Language Models by
  Asking Questions to Detect User's Intentions","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs), such as ChatGPT, have recently been applied to
various NLP tasks due to its open-domain generation capabilities. However,
there are two issues with applying LLMs to dialogue tasks. 1. During the
dialogue process, users may have implicit intentions that might be overlooked
by LLMs. Consequently, generated responses couldn't align with the user's
intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.
In certain specific domains, their knowledge may be incomplete, and LLMs cannot
update the latest knowledge in real-time. To tackle these issues, we propose a
framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by
asking questions to \textbf{D}etect user's \textbf{I}mplicit
in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions
related to the dialogue context as the potential user's intention; Then, EDIT
answers those questions by interacting with LLMs and searching in
domain-specific knowledge bases respectively, and use LLMs to choose the proper
answers to questions as extra knowledge; Finally, EDIT enhances response
generation by explicitly integrating those extra knowledge. Besides, previous
question generation works only focus on asking questions with answers in
context. In order to ask open questions, we construct a Context-Open-Question
(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and
Holl-E), EDIT outperformed other LLMs.
","2023-10-06","2310.03293v1.pdf"
"2310.03302","Jian Vora","Qian Huang, Jian Vora, Percy Liang, Jure Leskovec","Benchmarking Large Language Models As AI Research Agents","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Scientific experimentation involves an iterative process of creating
hypotheses, designing experiments, running experiments, and analyzing the
results. Can we build AI research agents to perform these long-horizon tasks?
To take a step towards building and evaluating research agents on such
open-ended decision-making tasks, we focus on the problem of machine learning
engineering: given a task description and a dataset, build a high-performing
model. In this paper, we propose MLAgentBench, a suite of ML tasks for
benchmarking AI research agents. Agents can perform actions like
reading/writing files, executing code, and inspecting outputs. With these
actions, agents could run experiments, analyze the results, and modify the code
of entire machine learning pipelines, such as data processing, architecture,
training processes, etc. The benchmark then automatically evaluates the agent's
performance objectively over various metrics related to performance and
efficiency. We also design an LLM-based research agent to automatically perform
experimentation loops in such an environment. Empirically, we find that a
GPT-4-based research agent can feasibly build compelling ML models over many
tasks in MLAgentBench, displaying highly interpretable plans and actions.
However, the success rates vary considerably; they span from almost 90\% on
well-established older datasets to as low as 10\% on recent Kaggle Challenges
-- unavailable during the LLM model's pretraining -- and even 0\% on newer
research challenges like BabyLM. Finally, we identify several key challenges
for LLM-based research agents such as long-term planning and hallucination. Our
code is released at https://github.com/snap-stanford/MLAgentBench.
","2023-10-06","2310.03302v1.pdf"
"2310.03304","Danqing Wang","Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei
  Li, Yuandong Tian","Learning Personalized Story Evaluation","19 pages","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  While large language models (LLMs) have shown impressive results for more
objective tasks such as QA and retrieval, it remains nontrivial to evaluate
their performance on open-ended text generation for reasons including (1) data
contamination; (2) multi-dimensional evaluation criteria; and (3)
subjectiveness stemming from reviewers' personal preferences. To address such
issues, we propose to model personalization in an uncontaminated open-ended
generation assessment. We create two new datasets Per-MPST and Per-DOC for
personalized story evaluation, by re-purposing existing datasets with proper
anonymization and new personalized labels. We further develop a personalized
story evaluation model PERSE to infer reviewer preferences and provide a
personalized evaluation. Specifically, given a few exemplary reviews from a
particular reviewer, PERSE predicts either a detailed review or fine-grained
comparison in several aspects (such as interestingness and surprise) for that
reviewer on a new text input. Experimental results show that PERSE outperforms
GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on
pairwise preference prediction accuracy. Both datasets and code will be
released.
","2023-10-11","2310.03304v1.pdf"
"2310.03309","Shaotian Yan","Shaotian Yan, Chen Shen, Junjie Liu and Jieping Ye","Concise and Organized Perception Facilitates Large Language Models for
  Deductive Reasoning","Under Review","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Exploiting large language models (LLMs) to tackle deductive reasoning has
garnered growing attention. It still remains highly challenging to achieve
satisfactory results in complex deductive problems, characterized by plenty of
premises (i.e., facts or rules) entailing intricate relationships among
entities and requiring multi-hop reasoning. One intuitive solution is to
decompose the original task into smaller sub-tasks, and then chain the multiple
casual reasoning steps together in a forward (e.g., Selection-Inference) or
backward (e.g., LAMBADA) direction. However, these techniques inevitably
necessitate a large number of overall stages, leading to computationally
expensive operations and a higher possibility of making misleading steps. In
addition to stage-by-stage decomposition, we draw inspiration from another
aspect of human problem-solving. Humans tend to distill the most relevant
information and organize their thoughts systematically (e.g., creating mind
maps), which assists them in answering questions or drawing conclusions
precisely and quickly. In light of this, we propose a novel reasoning approach
named Concise and Organized Perception (COP). COP carefully analyzes the given
statements to efficiently identify the most pertinent information while
eliminating redundancy. It then prompts the LLMs in a more organized form that
adapts to the model's inference process. By perceiving concise and organized
proofs, the deductive reasoning abilities of LLMs can be better elicited, and
the risk of acquiring errors caused by excessive reasoning stages is mitigated.
Furthermore, our approach can be combined with the aforementioned ones to
further boost their performance. Extensive experimental results on three
popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD)
show that COP significantly outperforms previous state-of-the-art methods.
","2023-10-06","2310.03309v1.pdf"
"2310.03320","Zifeng Wang","Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, Vassilis N.
  Ioannidis, Huzefa Rangwala, Rishita Anubhai","BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph","this paper needs further internal review for being published","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Foundation models (FMs) are able to leverage large volumes of unlabeled data
to demonstrate superior performance across a wide range of tasks. However, FMs
developed for biomedical domains have largely remained unimodal, i.e.,
independently trained and used for tasks on protein sequences alone, small
molecule structures alone, or clinical data alone. To overcome this limitation
of biomedical FMs, we present BioBridge, a novel parameter-efficient learning
framework, to bridge independently trained unimodal FMs to establish multimodal
behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn
transformations between one unimodal FM and another without fine-tuning any
underlying unimodal FMs. Our empirical results demonstrate that BioBridge can
beat the best baseline KG embedding methods (on average by around 76.3%) in
cross-modal retrieval tasks. We also identify BioBridge demonstrates
out-of-domain generalization ability by extrapolating to unseen modalities or
relations. Additionally, we also show that BioBridge presents itself as a
general purpose retriever that can aid biomedical multimodal question answering
as well as enhance the guided generation of novel drugs.
","2023-10-16","2310.03320v1.pdf"
"2310.03324","Jie-Jing Shao","Jie-Jing Shao, Jiang-Xin Shi, Xiao-Wen Yang, Lan-Zhe Guo, Yu-Feng Li","Investigating the Limitation of CLIP Models: The Worst-Performing
  Categories","","","","","cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Contrastive Language-Image Pre-training (CLIP) provides a foundation model by
integrating natural language into visual concepts, enabling zero-shot
recognition on downstream tasks. It is usually expected that satisfactory
overall accuracy can be achieved across numerous domains through well-designed
textual prompts. However, we found that their performance in the worst
categories is significantly inferior to the overall performance. For example,
on ImageNet, there are a total of 10 categories with class-wise accuracy as low
as 0\%, even though the overall performance has achieved 64.1\%. This
phenomenon reveals the potential risks associated with using CLIP models,
particularly in risk-sensitive applications where specific categories hold
significant importance. To address this issue, we investigate the alignment
between the two modalities in the CLIP model and propose the Class-wise
Matching Margin (\cmm) to measure the inference confusion. \cmm\ can
effectively identify the worst-performing categories and estimate the potential
performance of the candidate prompts. We further query large language models to
enrich descriptions of worst-performing categories and build a weighted
ensemble to highlight the efficient prompts. Experimental results clearly
verify the effectiveness of our proposal, where the accuracy on the worst-10
categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,
laborious optimization, or access to labeled validation data.
","2023-10-06","2310.03324v1.pdf"
"2310.03328","Zhen Wan","Zhen wan, Yating Zhang, Yexiang Wang, Fei Cheng, Sadao Kurohashi","Reformulating Domain Adaptation of Large Language Models as
  Adapt-Retrieve-Revise","Under submission to ICLR 2024","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models (LLMs) like GPT-4 have recently demonstrated
astonishing zero-shot capabilities in general domain tasks, they often generate
content with hallucinations in specific domains such as Chinese law, hindering
their application in these areas. This is typically due to the absence of
training data that encompasses such a specific domain, preventing GPT-4 from
acquiring in-domain knowledge. A pressing challenge is that it's not plausible
to continue training LLMs of such scale on in-domain data.
  This paper introduces a simple and effective domain adaptation framework for
GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process.
The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain
by continuing learning on in-domain data. When solving a task, we leverage the
adapted LLM to generate a draft answer given a task query. Then, the draft
answer will be used to \textbf{retrieve} supporting evidence candidates from an
external in-domain knowledge base. Finally, the draft answer and retrieved
evidence are concatenated into a whole prompt to let GPT-4 assess the evidence
and \textbf{revise} the draft answer to generate the final answer.
  Our proposal combines the advantages of the efficiency of adapting a smaller
7B model with the evidence-assessing capability of GPT-4 and effectively
prevents GPT-4 from generating hallucinatory content. In the zero-shot setting
of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to
the direct generation by GPT-4. When compared to two stronger retrieval-based
baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be
released
","2023-10-16","2310.03328v1.pdf"
"2310.03331","Chiwun Yang","Timothy Chu, Zhao Song, Chiwun Yang","Fine-tune Language Models to Approximate Unbiased In-context Learning","","","","","cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In-context learning (ICL) is an astonishing emergent ability of large
language models (LLMs). By presenting a prompt that includes multiple
input-output pairs as examples and introducing a new query input, models can
generate the corresponding output. However, the performance of models heavily
relies on the quality of the input prompt when implementing in-context
learning. Biased or imbalanced input prompts can significantly degrade the
performance of language models. To address this issue, we introduce a
reweighted algorithm called RICL (Reweighted In-context Learning). This
algorithm fine-tunes language models using an unbiased validation set to
determine the optimal weight for each input-output example to approximate
unbiased in-context learning. Furthermore, we also introduce a low-cost
reweighted algorithm, a linear optimal weight approximation algorithm called
LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm
requires minimal training cost while providing effective results. We prove the
convergence of our algorithm and validate its performance through experiments
conducted on a numerical dataset. The experimental findings reveal a
substantial improvement in comparison to benchmarks including the performance
of casual prompt-based in-context learning and the performance of a classic
fine-tuning method.
","2023-10-06","2310.03331v1.pdf"
"2310.03368","Qinyuan Cheng","Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu,
  Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, Xipeng Qiu","Evaluating Hallucinations in Chinese Large Language Models","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.
","2023-10-26","2310.03368v1.pdf"
"2310.03376","Jennifer D'Souza","Anisa Rula and Jennifer D'Souza","Procedural Text Mining with Large Language Models","8 pages, 4 figures, Accepted to The Twelfth International Conference
  on Knowledge Capture (K-Cap 2023)","","","","cs.CL cs.AI cs.IT math.IT","http://creativecommons.org/licenses/by-sa/4.0/","  Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.
","2023-10-06","2310.03376v1.pdf"
"2310.03400","Huan Ma","Huan Ma, Changqing Zhang, Huazhu Fu, Peilin Zhao, Bingzhe Wu","Adapting Large Language Models for Content Moderation: Pitfalls in Data
  Engineering and Supervised Fine-tuning","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Nowadays, billions of people engage in communication and express their
opinions on the internet daily. Unfortunately, not all of these expressions are
friendly or compliant, making content moderation an indispensable task. With
the successful development of Large Language Models (LLMs) in recent years,
LLM-based methods have become a feasible solution for handling tasks in various
domains. However, in the field of content moderation, there is still a lack of
detailed work that systematically introduces implementation details. In this
paper, we introduce how to fine-tune an LLM model that can be privately
deployed for content moderation. Specifically, we discuss whether incorporating
reasons during the fine-tuning process would be better or if it should be
treated as a classification task directly. We also explore the benefits of
utilizing reasons generated by more powerful LLMs for fine-tuning privately
deployed models and the impact of different processing approaches when the
answers generated by the more powerful LLMs are incorrect. We report the entire
research process and the key findings in this paper, hoping to provide valuable
experience for researchers who are fine-tuning privately deployed models in
their domain-specific research.
","2023-10-06","2310.03400v1.pdf"
"2310.03414","LItton Jose Kurisinkel","Litton J Kurisinkel, Nancy F. Chen","LLM Based Multi-Document Summarization Exploiting Main-Event Biased
  Monotone Submodular Content Extraction","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Multi-document summarization is a challenging task due to its inherent
subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4
among DUC-2004 reference summaries. In this work, we aim to enhance the
objectivity of news summarization by focusing on the main event of a group of
related news documents and presenting it coherently with sufficient context.
Our primary objective is to succinctly report the main event, ensuring that the
summary remains objective and informative. To achieve this, we employ an
extract-rewrite approach that incorporates a main-event biased
monotone-submodular function for content selection. This enables us to extract
the most crucial information related to the main event from the document
cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for
rewriting the extracted content into a coherent text. The evaluation using
objective metrics and human evaluators confirms the effectiveness of our
approach, as it surpasses potential baselines, demonstrating excellence in both
content coverage, coherence, and informativeness.
","2023-10-06","2310.03414v1.pdf"
"2310.03424","Thiago Fraga Da Silva","Leonardo Emili, Thiago Fraga-Silva, Ernest Pusateri, Markus
  Nu{\ss}baum-Thom, Youssef Oualil","Neural Language Model Pruning for Automatic Speech Recognition","8 pages, 3 figures","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study model pruning methods applied to Transformer-based neural network
language models for automatic speech recognition. We explore three aspects of
the pruning frame work, namely criterion, method and scheduler, analyzing their
contribution in terms of accuracy and inference speed. To the best of our
knowledge, such in-depth analyses on large-scale recognition systems has not
been reported in the literature. In addition, we propose a variant of low-rank
approximation suitable for incrementally compressing models, and delivering
multiple models with varied target sizes. Among other results, we show that a)
data-driven pruning outperforms magnitude-driven in several scenarios; b)
incremental pruning achieves higher accuracy compared to one-shot pruning,
especially when targeting smaller sizes; and c) low-rank approximation presents
the best trade-off between size reduction and inference speed-up for moderate
compression.
","2023-10-06","2310.03424v1.pdf"
"2310.03473","LItton Jose Kurisinkel","Litton J Kurisinkel, Nancy F chen","Controllable Multi-document Summarization: Coverage & Coherence
  Intuitive Policy with Large Language Model Based Rewards","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Memory-efficient large language models are good at refining text input for
better readability. However, controllability is a matter of concern when it
comes to text generation tasks with long inputs, such as multi-document
summarization. In this work, we investigate for a generic controllable approach
for multi-document summarization that leverages the capabilities of LLMs to
refine the text. In particular, we train a controllable content extraction
scheme to extract the text that will be refined by an LLM. The scheme is
designed with a novel coverage and coherence intuitive policy, which is duly
rewarded by a passively trained LLM. Our approach yields competitive results in
the evaluation using ROUGE metrics and outperforms potential baselines in
coherence, as per human evaluation.
","2023-10-06","2310.03473v1.pdf"
"2310.03477","Francois Remy","Fran\c{c}ois Remy, Pieter Delobelle, Bettina Berendt, Kris Demuynck,
  Thomas Demeester","Tik-to-Tok: Translating Language Models One Token at a Time: An
  Embedding Initialization Strategy for Efficient Language Adaptation","As first reviewed at TACL","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Training monolingual language models for low and mid-resource languages is
made challenging by limited and often inadequate pretraining data. In this
study, we propose a novel model conversion strategy to address this issue,
adapting high-resources monolingual language models to a new target language.
By generalizing over a word translation dictionary encompassing both the source
and target languages, we map tokens from the target tokenizer to semantically
similar tokens from the source language tokenizer. This one-to-many token
mapping improves tremendously the initialization of the embedding table for the
target language. We conduct experiments to convert high-resource models to mid-
and low-resource languages, namely Dutch and Frisian. These converted models
achieve a new state-of-the-art performance on these languages across all sorts
of downstream tasks. By reducing significantly the amount of data and time
required for training state-of-the-art models, our novel model conversion
strategy has the potential to benefit many languages worldwide.
","2023-10-06","2310.03477v1.pdf"
"2310.03498","Constantinos Patsakis","Anargyros Chrysanthou, Yorgos Pantis, Constantinos Patsakis","The Anatomy of Deception: Technical and Human Perspectives on a
  Large-scale Phishing Campaign","","","","","cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In an era dominated by digital interactions, phishing campaigns have evolved
to exploit not just technological vulnerabilities but also human traits. This
study takes an unprecedented deep dive into large-scale phishing campaigns
aimed at Meta's users, offering a dual perspective on the technical mechanics
and human elements involved. Analysing data from over 25,000 victims worldwide,
we highlight the nuances of these campaigns, from the intricate techniques
deployed by the attackers to the sentiments and behaviours of those who were
targeted. Unlike prior research conducted in controlled environments, this
investigation capitalises on the vast, diverse, and genuine data extracted
directly from active phishing campaigns, allowing for a more holistic
understanding of the drivers, facilitators, and human factors. Through the
application of advanced computational techniques, including natural language
processing and machine learning, this work unveils critical insights into the
psyche of victims and the evolving tactics of modern phishers. Our analysis
illustrates very poor password selection choices from the victims but also
persistence in the revictimisation of a significant part of the users. Finally,
we reveal many correlations regarding demographics, timing, sentiment, emotion,
and tone of the victims' responses.
","2023-10-06","2310.03498v1.pdf"
"2310.03502","Andrey Kuznetsov","Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir
  Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko,
  Andrey Kuznetsov and Denis Dimitrov","Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and
  Latent Diffusion","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Text-to-image generation is a significant domain in modern computer vision
and has achieved substantial improvements through the evolution of generative
architectures. Among these, there are diffusion-based models that have
demonstrated essential quality enhancements. These models are generally split
into two categories: pixel-level and latent-level approaches. We present
Kandinsky1, a novel exploration of latent diffusion architecture, combining the
principles of the image prior models with latent diffusion techniques. The
image prior model is trained separately to map text embeddings to image
embeddings of CLIP. Another distinct feature of the proposed model is the
modified MoVQ implementation, which serves as the image autoencoder component.
Overall, the designed model contains 3.3B parameters. We also deployed a
user-friendly demo system that supports diverse generative modes such as
text-to-image generation, image fusion, text and image fusion, image variations
generation, and text-guided inpainting/outpainting. Additionally, we released
the source code and checkpoints for the Kandinsky models. Experimental
evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking
our model as the top open-source performer in terms of measurable image
generation quality.
","2023-10-06","2310.03502v1.pdf"
"2310.03511","Julia Westermayr","Rhyan Barrett, Julia Westermayr","Reinforcement learning for traversing chemical structure space:
  Optimizing transition states and minimum energy paths of molecules","30 pages, 10 figures","","","","physics.chem-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, deep learning has made remarkable strides, surpassing human
capabilities in tasks like strategy games, and it has found applications in
complex domains, including protein folding. In the realm of quantum chemistry,
machine learning methods have primarily served as predictive tools or design
aids using generative models, while reinforcement learning remains in its early
stages of exploration. This work introduces an actor-critic reinforcement
learning framework suitable for diverse optimization tasks, such as searching
for molecular structures with specific properties within conformational spaces.
As an example, we show an implementation of this scheme for calculating minimum
energy pathways of a Claisen rearrangement reaction and a number of SN2
reactions. Our results show that the algorithm is able to accurately predict
minimum energy pathways and thus, transition states, therefore providing the
first steps in using actor-critic methods to study chemical reactions.
","2023-10-06","2310.03511v1.pdf"
"2310.03533","Jie Zhang","Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho
  Sengupta, Shin Yoo, Jie M. Zhang","Large Language Models for Software Engineering: Survey and Open Problems","","","","","cs.SE","http://creativecommons.org/licenses/by/4.0/","  This paper provides a survey of the emerging area of Large Language Models
(LLMs) for Software Engineering (SE). It also sets out open research challenges
for the application of LLMs to technical problems faced by software engineers.
LLMs' emergent properties bring novelty and creativity with applications right
across the spectrum of Software Engineering activities including coding,
design, requirements, repair, refactoring, performance improvement,
documentation and analytics. However, these very same emergent properties also
pose significant technical challenges; we need techniques that can reliably
weed out incorrect solutions, such as hallucinations. Our survey reveals the
pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in
the development and deployment of reliable, efficient and effective LLM-based
SE.
","2023-10-12","2310.03533v1.pdf"
"2310.03560","Fergus Imrie","Fergus Imrie, Paulius Rauba, Mihaela van der Schaar","Redefining Digital Health Interfaces with Large Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their use remains comparatively limited due,
in part, to challenges surrounding usability and trust. Recently, Large
Language Models (LLMs) have emerged as general-purpose models with the ability
to process complex information and produce human-quality text, presenting a
wealth of potential applications in healthcare. Directly applying LLMs in
clinical settings is not straightforward, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLMs can utilize
external tools to provide a novel interface between clinicians and digital
technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLM
in clinical settings such as hallucinations. We illustrate our approach with
examples from cardiovascular disease and diabetes risk prediction, highlighting
the benefit compared to traditional interfaces for digital tools.
","2023-10-06","2310.03560v1.pdf"
"2310.03589","Federico Garza","Azul Garza, Max Mergenthaler-Canseco","TimeGPT-1","","","","","cs.LG stat.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we introduce TimeGPT, the first foundation model for time
series, capable of generating accurate predictions for diverse datasets not
seen during training. We evaluate our pre-trained model against established
statistical, machine learning, and deep learning methods, demonstrating that
TimeGPT zero-shot inference excels in performance, efficiency, and simplicity.
Our study provides compelling evidence that insights from other domains of
artificial intelligence can be effectively applied to time series analysis. We
conclude that large-scale time series models offer an exciting opportunity to
democratize access to precise predictions and reduce uncertainty by leveraging
the capabilities of contemporary advancements in deep learning.
","2023-10-06","2310.03589v1.pdf"
"2310.03659","Thorsten H\""andler","Thorsten H\""andler","Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for
  Autonomous LLM-powered Multi-Agent Architectures","","","","","cs.AI cs.MA cs.SE","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.
","2023-10-06","2310.03659v1.pdf"
"2310.03666","Harshad Hegde","Nicolas Matentzoglu, J. Harry Caufield, Harshad B. Hegde, Justin T.
  Reese, Sierra Moxon, Hyeongsik Kim, Nomi L. Harris, Melissa A Haendel,
  Christopher J. Mungall","MapperGPT: Large Language Models for Linking and Mapping Entities","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
  Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
  We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.
","2023-10-06","2310.03666v1.pdf"
"2310.03668","Iker Garc\'ia-Ferrero","Oscar Sainz, Iker Garc\'ia-Ferrero, Rodrigo Agerri, Oier Lopez de
  Lacalle, German Rigau, Eneko Agirre","GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction","","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Large Language Models (LLMs) combined with instruction tuning have made
significant progress when generalizing to unseen tasks. However, they have been
less successful in Information Extraction (IE), lagging behind task-specific
models. Typically, IE tasks are characterized by complex annotation guidelines
which describe the task and give examples to humans. Previous attempts to
leverage such information have failed, even with the largest models, as they
are not able to follow the guidelines out-of-the-box. In this paper we propose
GoLLIE (Guideline-following Large Language Model for IE), a model able to
improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to
comply with annotation guidelines. Comprehensive evaluation empirically
demonstrates that GoLLIE is able to generalize to and follow unseen guidelines,
outperforming previous attempts at zero-shot information extraction. The
ablation study shows that detailed guidelines is key for good results.
","2023-10-09","2310.03668v1.pdf"
"2310.03684","Alexander Robey","Alexander Robey and Eric Wong and Hamed Hassani and George J. Pappas","SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks","","","","","cs.LG cs.AI stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM.
","2023-10-16","2310.03684v1.pdf"
"2310.03686","Anna Langedijk","Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap
  Jumelet","DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In recent years, many interpretability methods have been proposed to help
interpret the internal states of Transformer-models, at different levels of
precision and complexity. Here, to analyze encoder-decoder Transformers, we
propose a simple, new method: DecoderLens. Inspired by the LogitLens (for
decoder-only Transformers), this method involves allowing the decoder to
cross-attend representations of intermediate encoder layers instead of using
the final encoder output, as is normally done in encoder-decoder models. The
method thus maps previously uninterpretable vector representations to
human-interpretable sequences of words or symbols. We report results from the
DecoderLens applied to models trained on question answering, logical reasoning,
speech recognition and machine translation. The DecoderLens reveals several
specific subtasks that are solved at low or intermediate layers, shedding new
light on the information flow inside the encoder component of this important
class of models.
","2023-10-06","2310.03686v1.pdf"
"2310.03691","Damien Masson","Damien Masson, Sylvain Malacria, G\'ery Casiez, Daniel Vogel","DirectGPT: A Direct Manipulation Interface to Interact with Large
  Language Models","","","","","cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We characterize and demonstrate how the principles of direct manipulation can
improve interaction with large language models. This includes: continuous
representation of generated objects of interest; reuse of prompt syntax in a
toolbar of commands; manipulable outputs to compose or control the effect of
prompts; and undo mechanisms. This idea is exemplified in DirectGPT, a user
interface layer on top of ChatGPT that works by transforming direct
manipulation actions to engineered prompts. A study shows participants were 50%
faster and relied on 50% fewer and 72% shorter prompts to edit text, code, and
vector images compared to baseline ChatGPT. Our work contributes a validated
approach to integrate LLMs into traditional software using direct manipulation.
","2023-10-06","2310.03691v1.pdf"
"2310.03693","Xiangyu Qi","Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
  Mittal, Peter Henderson","Fine-tuning Aligned Language Models Compromises Safety, Even When Users
  Do Not Intend To!","","","","","cs.CL cs.AI cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Optimizing large language models (LLMs) for downstream use cases often
involves the customization of pre-trained LLMs through further fine-tuning.
Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5
Turbo on custom datasets also encourage this practice. But, what are the safety
costs associated with such custom fine-tuning? We note that while existing
safety alignment infrastructures can restrict harmful behaviors of LLMs at
inference time, they do not cover safety risks when fine-tuning privileges are
extended to end-users. Our red teaming studies find that the safety alignment
of LLMs can be compromised by fine-tuning with only a few adversarially
designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety
guardrails by fine-tuning it on only 10 such examples at a cost of less than
$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful
instructions. Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used datasets can
also inadvertently degrade the safety alignment of LLMs, though to a lesser
extent. These findings suggest that fine-tuning aligned LLMs introduces new
safety risks that current safety infrastructures fall short of addressing --
even if a model's initial safety alignment is impeccable, it is not necessarily
to be maintained after custom fine-tuning. We outline and critically analyze
potential mitigations and advocate for further research efforts toward
reinforcing safety protocols for the custom fine-tuning of aligned LLMs.
","2023-10-06","2310.03693v1.pdf"
"2310.03708","Jie Liu","Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue,
  Wanli Ouyang, Yu Qiao","Beyond One-Preference-for-All: Multi-Objective Direct Preference
  Optimization for Language Models","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A single language model (LM), despite aligning well with an average labeler
through reinforcement learning from human feedback (RLHF), may not universally
suit diverse human preferences. Recent approaches thus pursue customization,
training separate principle-based reward models to represent different
alignment objectives (e.g. helpfulness, harmlessness, or honesty). Different
LMs can then be trained for different preferences through multi-objective RLHF
(MORLHF) with different objective weightings. Yet, RLHF is unstable and
resource-heavy, especially for MORLHF with diverse and usually conflicting
objectives. In this paper, we present Multi-Objective Direct Preference
Optimization (MODPO), an RL-free algorithm that extends Direct Preference
Optimization (DPO) for multiple alignment objectives. Essentially, MODPO folds
LM learning directly into reward modeling, aligning LMs with the weighted sum
of all principle-based rewards using pure cross-entropy loss. While
theoretically guaranteed to produce the same optimal solutions as MORLHF, MODPO
is practically more stable and computationally efficient, obviating value
function modeling and online sample collection. Empirical results in safety
alignment and long-form question answering confirm that MODPO matches or
outperforms existing methods, consistently producing one of the most
competitive LM fronts that cater to diverse preferences with 3 times fewer
computations compared with MORLHF.
","2023-10-18","2310.03708v1.pdf"
"2310.03710","Kyle Montgomery","Nicholas Crispino and Kyle Montgomery and Fankun Zeng and Dawn Song
  and Chenguang Wang","Agent Instructs Large Language Models to be General Zero-Shot Reasoners","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
","2023-10-06","2310.03710v1.pdf"
"2310.03714","Omar Khattab","Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav
  Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi,
  Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts","DSPy: Compiling Declarative Language Model Calls into Self-Improving
  Pipelines","","","","","cs.CL cs.AI cs.IR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The ML community is rapidly exploring techniques for prompting language
models (LMs) and for stacking them into pipelines that solve complex tasks.
Unfortunately, existing LM pipelines are typically implemented using hard-coded
""prompt templates"", i.e. lengthy strings discovered via trial and error. Toward
a more systematic approach for developing and optimizing LM pipelines, we
introduce DSPy, a programming model that abstracts LM pipelines as text
transformation graphs, i.e. imperative computational graphs where LMs are
invoked through declarative modules. DSPy modules are parameterized, meaning
they can learn (by creating and collecting demonstrations) how to apply
compositions of prompting, finetuning, augmentation, and reasoning techniques.
We design a compiler that will optimize any DSPy pipeline to maximize a given
metric. We conduct two case studies, showing that succinct DSPy programs can
express and optimize sophisticated LM pipelines that reason about math word
problems, tackle multi-hop retrieval, answer complex questions, and control
agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and
llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot
prompting (generally by over 25% and 65%, respectively) and pipelines with
expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top
of that, DSPy programs compiled to open and relatively small LMs like
770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely
on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at
https://github.com/stanfordnlp/dspy
","2023-10-06","2310.03714v1.pdf"
"2310.03715","Nestor Maslej","Nestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy,
  Katrina Ligett, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles,
  Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault","Artificial Intelligence Index Report 2023","","","","","cs.AI cs.CY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Welcome to the sixth edition of the AI Index Report. This year, the report
introduces more original data than any previous edition, including a new
chapter on AI public opinion, a more thorough technical performance chapter,
original analysis about large language and multimodal models, detailed trends
in global AI legislation records, a study of the environmental impact of AI
systems, and more. The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Our mission is to provide
unbiased, rigorously vetted, broadly sourced data in order for policymakers,
researchers, executives, journalists, and the general public to develop a more
thorough and nuanced understanding of the complex field of AI. The report aims
to be the world's most credible and authoritative source for data and insights
about AI.
","2023-10-06","2310.03715v1.pdf"
"2310.03716","Prasann Singhal","Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett","A Long Way to Go: Investigating Length Correlations in RLHF","20 pages, 12 figures","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Great successes have been reported using Reinforcement Learning from Human
Feedback (RLHF) to align large language models. Open-source preference datasets
and reward models have enabled wider experimentation beyond generic chat
settings, particularly to make systems more ""helpful"" for tasks like web
question answering, summarization, and multi-turn dialogue. When optimizing for
helpfulness, RLHF has been consistently observed to drive models to produce
longer outputs. This paper demonstrates that optimizing for response length is
a significant factor behind RLHF's reported improvements in these settings.
First, we study the relationship between reward and length for reward models
trained on three open-source preference datasets for helpfulness. Here, length
correlates strongly with reward, and improvements in reward score are driven in
large part by shifting the distribution over output lengths. We then explore
interventions during both RL and reward model learning to see if we can achieve
the same downstream improvements as RLHF without increasing length. While our
interventions mitigate length increases, they aren't uniformly effective across
settings. Furthermore, we find that even running RLHF with a reward based
solely on length can reproduce most of the downstream improvements over the
initial policy model, showing that reward models in these settings have a long
way to go.
","2023-10-06","2310.03716v1.pdf"
"2310.03720","Paloma Sodhi","Paloma Sodhi, S.R.K. Branavan, Ryan McDonald","HeaP: Hierarchical Policies for Web Actions using LLMs","38 pages, 14 figures","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have demonstrated remarkable capabilities in
performing a range of instruction following tasks in few and zero-shot
settings. However, teaching LLMs to perform tasks on the web presents
fundamental challenges -- combinatorially large open-world tasks and variations
across web interfaces. We tackle these challenges by leveraging LLMs to
decompose web tasks into a collection of sub-tasks, each of which can be solved
by a low-level, closed-loop policy. These policies constitute a shared grammar
across tasks, i.e., new web tasks can be expressed as a composition of these
policies. We propose a novel framework, Hierarchical Policies for Web Actions
using LLMs (HeaP), that learns a set of hierarchical LLM prompts from
demonstrations for planning high-level tasks and executing them via a sequence
of low-level policies. We evaluate HeaP against a range of baselines on a suite
of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as
live website interactions, and show that it is able to outperform prior works
using orders of magnitude less data.
","2023-10-06","2310.03720v1.pdf"
"2310.03731","Aojun Zhou","Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi,
  Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li","MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical
  Reasoning","The state-of-the-art open-source language models for mathematical
  reasoning","","","","cs.CL cs.AI cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recently released GPT-4 Code Interpreter has demonstrated remarkable
proficiency in solving challenging math problems, primarily attributed to its
ability to seamlessly reason with natural language, generate code, execute
code, and continue reasoning based on the execution output. In this paper, we
present a method to fine-tune open-source language models, enabling them to use
code for modeling and deriving math equations and, consequently, enhancing
their mathematical reasoning abilities. We propose a method of generating novel
and high-quality datasets with math problems and their code-based solutions,
referred to as MathCodeInstruct. Each solution interleaves natural language,
code, and execution results. We also introduce a customized supervised
fine-tuning and inference approach. This approach yields the MathCoder models,
a family of models capable of generating code-based solutions for solving
challenging math problems. Impressively, the MathCoder models achieve
state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K
(83.9%) datasets, substantially outperforming other open-source alternatives.
Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K
and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The
dataset and models will be released at https://github.com/mathllm/MathCoder.
","2023-10-06","2310.03731v1.pdf"
"2310.03734","Tianhong Li","Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred
  Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan","Leveraging Unpaired Data for Vision-Language Generative Models via Cycle
  Consistency","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Current vision-language generative models rely on expansive corpora of paired
image-text data to attain optimal performance and generalization capabilities.
However, automatically collecting such data (e.g. via large-scale web scraping)
leads to low quality and poor image-text correlation, while human annotation is
more accurate but requires significant manual effort and expense. We introduce
$\textbf{ITIT}$ ($\textbf{I}$n$\textbf{T}$egrating $\textbf{I}$mage
$\textbf{T}$ext): an innovative training paradigm grounded in the concept of
cycle consistency which allows vision-language training on unpaired image and
text data. ITIT is comprised of a joint image-text encoder with disjoint image
and text decoders that enable bidirectional image-to-text and text-to-image
generation in a single framework. During training, ITIT leverages a small set
of paired image-text data to ensure its output matches the input reasonably
well in both directions. Simultaneously, the model is also trained on much
larger datasets containing only images or texts. This is achieved by enforcing
cycle consistency between the original unpaired samples and the cycle-generated
counterparts. For instance, it generates a caption for a given input image and
then uses the caption to create an output image, and enforces similarity
between the input and output images. Our experiments show that ITIT with
unpaired datasets exhibits similar scaling behavior as using high-quality
paired data. We demonstrate image generation and captioning performance on par
with state-of-the-art text-to-image and image-to-text models with orders of
magnitude fewer (only 3M) paired image-text data.
","2023-10-06","2310.03734v1.pdf"
"2310.03738","Stefan Smeu","Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu","Stylist: Style-Driven Feature Ranking for Robust Novelty Detection","","","","","cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Novelty detection aims at finding samples that differ in some form from the
distribution of seen samples. But not all changes are created equal. Data can
suffer a multitude of distribution shifts, and we might want to detect only
some types of relevant changes. Similar to works in out-of-distribution
generalization, we propose to use the formalization of separating into semantic
or content changes, that are relevant to our task, and style changes, that are
irrelevant. Within this formalization, we define the robust novelty detection
as the task of finding semantic changes while being robust to style
distributional shifts. Leveraging pretrained, large-scale model
representations, we introduce Stylist, a novel method that focuses on dropping
environment-biased features. First, we compute a per-feature score based on the
feature distribution distances between environments. Next, we show that our
selection manages to remove features responsible for spurious correlations and
improve novelty detection performance. For evaluation, we adapt domain
generalization datasets to our task and analyze the methods behaviors. We
additionally built a large synthetic dataset where we have control over the
spurious correlations degree. We prove that our selection mechanism improves
novelty detection algorithms across multiple datasets, containing both
stylistic and content shifts.
","2023-10-06","2310.03738v1.pdf"
"2310.03739","Mihir Prabhudesai","Mihir Prabhudesai and Anirudh Goyal and Deepak Pathak and Katerina
  Fragkiadaki","Aligning Text-to-Image Diffusion Models with Reward Backpropagation","Code available at https://align-prop.github.io/","","","","cs.CV cs.AI cs.LG cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
","2023-10-06","2310.03739v1.pdf"
"2310.03744","Haotian Liu","Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee","Improved Baselines with Visual Instruction Tuning","Tech report, 4 pages. LLaVA project page: https://llava-vl.github.io","","","","cs.CV cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large multimodal models (LMM) have recently shown encouraging progress with
visual instruction tuning. In this note, we show that the fully-connected
vision-language cross-modal connector in LLaVA is surprisingly powerful and
data-efficient. With simple modifications to LLaVA, namely, using
CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA
data with simple response formatting prompts, we establish stronger baselines
that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint
uses merely 1.2M publicly available data, and finishes full training in ~1 day
on a single 8-A100 node. We hope this can make state-of-the-art LMM research
more accessible. Code and model will be publicly available.
","2023-10-06","2310.03744v1.pdf"
"2310.03777","Saifullah Saifullah","Saifullah Saifullah (1 and 2), Stefan Agne (2 and 3), Andreas Dengel
  (1 and 2), Sheraz Ahmed (2 and 3) ((1) Department of Computer Science,
  University of Kaiserslautern-Landau, Kaiserslautern, Rhineland-Palatinate,
  Germany, (2) German Research Center for Artificial Intelligence, DFKI GmbH,
  Kaiserslautern, Rhineland-Palatinate, Germany, (3) DeepReader GmbH,
  Kaiserlautern, Germany)","PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In this paper, we introduce strategies for developing private Key Information
Extraction (KIE) systems by leveraging large pretrained document foundation
models in conjunction with differential privacy (DP), federated learning (FL),
and Differentially Private Federated Learning (DP-FL). Through extensive
experimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts,
XFUND, and DOCILE), we demonstrate that large document foundation models can be
effectively fine-tuned for the KIE task under private settings to achieve
adequate performance while maintaining strong privacy guarantees. Moreover, by
thoroughly analyzing the impact of various training and model parameters on
model performance, we propose simple yet effective guidelines for achieving an
optimal privacy-utility trade-off for the KIE task under global DP. Finally, we
introduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling
global DP from a standalone context to a multi-client federated environment. We
conduct a comprehensive evaluation of the algorithm across various client and
privacy settings, and demonstrate its capability to achieve comparable
performance and privacy guarantees to standalone DP, even when accommodating an
increasing number of participating clients. Overall, our study offers valuable
insights into the development of private KIE systems, and highlights the
potential of document foundation models for privacy-preserved Document AI
applications. To the best of authors' knowledge, this is the first work that
explores privacy preserved document KIE using document foundation models.
","2023-10-09","2310.03777v1.pdf"
"2310.03780","Adish Singla","Tung Phung, Victor-Alexandru P\u{a}durean, Anjali Singh, Christopher
  Brooks, Jos\'e Cambronero, Sumit Gulwani, Adish Singla, Gustavo Soares","Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4
  Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor'' model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student'' model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.
","2023-10-09","2310.03780v1.pdf"
"2310.03789","Noa Rubin","Noa Rubin, Inbar Seroussi, Zohar Ringel","Droplets of Good Representations: Grokking as a First Order Phase
  Transition in Two Layer Networks","","","","","stat.ML cond-mat.dis-nn cs.LG","http://creativecommons.org/licenses/by/4.0/","  A key property of deep neural networks (DNNs) is their ability to learn new
features during training. This intriguing aspect of deep learning stands out
most clearly in recently reported Grokking phenomena. While mainly reflected as
a sudden increase in test accuracy, Grokking is also believed to be a beyond
lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here
we apply a recent development in the theory of feature learning, the adaptive
kernel approach, to two teacher-student models with cubic-polynomial and
modular addition teachers. We provide analytical predictions on feature
learning and Grokking properties of these models and demonstrate a mapping
between Grokking and the theory of phase transitions. We show that after
Grokking, the state of the DNN is analogous to the mixed phase following a
first-order phase transition. In this mixed phase, the DNN generates useful
internal representations of the teacher that are sharply distinct from those
before the transition.
","2023-10-09","2310.03789v1.pdf"
"2310.03838","Harsh Chaudhari","Harsh Chaudhari, Giorgio Severi, Alina Oprea, Jonathan Ullman","Chameleon: Increasing Label-Only Membership Leakage with Adaptive
  Poisoning","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  The integration of machine learning (ML) in numerous critical applications
introduces a range of privacy concerns for individuals who provide their
datasets for model training. One such privacy risk is Membership Inference
(MI), in which an attacker seeks to determine whether a particular data sample
was included in the training dataset of a model. Current state-of-the-art MI
attacks capitalize on access to the model's predicted confidence scores to
successfully perform membership inference, and employ data poisoning to further
enhance their effectiveness. In this work, we focus on the less explored and
more realistic label-only setting, where the model provides only the predicted
label on a queried sample. We show that existing label-only MI attacks are
ineffective at inferring membership in the low False Positive Rate (FPR)
regime. To address this challenge, we propose a new attack Chameleon that
leverages a novel adaptive data poisoning strategy and an efficient query
selection method to achieve significantly more accurate membership inference
than existing label-only attacks, especially at low FPRs.
","2023-10-09","2310.03838v1.pdf"
"2310.03842","Pranam Chatterjee","Tianlai Chen, Sarah Pertsemlidis, Venkata Srikar Kavirayuni, Pranay
  Vure, Rishab Pulugurta, Ashley Hsu, Sophia Vincoff, Vivian Yudistyra, Lauren
  Hong, Tian Wang, Elena Haarer, Lin Zhao, Pranam Chatterjee","PepMLM: Target Sequence-Conditioned Generation of Peptide Binders via
  Masked Language Modeling","","","","","q-bio.BM","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Target proteins that lack accessible binding pockets and conformational
stability have posed increasing challenges for drug development. Induced
proximity strategies, such as PROTACs and molecular glues, have thus gained
attention as pharmacological alternatives, but still require small molecule
docking at binding pockets for targeted protein degradation (TPD). The
computational design of protein-based binders presents unique opportunities to
access undruggable targets, but have often relied on stable 3D structures or
predictions for effective binder generation. Recently, we have leveraged the
expressive latent spaces of protein language models (pLMs) for the
prioritization of peptide binders from sequence alone, which we have then fused
to E3 ubiquitin ligase domains, creating a CRISPR-analogous TPD system for
target proteins. However, our methods rely on training discriminator models for
ranking heuristically or unconditionally-derived guide peptides for their
target binding capability. In this work, we introduce PepMLM, a purely target
sequence-conditioned de novo generator of linear peptide binders. By employing
a novel masking strategy that uniquely positions cognate peptide sequences at
the terminus of target protein sequences, PepMLM tasks the state-of-the-art
ESM-2 pLM to fully reconstruct the binder region, achieving low perplexities
matching or improving upon previously-validated peptide-protein sequence pairs.
After successful in silico benchmarking with AlphaFold-Multimer, we
experimentally verify PepMLM's efficacy via fusion of model-derived peptides to
E3 ubiquitin ligase domains, demonstrating endogenous degradation of target
substrates in cellular models. In total, PepMLM enables the generative design
of candidate binders to any target protein, without the requirement of target
structure, empowering downstream programmable proteome editing applications.
","2023-10-09","2310.03842v1.pdf"
"2310.03874","Jason Holmes PhD","Jason Holmes, Lian Zhang, Yuzhen Ding, Hongying Feng, Zhengliang Liu,
  Tianming Liu, William W. Wong, Sujay A. Vora, Jonathan B. Ashman, Wei Liu","Benchmarking a foundation LLM on its ability to re-label structure names
  in accordance with the AAPM TG-263 report","20 pages, 5 figures, 1 table","","","","physics.med-ph cs.CL","http://creativecommons.org/licenses/by/4.0/","  Purpose: To introduce the concept of using large language models (LLMs) to
re-label structure names in accordance with the American Association of
Physicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a
benchmark for future studies to reference.
  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4
application programming interface (API) was implemented as a Digital Imaging
and Communications in Medicine (DICOM) storage server, which upon receiving a
structure set DICOM file, prompts GPT-4 to re-label the structure names of both
target volumes and normal tissues according to the AAPM TG-263. Three disease
sites, prostate, head and neck, and thorax were selected for evaluation. For
each disease site category, 150 patients were randomly selected for manually
tuning the instructions prompt (in batches of 50) and 50 patients were randomly
selected for evaluation. Structure names that were considered were those that
were most likely to be relevant for studies utilizing structure contours for
many patients.
  Results: The overall re-labeling accuracy of both target volumes and normal
tissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and
96.9% respectively. Re-labeling of target volumes was less accurate on average
except for prostate - 100%, 93.1%, and 91.1% respectively.
  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of
both target volumes and normal tissues as presented in this work, LLMs are
poised to be the preferred method for standardizing structure names in
radiation oncology, especially considering the rapid advancements in LLM
capabilities that are likely to continue.
","2023-10-09","2310.03874v1.pdf"
"2310.03878","Yao Dou","Yao Dou, Philippe Laban, Claire Gardent, Wei Xu","Automatic and Human-AI Interactive Text Generation","To appear at ACL 2024, Tutorial","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  In this tutorial, we focus on text-to-text generation, a class of natural
language generation (NLG) tasks, that takes a piece of text as input and then
generates a revision that is improved according to some specific criteria
(e.g., readability or linguistic styles), while largely retaining the original
meaning and the length of the text. This includes many useful applications,
such as text simplification, paraphrase generation, style transfer, etc. In
contrast to text summarization and open-ended text completion (e.g., story),
the text-to-text generation tasks we discuss in this tutorial are more
constrained in terms of semantic consistency and targeted language styles. This
level of control makes these tasks ideal testbeds for studying the ability of
models to generate text that is both semantically adequate and stylistically
appropriate. Moreover, these tasks are interesting from a technical standpoint,
as they require complex combinations of lexical and syntactical
transformations, stylistic control, and adherence to factual knowledge, -- all
at once. With a special focus on text simplification and revision, this
tutorial aims to provide an overview of the state-of-the-art natural language
generation research from four major aspects -- Data, Models, Human-AI
Collaboration, and Evaluation -- and to discuss and showcase a few significant
and recent advances: (1) the use of non-retrogressive approaches; (2) the shift
from fine-tuning to prompting with large language models; (3) the development
of new learnable metric and fine-grained human evaluation framework; (4) a
growing body of studies and datasets on non-English languages; (5) the rise of
HCI+NLP+Accessibility interdisciplinary research to create real-world writing
assistant systems.
","2023-10-09","2310.03878v1.pdf"
"2310.03903","Saaket Agashe","Saaket Agashe, Yue Fan, Xin Eric Wang","Evaluating Multi-Agent Coordination Abilities in Large Language Models","","","","","cs.CL cs.MA","http://creativecommons.org/licenses/by/4.0/","  A pivotal aim in contemporary AI research is to develop agents proficient in
multi-agent coordination, enabling effective collaboration with both humans and
other systems. Large Language Models (LLMs), with their notable ability to
understand, generate, and interpret language in a human-like manner, stand out
as promising candidates for the development of such agents. In this study, we
build and assess the effectiveness of agents crafted using LLMs in various
coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework,
specifically designed to enable LLMs to play coordination games. With the
LLM-Co framework, we conduct our evaluation with three game environments and
organize the evaluation into five aspects: Theory of Mind, Situated Reasoning,
Sustained Coordination, Robustness to Partners, and Explicit Assistance. First,
the evaluation of the Theory of Mind and Situated Reasoning reveals the
capabilities of LLM to infer the partner's intention and reason actions
accordingly. Then, the evaluation around Sustained Coordination and Robustness
to Partners further showcases the ability of LLMs to coordinate with an unknown
partner in complex long-horizon tasks, outperforming Reinforcement Learning
baselines. Lastly, to test Explicit Assistance, which refers to the ability of
an agent to offer help proactively, we introduce two novel layouts into the
Overcooked-AI benchmark, examining if agents can prioritize helping their
partners, sacrificing time that could have been spent on their tasks. This
research underscores the promising capabilities of LLMs in sophisticated
coordination environments and reveals the potential of LLMs in building strong
real-world agents for multi-agent coordination.
","2023-10-09","2310.03903v1.pdf"
"2310.03913","Tatsuya Matsushima","Chikaha Tsuji, Dai Komukai, Mimo Shirasaka, Hikaru Wada, Tsunekazu
  Omija, Aoi Horo, Daiki Furuta, Saki Yamaguchi, So Ikoma, Soshi Tsunashima,
  Masato Kobayashi, Koki Ishimoto, Yuya Ikeda, Tatsuya Matsushima, Yusuke
  Iwasawa, Yutaka Matsuo","TRAIL Team Description Paper for RoboCup@Home 2023","","","","","cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Our team, TRAIL, consists of AI/ML laboratory members from The University of
Tokyo. We leverage our extensive research experience in state-of-the-art
machine learning to build general-purpose in-home service robots. We previously
participated in two competitions using Human Support Robot (HSR): RoboCup@Home
Japan Open 2020 (DSPL) and World Robot Summit 2020, equivalent to RoboCup World
Tournament. Throughout the competitions, we showed that a data-driven approach
is effective for performing in-home tasks. Aiming for further development of
building a versatile and fast-adaptable system, in RoboCup @Home 2023, we unify
three technologies that have recently been evaluated as components in the
fields of deep learning and robot learning into a real household robot system.
In addition, to stimulate research all over the RoboCup@Home community, we
build a platform that manages data collected from each site belonging to the
community around the world, taking advantage of the characteristics of the
community.
","2023-10-09","2310.03913v1.pdf"
"2310.03951","Deren Lei","Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching,
  Eslam Kamal","Chain of Natural Language Inference for Reducing Large Language Model
  Ungrounded Hallucinations","The source code is available at
  https://github.com/microsoft/CoNLI_hallucination","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) can generate fluent natural language texts when
given relevant documents as background context. This ability has attracted
considerable interest in developing industry applications of LLMs. However,
LLMs are prone to generate hallucinations that are not supported by the
provided sources. In this paper, we propose a hierarchical framework to detect
and mitigate such ungrounded hallucination. Our framework uses Chain of Natural
Language Inference (CoNLI) for hallucination detection and hallucination
reduction via post-editing. Our approach achieves state-of-the-art performance
on hallucination detection and enhances text quality through rewrite, using
LLMs without any fine-tuning or domain-specific prompt engineering. We show
that this simple plug-and-play framework can serve as an effective choice for
hallucination detection and reduction, achieving competitive performance across
various contexts.
","2023-10-11","2310.03951v1.pdf"
"2310.03957","Victor Akinwande","Victor Akinwande, Yiding Jiang, Dylan Sam, J. Zico Kolter","Understanding prompt engineering may not require rethinking
  generalization","","","","","cs.LG cs.CV","http://creativecommons.org/licenses/by/4.0/","  Zero-shot learning in prompted vision-language models, the practice of
crafting prompts to build classifiers without an explicit training process, has
achieved impressive performance in many settings. This success presents a
seemingly surprising observation: these methods suffer relatively little from
overfitting, i.e., when a prompt is manually engineered to achieve low error on
a given training set (thus rendering the method no longer actually zero-shot),
the approach still performs well on held-out test data. In this paper, we show
that we can explain such performance well via recourse to classical PAC-Bayes
bounds. Specifically, we show that the discrete nature of prompts, combined
with a PAC-Bayes prior given by a language model, results in generalization
bounds that are remarkably tight by the standards of the literature: for
instance, the generalization bound of an ImageNet classifier is often within a
few percentage points of the true test error. We demonstrate empirically that
this holds for existing handcrafted prompts and prompts generated through
simple greedy search. Furthermore, the resulting bound is well-suited for model
selection: the models with the best bound typically also have the best test
performance. This work thus provides a possible justification for the
widespread practice of prompt engineering, even if it seems that such methods
could potentially overfit the training data.
","2023-10-09","2310.03957v1.pdf"
"2310.03965","Junchi Yu","Junchi Yu, Ran He, Rex Ying","Thought Propagation: An Analogical Approach to Complex Reasoning with
  Large Language Models","","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large Language Models (LLMs) have achieved remarkable success in reasoning
tasks with the development of prompting methods. However, existing prompting
approaches cannot reuse insights of solving similar problems and suffer from
accumulated errors in multi-step reasoning, since they prompt LLMs to reason
\textit{from scratch}. To address these issues, we propose
\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous
problems and leverages their solutions to enhance the complex reasoning ability
of LLMs. These analogous problems are related to the input one, with reusable
solutions and problem-solving strategies. Thus, it is promising to propagate
insights of solving previous analogous problems to inspire new problem-solving.
To achieve this, TP first prompts LLMs to propose and solve a set of analogous
problems that are related to the input one. Then, TP reuses the results of
analogous problems to directly yield a new solution or derive a
knowledge-intensive plan for execution to amend the initial solution obtained
from scratch. TP is compatible with existing prompting approaches, allowing
plug-and-play generalization and enhancement in a wide range of tasks without
much labor in task-specific prompt engineering. Experiments across three
challenging tasks demonstrate TP enjoys a substantial improvement over the
baselines by an average of 12\% absolute increase in finding the optimal
solutions in Shortest-path Reasoning, 13\% improvement of human preference in
Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent
Planning.
","2023-10-10","2310.03965v1.pdf"
"2310.03976","Yue Fu","Yue Fu, Sami Foell, Xuhai Xu, Alexis Hiniker","From Text to Self: Users' Perceptions of Potential of AI on
  Interpersonal Communication and Self","","","","","cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the rapidly evolving landscape of AI-mediated communication (AIMC), tools
powered by Large Language Models (LLMs) are becoming integral to interpersonal
communication. Employing a mixed-methods approach, we conducted a one-week
diary and interview study to explore users' perceptions of these tools' ability
to: 1) support interpersonal communication in the short-term, and 2) lead to
potential long-term effects. Our findings indicate that participants view AIMC
support favorably, citing benefits such as increased communication confidence,
and finding precise language to express their thoughts, navigating linguistic
and cultural barriers. However, the study also uncovers current limitations of
AIMC tools, including verbosity, unnatural responses, and excessive emotional
intensity. These shortcomings are further exacerbated by user concerns about
inauthenticity and potential overreliance on the technology. Furthermore, we
identified four key communication spaces delineated by communication stakes
(high or low) and relationship dynamics (formal or informal) that
differentially predict users' attitudes toward AIMC tools. Specifically,
participants found the tool is more suitable for communicating in formal
relationships than informal ones and more beneficial in high-stakes than
low-stakes communication.
","2023-10-09","2310.03976v1.pdf"
"2310.03986","Md Kaykobad Reza","Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif","Robust Multimodal Learning with Missing Modalities via
  Parameter-Efficient Adaptation","18 pages, 3 figures, 11 tables","","","","cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose simple and parameter-efficient adaptation
procedures for pretrained multimodal networks. In particular, we exploit
low-rank adaptation and modulation of intermediate features to compensate for
the missing modalities. We demonstrate that such adaptation can partially
bridge performance drop due to missing modalities and outperform independent,
dedicated networks trained for the available modality combinations in some
cases. The proposed adaptation requires extremely small number of parameters
(e.g., fewer than 0.7% of the total parameters in most experiments). We conduct
a series of experiments to highlight the robustness of our proposed method
using diverse datasets for RGB-thermal and RGB-Depth semantic segmentation,
multimodal material segmentation, and multimodal sentiment analysis tasks. Our
proposed method demonstrates versatility across various tasks and datasets, and
outperforms existing methods for robust multimodal learning with missing
modalities.
","2023-10-16","2310.03986v1.pdf"
"2310.03991","Abe Bohan Hou","Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung
  Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, and
  Yulia Tsvetkov","SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text
  Generation","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Existing watermarking algorithms are vulnerable to paraphrase attacks because
of their token-level design. To address this issue, we propose SemStamp, a
robust sentence-level semantic watermarking algorithm based on
locality-sensitive hashing (LSH), which partitions the semantic space of
sentences. The algorithm encodes and LSH-hashes a candidate sentence generated
by an LLM, and conducts sentence-level rejection sampling until the sampled
sentence falls in watermarked partitions in the semantic embedding space. A
margin-based constraint is used to enhance its robustness. To show the
advantages of our algorithm, we propose a ""bigram"" paraphrase attack using the
paraphrase that has the fewest bigram overlaps with the original sentence. This
attack is shown to be effective against the existing token-level watermarking
method. Experimental results show that our novel semantic watermark algorithm
is not only more robust than the previous state-of-the-art method on both
common and bigram paraphrase attacks, but also is better at preserving the
quality of generation.
","2023-10-09","2310.03991v1.pdf"
"2310.04003","Zihan Chen","Zihan Chen, Howard H. Yang, Y. C. Tay, Kai Fong Ernest Chong, and Tony
  Q. S. Quek","The Role of Federated Learning in a Wireless World with Foundation
  Models","8 pages, 5 figures, 1 table","","","","cs.NI cs.DC cs.LG cs.SY eess.SY","http://creativecommons.org/publicdomain/zero/1.0/","  Foundation models (FMs) are general-purpose artificial intelligence (AI)
models that have recently enabled multiple brand-new generative AI
applications. The rapid advances in FMs serve as an important contextual
backdrop for the vision of next-generation wireless networks, where federated
learning (FL) is a key enabler of distributed network intelligence. Currently,
the exploration of the interplay between FMs and FL is still in its nascent
stage. Naturally, FMs are capable of boosting the performance of FL, and FL
could also leverage decentralized data and computing resources to assist in the
training of FMs. However, the exceptionally high requirements that FMs have for
computing resources, storage, and communication overhead would pose critical
challenges to FL-enabled wireless networks. In this article, we explore the
extent to which FMs are suitable for FL over wireless networks, including a
broad overview of research challenges and opportunities. In particular, we
discuss multiple new paradigms for realizing future intelligent networks that
integrate FMs and FL. We also consolidate several broad research directions
associated with these paradigms.
","2023-10-09","2310.04003v1.pdf"
"2310.04017","Yijia Xiao","Rakesh Bal, Yijia Xiao, Wei Wang","PGraphDTA: Improving Drug Target Interaction Prediction using Protein
  Language Models and Contact Maps","11 pages, 5 figures, 4 tables","","","","cs.LG q-bio.QM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Developing and discovering new drugs is a complex and resource-intensive
endeavor that often involves substantial costs, time investment, and safety
concerns. A key aspect of drug discovery involves identifying novel drug-target
(DT) interactions. Existing computational methods for predicting DT
interactions have primarily focused on binary classification tasks, aiming to
determine whether a DT pair interacts or not. However, protein-ligand
interactions exhibit a continuum of binding strengths, known as binding
affinity, presenting a persistent challenge for accurate prediction. In this
study, we investigate various techniques employed in Drug Target Interaction
(DTI) prediction and propose novel enhancements to enhance their performance.
Our approaches include the integration of Protein Language Models (PLMs) and
the incorporation of Contact Map information as an inductive bias within
current models. Through extensive experimentation, we demonstrate that our
proposed approaches outperform the baseline models considered in this study,
presenting a compelling case for further development in this direction. We
anticipate that the insights gained from this work will significantly narrow
the search space for potential drugs targeting specific proteins, thereby
accelerating drug discovery. Code and data for PGraphDTA are available at
https://anonymous.4open.science/r/PGraphDTA.
","2023-10-09","2310.04017v1.pdf"
"2310.04027","Hongyang Yang","Boyu Zhang, Hongyang Yang, Tianyu Zhou, Ali Babar, Xiao-Yang Liu","Enhancing Financial Sentiment Analysis via Retrieval Augmented Large
  Language Models","ACM International Conference on AI in Finance (ICAIF) 2023","","","","cs.CL q-fin.ST q-fin.TR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Financial sentiment analysis is critical for valuation and investment
decision-making. Traditional NLP models, however, are limited by their
parameter size and the scope of their training datasets, which hampers their
generalization capabilities and effectiveness in this field. Recently, Large
Language Models (LLMs) pre-trained on extensive corpora have demonstrated
superior performance across various NLP tasks due to their commendable
zero-shot abilities. Yet, directly applying LLMs to financial sentiment
analysis presents challenges: The discrepancy between the pre-training
objective of LLMs and predicting the sentiment label can compromise their
predictive performance. Furthermore, the succinct nature of financial news,
often devoid of sufficient context, can significantly diminish the reliability
of LLMs' sentiment analysis. To address these challenges, we introduce a
retrieval-augmented LLMs framework for financial sentiment analysis. This
framework includes an instruction-tuned LLMs module, which ensures LLMs behave
as predictors of sentiment labels, and a retrieval-augmentation module which
retrieves additional context from reliable external sources. Benchmarked
against traditional models and LLMs like ChatGPT and LLaMA, our approach
achieves 15\% to 48\% performance gain in accuracy and F1 score.
","2023-10-09","2310.04027v1.pdf"
"2310.04047","Quazi Ishtiaque Mahmud","Quazi Ishtiaque Mahmud, Ali TehraniJamsaz, Hung D Phan, Nesreen K.
  Ahmed and Ali Jannesari","AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large
  Language Models","10 pages","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Parallelizing sequentially written programs is a challenging task. Even
experienced developers need to spend considerable time finding parallelism
opportunities and then actually writing parallel versions of sequentially
written programs. To address this issue, we present AUTOPARLLM, a framework for
automatically discovering parallelism and generating the parallel version of
the sequentially written program. Our framework consists of two major
components: i) a heterogeneous Graph Neural Network (GNN) based parallelism
discovery and parallel pattern detection module, and ii) an LLM-based code
generator to generate the parallel counterpart of the sequential programs. We
use the GNN to learn the flow-aware characteristics of the programs to identify
parallel regions in sequential programs and then construct an enhanced prompt
using the GNN's results for the LLM-based generator to finally produce the
parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11
applications of 2 well-known benchmark suites: NAS Parallel Benchmark and
Rodinia Benchmark. Our results show that AUTOPARLLM is indeed effective in
improving the state-of-the-art LLM-based models for the task of parallel code
generation in terms of multiple code generation metrics. AUTOPARLLM also
improves the average runtime of the parallel code generated by the
state-of-the-art LLMs by as high as 3.4% and 2.9% for the NAS Parallel
Benchmark and Rodinia Benchmark respectively. Additionally, to overcome the
issue that well-known metrics for translation evaluation have not been
optimized to evaluate the quality of the generated parallel code, we propose
OMPScore for evaluating the quality of the generated code. We show that
OMPScore exhibits a better correlation with human judgment than existing
metrics, measured by up to 75% improvement of Spearman correlation.
","2023-10-10","2310.04047v1.pdf"
"2310.04064","Zhao Song","Josh Alman, Zhao Song","How to Capture Higher-order Correlations? Generalizing Matrix Softmax
  Attention to Kronecker Computation","","","","","cs.DS cs.CC cs.CL cs.LG stat.ML","http://creativecommons.org/licenses/by-nc-sa/4.0/","  In the classical transformer attention scheme, we are given three $n \times
d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is
to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D =
\mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. In this work, we study a
generalization of attention which captures triple-wise correlations. This
generalization is able to solve problems about detecting triple-wise
connections that were shown to be impossible for transformers. The potential
downside of this generalization is that it appears as though computations are
even more difficult, since the straightforward algorithm requires cubic time in
$n$. However, we show that in the bounded-entry setting (which arises in
practice, and which is well-studied in both theory and practice), there is
actually a near-linear time algorithm. More precisely, we show that bounded
entries are both necessary and sufficient for quickly performing generalized
computations:
  $\bullet$ On the positive side, if all entries of the input matrices are
bounded above by $o(\sqrt[3]{\log n})$ then we show how to approximate the
``tensor-type'' attention matrix in $n^{1+o(1)}$ time.
  $\bullet$ On the negative side, we show that if the entries of the input
matrices may be as large as $\Omega(\sqrt[3]{\log n})$, then there is no
algorithm that runs faster than $n^{3-o(1)}$ (assuming the Strong Exponential
Time Hypothesis from fine-grained complexity theory).
  We also show that our construction, algorithms, and lower bounds naturally
generalize to higher-order tensors and correlations. Interestingly, the higher
the order of the tensors, the lower the bound on the entries needs to be for an
efficient algorithm. Our results thus yield a natural tradeoff between the
boundedness of the entries, and order of the tensor one may use for more
expressive, efficient attention computation.
","2023-10-09","2310.04064v1.pdf"
"2310.04072","Philipp Hacker","Philipp Hacker","AI Regulation in Europe: From the AI Act to Future Regulatory Challenges","Final version forthcoming in: Ifeoma Ajunwa & Jeremias Adams-Prassl
  (eds), Oxford Handbook of Algorithmic Governance and the Law, Oxford
  University Press, 2024","","","","cs.CY cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This chapter provides a comprehensive discussion on AI regulation in the
European Union, contrasting it with the more sectoral and self-regulatory
approach in the UK. It argues for a hybrid regulatory strategy that combines
elements from both philosophies, emphasizing the need for agility and safe
harbors to ease compliance. The paper examines the AI Act as a pioneering
legislative effort to address the multifaceted challenges posed by AI,
asserting that, while the Act is a step in the right direction, it has
shortcomings that could hinder the advancement of AI technologies. The paper
also anticipates upcoming regulatory challenges, such as the management of
toxic content, environmental concerns, and hybrid threats. It advocates for
immediate action to create protocols for regulated access to high-performance,
potentially open-source AI systems. Although the AI Act is a significant
legislative milestone, it needs additional refinement and global collaboration
for the effective governance of rapidly evolving AI technologies.
","2023-10-09","2310.04072v1.pdf"
"2310.04181","Sanket Kalwar Mr","Sanket Kalwar, Mihir Ungarala, Shruti Jain, Aaron Monis, Krishna Reddy
  Konda, Sourav Garg, K Madhava Krishna","DiffPrompter: Differentiable Implicit Visual Prompts for
  Semantic-Segmentation in Adverse Conditions","","","","","cs.CV cs.RO","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Semantic segmentation in adverse weather scenarios is a critical task for
autonomous driving systems. While foundation models have shown promise, the
need for specialized adaptors becomes evident for handling more challenging
scenarios. We introduce DiffPrompter, a novel differentiable visual and latent
prompting mechanism aimed at expanding the learning capabilities of existing
adaptors in foundation models. Our proposed $\nabla$HFC image processing block
excels particularly in adverse weather conditions, where conventional methods
often fall short. Furthermore, we investigate the advantages of jointly
training visual and latent prompts, demonstrating that this combined approach
significantly enhances performance in out-of-distribution scenarios. Our
differentiable visual prompts leverage parallel and series architectures to
generate prompts, effectively improving object segmentation tasks in adverse
conditions. Through a comprehensive series of experiments and evaluations, we
provide empirical evidence to support the efficacy of our approach. Project
page at https://diffprompter.github.io.
","2023-10-09","2310.04181v1.pdf"
"2310.04205","Anupam Purwar","Anupam Purwar and Rahul Sundar","Keyword Augmented Retrieval: Novel framework for Information Retrieval
  integrated with speech interface","","","","","cs.IR cs.AI cs.CL cs.HC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Retrieving answers in a quick and low cost manner without hallucinations from
a combination of structured and unstructured data using Language models is a
major hurdle which prevents employment of Language models in knowledge
retrieval automation. This becomes accentuated when one wants to integrate a
speech interface. Besides, for commercial search and chatbot applications,
complete reliance on commercial large language models (LLMs) like GPT 3.5 etc.
can be very costly. In this work, authors have addressed this problem by first
developing a keyword based search framework which augments discovery of the
context to be provided to the large language model. The keywords in turn are
generated by LLM and cached for comparison with keywords generated by LLM
against the query raised. This significantly reduces time and cost to find the
context within documents. Once the context is set, LLM uses that to provide
answers based on a prompt tailored for Q&A. This research work demonstrates
that use of keywords in context identification reduces the overall inference
time and cost of information retrieval. Given this reduction in inference time
and cost with the keyword augmented retrieval framework, a speech based
interface for user input and response readout was integrated. This allowed a
seamless interaction with the language model.
","2023-10-09","2310.04205v1.pdf"
"2310.04230","Jiarui Jin","Jiarui Jin, Xianyu Chen, Fanghua Ye, Mengyue Yang, Yue Feng, Weinan
  Zhang, Yong Yu, Jun Wang","Lending Interaction Wings to Recommender Systems with Conversational
  Agents","NeurIPS 2023","","","","cs.IR","http://creativecommons.org/licenses/by-sa/4.0/","  Recommender systems trained on offline historical user behaviors are
embracing conversational techniques to online query user preference. Unlike
prior conversational recommendation approaches that systemically combine
conversational and recommender parts through a reinforcement learning
framework, we propose CORE, a new offline-training and online-checking paradigm
that bridges a COnversational agent and REcommender systems via a unified
uncertainty minimization framework. It can benefit any recommendation platform
in a plug-and-play style. Here, CORE treats a recommender system as an offline
relevance score estimator to produce an estimated relevance score for each
item; while a conversational agent is regarded as an online relevance score
checker to check these estimated scores in each session. We define uncertainty
as the summation of unchecked relevance scores. In this regard, the
conversational agent acts to minimize uncertainty via querying either
attributes or items. Based on the uncertainty minimization framework, we derive
the expected certainty gain of querying each attribute and item, and develop a
novel online decision tree algorithm to decide what to query at each turn.
Experimental results on 8 industrial datasets show that CORE could be
seamlessly employed on 9 popular recommendation approaches. We further
demonstrate that our conversational agent could communicate as a human if
empowered by a pre-trained large language model.
","2023-10-09","2310.04230v1.pdf"
"2310.04270","Md Tahmid Rahman Laskar","Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang","A Comprehensive Evaluation of Large Language Models on Benchmark
  Biomedical Text Processing Tasks","Extended version of the following BioNLP paper:
  https://aclanthology.org/2023.bionlp-1.30/ (arXiv:2306.04504). arXiv admin
  note: substantial text overlap with arXiv:2306.04504","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recently, Large Language Models (LLM) have demonstrated impressive capability
to solve a wide range of tasks. However, despite their success across various
tasks, no prior work has investigated their capability in the biomedical domain
yet. To this end, this paper aims to evaluate the performance of LLMs on
benchmark biomedical tasks. For this purpose, we conduct a comprehensive
evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets.
To the best of our knowledge, this is the first work that conducts an extensive
evaluation and comparison of various LLMs in the biomedical domain.
Interestingly, we find based on our evaluation that in biomedical datasets that
have smaller training sets, zero-shot LLMs even outperform the current
state-of-the-art fine-tuned biomedical models. This suggests that pretraining
on large text corpora makes LLMs quite specialized even in the biomedical
domain. We also find that not a single LLM can outperform other LLMs in all
tasks, with the performance of different LLMs may vary depending on the task.
While their performance is still quite poor in comparison to the biomedical
models that were fine-tuned on large training sets, our findings demonstrate
that LLMs have the potential to be a valuable tool for various biomedical tasks
that lack large annotated data.
","2023-10-11","2310.04270v1.pdf"
"2310.04276","Ilker Yildirim","Ilker Yildirim, L.A. Paul","From task structures to world models: What do LLMs know?","","","","","cs.AI q-bio.NC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In what sense does a large language model have knowledge? The answer to this
question extends beyond the capabilities of a particular AI system, and
challenges our assumptions about the nature of knowledge and intelligence. We
answer by granting LLMs ""instrumental knowledge""; knowledge defined by a
certain set of abilities. We then ask how such knowledge is related to the more
ordinary, ""worldly"" knowledge exhibited by human agents, and explore this in
terms of the degree to which instrumental knowledge can be said to incorporate
the structured world models of cognitive science. We discuss ways LLMs could
recover degrees of worldly knowledge, and suggest such recovery will be
governed by an implicit, resource-rational tradeoff between world models and
task demands.
","2023-10-09","2310.04276v1.pdf"
"2310.04280","Maksim Papenkov","Maksim Papenkov and Chris Meredith and Claire Noel and Jai Padalkar
  and Temple Hendrickson and Daniel Nitiutomo and Thomas Farrell","Multi-Industry Simplex : A Probabilistic Extension of GICS","15 pages, 10 figures","","","","q-fin.PM","http://creativecommons.org/licenses/by/4.0/","  Accurate industry classification is a critical tool for many asset management
applications. While the current industry gold-standard GICS (Global Industry
Classification Standard) has proven to be reliable and robust in many settings,
it has limitations that cannot be ignored. Fundamentally, GICS is a
single-industry model, in which every firm is assigned to exactly one group -
regardless of how diversified that firm may be. This approach breaks down for
large conglomerates like Amazon, which have risk exposure spread out across
multiple sectors. We attempt to overcome these limitations by developing MIS
(Multi-Industry Simplex), a probabilistic model that can flexibly assign a firm
to as many industries as can be supported by the data. In particular, we
utilize topic modeling, an natural language processing approach that utilizes
business descriptions to extract and identify corresponding industries. Each
identified industry comes with a relevance probability, allowing for high
interpretability and easy auditing, circumventing the black-box nature of
alternative machine learning approaches. We describe this model in detail and
provide two use-cases that are relevant to asset management - thematic
portfolios and nearest neighbor identification. While our approach has
limitations of its own, we demonstrate the viability of probabilistic industry
classification and hope to inspire future research in this field.
","2023-10-09","2310.04280v1.pdf"
"2310.04292","Dominic Masters","Dominique Beaini, Shenyang Huang, Joao Alex Cunha, Zhiyi Li, Gabriela
  Moisescu-Pareja, Oleksandr Dymov, Samuel Maddrell-Mander, Callum McLean,
  Frederik Wenkel, Luis M\""uller, Jama Hussein Mohamud, Ali Parviz, Michael
  Craig, Micha{\l} Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini,
  Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume
  Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis,
  Mirco Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Therence Bois,
  Andrew Fitzgibbon, B{\l}a\.zej Banaszewski, Chad Martin, Dominic Masters","Towards Foundational Models for Molecular Learning on Large-Scale
  Multi-Task Datasets","","","","","cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Recently, pre-trained foundation models have enabled significant advancements
in multiple fields. In molecular machine learning, however, where datasets are
often hand-curated, and hence typically small, the lack of datasets with
labeled features, and codebases to manage those datasets, has hindered the
development of foundation models. In this work, we present seven novel datasets
categorized by size into three distinct categories: ToyMix, LargeMix and
UltraLarge. These datasets push the boundaries in both the scale and the
diversity of supervised labels for molecular learning. They cover nearly 100
million molecules and over 3000 sparsely defined tasks, totaling more than 13
billion individual labels of both quantum and biological nature. In comparison,
our datasets contain 300 times more data points than the widely used OGB-LSC
PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In
addition, to support the development of foundational models based on our
proposed datasets, we present the Graphium graph machine learning library which
simplifies the process of building and training molecular machine learning
models for multi-task and multi-level molecular datasets. Finally, we present a
range of baseline results as a starting point of multi-task and multi-level
training on these datasets. Empirically, we observe that performance on
low-resource biological datasets show improvement by also training on large
amounts of quantum data. This indicates that there may be potential in
multi-task and multi-level training of a foundation model and fine-tuning it to
resource-constrained downstream tasks.
","2023-10-19","2310.04292v1.pdf"
"2310.04304","Ahmed R. Sadik Dr.-Ing.","Ahmed R. Sadik, Sebastian Brulin, Markus Olhofer","Coding by Design: GPT-4 empowers Agile Model Driven Development","","","","","cs.SE cs.AI cs.FL cs.MA cs.PL","http://creativecommons.org/licenses/by/4.0/","  Generating code from a natural language using Large Language Models (LLMs)
such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's
evident that this approach has its own limitations. The inherent ambiguity of
natural language presents challenges for complex software designs. Accordingly,
our research offers an Agile Model-Driven Development (MDD) approach that
enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes
""Agility"" as a significant contribution to the current MDD method, particularly
when the model undergoes changes or needs deployment in a different programming
language. Thus, we present a case-study showcasing a multi-agent simulation
system of an Unmanned Vehicle Fleet. In the first and second layer of our
approach, we constructed a textual representation of the case-study using
Unified Model Language (UML) diagrams. In the next layer, we introduced two
sets of constraints that minimize model ambiguity. Object Constraints Language
(OCL) is applied to fine-tune the code constructions details, while FIPA
ontology is used to shape communication semantics and protocols. Ultimately,
leveraging GPT-4, our last layer auto-generates code in both Java and Python.
The Java code is deployed within the JADE framework, while the Python code is
deployed in PADE framework. Concluding our research, we engaged in a
comprehensive evaluation of the generated code. From a behavioural standpoint,
the auto-generated code aligned perfectly with the expected UML sequence
diagram. Structurally, we compared the complexity of code derived from UML
diagrams constrained solely by OCL to that influenced by both OCL and
FIPA-ontology. Results indicate that ontology-constrained model produce
inherently more intricate code, but it remains manageable and low-risk for
further testing and maintenance.
","2023-10-09","2310.04304v1.pdf"
"2310.04353","Amitayush Thakur","Amitayush Thakur, Yeming Wen, Swarat Chaudhuri","A Language-Agent Approach to Formal Theorem-Proving","","","","","cs.LG cs.AI cs.LO cs.PL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language agents, which use a large language model (LLM) capable of in-context
learning to interact with an external environment, have recently emerged as a
promising approach to control tasks. We present the first language-agent
approach to formal theorem-proving. Our method, COPRA, uses a high-capacity,
black-box LLM (GPT-4) as part of a policy for a stateful backtracking search.
During the search, the policy can select proof tactics and retrieve lemmas and
definitions from an external database. Each selected tactic is executed in the
underlying proof framework, and the execution feedback is used to build the
prompt for the next policy invocation. The search also tracks selected
information from its history and uses it to reduce hallucinations and
unnecessary LLM queries.
  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks
from the Compcert project. On these benchmarks, COPRA is significantly better
than one-shot invocations of GPT-4, as well as state-of-the-art models
fine-tuned on proof data, at finding correct proofs quickly.
","2023-10-09","2310.04353v1.pdf"
"2310.04373","Theodore Moskovitz","Ted Moskovitz, Aaditya K. Singh, DJ Strouse, Tuomas Sandholm, Ruslan
  Salakhutdinov, Anca D. Dragan, Stephen McAleer","Confronting Reward Model Overoptimization with Constrained RLHF","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large language models are typically aligned with human preferences by
optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However,
human preferences are multi-faceted, and it is increasingly common to derive
reward from a composition of simpler reward models which each capture a
different aspect of language quality. This itself presents a challenge, as it
is difficult to appropriately weight these component RMs when combining them.
Compounding this difficulty, because any RM is only a proxy for human
evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein
past a certain point, accumulating higher reward is associated with worse human
ratings. In this paper, we perform, to our knowledge, the first study on
overoptimization in composite RMs, showing that correlation between component
RMs has a significant effect on the locations of these points. We then
introduce an approach to solve this issue using constrained reinforcement
learning as a means of preventing the agent from exceeding each RM's threshold
of usefulness. Our method addresses the problem of weighting component RMs by
learning dynamic weights, naturally expressed by Lagrange multipliers. As a
result, each RM stays within the range at which it is an effective proxy,
improving evaluation performance. Finally, we introduce an adaptive method
using gradient-free optimization to identify and optimize towards these points
during a single run.
","2023-10-11","2310.04373v1.pdf"
"2310.04406","Andy Zhou","Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong
  Wang","Language Agent Tree Search Unifies Reasoning Acting and Planning in
  Language Models","Website and code can be found at
  https://andyz245.github.io/LanguageAgentTreeSearch","","","","cs.AI cs.CL cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models (LLMs) have demonstrated impressive performance
on a range of decision-making tasks, they rely on simple acting processes and
fall short of broad deployment as autonomous agents. We introduce LATS
(Language Agent Tree Search), a general framework that synergizes the
capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration
from Monte Carlo tree search in model-based reinforcement learning, LATS
employs LLMs as agents, value functions, and optimizers, repurposing their
latent strengths for enhanced decision-making. What is crucial in this method
is the use of an environment for external feedback, which offers a more
deliberate and adaptive problem-solving mechanism that moves beyond the
limitations of existing techniques. Our experimental evaluation across diverse
domains, such as programming, HotPotQA, and WebShop, illustrates the
applicability of LATS for both reasoning and acting. In particular, LATS
achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of
75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness
and generality of our method.
","2023-10-09","2310.04406v1.pdf"
"2310.04407","Ge Gao","Ge Gao, Jonathan D. Chang, Claire Cardie, Kiant\'e Brantley, Thorsten
  Joachim","Policy-Gradient Training of Language Models for Ranking","","","","","cs.CL cs.AI cs.IR cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Text retrieval plays a crucial role in incorporating factual knowledge for
decision making into language processing pipelines, ranging from chat-based web
search to question answering systems. Current state-of-the-art text retrieval
models leverage pre-trained large language models (LLMs) to achieve competitive
performance, but training LLM-based retrievers via typical contrastive losses
requires intricate heuristics, including selecting hard negatives and using
additional supervision as learning signals. This reliance on heuristics stems
from the fact that the contrastive loss itself is heuristic and does not
directly optimize the downstream metrics of decision quality at the end of the
processing pipeline. To address this issue, we introduce Neural PG-RANK, a
novel training algorithm that learns to rank by instantiating a LLM as a
Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for
end-to-end training of retrieval models as part of larger decision systems via
policy gradient, with little reliance on complex heuristics, and it effectively
unifies the training objective with downstream decision-making quality. We
conduct extensive experiments on various text retrieval benchmarks. The results
demonstrate that when the training objective aligns with the evaluation setup,
Neural PG-RANK yields remarkable in-domain performance improvement, with
substantial out-of-domain generalization to some critical datasets employed in
downstream question answering tasks.
","2023-10-09","2310.04407v1.pdf"
"2310.04408","Fangyuan Xu","Fangyuan Xu, Weijia Shi, Eunsol Choi","RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective
  Augmentation","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Retrieving documents and prepending them in-context at inference time
improves performance of language model (LMs) on a wide range of tasks. However,
these documents, often spanning hundreds of words, make inference substantially
more expensive. We propose compressing the retrieved documents into textual
summaries prior to in-context integration. This not only reduces the
computational costs but also relieves the burden of LMs to identify relevant
information in long retrieved documents. We present two compressors -- an
extractive compressor which selects useful sentences from retrieved documents
and an abstractive compressor which generates summaries by synthesizing
information from multiple documents. Both compressors are trained to improve
LMs' performance on end tasks when the generated summaries are prepended to the
LMs' input, while keeping the summary concise.If the retrieved documents are
irrelevant to the input or offer no additional information to LM, our
compressor can return an empty string, implementing selective augmentation.We
evaluate our approach on language modeling task and open domain question
answering task. We achieve a compression rate of as low as 6% with minimal loss
in performance for both tasks, significantly outperforming the off-the-shelf
summarization models. We show that our compressors trained for one LM can
transfer to other LMs on the language modeling task and provide summaries
largely faithful to the retrieved documents.
","2023-10-09","2310.04408v1.pdf"
"2310.04415","Maksym Andriushchenko","Maksym Andriushchenko and Francesco D'Angelo and Aditya Varre and
  Nicolas Flammarion","Why Do We Need Weight Decay in Modern Deep Learning?","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Weight decay is a broadly used technique for training state-of-the-art deep
networks, including large language models. Despite its widespread usage, its
role remains poorly understood. In this work, we highlight that the role of
weight decay in modern deep learning is different from its regularization
effect studied in classical learning theory. For overparameterized deep
networks, we show how weight decay modifies the optimization dynamics enhancing
the ever-present implicit regularization of SGD via the loss stabilization
mechanism. In contrast, for underparameterized large language models trained
with nearly online SGD, we describe how weight decay balances the bias-variance
tradeoff in stochastic optimization leading to lower training loss. Moreover,
we show that weight decay also prevents sudden loss divergences for bfloat16
mixed-precision training which is a crucial tool for LLM training. Overall, we
present a unifying perspective from ResNets on vision tasks to LLMs: weight
decay is never useful as an explicit regularizer but instead changes the
training dynamics in a desirable way. Our code is available at
https://github.com/tml-epfl/why-weight-decay.
","2023-10-09","2310.04415v1.pdf"
"2310.04418","Shanda Li","Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago
  Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh
  Bhojanapalli","Functional Interpolation for Relative Positions Improves Long Context
  Transformers","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Preventing the performance decay of Transformers on inputs longer than those
used for training has been an important challenge in extending the context
length of these models. Though the Transformer architecture has fundamentally
no limits on the input sequence lengths it can process, the choice of position
encoding used during training can limit the performance of these models on
longer inputs. We propose a novel functional relative position encoding with
progressive interpolation, FIRE, to improve Transformer generalization to
longer contexts. We theoretically prove that this can represent some of the
popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We
next empirically show that FIRE models have better generalization to longer
contexts on both zero-shot language modeling and long text benchmarks.
","2023-10-09","2310.04418v1.pdf"
"2310.04420","Andrew Luo","Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe","BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex
  Selectivity","","","","","cs.LG q-bio.NC","http://creativecommons.org/licenses/by/4.0/","  Understanding the functional organization of higher visual cortex is a
central focus in neuroscience. Past studies have primarily mapped the visual
and semantic selectivity of neural populations using hand-selected stimuli,
which may potentially bias results towards pre-existing hypotheses of visual
cortex functionality. Moving beyond conventional approaches, we introduce a
data-driven method that generates natural language descriptions for images
predicted to maximally activate individual voxels of interest. Our method --
Semantic Captioning Using Brain Alignments (""BrainSCUBA"") -- builds upon the
rich embedding space learned by a contrastive vision-language model and
utilizes a pre-trained large language model to generate interpretable captions.
We validate our method through fine-grained voxel-level captioning across
higher-order visual regions. We further perform text-conditioned image
synthesis with the captions, and show that our images are semantically coherent
and yield high predicted activations. Finally, to demonstrate how our method
enables scientific discovery, we perform exploratory investigations on the
distribution of ""person"" representations in the brain, and discover
fine-grained semantic selectivity in body-selective areas. Unlike earlier
studies that decode text, our method derives voxel-wise captions of semantic
selectivity. Our results show that BrainSCUBA is a promising means for
understanding functional preferences in the brain, and provides motivation for
further hypothesis-driven investigation of visual cortex.
","2023-10-09","2310.04420v1.pdf"
"2310.04425","Petar Radanliev","Petar Radanliev, David De Roure, Omar Santos","Red Teaming Generative AI/NLP, the BB84 quantum cryptography protocol
  and the NIST-approved Quantum-Resistant Cryptographic Algorithms","","","","","cs.CY cs.CR cs.ET cs.LG","http://creativecommons.org/licenses/by/4.0/","  In the contemporary digital age, Quantum Computing and Artificial
Intelligence (AI) convergence is reshaping the cyber landscape, introducing
unprecedented opportunities and potential vulnerabilities.This research,
conducted over five years, delves into the cybersecurity implications of this
convergence, with a particular focus on AI/Natural Language Processing (NLP)
models and quantum cryptographic protocols, notably the BB84 method and
specific NIST-approved algorithms. Utilising Python and C++ as primary
computational tools, the study employs a ""red teaming"" approach, simulating
potential cyber-attacks to assess the robustness of quantum security measures.
Preliminary research over 12 months laid the groundwork, which this study seeks
to expand upon, aiming to translate theoretical insights into actionable,
real-world cybersecurity solutions. Located at the University of Oxford's
technology precinct, the research benefits from state-of-the-art infrastructure
and a rich collaborative environment. The study's overarching goal is to ensure
that as the digital world transitions to quantum-enhanced operations, it
remains resilient against AI-driven cyber threats. The research aims to foster
a safer, quantum-ready digital future through iterative testing, feedback
integration, and continuous improvement. The findings are intended for broad
dissemination, ensuring that the knowledge benefits academia and the global
community, emphasising the responsible and secure harnessing of quantum
technology.
","2023-10-10","2310.04425v1.pdf"
"2310.04427","Prashnna Ghimire","Prashnna Ghimire, Kyungki Kim, Manoj Acharya","Generative AI in the Construction Industry: Opportunities & Challenges","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  In the last decade, despite rapid advancements in artificial intelligence
(AI) transforming many industry practices, construction largely lags in
adoption. Recently, the emergence and rapid adoption of advanced large language
models (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown
great potential and sparked considerable global interest. However, the current
surge lacks a study investigating the opportunities and challenges of
implementing Generative AI (GenAI) in the construction sector, creating a
critical knowledge gap for researchers and practitioners. This underlines the
necessity to explore the prospects and complexities of GenAI integration.
Bridging this gap is fundamental to optimizing GenAI's early-stage adoption
within the construction sector. Given GenAI's unprecedented capabilities to
generate human-like content based on learning from existing content, we reflect
on two guiding questions: What will the future bring for GenAI in the
construction industry? What are the potential opportunities and challenges in
implementing GenAI in the construction industry? This study delves into
reflected perception in literature, analyzes the industry perception using
programming-based word cloud and frequency analysis, and integrates authors'
opinions to answer these questions. This paper recommends a conceptual GenAI
implementation framework, provides practical recommendations, summarizes future
research questions, and builds foundational literature to foster subsequent
research expansion in GenAI within the construction and its allied architecture
& engineering domains.
","2023-10-10","2310.04427v1.pdf"
"2310.04438","Golam Md Muktadir","Golam Md Muktadir","A Brief History of Prompt: Leveraging Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  This paper presents a comprehensive exploration of the evolution of prompt
engineering and generation in the field of natural language processing (NLP).
Starting from the early language models and information retrieval systems, we
trace the key developments that have shaped prompt engineering over the years.
The introduction of attention mechanisms in 2015 revolutionized language
understanding, leading to advancements in controllability and
context-awareness. Subsequent breakthroughs in reinforcement learning
techniques further enhanced prompt engineering, addressing issues like exposure
bias and biases in generated text. We examine the significant contributions in
2018 and 2019, focusing on fine-tuning strategies, control codes, and
template-based generation. The paper also discusses the growing importance of
fairness, human-AI collaboration, and low-resource adaptation. In 2020 and
2021, contextual prompting and transfer learning gained prominence, while 2022
and 2023 witnessed the emergence of advanced techniques like unsupervised
pre-training and novel reward shaping. Throughout the paper, we reference
specific research studies that exemplify the impact of various developments on
prompt engineering. The journey of prompt engineering continues, with ethical
considerations being paramount for the responsible and inclusive future of AI
systems.
","2023-10-10","2310.04438v1.pdf"
"2310.04444","Aman Bhargava","Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson","What's the Magic Word? A Control Theory of LLM Prompting","18 pages, 8 figures. Under review for ICLR 2024","","","","cs.CL cs.AI cs.LG cs.NE","http://creativecommons.org/licenses/by/4.0/","  Prompt engineering is effective and important in the deployment of LLMs but
is poorly understood mathematically. Here, we formalize prompt engineering as
an optimal control problem on LLMs -- where the prompt is considered a control
variable for modulating the output distribution of the LLM. Within this
framework, we ask a simple question: given a sequence of tokens, does there
always exist a prompt we can prepend that will steer the LLM toward accurately
predicting the final token? We call such an optimal prompt the magic word since
prepending the prompt causes the LLM to output the correct answer. If magic
words exist, can we find them? If so, what are their properties? We offer
analytic analysis on the controllability of the self-attention head where we
prove a bound on controllability as a function of the singular values of its
weight matrices. We take inspiration from control theory to propose a metric
called $k-\epsilon$ controllability to characterize LLM steerability. We
compute the $k-\epsilon$ controllability of a panel of large language models,
including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language
modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist
for over 97% of WikiText instances surveyed for each model.
","2023-10-11","2310.04444v1.pdf"
"2310.04445","Muhammad Shah","Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier,
  Ankit Shah, Joseph Konan, Dareen Alharthi, Hazim T Bukhari, Massa Baali,
  Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh","LoFT: Local Proxy Fine-tuning For Improving Transferability Of
  Adversarial Attacks Against Large Language Model","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  It has been shown that Large Language Model (LLM) alignments can be
circumvented by appending specially crafted attack suffixes with harmful
queries to elicit harmful responses. To conduct attacks against private target
models whose characterization is unknown, public models can be used as proxies
to fashion the attack, with successful attacks being transferred from public
proxies to private target models. The success rate of attack depends on how
closely the proxy model approximates the private model. We hypothesize that for
attacks to be transferrable, it is sufficient if the proxy can approximate the
target model in the neighborhood of the harmful query. Therefore, in this
paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning
proxy models on similar queries that lie in the lexico-semantic neighborhood of
harmful queries to decrease the divergence between the proxy and target models.
First, we demonstrate three approaches to prompt private target models to
obtain similar queries given harmful queries. Next, we obtain data for local
fine-tuning by eliciting responses from target models for the generated similar
queries. Then, we optimize attack suffixes to generate attack prompts and
evaluate the impact of our local fine-tuning on the attack's success rate.
Experiments show that local fine-tuning of proxy models improves attack
transferability and increases attack success rate by $39\%$, $7\%$, and $0.5\%$
(absolute) on target models ChatGPT, GPT-4, and Claude respectively.
","2023-10-24","2310.04445v1.pdf"
"2310.04450","Nutchanon Yongsatianchot","Nutchanon Yongsatianchot, Parisa Ghanad Torshizi, Stacy Marsella","Investigating Large Language Models' Perception of Emotion Using
  Appraisal Theory","","11th International Conference on Affective Computing and
  Intelligent Interaction Workshop and Demo (ACIIW) 2023 1-8","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLM) like ChatGPT have significantly advanced in
recent years and are now being used by the general public. As more people
interact with these systems, improving our understanding of these black box
models is crucial, especially regarding their understanding of human
psychological aspects. In this work, we investigate their emotion perception
through the lens of appraisal and coping theory using the Stress and Coping
Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting
of multiple stories that evolve over time and differ in key appraisal variables
such as controllability and changeability. We applied SCPQ to three recent LLMs
from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with
predictions from the appraisal theory and human data. The results show that
LLMs' responses are similar to humans in terms of dynamics of appraisal and
coping, but their responses did not differ along key appraisal dimensions as
predicted by the theory and data. The magnitude of their responses is also
quite different from humans in several variables. We also found that GPTs can
be quite sensitive to instruction and how questions are asked. This work adds
to the growing literature evaluating the psychological aspects of LLMs and
helps enrich our understanding of the current models.
","2023-10-10","2310.04450v1.pdf"
"2310.04454","Eren Unlu Ph. D.","Eren Unlu","Spherical Position Encoding for Transformers","5 pages, 2 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Position encoding is the primary mechanism which induces notion of sequential
order for input tokens in transformer architectures. Even though this
formulation in the original transformer paper has yielded plausible performance
for general purpose language understanding and generation, several new
frameworks such as Rotary Position Embedding (RoPE) are proposed for further
enhancement. In this paper, we introduce the notion of ""geotokens"" which are
input elements for transformer architectures, each representing an information
related to a geological location. Unlike the natural language the sequential
position is not important for the model but the geographical coordinates are.
In order to induce the concept of relative position for such a setting and
maintain the proportion between the physical distance and distance on embedding
space, we formulate a position encoding mechanism based on RoPE architecture
which is adjusted for spherical coordinates.
","2023-10-10","2310.04454v1.pdf"
"2310.04474","Yinger Zhang","Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, Jing Zheng","Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning","","","","","cs.SE cs.AI cs.PL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While enabling large language models to implement function calling (known as
APIs) can greatly enhance the performance of LLMs, function calling is still a
challenging task due to the complicated relations between different APIs,
especially in a context-learning setting without fine-tuning. This paper
proposes a simple yet controllable target-driven approach called Reverse Chain
to empower LLMs with capabilities to use external APIs with only prompts. Given
that most open-source LLMs have limited tool-use or tool-plan capabilities,
LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API
selection and argument completion, and a generic rule is employed to implement
a controllable multiple functions calling. In this generic rule, after
selecting a final API to handle a given task via LLMs, we first ask LLMs to
fill the required arguments from user query and context. Some missing arguments
could be further completed by letting LLMs select another API based on API
description before asking user. This process continues until a given task is
completed. Extensive numerical experiments indicate an impressive capability of
Reverse Chain on implementing multiple function calling. Interestingly enough,
the experiments also reveal that tool-use capabilities of the existing LLMs,
e.g., ChatGPT, can be greatly improved via Reverse Chain.
","2023-10-11","2310.04474v1.pdf"
"2310.04475","Guy Tennenholtz","Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Jihwan Jeong, Lior Shani,
  Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier","Demystifying Embedding Spaces using Large Language Models","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Embeddings have become a pivotal means to represent complex, multi-faceted
information about entities, concepts, and relationships in a condensed and
useful format. Nevertheless, they often preclude direct interpretation. While
downstream tasks make use of these compressed representations, meaningful
interpretation usually requires visualization using dimensionality reduction or
specialized machine learning interpretability methods. This paper addresses the
challenge of making such embeddings more interpretable and broadly useful, by
employing Large Language Models (LLMs) to directly interact with embeddings --
transforming abstract vectors into understandable narratives. By injecting
embeddings into LLMs, we enable querying and exploration of complex embedding
data. We demonstrate our approach on a variety of diverse tasks, including:
enhancing concept activation vectors (CAVs), communicating novel embedded
entities, and decoding user preferences in recommender systems. Our work
couples the immense information potential of embeddings with the interpretative
power of LLMs.
","2023-10-10","2310.04475v1.pdf"
"2310.04480","Thanh Gia Hieu Khuong","Thanh Gia Hieu Khuong (TAU, LISN), Benedictus Kent Rachmat (TAU, LISN)","Auto-survey Challenge","","Junior Conference on Data Science and Engineering 2023, Sep 2023,
  Orsay, France","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a novel platform for evaluating the capability of Large Language
Models (LLMs) to autonomously compose and critique survey papers spanning a
vast array of disciplines including sciences, humanities, education, and law.
Within this framework, AI systems undertake a simulated peer-review mechanism
akin to traditional scholarly journals, with human organizers serving in an
editorial oversight capacity. Within this framework, we organized a competition
for the AutoML conference 2023. Entrants are tasked with presenting stand-alone
models adept at authoring articles from designated prompts and subsequently
appraising them. Assessment criteria include clarity, reference
appropriateness, accountability, and the substantive value of the content. This
paper presents the design of the competition, including the implementation
baseline submissions and methods of evaluation.
","2023-10-11","2310.04480v1.pdf"
"2310.04483","Changhun Lee","Changhun Lee and Chiehyeon Lim","A Bi-objective Perspective on Controllable Language Models: Reward
  Dropout Improves Off-policy Control Performance","25 pages, 14 figures, conference","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We study the theoretical aspects of CLMs (Controllable Language Models) from
a bi-objective optimization perspective. Specifically, we consider the CLMs as
an off-policy RL problem that requires simultaneously maximizing the reward and
likelihood objectives. Our main contribution consists of three parts. First, we
establish the theoretical foundations of CLM by presenting reward upper bound
and Pareto improvement/optimality conditions. Second, we analyze conditions
that improve and violate Pareto optimality itself, respectively. Finally, we
propose Reward Dropout, a simple yet powerful method to guarantee policy
improvement based on a Pareto improvement condition. Our theoretical outcomes
are supported by not only deductive proofs but also empirical results. The
performance of Reward Dropout was evaluated on five CLM benchmark datasets, and
it turns out that the Reward Dropout significantly improves the performance of
CLMs.
","2023-10-10","2310.04483v1.pdf"
"2310.04484","Qianle Wang","Wanyun Cui, Qianle Wang","Ada-Instruct: Adapting Instruction Generators for Complex Reasoning","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generating diverse and sophisticated instructions for downstream tasks by
Large Language Models (LLMs) is pivotal for advancing the effect. Current
approaches leverage closed-source LLMs, employing in-context prompting for
instruction generation. However, in this paper, we found that in-context
prompting cannot generate complex instructions with length $\ge 100$ for tasks
like code completion.
  To solve this problem, we introduce Ada-Instruct, an adaptive instruction
generator developed by fine-tuning open-source LLMs. Our pivotal finding
illustrates that fine-tuning open-source LLMs with a mere ten samples generates
long instructions that maintain distributional consistency for complex
reasoning tasks. We empirically validated Ada-Instruct's efficacy across
different applications, including code completion, mathematical reasoning, and
commonsense reasoning. The results underscore Ada-Instruct's superiority,
evidencing its improvements over its base models, current self-instruct
methods, and other state-of-the-art models.
","2023-10-11","2310.04484v1.pdf"
"2310.04535","Zixi Zhang","Zixi Zhang, Greg Chadwick, Hugo McNally, Yiren Zhao, Robert Mullins","LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation","","","","","cs.LG cs.AR","http://creativecommons.org/licenses/by/4.0/","  Test stimuli generation has been a crucial but labor-intensive task in
hardware design verification. In this paper, we revolutionize this process by
harnessing the power of large language models (LLMs) and present a novel
benchmarking framework, LLM4DV. This framework introduces a prompt template for
interactively eliciting test stimuli from the LLM, along with four innovative
prompting improvements to support the pipeline execution and further enhance
its performance. We compare LLM4DV to traditional constrained-random testing
(CRT), using three self-designed design-under-test (DUT) modules. Experiments
demonstrate that LLM4DV excels in efficiently handling straightforward DUT
scenarios, leveraging its ability to employ basic mathematical reasoning and
pre-trained knowledge. While it exhibits reduced efficiency in complex task
settings, it still outperforms CRT in relative terms. The proposed framework
and the DUT modules used in our experiments will be open-sourced upon
publication.
","2023-10-10","2310.04535v1.pdf"
"2310.04550","Chen Liang","Chen Liang, Jiahui Yu, Ming-Hsuan Yang, Matthew Brown, Yin Cui, Tuo
  Zhao, Boqing Gong, Tianyi Zhou","Module-wise Adaptive Distillation for Multimodality Foundation Models","","","","","cs.CV cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pre-trained multimodal foundation models have demonstrated remarkable
generalizability but pose challenges for deployment due to their large sizes.
One effective approach to reducing their sizes is layerwise distillation,
wherein small student models are trained to match the hidden representations of
large teacher models at each layer. Motivated by our observation that certain
architecture components, referred to as modules, contribute more significantly
to the student's performance than others, we propose to track the contributions
of individual modules by recording the loss decrement after distillation each
module and choose the module with a greater contribution to distill more
frequently. Such an approach can be naturally formulated as a multi-armed
bandit (MAB) problem, where modules and loss decrements are considered as arms
and rewards, respectively. We then develop a modified-Thompson sampling
algorithm named OPTIMA to address the nonstationarity of module contributions
resulting from model updating. Specifically, we leverage the observed
contributions in recent history to estimate the changing contribution of each
module and select modules based on these estimations to maximize the cumulative
contribution. We evaluate the effectiveness of OPTIMA through distillation
experiments on various multimodal understanding and image captioning tasks,
using the CoCa-Large model (Yu et al., 2022) as the teacher model.
","2023-10-10","2310.04550v1.pdf"
"2310.04557","Zining Zhu","Zining Zhu, Frank Rudzicz","Measuring Information in Text Explanations","22 pages, 7 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text-based explanation is a particularly promising approach in explainable
AI, but the evaluation of text explanations is method-dependent. We argue that
placing the explanations on an information-theoretic framework could unify the
evaluations of two popular text explanation methods: rationale and natural
language explanations (NLE). This framework considers the post-hoc text
pipeline as a series of communication channels, which we refer to as
``explanation channels''. We quantify the information flow through these
channels, thereby facilitating the assessment of explanation characteristics.
We set up tools for quantifying two information scores: relevance and
informativeness. We illustrate what our proposed information scores measure by
comparing them against some traditional evaluation metrics. Our
information-theoretic scores reveal some unique observations about the
underlying mechanisms of two representative text explanations. For example, the
NLEs trade-off slightly between transmitting the input-related information and
the target-related information, whereas the rationales do not exhibit such a
trade-off mechanism. Our work contributes to the ongoing efforts in
establishing rigorous and standardized evaluation criteria in the rapidly
evolving field of explainable AI.
","2023-10-10","2310.04557v1.pdf"
"2310.04560","Bahare Fatemi","Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi","Talk like a Graph: Encoding Graphs for Large Language Models","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Graphs are a powerful tool for representing and analyzing complex
relationships in real-world applications such as social networks, recommender
systems, and computational finance. Reasoning on graphs is essential for
drawing inferences about the relationships between entities in a complex
system, and to identify hidden patterns and trends. Despite the remarkable
progress in automated reasoning with natural text, reasoning on graphs with
large language models (LLMs) remains an understudied problem. In this work, we
perform the first comprehensive study of encoding graph-structured data as text
for consumption by LLMs. We show that LLM performance on graph reasoning tasks
varies on three fundamental levels: (1) the graph encoding method, (2) the
nature of the graph task itself, and (3) interestingly, the very structure of
the graph considered. These novel results provide valuable insight on
strategies for encoding graphs as text. Using these insights we illustrate how
the correct choice of encoders can boost performance on graph reasoning tasks
inside LLMs by 4.8% to 61.8%, depending on the task.
","2023-10-10","2310.04560v1.pdf"
"2310.04564","Seyed Iman Mirzadeh","Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel
  Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad Farajtabar","ReLU Strikes Back: Exploiting Activation Sparsity in Large Language
  Models","preprint","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Large Language Models (LLMs) with billions of parameters have drastically
transformed AI applications. However, their demanding computation during
inference has raised significant challenges for deployment on
resource-constrained devices. Despite recent trends favoring alternative
activation functions such as GELU or SiLU, known for increased computation,
this study strongly advocates for reinstating ReLU activation in LLMs. We
demonstrate that using the ReLU activation function has a negligible impact on
convergence and performance while significantly reducing computation and weight
transfer. This reduction is particularly valuable during the memory-bound
inference step, where efficiency is paramount. Exploring sparsity patterns in
ReLU-based LLMs, we unveil the reutilization of activated neurons for
generating new tokens and leveraging these insights, we propose practical
strategies to substantially reduce LLM inference computation up to three times,
using ReLU activations with minimal performance trade-offs.
","2023-10-10","2310.04564v1.pdf"
"2310.04573","Sia Gholami","Sia Gholami, Marwan Omar","Can pruning make Large Language Models more efficient?","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Transformer models have revolutionized natural language processing with their
unparalleled ability to grasp complex contextual relationships. However, the
vast number of parameters in these models has raised concerns regarding
computational efficiency, environmental impact, and deployability on
resource-limited platforms. To address these challenges, this paper
investigates the application of weight pruning-a strategic reduction of model
parameters based on their significance-as an optimization strategy for
Transformer architectures. Through extensive experimentation, we explore
various pruning methodologies, highlighting their impact on model performance,
size, and computational demands. Our findings suggest that with judicious
selection of pruning hyperparameters, significant reductions in model size are
attainable without considerable compromise on performance. Moreover, when
coupled with post-pruning fine-tuning strategies, some pruned models even
exhibit enhanced generalization capabilities. This work seeks to bridge the gap
between model efficiency and performance, paving the way for more scalable and
environmentally responsible deep learning applications.
","2023-10-10","2310.04573v1.pdf"
"2310.04579","Tao Li","Tao Li, Juan Guevara, Xinghong Xie, and Quanyan Zhu","Self-Confirming Transformer for Locally Consistent Online Adaptation in
  Multi-Agent Reinforcement Learning","8 figures, 6 tables","","","","cs.LG cs.MA","http://creativecommons.org/licenses/by/4.0/","  Offline reinforcement learning (RL) leverages previously collected data to
extract policies that return satisfying performance in online environments.
However, offline RL suffers from the distribution shift between the offline
dataset and the online environment. In the multi-agent RL (MARL) setting, this
distribution shift may arise from the nonstationary opponents (exogenous agents
beyond control) in the online testing who display distinct behaviors from those
recorded in the offline dataset. Hence, the key to the broader deployment of
offline MARL is the online adaptation to nonstationary opponents. Recent
advances in large language models have demonstrated the surprising
generalization ability of the transformer architecture in sequence modeling,
which prompts one to wonder \textit{whether the offline-trained transformer
policy adapts to nonstationary opponents during online testing}. This work
proposes the self-confirming loss (SCL) in offline transformer training to
address the online nonstationarity, which is motivated by the self-confirming
equilibrium (SCE) in game theory. The gist is that the transformer learns to
predict the opponents' future moves based on which it acts accordingly. As a
weaker variant of Nash equilibrium (NE), SCE (equivalently, SCL) only requires
local consistency: the agent's local observations do not deviate from its
conjectures, leading to a more adaptable policy than the one dictated by NE
focusing on global optimality. We evaluate the online adaptability of the
self-confirming transformer (SCT) by playing against nonstationary opponents
employing a variety of policies, from the random one to the benchmark MARL
policies. Experimental results demonstrate that SCT can adapt to nonstationary
opponents online, achieving higher returns than vanilla transformers and
offline MARL baselines.
","2023-10-10","2310.04579v1.pdf"
"2310.04595","Surjya Ray","Surjya Ray, Pratik Mehta, Hongen Zhang, Ada Chaman, Jian Wang,
  Chung-Jen Ho, Michael Chiou, Tashfeen Suleman","Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical
  Data for Medical Coding with Large Language Models","16 pages,3 figures, 3 tables","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The precipitous rise and adoption of Large Language Models (LLMs) have
shattered expectations with the fastest adoption rate of any consumer-facing
technology in history. Healthcare, a field that traditionally uses NLP
techniques, was bound to be affected by this meteoric rise. In this paper, we
gauge the extent of the impact by evaluating the performance of LLMs for the
task of medical coding on real-life noisy data. We conducted several
experiments on MIMIC III and IV datasets with encoder-based LLMs, such as BERT.
Furthermore, we developed Segmented Harmonic Loss, a new loss function to
address the extreme class imbalance that we found to prevail in most medical
data in a multi-label scenario by segmenting and decoupling co-occurring
classes of the dataset with a new segmentation algorithm. We also devised a
technique based on embedding similarity to tackle noisy data. Our experimental
results show that when trained with the proposed loss, the LLMs achieve
significant performance gains even on noisy long-tailed datasets, outperforming
the F1 score of the state-of-the-art by over ten percentage points.
","2023-10-10","2310.04595v1.pdf"
"2310.04607","Murali Emani","Murali Emani, Sam Foreman, Varuni Sastry, Zhen Xie, Siddhisanket
  Raskar, William Arnold, Rajeev Thakur, Venkatram Vishwanath, Michael E. Papka","A Comprehensive Performance Study of Large Language Models on Novel AI
  Accelerators","","","","","cs.PF cs.AI cs.AR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Artificial intelligence (AI) methods have become critical in scientific
applications to help accelerate scientific discovery. Large language models
(LLMs) are being considered as a promising approach to address some of the
challenging problems because of their superior generalization capabilities
across domains. The effectiveness of the models and the accuracy of the
applications is contingent upon their efficient execution on the underlying
hardware infrastructure. Specialized AI accelerator hardware systems have
recently become available for accelerating AI applications. However, the
comparative performance of these AI accelerators on large language models has
not been previously studied. In this paper, we systematically study LLMs on
multiple AI accelerators and GPUs and evaluate their performance
characteristics for these models. We evaluate these systems with (i) a
micro-benchmark using a core transformer block, (ii) a GPT- 2 model, and (iii)
an LLM-driven science use case, GenSLM. We present our findings and analyses of
the models' performance to better understand the intrinsic capabilities of AI
accelerators. Furthermore, our analysis takes into account key factors such as
sequence lengths, scaling behavior, sparsity, and sensitivity to gradient
accumulation steps.
","2023-10-10","2310.04607v1.pdf"
"2310.04610","Conglong Li","Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang
  Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad
  Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, Pete
  Luferenko, Divya Kumar, Jonathan Weyn, Ruixiong Zhang, Sylwester Klocek,
  Volodymyr Vragov, Mohammed AlQuraishi, Gustaf Ahdritz, Christina Floristean,
  Cristina Negri, Rao Kotamarthi, Venkatram Vishwanath, Arvind Ramanathan, Sam
  Foreman, Kyle Hippe, Troy Arcomano, Romit Maulik, Maxim Zvyagin, Alexander
  Brace, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo
  Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski,
  Logan Ward, Valerie Hayot, Murali Emani, Zhen Xie, Diangen Lin, Maulik
  Shukla, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin,
  Prasanna Balaprakash, Gina Tourassi, John Gounley, Heidi Hanson, Thomas E
  Potok, Massimiliano Lupo Pasini, Kate Evans, Dan Lu, Dalton Lunga, Junqi Yin,
  Sajal Dash, Feiyi Wang, Mallikarjun Shankar, Isaac Lyngaas, Xiao Wang,
  Guojing Cong, Pei Zhang, Ming Fan, Siyan Liu, Adolfy Hoisie, Shinjae Yoo,
  Yihui Ren, William Tang, Kyle Felker, Alexey Svyatkovskiy, Hang Liu, Ashwin
  Aji, Angela Dalton, Michael Schulte, Karl Schulz, Yuntian Deng, Weili Nie,
  Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs,
  Anima Anandkumar, Rick Stevens","DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery
  through Sophisticated AI System Technologies","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  In the upcoming decade, deep learning may revolutionize the natural sciences,
enhancing our capacity to model and predict natural occurrences. This could
herald a new era of scientific exploration, bringing significant advancements
across sectors from drug development to renewable energy. To answer this call,
we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to
build unique capabilities through AI system technology innovations to help
domain experts to unlock today's biggest science mysteries. By leveraging
DeepSpeed's current technology pillars (training, inference and compression) as
base technology enablers, DeepSpeed4Science will create a new set of AI system
technologies tailored for accelerating scientific discoveries by addressing
their unique complexity beyond the common technical approaches used for
accelerating generic large language models (LLMs). In this paper, we showcase
the early progress we made with DeepSpeed4Science in addressing two of the
critical system challenges in structural biology research.
","2023-10-13","2310.04610v1.pdf"
"2310.04621","Fred Hohman","Fred Hohman, Mary Beth Kery, Donghao Ren, Dominik Moritz","Model Compression in Practice: Lessons Learned from Practitioners
  Creating On-device Machine Learning Experiences","","","","","cs.HC cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  On-device machine learning (ML) promises to improve the privacy,
responsiveness, and proliferation of new, intelligent user experiences by
moving ML computation onto everyday personal devices. However, today's large ML
models must be drastically compressed to run efficiently on-device, a hurtle
that requires deep, yet currently niche expertise. To engage the broader
human-centered ML community in on-device ML experiences, we present the results
from an interview study with 30 experts at Apple that specialize in producing
efficient models. We compile tacit knowledge that experts have developed
through practical experience with model compression across different hardware
platforms. Our findings offer pragmatic considerations missing from prior work,
covering the design process, trade-offs, and technical strategies that go into
creating efficient models. Finally, we distill design recommendations for
tooling to help ease the difficulty of this work and bring on-device ML into to
more widespread practice.
","2023-10-10","2310.04621v1.pdf"
"2310.04625","Arthur Conmy","Callum McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, Neel
  Nanda","Copy Suppression: Comprehensively Understanding an Attention Head","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present a single attention head in GPT-2 Small that has one main role
across the entire training distribution. If components in earlier layers
predict a certain token, and this token appears earlier in the context, the
head suppresses it: we call this copy suppression. Attention Head 10.7 (L10H7)
suppresses naive copying behavior which improves overall model calibration.
This explains why multiple prior works studying certain narrow tasks found
negative heads that systematically favored the wrong answer. We uncover the
mechanism that the Negative Heads use for copy suppression with weights-based
evidence and are able to explain 76.9% of the impact of L10H7 in GPT-2 Small.
To the best of our knowledge, this is the most comprehensive description of the
complete role of a component in a language model to date. One major effect of
copy suppression is its role in self-repair. Self-repair refers to how ablating
crucial model components results in downstream neural network parts
compensating for this ablation. Copy suppression leads to self-repair: if an
initial overconfident copier is ablated, then there is nothing to suppress. We
show that self-repair is implemented by several mechanisms, one of which is
copy suppression, which explains 39% of the behavior in a narrow task.
Interactive visualisations of the copy suppression phenomena may be seen at our
web app https://copy-suppression.streamlit.app/
","2023-10-10","2310.04625v1.pdf"
"2310.04627","Liam Collins","Liam Collins, Shanshan Wu, Sewoong Oh, Khe Chai Sim","Profit: Benchmarking Personalization and Robustness Trade-off in
  Federated Prompt Tuning","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In many applications of federated learning (FL), clients desire models that
are personalized using their local data, yet are also robust in the sense that
they retain general global knowledge. However, the presence of data
heterogeneity across clients induces a fundamental trade-off between
personalization (i.e., adaptation to a local distribution) and robustness
(i.e., not forgetting previously learned general knowledge). It is critical to
understand how to navigate this personalization vs robustness trade-off when
designing federated systems, which are increasingly moving towards a paradigm
of fine-tuning large foundation models. Due to limited computational and
communication capabilities in most federated settings, this foundation model
fine-tuning must be done using parameter-efficient fine-tuning (PEFT)
approaches. While some recent work has studied federated approaches to PEFT,
the personalization vs robustness trade-off of federated PEFT has been largely
unexplored. In this work, we take a step towards bridging this gap by
benchmarking fundamental FL algorithms -- FedAvg and FedSGD plus
personalization (via client local fine-tuning) -- applied to one of the most
ubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning --
in a multitude of hyperparameter settings under varying levels of data
heterogeneity. Our results show that federated-trained prompts can be
surprisingly robust when using a small learning rate with many local epochs for
personalization, especially when using an adaptive optimizer as the client
optimizer during federated training. We also demonstrate that simple approaches
such as adding regularization and interpolating two prompts are effective in
improving the personalization vs robustness trade-off in computation-limited
settings with few local updates allowed for personalization.
","2023-10-10","2310.04627v1.pdf"
"2310.04628","Liam Magee","Vanicka Arora, Liam Magee, Luke Munn","(Re)framing Built Heritage through the Machinic Gaze","18 pages, 5 figures","","","","cs.CY","http://creativecommons.org/licenses/by-sa/4.0/","  Built heritage has been both subject and product of a gaze that has been
sustained through moments of colonial fixation on ruins and monuments,
technocratic examination and representation, and fetishisation by aglobal
tourist industry. We argue that the recent proliferation of machine learning
and vision technologies create new scopic regimes for heritage: storing and
retrieving existing images from vast digital archives, and further imparting
their own distortions upon its visual representation. We introduce the term
`machinic gaze' to conceptualise the reconfiguration of heritage representation
via AI models. To explore how this gaze reframes heritage, we deploy an
image-text-image pipeline that reads, interprets, and resynthesizes images of
several UNESCO World Heritage Sites. Employing two concepts from media studies
-- heteroscopia and anamorphosis -- we describe the reoriented perspective that
machine vision systems introduce. We propose that the machinic gaze highlights
the artifice of the human gaze and its underlying assumptions and practices
that combine to form established notions of heritage.
","2023-10-10","2310.04628v1.pdf"
"2310.04631","Matin Amoozadeh","Matin Amoozadeh, David Daniels, Daye Nam, Stella Chen, Michael Hilton,
  Sruti Srinivasa Ragavan and Mohammad Amin Alipour","Trust in Generative AI among students: An Exploratory Study","Accepted at SIGCSE 2024","","","","cs.HC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Generative artificial systems (GenAI) have experienced exponential growth in
the past couple of years. These systems offer exciting capabilities, such as
generating programs, that students can well utilize for their learning. Among
many dimensions that might affect the effective adoption of GenAI, in this
paper, we investigate students' \textit{trust}. Trust in GenAI influences the
extent to which students adopt GenAI, in turn affecting their learning. In this
study, we surveyed 253 students at two large universities to understand how
much they trust \genai tools and their feedback on how GenAI impacts their
performance in CS courses. Our results show that students have different levels
of trust in GenAI. We also observe different levels of confidence and
motivation, highlighting the need for further understanding of factors
impacting trust.
","2023-10-10","2310.04631v1.pdf"
"2310.04668","Zhikai Chen","Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang
  Zhang, Hui Liu, Jiliang Tang","Label-free Node Classification on Graphs with Large Language Models
  (LLMS)","The code will be available soon via
  https://github.com/CurryTang/LLMGNN","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, there have been remarkable advancements in node
classification achieved by Graph Neural Networks (GNNs). However, they
necessitate abundant high-quality labels to ensure promising performance. In
contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency
on text-attributed graphs. Yet, they face challenges in efficiently processing
structural data and suffer from high inference costs. In light of these
observations, this work introduces a label-free node classification on graphs
with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs
while mitigating their limitations. Specifically, LLMs are leveraged to
annotate a small portion of nodes and then GNNs are trained on LLMs'
annotations to make predictions for the remaining large portion of nodes. The
implementation of LLM-GNN faces a unique challenge: how can we actively select
nodes for LLMs to annotate and consequently enhance the GNN training? How can
we leverage LLMs to obtain annotations of high quality, representativeness, and
diversity, thereby enhancing GNN performance with less cost? To tackle this
challenge, we develop an annotation quality heuristic and leverage the
confidence scores derived from LLMs to advanced node selection. Comprehensive
experimental results validate the effectiveness of LLM-GNN. In particular,
LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \products with
a cost less than 1 dollar.
","2023-10-16","2310.04668v1.pdf"
"2310.04671","Korawat Charoenpitaks Mr.","Korawat Charoenpitaks, Van-Quang Nguyen, Masanori Suganuma, Masahiro
  Takahashi, Ryoma Niihara, Takayuki Okatani","Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem
  Formulation and Dataset","Main Paper: 10 pages, Supplementary Materials: 25 pages","","","","cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This paper addresses the problem of predicting hazards that drivers may
encounter while driving a car. We formulate it as a task of anticipating
impending accidents using a single input image captured by car dashcams. Unlike
existing approaches to driving hazard prediction that rely on computational
simulations or anomaly detection from videos, this study focuses on high-level
inference from static images. The problem needs predicting and reasoning about
future events based on uncertain observations, which falls under visual
abductive reasoning. To enable research in this understudied area, a new
dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is
created. The dataset consists of 15K dashcam images of street scenes, and each
image is associated with a tuple containing car speed, a hypothesized hazard
description, and visual entities present in the scene. These are annotated by
human annotators, who identify risky scenes and provide descriptions of
potential accidents that could occur a few seconds later. We present several
baseline methods and evaluate their performance on our dataset, identifying
remaining issues and discussing future directions. This study contributes to
the field by introducing a novel problem formulation and dataset, enabling
researchers to explore the potential of multi-modal AI for driving hazard
prediction.
","2023-10-11","2310.04671v1.pdf"
"2310.04672","Ziheng Wu","Ziheng Wu, Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Xing Shi, Jun Huang","EasyPhoto: Your Smart AI Photo Generator","7 pages, 7 figures","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Stable Diffusion web UI (SD-WebUI) is a comprehensive project that provides a
browser interface based on Gradio library for Stable Diffusion models. In this
paper, We propose a novel WebUI plugin called EasyPhoto, which enables the
generation of AI portraits. By training a digital doppelganger of a specific
user ID using 5 to 20 relevant images, the finetuned model (according to the
trained LoRA model) allows for the generation of AI photos using arbitrary
templates. Our current implementation supports the modification of multiple
persons and different photo styles. Furthermore, we allow users to generate
fantastic template image with the strong SDXL model, enhancing EasyPhoto's
capabilities to deliver more diverse and satisfactory results. The source code
for EasyPhoto is available at: https://github.com/aigc-apps/sd-webui-EasyPhoto.
We also support a webui-free version by using diffusers:
https://github.com/aigc-apps/EasyPhoto. We are continuously enhancing our
efforts to expand the EasyPhoto pipeline, making it suitable for any
identification (not limited to just the face), and we enthusiastically welcome
any intriguing ideas or suggestions.
","2023-10-10","2310.04672v1.pdf"
"2310.04673","Zhihao Du","Jiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li,
  Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng, Chang Zhou,
  Zhijie Yan, Shiliang Zhang","LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT","10 pages, under review","","","","cs.SD cs.AI cs.LG cs.MM eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative Pre-trained Transformer (GPT) models have achieved remarkable
performance on various natural language processing tasks. However, there has
been limited research on applying similar frameworks to audio tasks. Previously
proposed large language models for audio tasks either lack sufficient
quantitative evaluations, or are limited to tasks for recognizing and
understanding audio content, or significantly underperform existing
state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified
GPT model for audio recognition, understanding, and generation. LauraGPT is a
versatile language model that can process both audio and text inputs and
generate outputs in either modalities. It can perform a wide range of tasks
related to content, semantics, paralinguistics, and audio-signal analysis. Some
of its noteworthy tasks include automatic speech recognition, speech-to-text
translation, text-to-speech synthesis, machine translation, speech enhancement,
automated audio captioning, speech emotion recognition, and spoken language
understanding. To achieve this goal, we use a combination of continuous and
discrete features for audio. We encode input audio into continuous
representations using an audio encoder and decode output audio from discrete
codec codes. We then fine-tune a large decoder-only Transformer-based language
model on multiple audio-to-text, text-to-audio, audio-to-audio, and
text-to-text tasks using a supervised multitask learning approach. Extensive
experiments show that LauraGPT achieves competitive or superior performance
compared to existing SOTA models on various audio processing benchmarks.
","2023-10-12","2310.04673v1.pdf"
"2310.04674","Taicheng Guo","Taicheng Guo, Changsheng Ma, Xiuying Chen, Bozhao Nan, Kehan Guo,
  Shichao Pei, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang","Modeling non-uniform uncertainty in Reaction Prediction via Boosting and
  Dropout","","","","","cs.LG physics.chem-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reaction prediction has been recognized as a critical task in synthetic
chemistry, where the goal is to predict the outcome of a reaction based on the
given reactants. With the widespread adoption of generative models, the
Variational Autoencoder(VAE) framework has typically been employed to tackle
challenges in reaction prediction, where the reactants are encoded as a
condition for the decoder, which then generates the product. Despite
effectiveness, these conditional VAE (CVAE) models still fail to adequately
account for the inherent uncertainty in reaction prediction, which primarily
stems from the stochastic reaction process. The principal limitations are
twofold. Firstly, in these CVAE models, the prior is independent of the
reactants, leading to a default wide and assumed uniform distribution variance
of the generated product. Secondly, reactants with analogous molecular
representations are presumed to undergo similar electronic transition
processes, thereby producing similar products. This hinders the ability to
model diverse reaction mechanisms effectively. Since the variance in outcomes
is inherently non-uniform, we are thus motivated to develop a framework that
generates reaction products with non-uniform uncertainty. Firstly, we eliminate
the latent variable in previous CVAE models to mitigate uncontrol-label noise.
Instead, we introduce randomness into product generation via boosting to
ensemble diverse models and cover the range of potential outcomes, and through
dropout to secure models with minor variations. Additionally, we design a
ranking method to union the predictions from boosting and dropout, prioritizing
the most plausible products. Experimental results on the largest reaction
prediction benchmark USPTO-MIT show the superior performance of our proposed
method in modeling the non-uniform uncertainty compared to baselines.
","2023-10-10","2310.04674v1.pdf"
"2310.04677","Rongzhao Zhang","Rongzhao Zhang, Zhian Bai, Ruoying Yu, Wenrao Pang, Lingyun Wang,
  Lifeng Zhu, Xiaofan Zhang, Huan Zhang, Weiguo Hu","AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with
  Imperfect Anatomical Knowledge","Submitted to Medical Image Analysis","","","","eess.IV cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  When delineating lesions from medical images, a human expert can always keep
in mind the anatomical structure behind the voxels. However, although
high-quality (though not perfect) anatomical information can be retrieved from
computed tomography (CT) scans with modern deep learning algorithms, it is
still an open problem how these automatically generated organ masks can assist
in addressing challenging lesion segmentation tasks, such as the segmentation
of colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided
segmentation framework to exploit the auto-generated organ masks to aid CRC
segmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation
(MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive
a more robust organ of interest (OOI) mask that may cover most of the
colon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch
sampling strategy by optimizing a heuristic gain function that considers both
the proximity of important regions (e.g., the tumor or organs of interest) and
sample diversity. Third, we design a novel self-supervised learning scheme
inspired by the topology of tubular organs like the colon to boost the model
performance further. Finally, we employ a masked loss scheme to guide the model
to focus solely on the essential learning region. We extensively evaluate the
proposed method on two CRC segmentation datasets, where substantial performance
improvement (5% to 9% in Dice) is achieved over current state-of-the-art
medical image segmentation models, and the ablation studies further evidence
the efficacy of every proposed component.
","2023-10-10","2310.04677v1.pdf"
"2310.04678","Kaicheng Wang","Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Prudhviraj Naidu, Leon
  Bergen, Ramamohan Paturi","DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based
  Queries","To appear in NeurIPS 2023 Datasets and Benchmarks Track","","","","cs.IR cs.CL","http://creativecommons.org/licenses/by/4.0/","  In scientific research, the ability to effectively retrieve relevant
documents based on complex, multifaceted queries is critical. Existing
evaluation datasets for this task are limited, primarily due to the high cost
and effort required to annotate resources that effectively represent complex
queries. To address this, we propose a novel task, Scientific DOcument
Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed
to handle the complex nature of user queries in scientific research. We
developed a benchmark dataset within the field of computer science, consisting
of 100 human-authored complex query cases. For each complex query, we assembled
a collection of 100 relevant documents and produced annotated relevance scores
for ranking them. Recognizing the significant labor of expert annotation, we
also introduce Anno-GPT, a scalable framework for validating the performance of
Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM
annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,
without compromising quality. Furthermore, due to the multi-tiered structure of
these complex queries, the DORIS-MAE dataset can be extended to over 4,000
sub-query test cases without requiring additional annotation. We evaluated 17
recent retrieval methods on DORIS-MAE, observing notable performance drops
compared to traditional datasets. This highlights the need for better
approaches to handle complex, multifaceted queries in scientific research. Our
dataset and codebase are available at
https://github.com/Real-Doris-Mae/Doris-Mae-Dataset.
","2023-10-11","2310.04678v1.pdf"
"2310.04680","Tian Jin","Tian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael
  Carbin, Jonathan Ragan-Kelley, Gintare Karolina Dziugaite","The Cost of Down-Scaling Language Models: Fact Recall Deteriorates
  before In-Context Learning","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  How does scaling the number of parameters in large language models (LLMs)
affect their core capabilities? We study two natural scaling techniques --
weight pruning and simply training a smaller or larger model, which we refer to
as dense scaling -- and their effects on two core capabilities of LLMs: (a)
recalling facts presented during pre-training and (b) processing information
presented in-context during inference. By curating a suite of tasks that help
disentangle these two capabilities, we find a striking difference in how these
two abilities evolve due to scaling. Reducing the model size by more than 30\%
(via either scaling approach) significantly decreases the ability to recall
facts seen in pre-training. Yet, a 60--70\% reduction largely preserves the
various ways the model can process in-context information, ranging from
retrieving answers from a long context to learning parameterized functions from
in-context exemplars. The fact that both dense scaling and weight pruning
exhibit this behavior suggests that scaling model size has an inherently
disparate effect on fact recall and in-context learning.
","2023-10-10","2310.04680v1.pdf"
"2310.04684","Ali Mili","Wided Ghardallou, Hessamaldin Mohammadi, Elijah Brick, Ali Mili","Invariant Relations: A Bridge from Programs to Equations","32 pages","","","","cs.LO cs.DM cs.PL cs.SE","http://creativecommons.org/licenses/by/4.0/","  Great advances in program analysis would be enabled if it were possible to
derive the function of a program from inputs to outputs (or from initial states
to final states, depending on how we model program semantics). Efforts to do so
have always stalled against the difficulty to derive the function of loops; the
expedient solution to capture the function of loops by unrolling them an
arbitrary number of iterations is clearly inadequate. In this paper, we propose
a relations-based method to derive the function of a C-like program, including
programs that have loops nested to an arbitrary level. To capture the semantics
of loops, we use the concept of invariant relation.
","2023-10-10","2310.04684v1.pdf"
"2310.04698","Siqi Du","Siqi Du, Shengjun Tang, Weixi Wang, Xiaoming Li, Renzhong Guo","Tree-GPT: Modular Large Language Model Expert System for Forest Remote
  Sensing Image Understanding and Interactive Analysis","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  This paper introduces a novel framework, Tree-GPT, which incorporates Large
Language Models (LLMs) into the forestry remote sensing data workflow, thereby
enhancing the efficiency of data analysis. Currently, LLMs are unable to
extract or comprehend information from images and may generate inaccurate text
due to a lack of domain knowledge, limiting their use in forestry data
analysis. To address this issue, we propose a modular LLM expert system,
Tree-GPT, that integrates image understanding modules, domain knowledge bases,
and toolchains. This empowers LLMs with the ability to comprehend images,
acquire accurate knowledge, generate code, and perform data analysis in a local
environment. Specifically, the image understanding module extracts structured
information from forest remote sensing images by utilizing automatic or
interactive generation of prompts to guide the Segment Anything Model (SAM) in
generating and selecting optimal tree segmentation results. The system then
calculates tree structural parameters based on these results and stores them in
a database. Upon receiving a specific natural language instruction, the LLM
generates code based on a thought chain to accomplish the analysis task. The
code is then executed by an LLM agent in a local environment and . For
ecological parameter calculations, the system retrieves the corresponding
knowledge from the knowledge base and inputs it into the LLM to guide the
generation of accurate code. We tested this system on several tasks, including
Search, Visualization, and Machine Learning Analysis. The prototype system
performed well, demonstrating the potential for dynamic usage of LLMs in
forestry research and environmental sciences.
","2023-10-10","2310.04698v1.pdf"
"2310.04716","Zhizheng Zhang","Zhizheng Zhang, Wenxuan Xie, Xiaoyi Zhang, Yan Lu","Reinforced UI Instruction Grounding: Towards a Generic UI Task
  Automation API","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent popularity of Large Language Models (LLMs) has opened countless
possibilities in automating numerous AI tasks by connecting LLMs to various
domain-specific models or APIs, where LLMs serve as dispatchers while
domain-specific models or APIs are action executors. Despite the vast numbers
of domain-specific models/APIs, they still struggle to comprehensively cover
super diverse automation demands in the interaction between human and User
Interfaces (UIs). In this work, we build a multimodal model to ground natural
language instructions in given UI screenshots as a generic UI task automation
executor. This metadata-free grounding model, consisting of a visual encoder
and a language decoder, is first pretrained on well studied document
understanding tasks and then learns to decode spatial information from UI
screenshots in a promptable way. To facilitate the exploitation of
image-to-text pretrained knowledge, we follow the pixel-to-sequence paradigm to
predict geometric coordinates in a sequence of tokens using a language decoder.
We further propose an innovative Reinforcement Learning (RL) based algorithm to
supervise the tokens in such sequence jointly with visually semantic metrics,
which effectively strengthens the spatial decoding capability of the
pixel-to-sequence paradigm. Extensive experiments demonstrate our proposed
reinforced UI instruction grounding model outperforms the state-of-the-art
methods by a clear margin and shows the potential as a generic UI task
automation API.
","2023-10-10","2310.04716v1.pdf"
"2310.04742","Anke Tang","Anke Tang, Li Shen, Yong Luo, Yibing Zhan, Han Hu, Bo Du, Yixin Chen,
  Dacheng Tao","Parameter Efficient Multi-task Model Fusion with Partial Linearization","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large pre-trained models have enabled significant advances in machine
learning and served as foundation components. Model fusion methods, such as
task arithmetic, have been proven to be powerful and scalable to incorporate
fine-tuned weights from different tasks into a multi-task model. However,
efficiently fine-tuning large pre-trained models on multiple downstream tasks
remains challenging, leading to inefficient multi-task model fusion. In this
work, we propose a novel method to improve multi-task fusion for
parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically,
our approach partially linearizes only the adapter modules and applies task
arithmetic over the linearized adapters. This allows us to leverage the the
advantages of model fusion over linearized fine-tuning, while still performing
fine-tuning and inference efficiently. We demonstrate that our partial
linearization technique enables a more effective fusion of multiple tasks into
a single model, outperforming standard adapter tuning and task arithmetic
alone. Experimental results demonstrate the capabilities of our proposed
partial linearization technique to effectively construct unified multi-task
models via the fusion of fine-tuned task vectors. We evaluate performance over
an increasing number of tasks and find that our approach outperforms standard
parameter-efficient fine-tuning techniques. The results highlight the benefits
of partial linearization for scalable and efficient multi-task model fusion.
","2023-10-11","2310.04742v1.pdf"
"2310.04743","Song Jiang","Song Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz,
  Yinglong Xia, Bugra Akyildiz, Yizhou Sun, Jinchao Li, Qifan Wang, Asli
  Celikyilmaz","Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning
  in Large Language Models","29 pages","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving
rationales, has impressively unlocked the reasoning potential of large language
models (LLMs). Yet, the standard CoT is less effective in problems demanding
multiple reasoning steps. This limitation arises from the complex reasoning
process in multi-step problems: later stages often depend on the results of
several steps earlier, not just the results of the immediately preceding step.
Such complexities suggest the reasoning process is naturally represented as a
graph. The almost linear and straightforward structure of CoT prompting,
however, struggles to capture this complex reasoning graph. To address this
challenge, we propose Residual Connection Prompting (RESPROMPT), a new
prompting strategy that advances multi-step reasoning in LLMs. Our key idea is
to reconstruct the reasoning graph within prompts. We achieve this by
integrating necessary connections-links present in the reasoning graph but
missing in the linear CoT flow-into the prompts. Termed ""residual connections"",
these links are pivotal in morphing the linear CoT structure into a graph
representation, effectively capturing the complex reasoning graphs inherent in
multi-step problems. We evaluate RESPROMPT on six benchmarks across three
diverse domains: math, sequential, and commonsense reasoning. For the
open-sourced LLaMA family of models, RESPROMPT yields a significant average
reasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.
Breakdown analysis further highlights RESPROMPT particularly excels in complex
multi-step reasoning: for questions demanding at least five reasoning steps,
RESPROMPT outperforms the best CoT based benchmarks by a remarkable average
improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive
ablation studies and analyses, we pinpoint how to most effectively build
residual connections.
","2023-10-10","2310.04743v1.pdf"
"2310.04750","Wenhao Li","Wenhao Li, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu","DiffNAS: Bootstrapping Diffusion Models by Prompting for Better
  Architectures","","","","","cs.AI cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diffusion models have recently exhibited remarkable performance on synthetic
data. After a diffusion path is selected, a base model, such as UNet, operates
as a denoising autoencoder, primarily predicting noises that need to be
eliminated step by step. Consequently, it is crucial to employ a model that
aligns with the expected budgets to facilitate superior synthetic performance.
In this paper, we meticulously analyze the diffusion model and engineer a base
model search approach, denoted ""DiffNAS"". Specifically, we leverage GPT-4 as a
supernet to expedite the search, supplemented with a search memory to enhance
the results. Moreover, we employ RFID as a proxy to promptly rank the
experimental outcomes produced by GPT-4. We also adopt a rapid-convergence
training strategy to boost search efficiency. Rigorous experimentation
corroborates that our algorithm can augment the search efficiency by 2 times
under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37
improvement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.
","2023-10-11","2310.04750v1.pdf"
"2310.04756","Vladimir Vava Gligorov","V.V. Gligorov, V. Rekovi\'c","Review of real-time data processing for collider experiments","21 pages, 11 figures, 1 table","","","","hep-ex physics.ins-det","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We review the status of, and prospects for, real-time data processing for
collider experiments in experimental High Energy Physics. We discuss the
historical evolution of data rates and volumes in the field and place them in
the context of data in other scientific domains and commercial applications. We
review the requirements for real-time processing of these data, and the
constraints they impose on the computing architectures used for such
processing. We describe the evolution of real-time processing over the past
decades with a particular focus on the Large Hadron Collider experiments and
their planned upgrades over the next decade. We then discuss how the scientific
trends in the field and commercial trends in computing architectures may
influence real-time processing over the coming decades.
","2023-10-10","2310.04756v1.pdf"
"2310.04780","Zhenglin Huang","Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao
  Wu, Xi Yang","IPMix: Label-Preserving Data Augmentation Method for Training Robust
  Classifiers","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Data augmentation has been proven effective for training high-accuracy
convolutional neural network classifiers by preventing overfitting. However,
building deep neural networks in real-world scenarios requires not only high
accuracy on clean data but also robustness when data distributions shift. While
prior methods have proposed that there is a trade-off between accuracy and
robustness, we propose IPMix, a simple data augmentation approach to improve
robustness without hurting clean accuracy. IPMix integrates three levels of
data augmentation (image-level, patch-level, and pixel-level) into a coherent
and label-preserving technique to increase the diversity of training data with
limited computational overhead. To further improve the robustness, IPMix
introduces structural complexity at different levels to generate more diverse
images and adopts the random mixing method for multi-scale information fusion.
Experiments demonstrate that IPMix outperforms state-of-the-art corruption
robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also
significantly improves the other safety measures, including robustness to
adversarial perturbations, calibration, prediction consistency, and anomaly
detection, achieving state-of-the-art or comparable results on several
benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.
","2023-10-18","2310.04780v1.pdf"
"2310.04782","Yuchen Yang","Yuchen Yang, Houqiang Li, Yanfeng Wang and Yu Wang","Improving the Reliability of Large Language Models by Leveraging
  Uncertainty-Aware In-Context Learning","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In recent years, large-scale language models (LLMs) have gained attention for
their impressive text generation capabilities. However, these models often face
the challenge of ""hallucination,"" which undermines their reliability. In this
study, we introduce an uncertainty-aware in-context learning framework to
empower the model to enhance or reject its output in response to uncertainty.
Human-defined methods for estimating uncertainty typically assume that
""uncertainty is lower when the model's response is correct compared to when it
is incorrect."" However, setting a precise threshold to distinguish correctness
is challenging. Therefore, we introduce uncertainty information as an
intermediary variable that implicitly influences the model's behavior. Our
innovative uncertainty-aware in-context learning framework involves fine-tuning
the LLM using a calibration dataset. Our aim is to improve the model's
responses by filtering out answers with high uncertainty while considering the
model's knowledge limitations. We evaluate the model's knowledge by examining
multiple responses to the same question for the presence of a correct answer.
When the model lacks relevant knowledge, the response should indicate that the
question cannot be answered. Conversely, when the model has relevant knowledge,
the response should provide the correct answer. Extensive experiments confirm
the effectiveness of our framework, leading to two key findings. First, the
logit output values of the LLM partly reflect inherent uncertainty. Second, our
model autonomously recognizes uncertainty, resulting in improved responses.
","2023-10-10","2310.04782v1.pdf"
"2310.04793","Hongyang Yang","Neng Wang, Hongyang Yang, Christina Dan Wang","FinGPT: Instruction Tuning Benchmark for Open-Source Large Language
  Models in Financial Datasets","","","","","cs.CL q-fin.TR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the swiftly expanding domain of Natural Language Processing (NLP), the
potential of GPT-based models for the financial sector is increasingly evident.
However, the integration of these models with financial datasets presents
challenges, notably in determining their adeptness and relevance. This paper
introduces a distinctive approach anchored in the Instruction Tuning paradigm
for open-source large language models, specifically adapted for financial
contexts. Through this methodology, we capitalize on the interoperability of
open-source models, ensuring a seamless and transparent integration. We begin
by explaining the Instruction Tuning paradigm, highlighting its effectiveness
for immediate integration. The paper presents a benchmarking scheme designed
for end-to-end training and testing, employing a cost-effective progression.
Firstly, we assess basic competencies and fundamental tasks, such as Named
Entity Recognition (NER) and sentiment analysis to enhance specialization.
Next, we delve into a comprehensive model, executing multi-task operations by
amalgamating all instructional tunings to examine versatility. Finally, we
explore the zero-shot capabilities by earmarking unseen tasks and incorporating
novel datasets to understand adaptability in uncharted terrains. Such a
paradigm fortifies the principles of openness and reproducibility, laying a
robust foundation for future investigations in open-source financial large
language models (FinLLMs).
","2023-10-10","2310.04793v1.pdf"
"2310.04799","Shih-Cheng Huang","Shih-Cheng Huang, Pin-Zu Li, Yu-Chi Hsu, Kuang-Ming Chen, Yu Tung Lin,
  Shih-Kai Hsiao, Richard Tzong-Han Tsai, Hung-yi Lee","Chat Vector: A Simple Approach to Equip LLMs With New Language Chat
  Capabilities","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the advancements in conversational AI, such as ChatGPT, this paper
focuses on exploring developing Large Language Models (LLMs) for non-English
languages, especially emphasizing alignment with human preferences. We
introduce a computationally efficient method, leveraging chat vector, to
synergize pre-existing knowledge and behaviors in LLMs, restructuring the
conventional training paradigm from continual pre-train -> SFT -> RLHF to
continual pre-train + chat vector. Our empirical studies, primarily focused on
Traditional Chinese, employ LLaMA2 as the base model and acquire the chat
vector by subtracting the pre-trained weights, LLaMA2, from the weights of
LLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability
of instruction following, and multi-turn dialogue demonstrates the chat
vector's superior efficacy in chatting. To confirm the adaptability of our
approach, we extend our experiments to include models pre-trained in both
Korean and Simplified Chinese, illustrating the versatility of our methodology.
Overall, we present a significant solution in aligning LLMs with human
preferences efficiently across various languages, accomplished by the chat
vector.
","2023-10-10","2310.04799v1.pdf"
"2310.04801","Yongrui Chen","Yongrui Chen, Shenyu Zhang, Guilin Qi, Xinnan Guo","Parameterizing Context: Unleashing the Power of Parameter-Efficient
  Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing","Accepted by NeurIPS-2023 (Poster)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Continual table semantic parsing aims to train a parser on a sequence of
tasks, where each task requires the parser to translate natural language into
SQL based on task-specific tables but only offers limited training examples.
Conventional methods tend to suffer from overfitting with limited supervision,
as well as catastrophic forgetting due to parameter updates. Despite recent
advancements that partially alleviate these issues through semi-supervised data
augmentation and retention of a few past examples, the performance is still
limited by the volume of unsupervised data and stored examples. To overcome
these challenges, this paper introduces a novel method integrating
\textit{parameter-efficient fine-tuning} (PEFT) and \textit{in-context tuning}
(ICT) for training a continual table semantic parser. Initially, we present a
task-adaptive PEFT framework capable of fully circumventing catastrophic
forgetting, which is achieved by freezing the pre-trained model backbone and
fine-tuning small-scale prompts. Building on this, we propose a teacher-student
framework-based solution. The teacher addresses the few-shot problem using ICT,
which procures contextual information by demonstrating a few training examples.
In turn, the student leverages the proposed PEFT framework to learn from the
teacher's output distribution, and subsequently compresses and saves the
contextual information to the prompts, eliminating the need to store any
training examples. Experimental evaluations on two benchmarks affirm the
superiority of our method over prevalent few-shot and continual learning
baselines across various metrics.
","2023-10-10","2310.04801v1.pdf"
"2310.04804","Weiwen Liu","Zhenhua Dong, Jieming Zhu, Weiwen Liu, Ruiming Tang","Ten Challenges in Industrial Recommender Systems","","","","","cs.IR cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Huawei's vision and mission is to build a fully connected intelligent world.
Since 2013, Huawei Noah's Ark Lab has helped many products build recommender
systems and search engines for getting the right information to the right
users. Every day, our recommender systems serve hundreds of millions of mobile
phone users and recommend different kinds of content and services such as apps,
news feeds, songs, videos, books, themes, and instant services. The big data
and various scenarios provide us with great opportunities to develop advanced
recommendation technologies. Furthermore, we have witnessed the technical trend
of recommendation models in the past ten years, from the shallow and simple
models like collaborative filtering, linear models, low rank models to deep and
complex models like neural networks, pre-trained language models. Based on the
mission, opportunities and technological trends, we have also met several hard
problems in our recommender systems. In this talk, we will share ten important
and interesting challenges and hope that the RecSys community can get inspired
and create better recommender systems.
","2023-10-10","2310.04804v1.pdf"
"2310.04815","Liangchen Luo","Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang,
  Lei Meng","Critique Ability of Large Language Models","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Critical thinking is essential for rational decision-making and
problem-solving. This skill hinges on the ability to provide precise and
reasoned critiques and is a hallmark of human intelligence. In the era of large
language models (LLMs), this study explores the ability of LLMs to deliver
accurate critiques across various tasks. We are interested in this topic as a
capable critic model could not only serve as a reliable evaluator, but also as
a source of supervised signals for model tuning. Particularly, if a model can
self-critique, it has the potential for autonomous self-improvement. To examine
this, we introduce a unified evaluation framework for assessing the critique
abilities of LLMs. We develop a benchmark called CriticBench, which comprises
3K high-quality natural language queries and corresponding model responses; and
annotate the correctness of these responses. The benchmark cover tasks such as
math problem-solving, code completion, and question answering. We evaluate
multiple LLMs on the collected dataset and our analysis reveals several
noteworthy insights: (1) Critique is generally challenging for most LLMs, and
this capability often emerges only when models are sufficiently large. (2) In
particular, self-critique is especially difficult. Even top-performing LLMs
struggle to achieve satisfactory performance. (3) Models tend to have lower
critique accuracy on problems where they are most uncertain. To this end, we
introduce a simple yet effective baseline named self-check, which leverages
self-critique to improve task performance for various models. We hope this
study serves as an initial exploration into understanding the critique
abilities of LLMs, and aims to inform future research, including the
development of more proficient critic models and the application of critiques
across diverse tasks.
","2023-10-10","2310.04815v1.pdf"
"2310.04824","Won Ik Cho","Won Ik Cho, Eunjung Cho, Kyunghyun Cho","PaperCard for Reporting Machine Assistance in Academic Writing","Accepted at EAAMO'23 as a poster presentation","","","","cs.CY","http://creativecommons.org/licenses/by-sa/4.0/","  Academic writing process has benefited from various technological
developments over the years including search engines, automatic translators,
and editing tools that review grammar and spelling mistakes. They have enabled
human writers to become more efficient in writing academic papers, for example
by helping with finding relevant literature more effectively and polishing
texts. While these developments have so far played a relatively assistive role,
recent advances in large-scale language models (LLMs) have enabled LLMs to play
a more major role in the writing process, such as coming up with research
questions and generating key contents. This raises critical questions
surrounding the concept of authorship in academia. ChatGPT, a
question-answering system released by OpenAI in November 2022, has demonstrated
a range of capabilities that could be utilised in producing academic papers.
The academic community will have to address relevant pressing questions,
including whether Artificial Intelligence (AI) should be merited authorship if
it made significant contributions in the writing process, or whether its use
should be restricted such that human authorship would not be undermined. In
this paper, we aim to address such questions, and propose a framework we name
""PaperCard"", a documentation for human authors to transparently declare the use
of AI in their writing process.
","2023-10-10","2310.04824v1.pdf"
"2310.04835","Xuhui Jiang","Xuhui Jiang, Chengjin Xu, Yinghan Shen, Xun Sun, Lumingyuan Tang,
  Saizhuo Wang, Zhongwu Chen, Yuanzhuo Wang, Jian Guo","On the Evolution of Knowledge Graphs: A Survey and Perspective","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Knowledge graphs (KGs) are structured representations of diversified
knowledge. They are widely used in various intelligent applications. In this
article, we provide a comprehensive survey on the evolution of various types of
knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs)
and techniques for knowledge extraction and reasoning. Furthermore, we
introduce the practical applications of different types of KGs, including a
case study in financial analysis. Finally, we propose our perspective on the
future directions of knowledge engineering, including the potential of
combining the power of knowledge graphs and large language models (LLMs), and
the evolution of knowledge extraction, reasoning, and representation.
","2023-10-11","2310.04835v1.pdf"
"2310.04861","Yiqiao Zhong","Jiajun Song and Yiqiao Zhong","Uncovering hidden geometry in Transformers via disentangling position
  and context","40 pages, 56 figures","","","","cs.LG cs.AI stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformers are widely used to extract complex semantic meanings from input
tokens, yet they usually operate as black-box models. In this paper, we present
a simple yet informative decomposition of hidden states (or embeddings) of
trained transformers into interpretable components. For any layer, embedding
vectors of input sequence samples are represented by a tensor $\boldsymbol{h}
\in \mathbb{R}^{C \times T \times d}$. Given embedding vector
$\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a
sequence (or context) $c \le C$, extracting the mean effects yields the
decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t +
\mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global
mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across
contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the
residual vector. For popular transformer architectures and diverse text
datasets, empirically we find pervasive mathematical structure: (1)
$(\mathbf{pos}_t)_{t}$ forms a low-dimensional, continuous, and often spiral
shape across layers, (2) $(\mathbf{ctx}_c)_c$ shows clear cluster structure
that falls into context topics, and (3) $(\mathbf{pos}_t)_{t}$ and
$(\mathbf{ctx}_c)_c$ are mutually incoherent -- namely $\mathbf{pos}_t$ is
almost orthogonal to $\mathbf{ctx}_c$ -- which is canonical in compressed
sensing and dictionary learning. This decomposition offers structural insights
about input formats in in-context learning (especially for induction heads) and
in arithmetic tasks.
","2023-10-10","2310.04861v1.pdf"
"2310.04870","Haoze Wu","Haoze Wu, Clark Barrett, Nina Narodytska","Lemur: Integrating Large Language Models in Automated Program
  Verification","Under submission","","","","cs.FL cs.AI cs.LG cs.LO","http://creativecommons.org/licenses/by-sa/4.0/","  The demonstrated code-understanding capability of LLMs raises the question of
whether they can be used for automated program verification, a task that often
demands high-level abstract reasoning about program properties, which is
challenging for verification tools. We propose a general methodology to combine
the power of LLMs and automated reasoners for automated program verification.
We formally describe this methodology as a set of derivation rules and prove
its soundness. We instantiate the calculus as a sound automated verification
procedure, which led to practical improvements on a set of synthetic and
competition benchmarks.
","2023-10-11","2310.04870v1.pdf"
"2310.04875","Cesare Campagnano","Gabriele Tolomei, Cesare Campagnano, Fabrizio Silvestri, Giovanni
  Trappolini","Prompt-to-OS (P2OS): Revolutionizing Operating Systems and
  Human-Computer Interaction with Integrated AI Generative Models","5 pages, 1 figure. Accepted at IEEE CogMI 2023 (IEEE International
  Conference on Cognitive Machine Intelligence)","","","","cs.LG cs.CL cs.CY cs.HC cs.OS","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In this paper, we present a groundbreaking paradigm for human-computer
interaction that revolutionizes the traditional notion of an operating system.
  Within this innovative framework, user requests issued to the machine are
handled by an interconnected ecosystem of generative AI models that seamlessly
integrate with or even replace traditional software applications. At the core
of this paradigm shift are large generative models, such as language and
diffusion models, which serve as the central interface between users and
computers. This pioneering approach leverages the abilities of advanced
language models, empowering users to engage in natural language conversations
with their computing devices. Users can articulate their intentions, tasks, and
inquiries directly to the system, eliminating the need for explicit commands or
complex navigation. The language model comprehends and interprets the user's
prompts, generating and displaying contextual and meaningful responses that
facilitate seamless and intuitive interactions.
  This paradigm shift not only streamlines user interactions but also opens up
new possibilities for personalized experiences. Generative models can adapt to
individual preferences, learning from user input and continuously improving
their understanding and response generation. Furthermore, it enables enhanced
accessibility, as users can interact with the system using speech or text,
accommodating diverse communication preferences.
  However, this visionary concept raises significant challenges, including
privacy, security, trustability, and the ethical use of generative models.
Robust safeguards must be in place to protect user data and prevent potential
misuse or manipulation of the language model.
  While the full realization of this paradigm is still far from being achieved,
this paper serves as a starting point for envisioning this transformative
potential.
","2023-10-10","2310.04875v1.pdf"
"2310.04880","Shashidhar Reddy Javaji","Krutika Sarode, Shashidhar Reddy Javaji, Vishal Kalakonnavar","Question-focused Summarization by Decomposing Articles into Facts and
  Opinions and Retrieving Entities","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  This research focuses on utilizing natural language processing techniques to
predict stock price fluctuations, with a specific interest in early detection
of economic, political, social, and technological changes that can be leveraged
for capturing market opportunities. The proposed approach includes the
identification of salient facts and events from news articles, then use these
facts to form tuples with entities which can be used to get summaries of market
changes for particular entity and then finally combining all the summaries to
form a final abstract summary of the whole article. The research aims to
establish relationships between companies and entities through the analysis of
Wikipedia data and articles from the Economist. Large Language Model GPT 3.5 is
used for getting the summaries and also forming the final summary. The ultimate
goal of this research is to develop a comprehensive system that can provide
financial analysts and investors with more informed decision-making tools by
enabling early detection of market trends and events.
","2023-10-10","2310.04880v1.pdf"
"2310.04892","Ines Zelch","Ines Zelch, Matthias Hagen, Martin Potthast","Commercialized Generative AI: A Critical Study of the Feasibility and
  Ethics of Generating Native Advertising Using Large Language Models in
  Conversational Web Search","Presented at OSSYM 2023","","","","cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  How will generative AI pay for itself? Unless charging users for access,
selling advertising is the only alternative. Especially in the multi-billion
dollar web search market with ads as the main source of revenue, the
introduction of a subscription model seems unlikely. The recent disruption of
search by generative large language models could thus ultimately be accompanied
by generated ads. Our concern is that the commercialization of generative AI in
general and large language models in particular could lead to native
advertising in the form of quite subtle brand or product placements. In web
search, the evolution of search engine results pages (SERPs) from traditional
lists of ``ten blue links'' (lists SERPs) to generated text with web page
references (text SERPs) may further blur the line between advertising-based and
organic search results, making it difficult for users to distinguish between
the two, depending on how advertising is integrated and disclosed. To raise
awareness of this potential development, we conduct a pilot study analyzing the
capabilities of current large language models to blend ads with organic search
results. Although the models still struggle to subtly frame ads in an unrelated
context, their potential is evident when integrating ads into related topics
which calls for further investigation.
","2023-10-10","2310.04892v1.pdf"
"2310.04897","Shan Ye","Shan Ye","Generative AI May Prefer to Present National-level Characteristics of
  Cities Based on Stereotypical Geographic Impressions at the Continental Level","9 pages, 3 figures","","","","cs.CY cs.AI","http://creativecommons.org/licenses/by/4.0/","  A simple experiment was conducted to test the ability of the Chinese-based
generative artificial intelligence (AI) platform, Wenxin Yige, to render images
of urban street views of different countries. The study found that images
generated by this AI platform may contain continental-level stereotypes in
terms of showing the level of economic development and modernization. Street
view images generated from Wenxin Yige do not adequately represent the diverse
range of urban landscapes found across different nations. Using these generated
images for geography education or outreach initiatives could inadvertently
strengthen people's existing stereotypical views about individual countries.
","2023-10-10","2310.04897v1.pdf"
"2310.04900","Anna Kukleva","Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt
  Schiele, Hilde Kuehne","HowToCaption: Prompting LLMs to Transform Video Annotations at Scale","https://github.com/ninatu/howtocaption","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instructional videos are an excellent source for learning multimodal
representations by leveraging video-subtitle pairs extracted with automatic
speech recognition systems (ASR) from the audio signal in the videos. However,
in contrast to human-annotated captions, both speech and subtitles naturally
differ from the visual content of the videos and thus provide only noisy
supervision for multimodal learning. As a result, large-scale annotation-free
web video training data remains sub-optimal for training text-video models. In
this work, we propose to leverage the capability of large language models
(LLMs) to obtain fine-grained video descriptions aligned with videos.
Specifically, we prompt an LLM to create plausible video descriptions based on
ASR narrations of the video for a large-scale instructional video dataset. To
this end, we introduce a prompting method that is able to take into account a
longer text of subtitles, allowing us to capture context beyond a single
sentence. To align the captions to the video temporally, we prompt the LLM to
generate timestamps for each produced caption based on the subtitles. In this
way, we obtain human-style video captions at scale without human supervision.
We apply our method to the subtitles of the HowTo100M dataset, creating a new
large-scale dataset, HowToCaption. Our evaluation shows that the resulting
captions not only significantly improve the performance over many different
benchmark datasets for text-video retrieval but also lead to a disentangling of
textual narration from the audio, boosting performance in text-video-audio
tasks.
","2023-10-10","2310.04900v1.pdf"
"2310.04914","Avinash Madasu","Avinash Madasu, Anahita Bhiwandiwalla, Vasudev Lal","Analyzing Zero-Shot Abilities of Vision-Language Models on Video
  Understanding Tasks","","","","","cs.CV cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Foundational multimodal models pre-trained on large scale image-text pairs or
video-text pairs or both have shown strong generalization abilities on
downstream tasks. However unlike image-text models, pretraining video-text
models is always not feasible due to the difficulty in collecting large-scale
clean and aligned data, and exponential computational costs involved in the
pretraining phase. Therefore, the pertinent question to ask is: Can image-text
models be adapted to video tasks and is there any benefit to using these models
over pretraining directly on videos? In this work, we focus on this question by
proposing a detailed study on the generalization abilities of image-text models
when evaluated on video understanding tasks in a zero-shot setting. We
investigate 9 foundational image-text models on a diverse set of video tasks
that include video action recognition (video AR), video retrieval (video RT),
video question answering (video QA), video multiple choice (video MC) and video
captioning (video CP). Our experiments show that image-text models exhibit
impressive performance on video AR, video RT and video MC. Furthermore, they
perform moderately on video captioning and poorly on video QA. These findings
shed a light on the benefits of adapting foundational image-text models to an
array of video tasks while avoiding the costly pretraining step.
","2023-10-10","2310.04914v1.pdf"
"2310.04921","Jiacheng Liu","Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi,
  Asli Celikyilmaz","Crystal: Introspective Reasoners Reinforced with Self-Feedback","EMNLP 2023 main conference","","","","cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Extensive work has shown that the performance and interpretability of
commonsense reasoning can be improved via knowledge-augmented reasoning
methods, where the knowledge that underpins the reasoning process is explicitly
verbalized and utilized. However, existing implementations, including
""chain-of-thought"" and its variants, fall short in capturing the introspective
nature of knowledge required in commonsense reasoning, and in accounting for
the mutual adaptation between the generation and utilization of knowledge. We
propose a novel method to develop an introspective commonsense reasoner,
Crystal. To tackle commonsense problems, it first introspects for knowledge
statements related to the given question, and subsequently makes an informed
prediction that is grounded in the previously introspected knowledge. The
knowledge introspection and knowledge-grounded reasoning modes of the model are
tuned via reinforcement learning to mutually adapt, where the reward derives
from the feedback given by the model itself. Experiments show that Crystal
significantly outperforms both the standard supervised finetuning and
chain-of-thought distilled methods, and enhances the transparency of the
commonsense reasoning process. Our work ultimately validates the feasibility
and potential of reinforcing a neural model with self-feedback.
","2023-10-19","2310.04921v1.pdf"
"2310.04928","Fajri Koto","Fajri Koto and Nurul Aisyah and Haonan Li and Timothy Baldwin","Large Language Models Only Pass Primary School Exams in Indonesia: A
  Comprehensive Test on IndoMMLU","Accepted at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Although large language models (LLMs) are often pre-trained on large-scale
multilingual texts, their reasoning abilities and real-world knowledge are
mainly evaluated based on English datasets. Assessing LLM capabilities beyond
English is increasingly vital but hindered due to the lack of suitable
datasets. In this work, we introduce IndoMMLU, the first multi-task language
understanding benchmark for Indonesian culture and languages, which consists of
questions from primary school to university entrance exams in Indonesia. By
employing professional teachers, we obtain 14,981 questions across 64 tasks and
education levels, with 46% of the questions focusing on assessing proficiency
in the Indonesian language and knowledge of nine local languages and cultures
in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass
the Indonesian primary school level, with limited knowledge of local Indonesian
languages and culture. Other smaller models such as BLOOMZ and Falcon perform
at even lower levels.
","2023-10-24","2310.04928v1.pdf"
"2310.04942","Zheng Zhang","Zheng Zhang, Hossein Amiri, Zhenke Liu, Andreas Z\""ufle, Liang Zhao","Large Language Models for Spatial Trajectory Patterns Mining","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Identifying anomalous human spatial trajectory patterns can indicate dynamic
changes in mobility behavior with applications in domains like infectious
disease monitoring and elderly care. Recent advancements in large language
models (LLMs) have demonstrated their ability to reason in a manner akin to
humans. This presents significant potential for analyzing temporal patterns in
human mobility. In this paper, we conduct empirical studies to assess the
capabilities of leading LLMs like GPT-4 and Claude-2 in detecting anomalous
behaviors from mobility data, by comparing to specialized methods. Our key
findings demonstrate that LLMs can attain reasonable anomaly detection
performance even without any specific cues. In addition, providing contextual
clues about potential irregularities could further enhances their prediction
efficacy. Moreover, LLMs can provide reasonable explanations for their
judgments, thereby improving transparency. Our work provides insights on the
strengths and limitations of LLMs for human spatial trajectory analysis.
","2023-10-10","2310.04942v1.pdf"
"2310.04944","Zheng Zhang","Yuntong Hu, Zheng Zhang, Liang Zhao","Beyond Text: A Deep Dive into Large Language Models' Ability on
  Understanding Graph Data","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have achieved impressive performance on many
natural language processing tasks. However, their capabilities on
graph-structured data remain relatively unexplored. In this paper, we conduct a
series of experiments benchmarking leading LLMs on diverse graph prediction
tasks spanning node, edge, and graph levels. We aim to assess whether LLMs can
effectively process graph data and leverage topological structures to enhance
performance, compared to specialized graph neural networks. Through varied
prompt formatting and task/dataset selection, we analyze how well LLMs can
interpret and utilize graph structures. By comparing LLMs' performance with
specialized graph models, we offer insights into the strengths and limitations
of employing LLMs for graph analytics. Our findings provide insights into LLMs'
capabilities and suggest avenues for further exploration in applying them to
graph analytics.
","2023-10-10","2310.04944v1.pdf"
"2310.04945","Zheng Zhang","Zheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun Ma, Yingtong Bu, Xun
  Zhou, Liang Zhao","Balancing Specialized and General Skills in LLMs: The Impact of Modern
  Tuning and Data Strategy","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  This paper introduces a multifaceted methodology for fine-tuning and
evaluating large language models (LLMs) for specialized monetization tasks. The
goal is to balance general language proficiency with domain-specific skills.
The methodology has three main components: 1) Carefully blending in-domain and
general-purpose data during fine-tuning to achieve an optimal balance between
general and specialized capabilities; 2) Designing a comprehensive evaluation
framework with 45 questions tailored to assess performance on functionally
relevant dimensions like reliability, consistency, and business impact; 3)
Analyzing how model size and continual training influence metrics to guide
efficient resource allocation during fine-tuning. The paper details the design,
data collection, analytical techniques, and results validating the proposed
frameworks. It aims to provide businesses and researchers with actionable
insights on effectively adapting LLMs for specialized contexts. We also intend
to make public the comprehensive evaluation framework, which includes the 45
tailored questions and their respective scoring guidelines, to foster
transparency and collaboration in adapting LLMs for specialized tasks.
","2023-10-10","2310.04945v1.pdf"
"2310.04948","Defu Cao","Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen
  Ye, Yan Liu","TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series
  Forecasting","35 pages, 20 figures, 17 tables","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The past decade has witnessed significant advances in time series modeling
with deep learning. While achieving state-of-the-art results, the
best-performing architectures vary highly across applications and domains.
Meanwhile, for natural language processing, the Generative Pre-trained
Transformer (GPT) has demonstrated impressive performance via training one
general-purpose model across various textual datasets. It is intriguing to
explore whether GPT-type architectures can be effective for time series,
capturing the intrinsic dynamic attributes and leading to significant accuracy
improvements. In this paper, we propose a novel framework, TEMPO, that can
effectively learn time series representations. We focus on utilizing two
essential inductive biases of the time series task for pre-trained models: (i)
decomposition of the complex interaction between trend, seasonal and residual
components; and (ii) introducing the selection-based prompts to facilitate
distribution adaptation in non-stationary time series. TEMPO expands the
capability for dynamically modeling real-world temporal phenomena from data
within diverse domains. Our experiments demonstrate the superior performance of
TEMPO over state-of-the-art methods on a number of time series benchmark
datasets. This performance gain is observed not only in standard supervised
learning settings but also in scenarios involving previously unseen datasets as
well as in scenarios with multi-modal inputs. This compelling finding
highlights TEMPO's potential to constitute a foundational model-building
framework.
","2023-10-13","2310.04948v1.pdf"
"2310.04951","Weixiang Yan","Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang","CodeTransOcean: A Comprehensive Multilingual Benchmark for Code
  Translation","Accepted by Findings of EMNLP 2023","","","","cs.AI cs.PL","http://creativecommons.org/licenses/by/4.0/","  Recent code translation techniques exploit neural machine translation models
to translate source code from one programming language to another to satisfy
production compatibility or to improve efficiency of codebase maintenance. Most
existing code translation datasets only focus on a single pair of popular
programming languages. To advance research on code translation and meet diverse
requirements of real-world applications, we construct CodeTransOcean, a
large-scale comprehensive benchmark that supports the largest variety of
programming languages for code translation. CodeTransOcean consists of three
novel multilingual datasets, namely, MultilingualTrans supporting translations
between multiple popular programming languages, NicheTrans for translating
between niche programming languages and popular ones, and LLMTrans for
evaluating executability of translated code by large language models (LLMs).
CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for
translating deep learning code across different frameworks. We develop
multilingual modeling approaches for code translation and demonstrate their
great potential in improving the translation quality of both low-resource and
high-resource language pairs and boosting the training efficiency. We also
propose a novel evaluation metric Debugging Success Rate@K for program-level
code translation. Last but not least, we evaluate LLM ChatGPT on our datasets
and investigate its potential for fuzzy execution predictions. We build
baselines for CodeTransOcean and analyze challenges of code translation for
guiding future research. The CodeTransOcean datasets and code are publicly
available at https://github.com/WeixiangYAN/CodeTransOcean.
","2023-10-26","2310.04951v1.pdf"
"2310.04959","Zihan Yu","Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen","Towards Better Chain-of-Thought Prompting Strategies: A Survey","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its
impressive strength when used as a prompting strategy for large language models
(LLM). Recent years, the prominent effect of CoT prompting has attracted
emerging research. However, there still lacks of a systematic summary about key
factors of CoT prompting and comprehensive guide for prompts utilizing. For a
deeper understanding about CoT prompting, we survey on a wide range of current
research, presenting a systematic and comprehensive analysis on several factors
that may influence the effect of CoT prompting, and introduce how to better
apply it in different applications under these discussions. We further analyze
the challenges and propose some future directions about CoT prompting. This
survey could provide an overall reference on related research.
","2023-10-10","2310.04959v1.pdf"
"2310.04963","Sunita Chandrasekaran","Christian Munley, Aaron Jarmusch and Sunita Chandrasekaran","LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. In this paper, we explore the capabilitity of
state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and
open-source alternatives like Meta AI Codellama, to automatically generate
tests and use these tests to validate and verify compiler implementations of a
directive-based programming paradigm, OpenACC. Our approach entails exploring
various prompt engineering techniques including a code template,
retrieval-augmented generation (RAG) with code template, expressive prompt
using RAG with code template, one-shot example, and RAG with one-shot example.
This paper focusses on (a) exploring the capabilities of the latest LLMs for
code generation, (b) investigating prompt and fine tuning methods, and (c)
analyzing the outcome of LLMs generated tests
","2023-10-10","2310.04963v1.pdf"
"2310.04965","Minqian Liu","Jingyuan Qi, Minqian Liu, Ying Shen, Zhiyang Xu, Lifu Huang","MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain
  Everyday Tasks","12 pages, 9 figures, 4 tables","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Automatically generating scripts (i.e. sequences of key steps described in
text) from video demonstrations and reasoning about the subsequent steps are
crucial to the modern AI virtual assistants to guide humans to complete
everyday tasks, especially unfamiliar ones. However, current methods for
generative script learning rely heavily on well-structured preceding steps
described in text and/or images or are limited to a certain domain, resulting
in a disparity with real-world user scenarios. To address these limitations, we
present a new benchmark challenge -- MultiScript, with two new tasks on
task-oriented multimodal script learning: (1) multimodal script generation, and
(2) subsequent step prediction. For both tasks, the input consists of a target
task name and a video illustrating what has been done to complete the target
task, and the expected output is (1) a sequence of structured step descriptions
in text based on the demonstration video, and (2) a single text description for
the subsequent step, respectively. Built from WikiHow, MultiScript covers
multimodal scripts in videos and text descriptions for over 6,655 human
everyday tasks across 19 diverse domains. To establish baseline performance on
MultiScript, we propose two knowledge-guided multimodal generative frameworks
that incorporate the task-related knowledge prompted from large language models
such as Vicuna. Experimental results show that our proposed approaches
significantly improve over the competitive baselines.
","2023-10-10","2310.04965v1.pdf"
"2310.04970","Nitin Kohli","Joshua E. Blumenstock and Nitin Kohli","Big Data Privacy in Emerging Market Fintech and Financial Services: A
  Research Agenda","","","10.26085/C3WK53","","cs.CR","http://creativecommons.org/licenses/by/4.0/","  The data revolution in low- and middle-income countries is quickly
transforming how companies approach emerging markets. As mobile phones and
mobile money proliferate, they generate new streams of data that enable
innovation in consumer finance, credit, and insurance. Already, this new
generation of products are being used by hundreds of millions of consumers,
often to use financial services for the first time. However, the collection,
analysis, and use of these data, particularly from economically disadvantaged
populations, raises serious privacy concerns. This white paper describes a
research agenda to advance our understanding of the problem and solution space
of data privacy in emerging market fintech and financial services. We highlight
five priority areas for research: conducting comprehensive landscape analyses;
understanding local definitions of ``data privacy''; documenting key sources of
risk, and potential technical solutions (such as differential privacy and
homomorphic encryption); improving non-technical approaches to data privacy
(such as policies and practices); and understanding the tradeoffs involved in
deploying privacy-enhancing solutions. Taken together, we hope this research
agenda will focus attention on the multi-faceted nature of privacy in emerging
markets, and catalyze efforts to develop responsible and consumer-oriented
approaches to data-intensive applications.
","2023-10-10","2310.04970v1.pdf"
"2310.04981","Robin Karlsson","Robin Karlsson, Francisco Lepe-Salazar, Kazuya Takeda","Compositional Semantics for Open Vocabulary Spatio-semantic
  Representations","Under review","","","","cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  General-purpose mobile robots need to complete tasks without exact human
instructions. Large language models (LLMs) is a promising direction for
realizing commonsense world knowledge and reasoning-based planning.
Vision-language models (VLMs) transform environment percepts into
vision-language semantics interpretable by LLMs. However, completing complex
tasks often requires reasoning about information beyond what is currently
perceived. We propose latent compositional semantic embeddings z* as a
principled learning-based knowledge representation for queryable
spatio-semantic memories. We mathematically prove that z* can always be found,
and the optimal z* is the centroid for any set Z. We derive a probabilistic
bound for estimating separability of related and unrelated semantics. We prove
that z* is discoverable by iterative optimization by gradient descent from
visual appearance and singular descriptions. We experimentally verify our
findings on four embedding spaces incl. CLIP and SBERT. Our results show that
z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics
for ideal uniformly distributed high-dimensional embeddings. We demonstrate
that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181
overlapping semantics by 42.23 mIoU, while improving conventional
non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared
with a popular SOTA model.
","2023-10-10","2310.04981v1.pdf"
"2310.04987","Cheng Yang","Cheng Yang, Deyu Bo, Jixi Liu, Yufei Peng, Boyu Chen, Haoran Dai, Ao
  Sun, Yue Yu, Yixin Xiao, Qi Zhang, Chunchen Wang, Yuxin Guo, Chuan Shi","Data-centric Graph Learning: A Survey","Working in Progress","","","","cs.LG cs.SI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The history of artificial intelligence (AI) has witnessed the significant
impact of high-quality data on various deep learning models, such as ImageNet
for AlexNet and ResNet. Recently, instead of designing more complex neural
architectures as model-centric approaches, the attention of AI community has
shifted to data-centric ones, which focuses on better processing data to
strengthen the ability of neural models. Graph learning, which operates on
ubiquitous topological data, also plays an important role in the era of deep
learning. In this survey, we comprehensively review graph learning approaches
from the data-centric perspective, and aim to answer two crucial questions: (1)
when to modify graph data and (2) how to modify graph data to unlock the
potential of various graph models. Accordingly, we propose a novel taxonomy
based on the stages in the graph learning pipeline, and highlight the
processing methods for different data structures in the graph data, i.e.,
topology, feature and label. Furthermore, we analyze some potential problems
embedded in graph data and discuss how to solve them in a data-centric manner.
Finally, we provide some promising future directions for data-centric graph
learning.
","2023-10-10","2310.04987v1.pdf"
"2310.04988","Vipula Rawte","Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M
  Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das","The Troubling Emergence of Hallucination in Large Language Models -- An
  Extensive Definition, Quantification, and Prescriptive Remediations","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The recent advancements in Large Language Models (LLMs) have garnered
widespread acclaim for their remarkable emerging capabilities. However, the
issue of hallucination has parallelly emerged as a by-product, posing
significant concerns. While some recent endeavors have been made to identify
and mitigate different types of hallucination, there has been a limited
emphasis on the nuanced categorization of hallucination and associated
mitigation methods. To address this gap, we offer a fine-grained discourse on
profiling hallucination based on its degree, orientation, and category, along
with offering strategies for alleviation. As such, we define two overarching
orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining
(SL). To provide a more comprehensive understanding, both orientations are
further sub-categorized into intrinsic and extrinsic, with three degrees of
severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously
categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric
nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum,
and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a
publicly available dataset comprising of 75,000 samples generated using 15
contemporary LLMs along with human annotations for the aforementioned
categories. Finally, to establish a method for quantifying and to offer a
comparative spectrum that allows us to evaluate and rank LLMs based on their
vulnerability to producing hallucinations, we propose Hallucination
Vulnerability Index (HVI). We firmly believe that HVI holds significant value
as a tool for the wider NLP community, with the potential to serve as a rubric
in AI-related policy-making. In conclusion, we propose two solution strategies
for mitigating hallucinations.
","2023-10-24","2310.04988v1.pdf"
"2310.04991","Haogeng Liu","Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo
  Huang, Ran He, Hongxia Yang","Video-Teller: Enhancing Cross-Modal Generation with Fusion and
  Decoupling","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  This paper proposes Video-Teller, a video-language foundation model that
leverages multi-modal fusion and fine-grained modality alignment to
significantly enhance the video-to-text generation task. Video-Teller boosts
the training efficiency by utilizing frozen pretrained vision and language
modules. It capitalizes on the robust linguistic capabilities of large language
models, enabling the generation of both concise and elaborate video
descriptions. To effectively integrate visual and auditory information,
Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded
Q-Former which fuses information across frames and ASR texts. To better guide
video summarization, we introduce a fine-grained modality alignment objective,
where the cascaded Q-Former's output embedding is trained to align with the
caption/summary embedding created by a pretrained text auto-encoder.
Experimental results demonstrate the efficacy of our proposed video-language
foundation model in accurately comprehending videos and generating coherent and
precise language descriptions. It is worth noting that the fine-grained
alignment enhances the model's capabilities (4% improvement of CIDEr score on
MSR-VTT) with only 13% extra parameters in training and zero additional cost in
inference.
","2023-10-12","2310.04991v1.pdf"
"2310.04992","Jianing Qiu","Jianing Qiu, Jian Wu, Hao Wei, Peilun Shi, Minqing Zhang, Yunyun Sun,
  Lin Li, Hanruo Liu, Hongyi Liu, Simeng Hou, Yuyang Zhao, Xuehui Shi, Junfang
  Xian, Xiaoxia Qu, Sirui Zhu, Lijie Pan, Xiaoniao Chen, Xiaojia Zhang, Shuai
  Jiang, Kebing Wang, Chenlong Yang, Mingqiang Chen, Sujie Fan, Jianhua Hu,
  Aiguo Lv, Hui Miao, Li Guo, Shujun Zhang, Cheng Pei, Xiaojuan Fan, Jianqin
  Lei, Ting Wei, Junguo Duan, Chun Liu, Xiaobo Xia, Siqi Xiong, Junhong Li,
  Benny Lo, Yih Chung Tham, Tien Yin Wong, Ningli Wang, and Wu Yuan","VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for
  Generalist Ophthalmic Artificial Intelligence","","","","","eess.IV cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We present VisionFM, a foundation model pre-trained with 3.4 million
ophthalmic images from 560,457 individuals, covering a broad range of
ophthalmic diseases, modalities, imaging devices, and demography. After
pre-training, VisionFM provides a foundation to foster multiple ophthalmic
artificial intelligence (AI) applications, such as disease screening and
diagnosis, disease prognosis, subclassification of disease phenotype, and
systemic biomarker and disease prediction, with each application enhanced with
expert-level intelligence and accuracy. The generalist intelligence of VisionFM
outperformed ophthalmologists with basic and intermediate levels in jointly
diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale
ophthalmic disease diagnosis benchmark database, as well as a new large-scale
segmentation and detection benchmark database, VisionFM outperformed strong
baseline deep neural networks. The ophthalmic image representations learned by
VisionFM exhibited noteworthy explainability, and demonstrated strong
generalizability to new ophthalmic modalities, disease spectrum, and imaging
devices. As a foundation model, VisionFM has a large capacity to learn from
diverse ophthalmic imaging data and disparate datasets. To be commensurate with
this capacity, in addition to the real data used for pre-training, we also
generated and leveraged synthetic ophthalmic imaging data. Experimental results
revealed that synthetic data that passed visual Turing tests, can also enhance
the representation learning capability of VisionFM, leading to substantial
performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI
applications developed, validated, and demonstrated in this work, substantial
further applications can be achieved in an efficient and cost-effective manner
using VisionFM as the foundation.
","2023-10-10","2310.04992v1.pdf"
"2310.04993","Siqiao Xue","Siqiao Xue, Yan Wang, Zhixuan Chu, Xiaoming Shi, Caigao Jiang, Hongyan
  Hao, Gangwei Jiang, Xiaoyun Feng, James Y. Zhang, Jun Zhou","Prompt-augmented Temporal Point Process for Streaming Event Sequence","NeurIPS 2023 camera ready version","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Neural Temporal Point Processes (TPPs) are the prevalent paradigm for
modeling continuous-time event sequences, such as user activities on the web
and financial transactions. In real-world applications, event data is typically
received in a \emph{streaming} manner, where the distribution of patterns may
shift over time. Additionally, \emph{privacy and memory constraints} are
commonly observed in practical scenarios, further compounding the challenges.
Therefore, the continuous monitoring of a TPP to learn the streaming event
sequence is an important yet under-explored problem. Our work paper addresses
this challenge by adopting Continual Learning (CL), which makes the model
capable of continuously learning a sequence of tasks without catastrophic
forgetting under realistic constraints. Correspondingly, we propose a simple
yet effective framework, PromptTPP\footnote{Our code is available at {\small
\url{ https://github.com/yanyanSann/PromptTPP}}}, by integrating the base TPP
with a continuous-time retrieval prompt pool. The prompts, small learnable
parameters, are stored in a memory space and jointly optimized with the base
TPP, ensuring that the model learns event streams sequentially without
buffering past examples or task-specific attributes. We present a novel and
realistic experimental setup for modeling event streams, where PromptTPP
consistently achieves state-of-the-art performance across three real user
behavior datasets.
","2023-10-16","2310.04993v1.pdf"
"2310.05002","Yile Wang","Yile Wang, Peng Li, Maosong Sun, Yang Liu","Self-Knowledge Guided Retrieval Augmentation for Large Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have shown superior performance without
task-specific fine-tuning. Despite the success, the knowledge stored in the
parameters of LLMs could still be incomplete and difficult to update due to the
computational costs. As complementary, retrieval-based methods can offer
non-parametric world knowledge and improve the performance on tasks such as
question answering. However, we find that the retrieved knowledge does not
always help and even has a negative impact on original responses occasionally.
To better make use of both internal knowledge and external world knowledge, we
investigate eliciting the model's ability to recognize what they know and do
not know (which is also called self-knowledge) and propose Self-Knowledge
guided Retrieval augmentation (SKR), a simple yet effective method which can
let LLMs refer to the questions they have previously encountered and adaptively
call for external resources when dealing with new questions. We evaluate SKR on
multiple datasets and demonstrate that it outperforms chain-of-thought based
and fully retrieval-based methods by using either InstructGPT or ChatGPT.
","2023-10-10","2310.05002v1.pdf"
"2310.05007","Xiusi Chen","Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu
  Yu, Wei Wang","MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot
  Question Answering","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Few-shot question answering (QA) aims at achieving satisfactory results on
machine question answering when only a few training samples are available.
Recent advances mostly rely on the power of pre-trained large language models
(LLMs) and fine-tuning in specific settings. Although the pre-training stage
has already equipped LLMs with powerful reasoning capabilities, LLMs still need
to be fine-tuned to adapt to specific domains to achieve the best results. In
this paper, we propose to select the most informative data for fine-tuning,
thereby improving the efficiency of the fine-tuning process with comparative or
even better accuracy on the open-domain QA task. We present MinPrompt, a
minimal data augmentation framework for open-domain QA based on an approximate
graph algorithm and unsupervised question generation. We transform the raw text
into a graph structure to build connections between different factual
sentences, then apply graph algorithms to identify the minimal set of sentences
needed to cover the most information in the raw text. We then generate QA pairs
based on the identified sentence subset and train the model on the selected
sentences to obtain the final model. Empirical results on several benchmark
datasets and theoretical analysis show that MinPrompt is able to achieve
comparable or better results than baselines with a high degree of efficiency,
bringing improvements in F-1 scores by up to 27.5%.
","2023-10-10","2310.05007v1.pdf"
"2310.05010","Zejia Weng","Zuxuan Wu, Zejia Weng, Wujian Peng, Xitong Yang, Ang Li, Larry S.
  Davis, Yu-Gang Jiang","Building an Open-Vocabulary Video CLIP Model with Better Architectures,
  Optimization and Data","arXiv admin note: substantial text overlap with arXiv:2302.00624","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite significant results achieved by Contrastive Language-Image
Pretraining (CLIP) in zero-shot image recognition, limited effort has been made
exploring its potential for zero-shot video recognition. This paper presents
Open-VCLIP++, a simple yet effective framework that adapts CLIP to a strong
zero-shot video classifier, capable of identifying novel actions and events
during testing. Open-VCLIP++ minimally modifies CLIP to capture
spatial-temporal relationships in videos, thereby creating a specialized video
classifier while striving for generalization. We formally demonstrate that
training Open-VCLIP++ is tantamount to continual learning with zero historical
data. To address this problem, we introduce Interpolated Weight Optimization, a
technique that leverages the advantages of weight interpolation during both
training and testing. Furthermore, we build upon large language models to
produce fine-grained video descriptions. These detailed descriptions are
further aligned with video features, facilitating a better transfer of CLIP to
the video domain. Our approach is evaluated on three widely used action
recognition datasets, following a variety of zero-shot evaluation protocols.
The results demonstrate that our method surpasses existing state-of-the-art
techniques by significant margins. Specifically, we achieve zero-shot accuracy
scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets
respectively, outpacing the best-performing alternative methods by 8.5%, 8.2%,
and 12.3%. We also evaluate our approach on the MSR-VTT video-text retrieval
dataset, where it delivers competitive video-to-text and text-to-video
retrieval performance, while utilizing substantially less fine-tuning data
compared to other methods. Code is released at
https://github.com/wengzejia1/Open-VCLIP.
","2023-10-10","2310.05010v1.pdf"
"2310.05015","Li Lyna Zhang","Song Guo, Jiahang Xu, Li Lyna Zhang, Mao Yang","Compresso: Structured Pruning with Collaborative Prompting Learns
  Compact Large Language Models","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite the remarkable success of Large Language Models (LLMs), the massive
size poses significant deployment challenges, particularly on
resource-constrained hardware. While existing LLM compression methods focus on
quantization, pruning remains relatively unexplored due to the high cost of
training-based approaches and data collection challenges. One-shot pruning
methods, although cost-effective and data-free, have become dominant in LLM
pruning, but lead to performance decline under the structured pruning setting.
In this work, we introduce a new paradigm for structurally pruning LLMs, called
Compresso. Our approach, through the collaboration of the proposed
resource-efficient pruning algorithm and the LLM itself, learns optimal pruning
decisions during the training process. Compresso addresses the challenges of
expensive training costs and data collection by incorporating Low-Rank
Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning
process. Then, we further augment the pruning algorithm by introducing a
collaborative prompt that fosters collaboration between the LLM and the pruning
algorithm, significantly boosting the overall performance. To this end,
Compresso prunes LLaMA-7B to 5.4B, maintaining original performance and even
surpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments
demonstrate that Compresso significantly outperforms one-shot pruning baselines
across various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81%
higher scores on the commonsense reasoning, reading comprehension, MMLU, and
BBH benchmarks, respectively.
","2023-10-12","2310.05015v1.pdf"
"2310.05018","Utkarsh Pratiush","Sergei V. Kalinin, Yongtao Liu, Arpan Biswas, Gerd Duscher, Utkarsh
  Pratiush, Kevin Roccapriore, Maxim Ziatdinov and Rama Vasudevan","Human-in-the-loop: The future of Machine Learning in Automated Electron
  Microscopy","","","","","cond-mat.mtrl-sci cs.LG eess.IV","http://creativecommons.org/licenses/by/4.0/","  Machine learning methods are progressively gaining acceptance in the electron
microscopy community for de-noising, semantic segmentation, and dimensionality
reduction of data post-acquisition. The introduction of the APIs by major
instrument manufacturers now allows the deployment of ML workflows in
microscopes, not only for data analytics but also for real-time decision-making
and feedback for microscope operation. However, the number of use cases for
real-time ML remains remarkably small. Here, we discuss some considerations in
designing ML-based active experiments and pose that the likely strategy for the
next several years will be human-in-the-loop automated experiments (hAE). In
this paradigm, the ML learning agent directly controls beam position and image
and spectroscopy acquisition functions, and human operator monitors experiment
progression in real- and feature space of the system and tunes the policies of
the ML agent to steer the experiment towards specific objectives.
","2023-10-10","2310.05018v1.pdf"
"2310.05028","Guozheng Li","Guozheng Li and Peng Wang and Wenjun Ke","Revisiting Large Language Models as Zero-shot Relation Extractors","Findings of EMNLP 2023","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Relation extraction (RE) consistently involves a certain degree of labeled or
unlabeled data even if under zero-shot setting. Recent studies have shown that
large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt, which provides the possibility of extracting
relations from text without any data and parameter tuning. This work focuses on
the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.
On the one hand, we analyze the drawbacks of existing RE prompts and attempt to
incorporate recent prompt techniques such as chain-of-thought (CoT) to improve
zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a
simple prompt recursively using LLMs to transform RE inputs to the effective
question answering (QA) format. On the other hand, we conduct comprehensive
experiments on various benchmarks and settings to investigate the capabilities
of LLMs on zero-shot RE. Specifically, we have the following findings: (i)
\textsc{SumAsk} consistently and significantly improves LLMs performance on
different model sizes, benchmarks and settings; (ii) Zero-shot prompting with
ChatGPT achieves competitive or superior results compared with zero-shot and
fully supervised methods; (iii) LLMs deliver promising performance in
extracting overlapping relations; (iv) The performance varies greatly regarding
different relations. Different from small language models, LLMs are effective
in handling challenge none-of-the-above (NoTA) relation.
","2023-10-12","2310.05028v1.pdf"
"2310.05029","Howard Chen","Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz","Walking Down the Memory Maze: Beyond Context Limit through Interactive
  Reading","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have advanced in large strides due to the
effectiveness of the self-attention mechanism that processes and compares all
tokens at once. However, this mechanism comes with a fundamental issue -- the
predetermined context window is bound to be limited. Despite attempts to extend
the context window through methods like extrapolating the positional embedding,
using recurrence, or selectively retrieving essential parts of the long
sequence, long-text understanding continues to be a challenge. We propose an
alternative approach which instead treats the LLM as an interactive agent,
allowing it to decide how to read the text via iterative prompting. We
introduce MemWalker, a method that first processes the long context into a tree
of summary nodes. Upon receiving a query, the model navigates this tree in
search of relevant information, and responds once it gathers sufficient
information. On long-text question answering tasks our method outperforms
baseline approaches that use long context windows, recurrence, and retrieval.
We show that, beyond effective reading, MemWalker enhances explainability by
highlighting the reasoning steps as it interactively reads the text;
pinpointing the relevant text segments related to the query.
","2023-10-10","2310.05029v1.pdf"
"2310.05030","Vinija Jain","Megha Chakraborty, S.M Towhidul Islam Tonmoy, S M Mehedi Zaman, Krish
  Sharma, Niyar R Barman, Chandan Gupta, Shreya Gautam, Tanay Kumar, Vinija
  Jain, Aman Chadha, Amit P. Sheth, Amitava Das","Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as
  You May Think -- Introducing AI Detectability Index","EMNLP 2023 Main","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  With the rise of prolific ChatGPT, the risk and consequences of AI-generated
text has increased alarmingly. To address the inevitable question of ownership
attribution for AI-generated artifacts, the US Copyright Office released a
statement stating that 'If a work's traditional elements of authorship were
produced by a machine, the work lacks human authorship and the Office will not
register it'. Furthermore, both the US and the EU governments have recently
drafted their initial proposals regarding the regulatory framework for AI.
Given this cynosural spotlight on generative AI, AI-generated text detection
(AGTD) has emerged as a topic that has already received immediate attention in
research, with some initial methods having been proposed, soon followed by
emergence of techniques to bypass detection. This paper introduces the Counter
Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a
comprehensive evaluation of the robustness of existing AGTD techniques. Our
empirical findings unequivocally highlight the fragility of the proposed AGTD
methods under scrutiny. Amidst the extensive deliberations on policy-making for
regulating AI development, it is of utmost importance to assess the
detectability of content generated by LLMs. Thus, to establish a quantifiable
spectrum facilitating the evaluation and ranking of LLMs according to their
detectability levels, we propose the AI Detectability Index (ADI). We conduct a
thorough examination of 15 contemporary LLMs, empirically demonstrating that
larger LLMs tend to have a higher ADI, indicating they are less detectable
compared to smaller LLMs. We firmly believe that ADI holds significant value as
a tool for the wider NLP community, with the potential to serve as a rubric in
AI-related policy-making.
","2023-10-25","2310.05030v1.pdf"
"2310.05035","Min Cai","Haodi Zhang and Min Cai and Xinhe Zhang and Chen Jason Zhang and Rui
  Mao and Kaishun Wu","Self-Convinced Prompting: Few-Shot Question Answering with Repeated
  Introspection","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models (LLMs) such as ChatGPT and PaLM have demonstrated
remarkable performance in various language understanding and generation tasks,
their capabilities in complex reasoning and intricate knowledge utilization
still fall short of human-level proficiency. Recent studies have established
the effectiveness of prompts in steering LLMs towards generating desired
outputs. Building on these insights, we introduce a novel framework that
harnesses the potential of large-scale pre-trained language models, to
iteratively enhance performance of the LLMs. Our framework incorporates three
components: \textit{Normal CoT}, a \textit{Convincer}, and an
\textit{Answerer}. It processes the output of a typical few-shot
chain-of-thought prompt, assesses the correctness of the response, scrutinizes
the answer, refines the reasoning, and ultimately produces a new solution.
Experimental results on the 7 datasets of miscellaneous problems validate the
efficacy of the Self-Convince framework, achieving substantial improvements
compared to the baselines. This study contributes to the burgeoning body of
research focused on integrating pre-trained language models with tailored
prompts and iterative refinement processes to augment their performance in
complex tasks.
","2023-10-11","2310.05035v1.pdf"
"2310.05036","Min Cai","Jonathan Light and Min Cai and Sheng Shen and Ziniu Hu","From Text to Tactic: Evaluating LLMs Playing the Game of Avalon","","","","","cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
","2023-10-11","2310.05036v1.pdf"
"2310.05046","Yue Huang","Yue Huang and Lichao Sun","Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in
  Generation, Detection and Explanation","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The rampant spread of fake news has adversely affected society, resulting in
extensive research on curbing its spread. As a notable milestone in large
language models (LLMs), ChatGPT has gained significant attention due to its
exceptional natural language processing capabilities. In this study, we present
a thorough exploration of ChatGPT's proficiency in generating, explaining, and
detecting fake news as follows. Generation -- We employ four prompt methods to
generate fake news samples and prove the high quality of these samples through
both self-assessment and human evaluation. Explanation -- We obtain nine
features to characterize fake news based on ChatGPT's explanations and analyze
the distribution of these factors across multiple public datasets. Detection --
We examine ChatGPT's capacity to identify fake news. We explore its detection
consistency and then propose a reason-aware prompt method to improve its
performance. Although our experiments demonstrate that ChatGPT shows
commendable performance in detecting fake news, there is still room for its
improvement. Consequently, we further probe into the potential extra
information that could bolster its effectiveness in detecting fake news.
","2023-10-10","2310.05046v1.pdf"
"2310.05057","Yifan Jiang","Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati","BRAINTEASER: Lateral Thinking Puzzles for Large Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The success of language models has inspired the NLP community to attend to
tasks that require implicit and complex reasoning, relying on human-like
commonsense mechanisms. While such vertical thinking tasks have been relatively
popular, lateral thinking puzzles have received little attention. To bridge
this gap, we devise BRAINTEASER: a multiple-choice Question Answering task
designed to test the model's ability to exhibit lateral thinking and defy
default commonsense associations. We design a three-step procedure for creating
the first lateral thinking benchmark, consisting of data collection, distractor
generation, and generation of adversarial examples, leading to 1,100 puzzles
with high-quality annotations. To assess the consistency of lateral reasoning
by models, we enrich BRAINTEASER based on a semantic and contextual
reconstruction of its questions. Our experiments with state-of-the-art
instruction- and commonsense language models reveal a significant gap between
human and model performance, which is further widened when consistency across
adversarial formats is considered. We make all of our code and data available
to stimulate work on developing and evaluating lateral thinking models.
","2023-10-12","2310.05057v1.pdf"
"2310.05060","Tingkai Liu","Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo
  Huang, Ran He, Hongxia Yang","Video-CSR: Complex Video Digest Creation for Visual-Language Models","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  We present a novel task and human annotated dataset for evaluating the
ability for visual-language models to generate captions and summaries for
real-world video clips, which we call Video-CSR (Captioning, Summarization and
Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in
duration and covers a wide range of topics and interests. Each video clip
corresponds to 5 independently annotated captions (1 sentence) and summaries
(3-10 sentences). Given any video selected from the dataset and its
corresponding ASR information, we evaluate visual-language models on either
caption or summary generation that is grounded in both the visual and auditory
content of the video. Additionally, models are also evaluated on caption- and
summary-based retrieval tasks, where the summary-based retrieval task requires
the identification of a target video given excerpts of a corresponding summary.
Given the novel nature of the paragraph-length video summarization task, we
perform extensive comparative analyses of different existing evaluation metrics
and their alignment with human preferences. Finally, we propose a foundation
model with competitive generation and retrieval capabilities that serves as a
baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful
evaluation set in the age of large language models and complex multi-modal
tasks.
","2023-10-10","2310.05060v1.pdf"
"2310.05063","Gerald Woo","Gerald Woo, Chenghao Liu, Akshat Kumar, Doyen Sahoo","Pushing the Limits of Pre-training for Time Series Forecasting in the
  CloudOps Domain","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Time series has been left behind in the era of pre-training and transfer
learning. While research in the fields of natural language processing and
computer vision are enjoying progressively larger datasets to train massive
models, the most popular time series datasets consist of only tens of thousands
of time steps, limiting our ability to study the effectiveness of pre-training
and scaling. Recent studies have also cast doubt on the need for expressive
models and scale. To alleviate these issues, we introduce three large-scale
time series forecasting datasets from the cloud operations (CloudOps) domain,
the largest having billions of observations, enabling further study into
pre-training and scaling of time series models. We build the empirical
groundwork for studying pre-training and scaling of time series models and pave
the way for future research by identifying a promising candidate architecture.
We show that it is a strong zero-shot baseline and benefits from further
scaling, both in model and dataset size. Accompanying these datasets and
results is a suite of comprehensive benchmark results comparing classical and
deep learning baselines to our pre-trained method - achieving a 27% reduction
in error on the largest dataset. Code and datasets will be released.
","2023-10-11","2310.05063v1.pdf"
"2310.05066","Chaoxu Pang","Chaoxu Pang, Yixuan Cao, Qiang Ding, Ping Luo","Guideline Learning for In-context Information Extraction","EMNLP 2023 main conference","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) can perform a new task by merely conditioning on
task instructions and a few input-output examples, without optimizing any
parameters. This is called In-Context Learning (ICL). In-context Information
Extraction (IE) has recently garnered attention in the research community.
However, the performance of In-context IE generally lags behind the
state-of-the-art supervised expert models. We highlight a key reason for this
shortfall: underspecified task description. The limited-length context
struggles to thoroughly express the intricate IE task instructions and various
edge cases, leading to misalignment in task comprehension with humans. In this
paper, we propose a Guideline Learning (GL) framework for In-context IE which
reflectively learns and follows guidelines. During the learning phrase, GL
automatically synthesizes a set of guidelines based on a few error cases, and
during inference, GL retrieves helpful guidelines for better ICL. Moreover, we
propose a self-consistency-based active learning method to enhance the
efficiency of GL. Experiments on event extraction and relation extraction show
that GL can significantly improve the performance of in-context IE.
","2023-10-24","2310.05066v1.pdf"
"2310.05073","Yun Luo","Yun Luo and Zhen Yang and Fandong Meng and Yingjie Li and Jie Zhou and
  Yue Zhang","Enhancing Argument Structure Extraction with Efficient Leverage of
  Contextual Information","EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Argument structure extraction (ASE) aims to identify the discourse structure
of arguments within documents. Previous research has demonstrated that
contextual information is crucial for developing an effective ASE model.
However, we observe that merely concatenating sentences in a contextual window
does not fully utilize contextual information and can sometimes lead to
excessive attention on less informative sentences. To tackle this challenge, we
propose an Efficient Context-aware ASE model (ECASE) that fully exploits
contextual information by enhancing modeling capacity and augmenting training
data. Specifically, we introduce a sequence-attention module and
distance-weighted similarity loss to aggregate contextual information and
argumentative information. Additionally, we augment the training data by
randomly masking discourse markers and sentences, which reduces the model's
reliance on specific words or less informative sentences. Our experiments on
five datasets from various domains demonstrate that our model achieves
state-of-the-art performance. Furthermore, ablation studies confirm the
effectiveness of each module in our model.
","2023-10-10","2310.05073v1.pdf"
"2310.05074","Chengcheng Han","Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao,
  Baoyuan Wang","DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller
  Language Models","Accepted to EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the
reasoning capabilities of Large Language Models (LLMs) with at least 100
billion parameters. However, it is ineffective or even detrimental when applied
to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion
parameters. To address this limitation, we introduce Dialogue-guided
Chain-of-Thought (DialCoT) which employs a dialogue format to generate
intermediate reasoning steps, guiding the model toward the final answer.
Additionally, we optimize the model's reasoning path selection using the
Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning
capabilities. Our method offers several advantages compared to previous
approaches. Firstly, we transform the process of solving complex reasoning
questions by breaking them down into a series of simpler sub-questions,
significantly reducing the task difficulty and making it more suitable for
SLMs. Secondly, we optimize the model's reasoning path selection through the
PPO algorithm. We conduct comprehensive experiments on four arithmetic
reasoning datasets, demonstrating that our method achieves significant
performance improvements compared to state-of-the-art competitors.
","2023-10-24","2310.05074v1.pdf"
"2310.05079","Cheng Zhang","Cheng Zhang, Jianyi Cheng, Ilia Shumailov, George A. Constantinides,
  and Yiren Zhao","Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM
  Inference?","Accepted by EMNLP2023","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  The inference of Large language models (LLMs) requires immense computation
and memory resources. To curtail these costs, quantisation has merged as a
promising solution, but existing LLM quantisation mainly focuses on 8-bit. In
this work, we explore the statistical and learning properties of the LLM layer
and attribute the bottleneck of LLM quantisation to numerical scaling offsets.
To address this, we adapt block quantisations for LLMs, a family of methods
that share scaling factors across packed numbers. Block quantisations
efficiently reduce the numerical scaling offsets solely from an arithmetic
perspective, without additional treatments in the computational path. Our
nearly-lossless quantised 6-bit LLMs achieve a $19\times$ higher arithmetic
density and $5\times$ memory density than the float32 baseline, surpassing the
prior art 8-bit quantisation by $2.5\times$ in arithmetic density and
$1.2\times$ in memory density, without requiring any data calibration or
re-training. We also share our insights into sub-8-bit LLM quantisation,
including the mismatch between activation and weight distributions, optimal
fine-tuning strategies, and a lower quantisation granularity inherent in the
statistical properties of LLMs. The latter two tricks enable nearly-lossless
4-bit LLMs on downstream tasks. Our code is open-sourced.
","2023-10-24","2310.05079v1.pdf"
"2310.05092","Jun Gao","Jun Gao, Huan Zhao, Yice Zhang, Wei Wang, Changlong Yu, Ruifeng Xu","Benchmarking Large Language Models with Augmented Instructions for
  Fine-grained Information Extraction","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Information Extraction (IE) is an essential task in Natural Language
Processing. Traditional methods have relied on coarse-grained extraction with
simple instructions. However, with the emergence of Large Language Models
(LLMs), there is a need to adapt IE techniques to leverage the capabilities of
these models. This paper introduces a fine-grained IE benchmark dataset
tailored for LLMs, employing augmented instructions for each information type,
which includes task descriptions, extraction rules, output formats, and
examples. Through extensive evaluations, we observe that encoder-decoder
models, particularly T5 and FLAN-T5, perform well in generalizing to unseen
information types, while ChatGPT exhibits greater adaptability to new task
forms. Our results also indicate that performance is not solely dictated by
model scale, and highlight the significance of architecture, data diversity,
and learning techniques. This work paves the way for a more refined and
versatile utilization of LLMs in Information Extraction.
","2023-10-10","2310.05092v1.pdf"
"2310.05095","Tharindu Kumarage","Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan
  Liu","How Reliable Are AI-Generated-Text Detectors? An Assessment Framework
  Using Evasive Soft Prompts","Accepted to EMNLP 2023 (Findings)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In recent years, there has been a rapid proliferation of AI-generated text,
primarily driven by the release of powerful pre-trained language models (PLMs).
To address the issue of misuse associated with AI-generated text, various
high-performing detectors have been developed, including the OpenAI detector
and the Stanford DetectGPT. In our study, we ask how reliable these detectors
are. We answer the question by designing a novel approach that can prompt any
PLM to generate text that evades these high-performing detectors. The proposed
approach suggests a universal evasive prompt, a novel type of soft prompt,
which guides PLMs in producing ""human-like"" text that can mislead the
detectors. The novel universal evasive prompt is achieved in two steps: First,
we create an evasive soft prompt tailored to a specific PLM through prompt
tuning; and then, we leverage the transferability of soft prompts to transfer
the learned evasive soft prompt from one PLM to another. Employing multiple
PLMs in various writing tasks, we conduct extensive experiments to evaluate the
efficacy of the evasive soft prompts in their evasion of state-of-the-art
detectors.
","2023-10-10","2310.05095v1.pdf"
"2310.05103","Xianjun Yang","Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William Yang
  Wang, Wei Cheng","Zero-Shot Detection of Machine-Generated Codes","work in progress","","","","cs.CL cs.AI cs.CR cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This work proposes a training-free approach for the detection of
LLMs-generated codes, mitigating the risks associated with their indiscriminate
usage. To the best of our knowledge, our research is the first to investigate
zero-shot detection techniques applied to code generated by advanced black-box
LLMs like ChatGPT. Firstly, we find that existing training-based or zero-shot
text detectors are ineffective in detecting code, likely due to the unique
statistical properties found in code structures. We then modify the previous
zero-shot text detection method, DetectGPT (Mitchell et al., 2023) by utilizing
a surrogate white-box model to estimate the probability of the rightmost
tokens, allowing us to identify code snippets generated by language models.
Through extensive experiments conducted on the python codes of the CodeContest
and APPS dataset, our approach demonstrates its effectiveness by achieving
state-of-the-art detection results on text-davinci-003, GPT-3.5, and GPT-4
models. Moreover, our method exhibits robustness against revision attacks and
generalizes well to Java codes. We also find that the smaller code language
model like PolyCoder-160M performs as a universal code detector, outperforming
the billion-scale counterpart. The codes will be available at
https://github.com/ Xianjun-Yang/Code_detection.git
","2023-10-10","2310.05103v1.pdf"
"2310.05109","Yixin Chen","Yixin Chen, Shuai Zhang, Boran Han, Jiaya Jia","Lightweight In-Context Tuning for Multimodal Unified Models","Preprint","","","","cs.CV","http://creativecommons.org/licenses/by-sa/4.0/","  In-context learning (ICL) involves reasoning from given contextual examples.
As more modalities comes, this procedure is becoming more challenging as the
interleaved input modalities convolutes the understanding process. This is
exemplified by the observation that multimodal models often struggle to
effectively extrapolate from contextual examples to perform ICL. To address
these challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), a
lightweight module to enhance the ICL capabilities of multimodal unified
models. The proposed M$^2$IXT module perceives an expandable context window to
incorporate various labeled examples of multiple modalities (e.g., text, image,
and coordinates). It can be prepended to various multimodal unified models
(e.g., OFA, Unival, LLaVA) of different architectures and trained via a
mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and
datasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boost
the few-shot ICL performance significantly (e.g., 18\% relative increase for
OFA), and obtained state-of-the-art results across an array of tasks including
visual question answering, image captioning, visual grounding, and visual
entailment, while being considerably small in terms of model parameters (e.g.,
$\sim$$20\times$ smaller than Flamingo or MMICL), highlighting the flexibility
and effectiveness of M$^2$IXT as a multimodal in-context learner.
","2023-10-10","2310.05109v1.pdf"
"2310.05126","Jiabo Ye","Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu,
  Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex
  Lin, Fei Huang","UReader: Universal OCR-free Visually-situated Language Understanding
  with Multimodal Large Language Model","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text is ubiquitous in our visual world, conveying crucial information, such
as in documents, websites, and everyday photographs. In this work, we propose
UReader, a first exploration of universal OCR-free visually-situated language
understanding based on the Multimodal Large Language Model (MLLM). By
leveraging the shallow text recognition ability of the MLLM, we only finetuned
1.2% parameters and the training cost is much lower than previous work
following domain-specific pretraining and finetuning paradigms. Concretely,
UReader is jointly finetuned on a wide range of Visually-situated Language
Understanding tasks via a unified instruction format. To enhance the visual
text and semantic understanding, we further apply two auxiliary tasks with the
same format, namely text reading and key points generation tasks. We design a
shape-adaptive cropping module before the encoder-decoder architecture of MLLM
to leverage the frozen low-resolution vision encoder for processing
high-resolution images. Without downstream finetuning, our single model
achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated
language understanding tasks, across 5 domains: documents, tables, charts,
natural images, and webpage screenshots. Codes and instruction-tuning datasets
will be released.
","2023-10-10","2310.05126v1.pdf"
"2310.05128","Simon Chi Lok U","Simon Chi Lok U, Jie He, V\'ictor Guti\'errez-Basulto, Jeff Z. Pan","Instances and Labels: Hierarchy-aware Joint Supervised Contrastive
  Learning for Hierarchical Multi-Label Text Classification","18 pages; 10 figures. Published as a conference paper at EMNLP 2023
  Findings (Long Paper). Code and data available at
  https://github.com/simonucl/HJCL","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Hierarchical multi-label text classification (HMTC) aims at utilizing a label
hierarchy in multi-label classification. Recent approaches to HMTC deal with
the problem of imposing an over-constrained premise on the output space by
using contrastive learning on generated samples in a semi-supervised manner to
bring text and label embeddings closer. However, the generation of samples
tends to introduce noise as it ignores the correlation between similar samples
in the same batch. One solution to this issue is supervised contrastive
learning, but it remains an underexplored topic in HMTC due to its complex
structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a
$\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive
$\textbf{L}$earning method that bridges the gap between supervised contrastive
learning and HMTC. Specifically, we employ both instance-wise and label-wise
contrastive learning techniques and carefully construct batches to fulfill the
contrastive learning objective. Extensive experiments on four multi-path HMTC
datasets demonstrate that HJCL achieves promising results and the effectiveness
of Contrastive Learning on HMTC.
","2023-10-17","2310.05128v1.pdf"
"2310.05130","Guangsheng Bao","Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang","Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text
  via Conditional Probability Curvature","9 pages, 5 figures, 11 tables","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have shown the ability to produce fluent and
cogent content, presenting both productivity opportunities and societal risks.
To build trustworthy AI systems, it is imperative to distinguish between
machine-generated and human-authored content. The leading zero-shot detector,
DetectGPT, showcases commendable performance but is marred by its intensive
computational costs. In this paper, we introduce the concept of conditional
probability curvature to elucidate discrepancies in word choices between LLMs
and humans within a given context. Utilizing this curvature as a foundational
metric, we present Fast-DetectGPT, an optimized zero-shot detector, which
substitutes DetectGPT's perturbation step with a more efficient sampling step.
Our evaluations on various datasets, source models, and test conditions
indicate that Fast-DetectGPT not only outperforms DetectGPT in both the
white-box and black-box settings but also accelerates the detection process by
a factor of 340, as detailed in Table 1.
","2023-10-10","2310.05130v1.pdf"
"2310.05135","Akshaj Kumar Veldanda","Akshaj Kumar Veldanda, Fabian Grob, Shailja Thakur, Hammond Pearce,
  Benjamin Tan, Ramesh Karri, Siddharth Garg","Are Emily and Greg Still More Employable than Lakisha and Jamal?
  Investigating Algorithmic Hiring Bias in the Era of ChatGPT","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit
applicability across numerous tasks. One domain of interest is their use in
algorithmic hiring, specifically in matching resumes with job categories. Yet,
this introduces issues of bias on protected attributes like gender, race and
maternity status. The seminal work of Bertrand & Mullainathan (2003) set the
gold-standard for identifying hiring bias via field experiments where the
response rate for identical resumes that differ only in protected attributes,
e.g., racially suggestive names such as Emily or Lakisha, is compared. We
replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and
Llama) to evaluate bias (or lack thereof) on gender, race, maternity status,
pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)
matching resumes to job categories; and (2) summarizing resumes with employment
relevant information. Overall, LLMs are robust across race and gender. They
differ in their performance on pregnancy status and political affiliation. We
use contrastive input decoding on open-source LLMs to uncover potential sources
of bias.
","2023-10-10","2310.05135v1.pdf"
"2310.05136","Ronghao Dang","Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song,
  Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song","InstructDET: Diversifying Referring Object Detection with Generalized
  Instructions","27 pages (include Appendix) Technical Report","","","","cs.AI cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.
","2023-10-18","2310.05136v1.pdf"
"2310.05140","Yushan Qian","Yushan Qian, Wei-Nan Zhang, Ting Liu","Harnessing the Power of Large Language Models for Empathetic Response
  Generation: Empirical Investigations and Improvements","the Findings of EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.
","2023-10-10","2310.05140v1.pdf"
"2310.05143","Jindong Wang","Wang Lu, Hao Yu, Jindong Wang, Damien Teney, Haohan Wang, Yiqiang
  Chen, Qiang Yang, Xing Xie, Xiangyang Ji","ZooPFL: Exploring Black-box Foundation Models for Personalized Federated
  Learning","Technical report; 26 pages; code will be available at
  https://github.com/microsoft/PersonalizedFL","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  When personalized federated learning (FL) meets large foundation models, new
challenges arise from various limitations in resources. In addition to typical
limitations such as data, computation, and communication costs, access to the
models is also often limited. This paper endeavors to solve both the challenges
of limited resources and personalization. i.e., distribution shifts between
clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order
Optimization for Personalized Federated Learning. ZOOPFL avoids direct
interference with the foundation models and instead learns to adapt its inputs
through zeroth-order optimization. In addition, we employ simple yet effective
linear projections to remap its predictions for personalization. To reduce the
computation costs and enhance personalization, we propose input surgery to
incorporate an auto-encoder with low-dimensional and client-specific
embeddings. We provide theoretical support for ZOOPFL to analyze its
convergence. Extensive empirical experiments on computer vision and natural
language processing tasks using popular foundation models demonstrate its
effectiveness for FL on black-box foundation models.
","2023-10-10","2310.05143v1.pdf"
"2310.05146","John Chong Min Tan","John Chong Min Tan, Mehul Motani","Large Language Model (LLM) as a System of Multiple Expert Agents: An
  Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge","6 main pages, 1 page references, 18 pages appendix","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge
using Large Language Models (LLMs) as a system of multiple expert agents. Using
the flexibility of LLMs to be prompted to do various novel tasks using
zero-shot, few-shot, context-grounded prompting, we explore the feasibility of
using LLMs to solve the ARC Challenge. We firstly convert the input image into
multiple suitable text-based abstraction spaces. We then utilise the
associative power of LLMs to derive the input-output relationship and map this
to actions in the form of a working program, similar to Voyager / Ghost in the
MineCraft. In addition, we use iterative environmental feedback in order to
guide LLMs to solve the task. Our proposed approach achieves 50 solves out of
111 training set problems (45%) with just three abstraction spaces - grid,
object and pixel - and we believe that with more abstraction spaces and
learnable actions, we will be able to solve more.
","2023-10-10","2310.05146v1.pdf"
"2310.05149","Zhangyin Feng","Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin","Retrieval-Generation Synergy Augmented Large Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models augmented with task-relevant documents have
demonstrated impressive performance on knowledge-intensive tasks. However,
regarding how to obtain effective documents, the existing methods are mainly
divided into two categories. One is to retrieve from an external knowledge
base, and the other is to utilize large language models to generate documents.
We propose an iterative retrieval-generation collaborative framework. It is not
only able to leverage both parametric and non-parametric knowledge, but also
helps to find the correct reasoning path through retrieval-generation
interactions, which is very important for tasks that require multi-step
reasoning. We conduct experiments on four question answering datasets,
including single-hop QA and multi-hop QA tasks. Empirical results show that our
method significantly improves the reasoning ability of large language models
and outperforms previous baselines.
","2023-10-10","2310.05149v1.pdf"
"2310.05150","Phillip Schneider","Phillip Schneider, Nils Rehtanz, Kristiina Jokinen and Florian Matthes","From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for
  Conversational Exploratory Search","Accepted to PACLIC 2023","","","","cs.CL cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Exploratory search is an open-ended information retrieval process that aims
at discovering knowledge about a topic or domain rather than searching for a
specific answer or piece of information. Conversational interfaces are
particularly suitable for supporting exploratory search, allowing users to
refine queries and examine search results through interactive dialogues. In
addition to conversational search interfaces, knowledge graphs are also useful
in supporting information exploration due to their rich semantic representation
of data items. In this study, we demonstrate the synergistic effects of
combining knowledge graphs and conversational interfaces for exploratory
search, bridging the gap between structured and unstructured information
retrieval. To this end, we propose a knowledge-driven dialogue system for
exploring news articles by asking natural language questions and using the
graph structure to navigate between related topics. Based on a user study with
54 participants, we empirically evaluate the effectiveness of the graph-based
exploratory search and discuss design implications for developing such systems.
","2023-10-10","2310.05150v1.pdf"
"2310.05155","Cheng Qian","Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu","Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on
  Open-Source Model","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated remarkable progress in
utilizing tools, but their closed-source nature and high inference costs pose
limitations on their adaptability, necessitating a valid method that leverages
smaller, open-sourced models. In this paper, we introduce Toolink, a
comprehensive framework that performs task-solving by first creating a toolkit
and then integrating the planning and calling of tools through a
chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in
harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we
curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and
finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source
model with advanced tool-planning and tool-calling capabilities. Evaluation on
diverse tasks from BIG-bench demonstrates its CoS ability matches that of
ChatGPT while its performance surpasses the chain-of-thought approach. Further
studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase
its capability in using toolkits not explicitly tailored for the target task,
affirming its robustness in real-world scenarios. All codes and data are
released.
","2023-10-10","2310.05155v1.pdf"
"2310.05157","Yifan Wei","Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe
  Zhang, Jun Zhao, Kang Liu","MenatQA: A New Dataset for Testing the Temporal Comprehension and
  Reasoning Abilities of Large Language Models","Accepted to EMNLP 2023 Findings","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have shown nearly saturated performance on many
natural language processing (NLP) tasks. As a result, it is natural for people
to believe that LLMs have also mastered abilities such as time understanding
and reasoning. However, research on the temporal sensitivity of LLMs has been
insufficiently emphasized. To fill this gap, this paper constructs Multiple
Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors
(scope factor, order factor, counterfactual factor) with total 2,853 samples
for evaluating the time comprehension and reasoning abilities of LLMs. This
paper tests current mainstream LLMs with different parameter sizes, ranging
from billions to hundreds of billions. The results show most LLMs fall behind
smaller temporal reasoning models with different degree on these factors. In
specific, LLMs show a significant vulnerability to temporal biases and depend
heavily on the temporal information provided in questions. Furthermore, this
paper undertakes a preliminary investigation into potential improvement
strategies by devising specific prompts and leveraging external tools. These
approaches serve as valuable baselines or references for future research
endeavors.
","2023-10-10","2310.05157v1.pdf"
"2310.05161","Anej Svete","Anej Svete, Ryan Cotterell","Recurrent Neural Language Models as Probabilistic Finite-state Automata","9 pages","","","","cs.CL cs.CC cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Studying language models (LMs) in terms of well-understood formalisms allows
us to precisely characterize their abilities and limitations. Previous work has
investigated the representational capacity of recurrent neural network (RNN)
LMs in terms of their capacity to recognize unweighted formal languages.
However, LMs do not describe unweighted formal languages -- rather, they define
probability distributions over strings. In this work, we study what classes of
such probability distributions RNN LMs can represent, which allows us to make
more direct statements about their capabilities. We show that simple RNNs are
equivalent to a subclass of probabilistic finite-state automata, and can thus
model a strict subset of probability distributions expressible by finite-state
models. Furthermore, we study the space complexity of representing finite-state
LMs with RNNs. We show that, to represent an arbitrary deterministic
finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requires
$\Omega\left(N |\Sigma|\right)$ neurons. These results present a first step
towards characterizing the classes of distributions RNN LMs can represent and
thus help us understand their capabilities and limitations.
","2023-10-20","2310.05161v1.pdf"
"2310.05163","Binyuan Hui","Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang
  Wu, Yuanjun Laili","An Investigation of LLMs' Inefficacy in Understanding Converse Relations","Accepted by EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have achieved remarkable success in many formal
language oriented tasks, such as structural data-to-text and semantic parsing.
However current benchmarks mostly follow the data distribution of the
pre-training data of LLMs. Therefore, a natural question rises that do LLMs
really understand the structured semantics of formal languages. In this paper,
we investigate this problem on a special case, converse binary relation. We
introduce a new benchmark ConvRe focusing on converse relations, which contains
17 relations and 1240 triples extracted from popular knowledge graph completion
datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are
formulated as multi-choice question answering to evaluate LLMs' ability to
determine the matching between relations and associated text. For the
evaluation protocol, apart from different prompting methods, we further
introduce variants to the test text and few-shot example text. We conduct
experiments on three popular LLM families and have observed various scaling
trends. The results suggest that LLMs often resort to shortcut learning and
still face challenges on our proposed benchmark.
","2023-10-26","2310.05163v1.pdf"
"2310.05165","Xiao Pu","Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He","On the Zero-Shot Generalization of Machine-Generated Text Detectors","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The rampant proliferation of large language models, fluent enough to generate
text indistinguishable from human-written language, gives unprecedented
importance to the detection of machine-generated text. This work is motivated
by an important research question: How will the detectors of machine-generated
text perform on outputs of a new generator, that the detectors were not trained
on? We begin by collecting generation data from a wide range of LLMs, and train
neural detectors on data from each generator and test its performance on
held-out generators. While none of the detectors can generalize to all
generators, we observe a consistent and interesting pattern that the detectors
trained on data from a medium-size LLM can zero-shot generalize to the larger
version. As a concrete application, we demonstrate that robust detectors can be
built on an ensemble of training data from medium-sized models.
","2023-10-10","2310.05165v1.pdf"
"2310.05175","Shiwei Liu","Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia,
  Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei Liu","Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for
  Pruning LLMs to High Sparsity","","","","","cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large Language Models (LLMs), renowned for their remarkable performance,
present a challenge due to their colossal model size when it comes to practical
deployment. In response to this challenge, efforts have been directed toward
the application of traditional network pruning techniques to LLMs, uncovering a
massive number of parameters can be pruned in one-shot without hurting
performance. Building upon insights gained from pre-LLM models, prevailing LLM
pruning strategies have consistently adhered to the practice of uniformly
pruning all layers at equivalent sparsity. However, this observation stands in
contrast to the prevailing trends observed in the field of vision models, where
non-uniform layerwise sparsity typically yields substantially improved results.
To elucidate the underlying reasons for this disparity, we conduct a
comprehensive analysis of the distribution of token features within LLMs. In
doing so, we discover a strong correlation with the emergence of outliers,
defined as features exhibiting significantly greater magnitudes compared to
their counterparts in feature dimensions. Inspired by this finding, we
introduce a novel LLM pruning methodology that incorporates a tailored set of
non-uniform layerwise sparsity ratios specifically designed for LLM pruning,
termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL
is directly proportional to the outlier ratio observed within each layer,
facilitating a more effective alignment between layerwise weight sparsity and
outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family
and OPT, spanning various benchmarks, demonstrates the distinct advantages
offered by OWL over previous methods. For instance, our approach exhibits a
remarkable performance gain, surpassing the state-of-the-art Wanda and
SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%,
respectively.
","2023-10-10","2310.05175v1.pdf"
"2310.05177","Zhijiang Guo","Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S.
  Yu, Zhijiang Guo","Do Large Language Models Know about Facts?","20 pages, 8 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have recently driven striking performance
improvements across a range of natural language processing tasks. The factual
knowledge acquired during pretraining and instruction tuning can be useful in
various downstream tasks, such as question answering, and language generation.
Unlike conventional Knowledge Bases (KBs) that explicitly store factual
knowledge, LLMs implicitly store facts in their parameters. Content generated
by the LLMs can often exhibit inaccuracies or deviations from the truth, due to
facts that can be incorrectly induced or become obsolete over time. To this
end, we aim to comprehensively evaluate the extent and scope of factual
knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains
20K diverse factual questions that span different sources, timelines, domains,
regions, and languages. Furthermore, we investigate whether LLMs are able to
compose multiple facts, update factual knowledge temporally, reason over
multiple pieces of facts, identify subtle factual differences, and resist
adversarial examples. Extensive experiments on different sizes and types of
LLMs show that existing LLMs still lack factual knowledge and suffer from
various spurious correlations. We believe this is a critical bottleneck for
realizing trustworthy artificial intelligence. The dataset Pinocchio and our
codes will be publicly available.
","2023-10-10","2310.05177v1.pdf"
"2310.05178","Nii Osae Osae Dade","Nii Osae Osae Dade, Margaret Lartey-Quaye, Emmanuel Teye-Kofi Odonkor,
  Paul Ammah","Optimizing Large Language Models to Expedite the Development of Smart
  Contracts","","","","","cs.SE cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Programming has always been at the heart of technological innovation in the
21st century. With the advent of blockchain technologies and the proliferation
of web3 paradigms of decentralised applications, smart contracts have been very
instrumental in enabling developers to build applications that reside on
decentralised blockchains. Despite the huge interest and potential of smart
contracts, there is still a significant knowledge and skill gap that developers
need to cross in order to build web3 applications. In light of this, we
introduce MazzumaGPT, a large language model that has been optimised to
generate smart contract code and aid developers to scaffold development and
improve productivity. As part of this research, we outline the optimisation and
fine-tuning parameters, evaluate the model's performance on functional
correctness and address the limitations and broader impacts of our research.
","2023-10-10","2310.05178v1.pdf"
"2310.05185","Haoran Luo","Haoran Luo, Haihong E, Yuhao Yang, Tianyu Yao, Yikai Guo, Zichen Tang,
  Wentai Zhang, Kaiyang Wan, Shiyao Peng, Meina Song, Wei Lin","Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational
  Knowledge Graph Construction","Preprint","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Beyond traditional binary relational facts, n-ary relational knowledge graphs
(NKGs) are comprised of n-ary relational facts containing more than two
entities, which are closer to real-world facts with broader applications.
However, the construction of NKGs still significantly relies on manual labor,
and n-ary relation extraction still remains at a course-grained level, which is
always in a single schema and fixed arity of entities. To address these
restrictions, we propose Text2NKG, a novel fine-grained n-ary relation
extraction framework for n-ary relational knowledge graph construction. We
introduce a span-tuple classification approach with hetero-ordered merging to
accomplish fine-grained n-ary relation extraction in different arity.
Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational
schema, event-based schema, role-based schema, and hypergraph-based schema,
with high flexibility and practicality. Experimental results demonstrate that
Text2NKG outperforms the previous state-of-the-art model by nearly 20\% points
in the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in
the hyper-relational schema. Our code and datasets are publicly available.
","2023-10-16","2310.05185v1.pdf"
"2310.05189","Tanmoy Chakraborty","Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy
  Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio
  Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer,
  Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni","Factuality Challenges in the Era of Large Language Models","Our article offers a comprehensive examination of the challenges and
  risks associated with Large Language Models (LLMs), focusing on their
  potential impact on the veracity of information in today's digital landscape","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  The emergence of tools based on Large Language Models (LLMs), such as
OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered
immense public attention. These incredibly useful, natural-sounding tools mark
significant advances in natural language generation, yet they exhibit a
propensity to generate false, erroneous, or misleading content -- commonly
referred to as ""hallucinations."" Moreover, LLMs can be exploited for malicious
applications, such as generating false but credible-sounding content and
profiles at scale. This poses a significant challenge to society in terms of
the potential deception of users and the increasing dissemination of inaccurate
information. In light of these risks, we explore the kinds of technological
innovations, regulatory reforms, and AI literacy initiatives needed from
fact-checkers, news organizations, and the broader research and policy
communities. By identifying the risks, the imminent threats, and some viable
solutions, we seek to shed light on navigating various aspects of veracity in
the era of generative AI.
","2023-10-11","2310.05189v1.pdf"
"2310.05191","Jieun Han","Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Hyunseung Lim, Yoonsu
  Kim, Tak Yeon Lee, Hwajung Hong, Juho Kim, So-Yeon Ahn, Alice Oh","FABRIC: Automated Scoring and Feedback Generation for Essays","","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Automated essay scoring (AES) provides a useful tool for students and
instructors in writing classes by generating essay scores in real-time.
However, previous AES models do not provide more specific rubric-based scores
nor feedback on how to improve the essays, which can be even more important
than the overall scores for learning. We present FABRIC, a pipeline to help
students and instructors in English writing classes by automatically generating
1) the overall scores, 2) specific rubric-based scores, and 3) detailed
feedback on how to improve the essays. Under the guidance of English education
experts, we chose the rubrics for the specific scores as content, organization,
and language. The first component of the FABRIC pipeline is DREsS, a real-world
Dataset for Rubric-based Essay Scoring (DREsS). The second component is CASE, a
Corruption-based Augmentation Strategy for Essays, with which we can improve
the accuracy of the baseline model by 45.44%. The third component is EssayCoT,
the Essay Chain-of-Thought prompting strategy which uses scores predicted from
the AES model to generate better feedback. We evaluate the effectiveness of the
new dataset DREsS and the augmentation strategy CASE quantitatively and show
significant improvements over the models trained with existing datasets. We
evaluate the feedback generated by EssayCoT with English education experts to
show significant improvements in the helpfulness of the feedback across all
rubrics. Lastly, we evaluate the FABRIC pipeline with students in a college
English writing class who rated the generated scores and feedback with an
average of 6 on the Likert scale from 1 to 7.
","2023-10-10","2310.05191v1.pdf"
"2310.05193","Chenzhuang Du","Chenzhuang Du, Yue Zhao, Chonghua Liao, Jiacheng You, Jie Fu, Hang
  Zhao","Improving Discriminative Multi-Modal Learning with Large-Scale
  Pre-Trained Models","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper investigates how to better leverage large-scale pre-trained
uni-modal models to further enhance discriminative multi-modal learning. Even
when fine-tuned with only uni-modal data, these models can outperform previous
multi-modal models in certain tasks. It's clear that their incorporation into
multi-modal learning would significantly improve performance. However,
multi-modal learning with these models still suffers from insufficient learning
of uni-modal features, which weakens the resulting multi-modal model's
generalization ability. While fine-tuning uni-modal models separately and then
aggregating their predictions is straightforward, it doesn't allow for adequate
adaptation between modalities, also leading to sub-optimal results. To this
end, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By
freezing the weights of uni-modal fine-tuned models, adding extra trainable
rank decomposition matrices to them, and subsequently performing multi-modal
joint training, our method enhances adaptation between modalities and boosts
overall performance. We demonstrate the effectiveness of MMLoRA on three
dataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D),
vision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).
","2023-10-10","2310.05193v1.pdf"
"2310.05199","Wei Shen","Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi
  Zhang, Xuanjing Huang","Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning
  from Human Feedback","EMNLP findings 2023 (camera-ready) Length Bias in RLHF","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reinforcement learning from human feedback serves as a crucial bridge,
aligning large language models with human and societal values. This alignment
requires a vast corpus of human feedback to learn a reward model, which is
subsequently used to finetune language models. However, we have identified that
the reward model often finds shortcuts to bypass its intended objectives,
misleadingly assuming that humans prefer longer responses. The emergence of
length bias often induces the model to favor longer outputs, yet it doesn't
equate to an increase in helpful information within these outputs. In this
paper, we propose an innovative solution, applying the Product-of-Experts (PoE)
technique to separate reward modeling from the influence of sequence length. In
our framework, the main expert concentrates on understanding human intents,
while the biased expert targets the identification and capture of length bias.
To further enhance the learning of bias, we introduce perturbations into the
bias-focused expert, disrupting the flow of semantic information. Experimental
results validate the effectiveness of our approach, indicating that language
model performance is improved, irrespective of sequence length.
","2023-10-20","2310.05199v3.pdf"
"2310.05204","YunDa Tsai","Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, Shou-De Lin","Towards Optimizing with Large Language Models","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  In this work, we conduct an assessment of the optimization capabilities of
LLMs across various tasks and data sizes. Each of these tasks corresponds to
unique optimization domains, and LLMs are required to execute these tasks with
interactive prompting. That is, in each optimization step, the LLM generates
new solutions from the past generated solutions with their values, and then the
new solutions are evaluated and considered in the next optimization step.
Additionally, we introduce three distinct metrics for a comprehensive
assessment of task performance from various perspectives. These metrics offer
the advantage of being applicable for evaluating LLM performance across a broad
spectrum of optimization tasks and are less sensitive to variations in test
samples. By applying these metrics, we observe that LLMs exhibit strong
optimization capabilities when dealing with small-sized samples. However, their
performance is significantly influenced by factors like data size and values,
underscoring the importance of further research in the domain of optimization
tasks for LLMs.
","2023-10-10","2310.05204v1.pdf"
"2310.05209","Xiaoran Liu","Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin","Scaling Laws of RoPE-based Extrapolation","20 pages, 12 figures","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The extrapolation capability of Large Language Models (LLMs) based on Rotary
Position Embedding is currently a topic of considerable interest. The
mainstream approach to addressing extrapolation with LLMs involves modifying
RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the
original RoPE, with a larger value and providing longer fine-tuning text. In
this work, we first observe that fine-tuning a RoPE-based LLM with either a
smaller or larger base in pre-training context length could significantly
enhance its extrapolation performance. After that, we propose
\textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework
from the periodic perspective, to describe the relationship between the
extrapolation performance and base value as well as tuning context length. In
this process, we also explain the origin of the RoPE-based extrapolation issue
by \textbf{\textit{critical dimension for extrapolation}}. Besides these
observations and analyses, we achieve extrapolation up to 1 million context
length within only 16K training length on LLaMA2 7B and 13B.
","2023-10-10","2310.05209v1.pdf"
"2310.05216","Xintong Wang","Xintong Wang, Xiaoyu Li, Xingshan Li, and Chris Biemann","Probing Language Models from A Human Behavioral Perspective","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have emerged as dominant foundational models in
modern NLP. However, the understanding of their prediction process and internal
mechanisms, such as feed-forward networks and multi-head self-attention,
remains largely unexplored. In this study, we probe LLMs from a human
behavioral perspective, correlating values from LLMs with eye-tracking
measures, which are widely recognized as meaningful indicators of reading
patterns. Our findings reveal that LLMs exhibit a prediction pattern distinct
from that of RNN-based LMs. Moreover, with the escalation of FFN layers, the
capacity for memorization and linguistic knowledge encoding also surges until
it peaks, subsequently pivoting to focus on comprehension capacity. The
functions of self-attention are distributed across multiple heads. Lastly, we
scrutinize the gate mechanisms, finding that they control the flow of
information, with some gates promoting, while others eliminating information.
","2023-10-10","2310.05216v1.pdf"
"2310.05239","Reihaneh Mirjalili","Reihaneh Mirjalili, Michael Krawez, Simone Silenzi, Yannik Blei and
  Wolfram Burgard","LAN-grasp: Using Large Language Models for Semantic Object Grasping","","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  In this paper, we propose LAN-grasp, a novel approach towards more
appropriate semantic grasping. We use foundation models to provide the robot
with a deeper understanding of the objects, the right place to grasp an object,
or even the parts to avoid. This allows our robot to grasp and utilize objects
in a more meaningful and safe manner. We leverage the combination of a Large
Language Model, a Vision Language Model, and a traditional grasp planner to
generate grasps demonstrating a deeper semantic understanding of the objects.
We first prompt the Large Language Model about which object part is appropriate
for grasping. Next, the Vision Language Model identifies the corresponding part
in the object image. Finally, we generate grasp proposals in the region
proposed by the Vision Language Model. Building on foundation models provides
us with a zero-shot grasp method that can handle a wide range of objects
without the need for further training or fine-tuning. We evaluated our method
in real-world experiments on a custom object data set. We present the results
of a survey that asks the participants to choose an object part appropriate for
grasping. The results show that the grasps generated by our method are
consistently ranked higher by the participants than those generated by a
conventional grasping planner and a recent semantic grasping approach.
","2023-10-10","2310.05239v1.pdf"
"2310.05242","Zihao Wu","Tianyang Zhong, Wei Zhao, Yutong Zhang, Yi Pan, Peixin Dong, Zuowei
  Jiang, Xiaoyan Kui, Youlan Shang, Li Yang, Yaonai Wei, Longtao Yang, Hao
  Chen, Huan Zhao, Yuxiao Liu, Ning Zhu, Yiwei Li, Yisong Wang, Jiaqi Yao,
  Jiaqi Wang, Ying Zeng, Lei He, Chao Zheng, Zhixue Zhang, Ming Li, Zhengliang
  Liu, Haixing Dai, Zihao Wu, Lu Zhang, Shu Zhang, Xiaoyan Cai, Xintao Hu,
  Shijie Zhao, Xi Jiang, Xin Zhang, Xiang Li, Dajiang Zhu, Lei Guo, Dinggang
  Shen, Junwei Han, Tianming Liu, Jun Liu, Tuo Zhang","ChatRadio-Valuer: A Chat Large Language Model for Generalizable
  Radiology Report Generation Based on Multi-institution and Multi-system Data","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Radiology report generation, as a key step in medical image analysis, is
critical to the quantitative analysis of clinically informed decision-making
levels. However, complex and diverse radiology reports with cross-source
heterogeneity pose a huge generalizability challenge to the current methods
under massive data volume, mainly because the style and normativity of
radiology reports are obviously distinctive among institutions, body regions
inspected and radiologists. Recently, the advent of large language models (LLM)
offers great potential for recognizing signs of health conditions. To resolve
the above problem, we collaborate with the Second Xiangya Hospital in China and
propose ChatRadio-Valuer based on the LLM, a tailored model for automatic
radiology report generation that learns generalizable representations and
provides a basis pattern for model adaptation in sophisticated analysts' cases.
Specifically, ChatRadio-Valuer is trained based on the radiology reports from a
single institution by means of supervised fine-tuning, and then adapted to
disease diagnosis tasks for human multi-system evaluation (i.e., chest,
abdomen, muscle-skeleton, head, and maxillofacial $\&$ neck) from six different
institutions in clinical-level events. The clinical dataset utilized in this
study encompasses a remarkable total of \textbf{332,673} observations. From the
comprehensive results on engineering indicators, clinical efficacy and
deployment cost metrics, it can be shown that ChatRadio-Valuer consistently
outperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and
GPT-4 et al., in terms of the diseases diagnosis from radiology reports.
ChatRadio-Valuer provides an effective avenue to boost model generalization
performance and alleviate the annotation workload of experts to enable the
promotion of clinical AI applications in radiology reports.
","2023-10-11","2310.05242v1.pdf"
"2310.05249","Yu Huang","Yu Huang, Yuan Cheng, Yingbin Liang","In-Context Convergence of Transformers","74 pages, 1 figure","","","","cs.LG cs.AI math.OC stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformers have recently revolutionized many domains in modern machine
learning and one salient discovery is their remarkable in-context learning
capability, where models can solve an unseen task by utilizing task-specific
prompts without further parameters fine-tuning. This also inspired recent
theoretical studies aiming to understand the in-context learning mechanism of
transformers, which however focused only on linear transformers. In this work,
we take the first step toward studying the learning dynamics of a one-layer
transformer with softmax attention trained via gradient descent in order to
in-context learn linear function classes. We consider a structured data model,
where each token is randomly sampled from a set of feature vectors in either
balanced or imbalanced fashion. For data with balanced features, we establish
the finite-time convergence guarantee with near-zero prediction error by
navigating our analysis over two phases of the training dynamics of the
attention map. More notably, for data with imbalanced features, we show that
the learning dynamics take a stage-wise convergence process, where the
transformer first converges to a near-zero prediction error for the query
tokens of dominant features, and then converges later to a near-zero prediction
error for the query tokens of under-represented features, respectively via one
and four training phases. Our proof features new techniques for analyzing the
competing strengths of two types of attention weights, the change of which
determines different training phases.
","2023-10-10","2310.05249v1.pdf"
"2310.05253","Haoran Wang","Haoran Wang, Kai Shu","Explainable Claim Verification via Knowledge-Grounded Reasoning with
  Large Language Models","Findings of EMNLP 2023","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Claim verification plays a crucial role in combating misinformation. While
existing works on claim verification have shown promising results, a crucial
piece of the puzzle that remains unsolved is to understand how to verify claims
without relying on human-annotated data, which is expensive to create at a
large scale. Additionally, it is important for models to provide comprehensive
explanations that can justify their decisions and assist human fact-checkers.
This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)
Reasoning that can verify complex claims and generate explanations without the
need for annotated evidence using Large Language Models (LLMs). FOLK leverages
the in-context learning ability of LLMs to translate the claim into a
First-Order-Logic (FOL) clause consisting of predicates, each corresponding to
a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning
over a set of knowledge-grounded question-and-answer pairs to make veracity
predictions and generate explanations to justify its decision-making process.
This process makes our model highly explanatory, providing clear explanations
of its reasoning process in human-readable form. Our experiment results
indicate that FOLK outperforms strong baselines on three datasets encompassing
various claim verification challenges. Our code and data are available.
","2023-10-23","2310.05253v1.pdf"
"2310.05280","Yixin Wan","Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, Kai-Wei Chang","Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona
  Biases in Dialogue Systems","","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  Recent advancements in Large Language Models empower them to follow freeform
instructions, including imitating generic or specific demographic personas in
conversations. We define generic personas to represent demographic groups, such
as ""an Asian person"", whereas specific personas may take the form of specific
popular Asian names like ""Yumi"". While the adoption of personas enriches user
experiences by making dialogue systems more engaging and approachable, it also
casts a shadow of potential risk by exacerbating social biases within model
responses, thereby causing societal harm through interactions with users. In
this paper, we systematically study ""persona biases"", which we define to be the
sensitivity of dialogue models' harmful behaviors contingent upon the personas
they adopt. We categorize persona biases into biases in harmful expression and
harmful agreement, and establish a comprehensive evaluation framework to
measure persona biases in five aspects: Offensiveness, Toxic Continuation,
Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to
investigate persona biases by experimenting with UNIVERSALPERSONA, a
systematically constructed persona dataset encompassing various types of both
generic and specific model personas. Through benchmarking on four different
models -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers
significant persona biases in dialogue systems. Our findings also underscore
the pressing need to revisit the use of personas in dialogue agents to ensure
safe application.
","2023-10-24","2310.05280v1.pdf"
"2310.05286","Heinrich Peters","Heinrich Peters, Alireza Hashemi, James Rae","Generalizable Error Modeling for Search Relevance Data Annotation Tasks","","","","","cs.LG cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Human data annotation is critical in shaping the quality of machine learning
(ML) and artificial intelligence (AI) systems. One significant challenge in
this context is posed by annotation errors, as their effects can degrade the
performance of ML models. This paper presents a predictive error model trained
to detect potential errors in search relevance annotation tasks for three
industry-scale ML applications (music streaming, video streaming, and mobile
apps) and assesses its potential to enhance the quality and efficiency of the
data annotation process. Drawing on real-world data from an extensive search
relevance annotation program, we illustrate that errors can be predicted with
moderate model performance (AUC=0.65-0.75) and that model performance
generalizes well across applications (i.e., a global, task-agnostic model
performs on par with task-specific models). We present model explainability
analyses to identify which types of features are the main drivers of predictive
performance. Additionally, we demonstrate the usefulness of the model in the
context of auditing, where prioritizing tasks with high predicted error
probabilities considerably increases the amount of corrected annotation errors
(e.g., 40% efficiency gains for the music streaming application). These results
underscore that automated error detection models can yield considerable
improvements in the efficiency and quality of data annotation processes. Thus,
our findings reveal critical insights into effective error management in the
data annotation process, thereby contributing to the broader field of
human-in-the-loop ML.
","2023-10-10","2310.05286v1.pdf"
"2310.05295","Danyang Liu","Danyang Liu, Mirella Lapata, Frank Keller","Visual Storytelling with Question-Answer Plans","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Visual storytelling aims to generate compelling narratives from image
sequences. Existing models often focus on enhancing the representation of the
image sequence, e.g., with external knowledge sources or advanced graph
structures. Despite recent progress, the stories are often repetitive,
illogical, and lacking in detail. To mitigate these issues, we present a novel
framework which integrates visual representations with pretrained language
models and planning. Our model translates the image sequence into a visual
prefix, a sequence of continuous embeddings which language models can
interpret. It also leverages a sequence of question-answer pairs as a blueprint
plan for selecting salient visual concepts and determining how they should be
assembled into a narrative. Automatic and human evaluation on the VIST
benchmark (Huang et al., 2016) demonstrates that blueprint-based models
generate stories that are more coherent, interesting, and natural compared to
competitive baselines and state-of-the-art systems.
","2023-10-19","2310.05295v1.pdf"
"2310.05312","Tinghui Ouyang","Tinghui Ouyang, Hoang-Quoc Nguyen-Son, Huy H. Nguyen, Isao Echizen,
  Yoshiki Seo","Quality Assurance of A GPT-based Sentiment Analysis System: Adversarial
  Review Data Generation and Detection","","","","","cs.SE","http://creativecommons.org/publicdomain/zero/1.0/","  Large Language Models (LLMs) have been garnering significant attention of AI
researchers, especially following the widespread popularity of ChatGPT.
However, due to LLMs' intricate architecture and vast parameters, several
concerns and challenges regarding their quality assurance require to be
addressed. In this paper, a fine-tuned GPT-based sentiment analysis model is
first constructed and studied as the reference in AI quality analysis. Then,
the quality analysis related to data adequacy is implemented, including
employing the content-based approach to generate reasonable adversarial review
comments as the wrongly-annotated data, and developing surprise adequacy
(SA)-based techniques to detect these abnormal data. Experiments based on
Amazon.com review data and a fine-tuned GPT model were implemented. Results
were thoroughly discussed from the perspective of AI quality assurance to
present the quality analysis of an LLM model on generated adversarial textual
data and the effectiveness of using SA on anomaly detection in data quality
assurance.
","2023-10-10","2310.05312v1.pdf"
"2310.05317","Siyang Liu","Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, Rada
  Mihalcea","Enhancing Long-form Text Generation Efficacy with Task-adaptive
  Tokenization","Accepted at the main conference of The 2023 Conference on Empirical
  Methods in Natural Language Processing; 8 pages","The 2023 Conference on Empirical Methods in Natural Language
  Processing(EMNLP 2023)","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose task-adaptive tokenization as a way to adapt the generation
pipeline to the specifics of a downstream task and enhance long-form generation
in mental health. Inspired by insights from cognitive science, our
task-adaptive tokenizer samples variable segmentations from multiple outcomes,
with sampling probabilities optimized based on task-specific data. We introduce
a strategy for building a specialized vocabulary and introduce a vocabulary
merging protocol that allows for the integration of task-specific tokens into
the pre-trained model's tokenization step. Through extensive experiments on
psychological question-answering tasks in both Chinese and English, we find
that our task-adaptive tokenization approach brings a significant improvement
in generation performance while using up to 60% fewer tokens. Preliminary
experiments point to promising results when using our tokenization approach
with very large language models.
","2023-10-24","2310.05317v1.pdf"
"2310.05318","Meng Xiao","Xunxin Cai, Meng Xiao, Zhiyuan Ning, Yuanchun Zhou","Resolving the Imbalance Issue in Hierarchical Disciplinary Topic
  Inference via LLM-based Data Augmentation","6 pages, accepted by ICDM 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In addressing the imbalanced issue of data within the realm of Natural
Language Processing, text data augmentation methods have emerged as pivotal
solutions. This data imbalance is prevalent in the research proposals submitted
during the funding application process. Such imbalances, resulting from the
varying popularity of disciplines or the emergence of interdisciplinary
studies, significantly impede the precision of downstream topic models that
deduce the affiliated disciplines of these proposals. At the data level,
proposals penned by experts and scientists are inherently complex technological
texts, replete with intricate terminologies, which augmenting such specialized
text data poses unique challenges. At the system level, this, in turn,
compromises the fairness of AI-assisted reviewer assignment systems, which
raises a spotlight on solving this issue. This study leverages large language
models (Llama V1) as data generators to augment research proposals categorized
within intricate disciplinary hierarchies, aiming to rectify data imbalances
and enhance the equity of expert assignments. We first sample within the
hierarchical structure to find the under-represented class. Then we designed a
prompt for keyword-based research proposal generation. Our experiments attests
to the efficacy of the generated data, demonstrating that research proposals
produced using the prompts can effectively address the aforementioned issues
and generate high quality scientific text data, thus help the model overcome
the imbalanced issue.
","2023-10-17","2310.05318v1.pdf"
"2310.05337","Michal Lukasik","Michal Lukasik, Vaishnavh Nagarajan, Ankit Singh Rawat, Aditya Krishna
  Menon, Sanjiv Kumar","What do larger image classifiers memorise?","","","","","cs.LG cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The success of modern neural networks has prompted study of the connection
between memorisation and generalisation: overparameterised models generalise
well, despite being able to perfectly fit (memorise) completely random labels.
To carefully study this issue, Feldman proposed a metric to quantify the degree
of memorisation of individual training examples, and empirically computed the
corresponding memorisation profile of a ResNet on image classification
bench-marks. While an exciting first glimpse into what real-world models
memorise, this leaves open a fundamental question: do larger neural models
memorise more? We present a comprehensive empirical analysis of this question
on image classification benchmarks. We find that training examples exhibit an
unexpectedly diverse set of memorisation trajectories across model sizes: most
samples experience decreased memorisation under larger models, while the rest
exhibit cap-shaped or increasing memorisation. We show that various proxies for
the Feldman memorization score fail to capture these fundamental trends.
Lastly, we find that knowledge distillation, an effective and popular model
compression technique, tends to inhibit memorisation, while also improving
generalisation. Specifically, memorisation is mostly inhibited on examples with
increasing memorisation trajectories, thus pointing at how distillation
improves generalisation.
","2023-10-10","2310.05337v1.pdf"
"2310.05338","Holy Lovenia","Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung","Negative Object Presence Evaluation (NOPE) to Measure Object
  Hallucination in Vision-Language Models","","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Object hallucination poses a significant challenge in vision-language (VL)
models, often leading to the generation of nonsensical or unfaithful responses
with non-existent objects. However, the absence of a general measurement for
evaluating object hallucination in VL models has hindered our understanding and
ability to mitigate this issue. In this work, we present NOPE (Negative Object
Presence Evaluation), a novel benchmark designed to assess object hallucination
in VL models through visual question answering (VQA). We propose a
cost-effective and scalable approach utilizing large language models to
generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.
We extensively investigate the performance of 10 state-of-the-art VL models in
discerning the non-existence of objects in visual questions, where the ground
truth answers are denoted as NegP (e.g., ""none""). Additionally, we evaluate
their standard performance on visual questions on 9 other VQA datasets. Through
our experiments, we demonstrate that no VL model is immune to the vulnerability
of object hallucination, as all models achieve accuracy below 10\% on NegP.
Furthermore, we uncover that lexically diverse visual questions, question types
with large scopes, and scene-relevant objects capitalize the risk of object
hallucination in VL models.
","2023-10-10","2310.05338v1.pdf"
"2310.05344","Zhilin Wang","Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii
  Kuchaiev","SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to
  RLHF","Findings of EMNLP 2023","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Model alignment with human preferences is an essential step in making Large
Language Models (LLMs) helpful and consistent with human values. It typically
consists of supervised fine-tuning (SFT) and reinforcement learning from human
feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from
a complex training setup and its tendency to align the model with implicit
values that end users cannot control at run-time. Moreover, reward models in
RLHF stage commonly rely on single-dimensional feedback as opposed to explicit,
multifaceted signals that indicate attributes such as helpfulness, humor, and
toxicity. To address these limitations, we propose SteerLM, a supervised
fine-tuning method that empowers end-users to control responses during
inference. SteerLM conditions responses to conform to an explicitly defined
multi-dimensional set of attributes, thereby empowering a steerable AI capable
of generating helpful and high-quality responses while maintaining
customizability. Experiments show that SteerLM trained on open source datasets
generates responses that are preferred by human and automatic evaluators to
many state-of-the-art baselines trained with RLHF while being much easier to
train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B
","2023-10-10","2310.05344v1.pdf"
"2310.05350","Priyanka Ranade","Michael Benington, Leo Phan, Chris Pierre Paul, Evan Shoemaker,
  Priyanka Ranade, Torstein Collett, Grant Hodgson Perez, Christopher Krieger","Scaling Studies for Efficient Parameter Search and Parallelism for Large
  Language Model Pre-training","","Supercomputing 2023 (SC23) Student Research Poster Track","","","cs.DC cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  AI accelerator processing capabilities and memory constraints largely dictate
the scale in which machine learning workloads (e.g., training and inference)
can be executed within a desirable time frame. Training a state of the art,
transformer-based model today requires use of GPU-accelerated high performance
computers with high-speed interconnects. As datasets and models continue to
increase in size, computational requirements and memory demands for AI also
continue to grow. These challenges have inspired the development of distributed
algorithm and circuit-based optimization techniques that enable the ability to
progressively scale models in multi-node environments, efficiently minimize
neural network cost functions for faster convergence, and store more parameters
into a set number of available resources. In our research project, we focus on
parallel and distributed machine learning algorithm development, specifically
for optimizing the data processing and pre-training of a set of 5
encoder-decoder LLMs, ranging from 580 million parameters to 13 billion
parameters. We performed a fine-grained study to quantify the relationships
between three ML parallelism methods, specifically exploring Microsoft
DeepSpeed Zero Redundancy Optimizer (ZeRO) stages.
","2023-10-12","2310.05350v1.pdf"
"2310.05374","Jianqiao Lu","Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting
  Yeung, Xiao Chen","Improving End-to-End Speech Processing by Efficient Text Data
  Utilization with Latent Synthesis","15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings","","","","cs.CL cs.LG cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Training a high performance end-to-end speech (E2E) processing model requires
an enormous amount of labeled speech data, especially in the era of
data-centric artificial intelligence. However, labeled speech data are usually
scarcer and more expensive for collection, compared to textual data. We propose
Latent Synthesis (LaSyn), an efficient textual data utilization framework for
E2E speech processing models. We train a latent synthesizer to convert textual
data into an intermediate latent representation of a pre-trained speech model.
These pseudo acoustic representations of textual data augment acoustic data for
model training. We evaluate LaSyn on low-resource automatic speech recognition
(ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an
E2E baseline trained on LibriSpeech train-clean-100, with relative word error
rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our
E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for
slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM)
and EM-Tree accuracies on STOP respectively. With fewer parameters, the results
of LaSyn are competitive to published state-of-the-art works. The results
demonstrate the quality of the augmented training data.
","2023-10-25","2310.05374v3.pdf"
"2310.05375","Shanglin Li","Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming
  Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, Baochang Zhang","IPDreamer: Appearance-Controllable 3D Object Generation with Image
  Prompts","9 pages, 4 figures","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances in text-to-3D generation have been remarkable, with methods
such as DreamFusion leveraging large-scale text-to-image diffusion-based models
to supervise 3D generation. These methods, including the variational score
distillation proposed by ProlificDreamer, enable the synthesis of detailed and
photorealistic textured meshes. However, the appearance of 3D objects generated
by these methods is often random and uncontrollable, posing a challenge in
achieving appearance-controllable 3D objects. To address this challenge, we
introduce IPDreamer, a novel approach that incorporates image prompts to
provide specific and comprehensive appearance information for 3D object
generation. Our results demonstrate that IPDreamer effectively generates
high-quality 3D objects that are consistent with both the provided text and
image prompts, demonstrating its promising capability in
appearance-controllable 3D object generation.
","2023-10-10","2310.05375v1.pdf"
"2310.05380","Ashish Tiwari","Anirudh Khatry, Yasharth Bajpai, Priyanshu Gupta, Sumit Gulwani,
  Ashish Tiwari","Augmented Embeddings for Custom Retrievals","14 pages","","","","cs.IR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Information retrieval involves selecting artifacts from a corpus that are
most relevant to a given search query. The flavor of retrieval typically used
in classical applications can be termed as homogeneous and relaxed, where
queries and corpus elements are both natural language (NL) utterances
(homogeneous) and the goal is to pick most relevant elements from the corpus in
the Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed).
Recently, retrieval is being used extensively in preparing prompts for large
language models (LLMs) to enable LLMs to perform targeted tasks. These new
applications of retrieval are often heterogeneous and strict -- the queries and
the corpus contain different kinds of entities, such as NL and code, and there
is a need for improving retrieval at Top-K for small values of K, such as K=1
or 3 or 5. Current dense retrieval techniques based on pretrained embeddings
provide a general-purpose and powerful approach for retrieval, but they are
oblivious to task-specific notions of similarity of heterogeneous artifacts. We
introduce Adapted Dense Retrieval, a mechanism to transform embeddings to
enable improved task-specific, heterogeneous and strict retrieval. Adapted
Dense Retrieval works by learning a low-rank residual adaptation of the
pretrained black-box embedding. We empirically validate our approach by showing
improvements over the state-of-the-art general-purpose embeddings-based
baseline.
","2023-10-10","2310.05380v1.pdf"
"2310.05388","Zhihua Wen","Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang,
  Dongsheng Li","GROVE: A Retrieval-augmented Complex Story Generation Framework with A
  Forest of Evidence","Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Conditional story generation is significant in human-machine interaction,
particularly in producing stories with complex plots. While Large language
models (LLMs) perform well on multiple NLP tasks, including story generation,
it is challenging to generate stories with both complex and creative plots.
Existing methods often rely on detailed prompts to guide LLMs to meet target
conditions, which inadvertently restrict the creative potential of the
generated stories. We argue that leveraging information from exemplary
human-written stories facilitates generating more diverse plotlines. Delving
deeper into story details helps build complex and credible plots. In this
paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation
framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to
enhance stories' complexity. We build a retrieval repository for target
conditions to produce few-shot examples to prompt LLMs. Additionally, we design
an ``asking-why'' prompting scheme that extracts a forest of evidence,
providing compensation for the ambiguities that may occur in the generated
story. This iterative process uncovers underlying story backgrounds. Finally,
we select the most fitting chains of evidence from the evidence forest and
integrate them into the generated story, thereby enhancing the narrative's
complexity and credibility. Experimental results and numerous examples verify
the effectiveness of our method.
","2023-10-25","2310.05388v1.pdf"
"2310.05393","Weifeng Lin","Weifeng Lin, Ziheng Wu, Jiayu Chen, Wentao Yang, Mingxin Huang, Jun
  Huang, Lianwen Jin","Hierarchical Side-Tuning for Vision Transformers","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Fine-tuning pre-trained Vision Transformers (ViT) has consistently
demonstrated promising performance in the realm of visual recognition. However,
adapting large pre-trained models to various tasks poses a significant
challenge. This challenge arises from the need for each model to undergo an
independent and comprehensive fine-tuning process, leading to substantial
computational and memory demands. While recent advancements in
Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to
achieve superior performance compared to full fine-tuning with a smaller subset
of parameter updates, they tend to overlook dense prediction tasks such as
object detection and segmentation. In this paper, we introduce Hierarchical
Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various
downstream tasks effectively. Diverging from existing methods that exclusively
fine-tune parameters within input spaces or certain modules connected to the
backbone, we tune a lightweight and hierarchical side network (HSN) that
leverages intermediate activations extracted from the backbone and generates
multi-scale features to make predictions. To validate HST, we conducted
extensive experiments encompassing diverse visual tasks, including
classification, object detection, instance segmentation, and semantic
segmentation. Notably, our method achieves state-of-the-art average Top-1
accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.
When applied to object detection tasks on COCO testdev benchmark, HST even
surpasses full fine-tuning and obtains better performance with 49.7 box AP and
43.2 mask AP using Cascade Mask R-CNN.
","2023-10-11","2310.05393v1.pdf"
"2310.05414","Ci-Jyun Liang","Ci-Jyun Liang, Thai-Hoa Le, Youngjib Ham, Bharadwaj R. K. Mantha,
  Marvin H. Cheng, Jacob J. Lin","Ethics of Artificial Intelligence and Robotics in the Architecture,
  Engineering, and Construction Industry","109 pages, 5 figures, submitted to Automation in Construction","","","","cs.RO cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Artificial intelligence (AI) and robotics research and implementation emerged
in the architecture, engineering, and construction (AEC) industry to positively
impact project efficiency and effectiveness concerns such as safety,
productivity, and quality. This shift, however, warrants the need for ethical
considerations of AI and robotics adoption due to its potential negative
impacts on aspects such as job security, safety, and privacy. Nevertheless,
this did not receive sufficient attention, particularly within the academic
community. This research systematically reviews AI and robotics research
through the lens of ethics in the AEC community for the past five years. It
identifies nine key ethical issues namely job loss, data privacy, data
security, data transparency, decision-making conflict, acceptance and trust,
reliability and safety, fear of surveillance, and liability, by summarizing
existing literature and filtering it further based on its AEC relevance.
Furthermore, thirteen research topics along the process were identified based
on existing AEC studies that had direct relevance to the theme of ethics in
general and their parallels are further discussed. Finally, the current
challenges and knowledge gaps are discussed and seven specific future research
directions are recommended. This study not only signifies more stakeholder
awareness of this important topic but also provides imminent steps towards
safer and more efficient realization.
","2023-10-10","2310.05414v1.pdf"
"2310.05418","Zhilin Wang","Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu","Humanoid Agents: Platform for Simulating Human-like Generative Agents","Accepted at EMNLP System Demonstrations 2023","","","","cs.CL cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents
","2023-10-10","2310.05418v1.pdf"
"2310.05421","Keivalya Pandya","Keivalya Pandya and Mehfuza Holia","Automating Customer Service using LangChain: Building custom open-source
  GPT Chatbot for organizations","4 pages, 2 figures, Submitted to appear in the Proceedings of the 3rd
  International Conference on Women in Science & Technology Creating
  Sustainable Career (ICWSTCSC 2023)","","","","cs.CL cs.CY cs.LG","http://creativecommons.org/licenses/by/4.0/","  In the digital age, the dynamics of customer service are evolving, driven by
technological advancements and the integration of Large Language Models (LLMs).
This research paper introduces a groundbreaking approach to automating customer
service using LangChain, a custom LLM tailored for organizations. The paper
explores the obsolescence of traditional customer support techniques,
particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift
towards responsive, context-aware, and personalized customer interactions. The
heart of this innovation lies in the fusion of open-source methodologies, web
scraping, fine-tuning, and the seamless integration of LangChain into customer
service platforms. This open-source state-of-the-art framework, presented as
""Sahaay,"" demonstrates the ability to scale across industries and
organizations, offering real-time support and query resolution. Key elements of
this research encompass data collection via web scraping, the role of
embeddings, the utilization of Google's Flan T5 XXL, Base and Small language
models for knowledge retrieval, and the integration of the chatbot into
customer service platforms. The results section provides insights into their
performance and use cases, here particularly within an educational institution.
This research heralds a new era in customer service, where technology is
harnessed to create efficient, personalized, and responsive interactions.
Sahaay, powered by LangChain, redefines the customer-company relationship,
elevating customer retention, value extraction, and brand image. As
organizations embrace LLMs, customer service becomes a dynamic and
customer-centric ecosystem.
","2023-10-10","2310.05421v1.pdf"
"2310.05424","Sangmin Bae","Sangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young Yun","Fast and Robust Early-Exiting Framework for Autoregressive Language
  Models with Synchronized Parallel Decoding","EMNLP 2023 (Long)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  To tackle the high inference latency exhibited by autoregressive language
models, previous studies have proposed an early-exiting framework that
allocates adaptive computation paths for each token based on the complexity of
generating the subsequent token. However, we observed several shortcomings,
including performance degradation caused by a state copying mechanism or
numerous exit paths, and sensitivity to exit confidence thresholds.
Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework,
which incorporates a shallow-deep module and a synchronized parallel decoding.
Our framework enables faster inference by synchronizing the decoding process of
the current token with previously stacked early-exited tokens. Furthermore, as
parallel decoding allows us to observe predictions from both shallow and deep
models, we present a novel adaptive threshold estimator that exploits a Beta
mixture model to determine suitable confidence thresholds. We empirically
demonstrated the superiority of our proposed framework on extensive generation
tasks.
","2023-10-10","2310.05424v1.pdf"
"2310.05442","Robert Litschko","Robert Litschko, Max M\""uller-Eberstein, Rob van der Goot, Leon Weber,
  Barbara Plank","Establishing Trustworthiness: Rethinking Tasks and Model Evaluation","Accepted at EMNLP 2023 (Main Conference), camera-ready","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Language understanding is a multi-faceted cognitive capability, which the
Natural Language Processing (NLP) community has striven to model
computationally for decades. Traditionally, facets of linguistic intelligence
have been compartmentalized into tasks with specialized model architectures and
corresponding evaluation protocols. With the advent of large language models
(LLMs) the community has witnessed a dramatic shift towards general purpose,
task-agnostic approaches powered by generative models. As a consequence, the
traditional compartmentalized notion of language tasks is breaking down,
followed by an increasing challenge for evaluation and analysis. At the same
time, LLMs are being deployed in more real-world scenarios, including
previously unforeseen zero-shot setups, increasing the need for trustworthy and
reliable systems. Therefore, we argue that it is time to rethink what
constitutes tasks and model evaluation in NLP, and pursue a more holistic view
on language, placing trustworthiness at the center. Towards this goal, we
review existing compartmentalized approaches for understanding the origins of a
model's functional capacity, and provide recommendations for more multi-faceted
evaluation protocols.
","2023-10-24","2310.05442v1.pdf"
"2310.05446","Khaled ELKarazle","Khaled ELKarazle, Valliappan Raman, Caslon Chua and Patrick Then","RetSeg: Retention-based Colorectal Polyps Segmentation Network","Updated version with a PDF","","","","eess.IV cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Vision Transformers (ViTs) have revolutionized medical imaging analysis,
showcasing superior efficacy compared to conventional Convolutional Neural
Networks (CNNs) in vital tasks such as polyp classification, detection, and
segmentation. Leveraging attention mechanisms to focus on specific image
regions, ViTs exhibit contextual awareness in processing visual data,
culminating in robust and precise predictions, even for intricate medical
images. Moreover, the inherent self-attention mechanism in Transformers
accommodates varying input sizes and resolutions, granting an unprecedented
flexibility absent in traditional CNNs. However, Transformers grapple with
challenges like excessive memory usage and limited training parallelism due to
self-attention, rendering them impractical for real-time disease detection on
resource-constrained devices. In this study, we address these hurdles by
investigating the integration of the recently introduced retention mechanism
into polyp segmentation, introducing RetSeg, an encoder-decoder network
featuring multi-head retention blocks. Drawing inspiration from Retentive
Networks (RetNet), RetSeg is designed to bridge the gap between precise polyp
segmentation and resource utilization, particularly tailored for colonoscopy
images. We train and validate RetSeg for polyp segmentation employing two
publicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we
showcase RetSeg's promising performance across diverse public datasets,
including CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While
our work represents an early-stage exploration, further in-depth studies are
imperative to advance these promising findings.
","2023-10-17","2310.05446v1.pdf"
"2310.05450","Hongqiu Wu","Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang","Empower Nested Boolean Logic via Self-Supervised Curriculum Learning","Accepted by EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Beyond the great cognitive powers showcased by language models, it is crucial
to scrutinize whether their reasoning capabilities stem from strong
generalization or merely exposure to relevant data. As opposed to constructing
increasingly complex logic, this paper probes into the boolean logic, the root
capability of a logical reasoner. We find that any pre-trained language models
even including large language models only behave like a random selector in the
face of multi-nested boolean logic, a task that humans can handle with ease. To
empower language models with this fundamental capability, this paper proposes a
new self-supervised learning method \textit{Curriculum Logical Reasoning}
(\textsc{Clr}), where we augment the training data with nested boolean logic
chain step-by-step, and program the training from simpler logical patterns
gradually to harder ones. This new training paradigm allows language models to
effectively generalize to much harder and longer-hop logic, which can hardly be
learned through naive training. Furthermore, we show that boolean logic is a
great foundation for improving the subsequent general logical tasks.
","2023-10-10","2310.05450v1.pdf"
"2310.05452","Haotong Yang","Haotong Yang and Fanxu Meng and Zhouchen Lin and Muhan Zhang","Explaining the Complex Task Reasoning of Large Language Models with
  Template-Content Structure","","","","","cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The continuous evolution of pre-trained large language models with
ever-growing parameters and corpus sizes has augmented their capacity to solve
complex tasks. This ability, which obviates the necessity for task-specific
training or fine-tuning, relies on providing the model with a language
description or some task exemplars -- referred to the prompt -- that guide the
desired autoregressive generation. Despite the remarkable success, the
underlying mechanisms that facilitate such exceptional generalization abilities
remain an open question. In this paper, we present a novel framework that
formally conceptualizes answer generation for complex natural language tasks as
a hierarchical ``template-content'' structure. According to our modeling, there
exist pre-trained models that can automatically decompose tasks into
constituent steps during autoregressive generation, through language modeling
on a sufficiently large corpus, thereby solving them. Our framework offers an
explanatory tool for the complex reasoning abilities of large language models
from the perspective of modeling autoregressive generation tasks. Our
experiments show that practical models exhibit different behaviors for
``template'' and ``content'' providing support for our modeling.
","2023-10-10","2310.05452v1.pdf"
"2310.05464","Ricardo Knauer","Ricardo Knauer and Erik Rodner","Cost-Sensitive Best Subset Selection for Logistic Regression: A
  Mixed-Integer Conic Optimization Perspective","German Conference on Artificial Intelligence (K\""unstliche
  Intelligenz)","","10.1007/978-3-031-42608-7_10","","cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A key challenge in machine learning is to design interpretable models that
can reduce their inputs to the best subset for making transparent predictions,
especially in the clinical domain. In this work, we propose a certifiably
optimal feature selection procedure for logistic regression from a
mixed-integer conic optimization perspective that can take an auxiliary cost to
obtain features into account. Based on an extensive review of the literature,
we carefully create a synthetic dataset generator for clinical prognostic model
research. This allows us to systematically evaluate different heuristic and
optimal cardinality- and budget-constrained feature selection procedures. The
analysis shows key limitations of the methods for the low-data regime and when
confronted with label noise. Our paper not only provides empirical
recommendations for suitable methods and dataset designs, but also paves the
way for future research in the area of meta-learning.
","2023-10-10","2310.05464v1.pdf"
"2310.05470","Junlong Li","Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei
  Liu","Generative Judge for Evaluating Alignment","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The rapid development of Large Language Models (LLMs) has substantially
expanded the range of tasks they can address. In the field of Natural Language
Processing (NLP), researchers have shifted their focus from conventional NLP
tasks (e.g., sequence tagging and parsing) towards tasks that revolve around
aligning with human needs (e.g., brainstorming and email writing). This shift
in task distribution imposes new requirements on evaluating these aligned
models regarding generality (i.e., assessing performance across diverse
scenarios), flexibility (i.e., examining under different protocols), and
interpretability (i.e., scrutinizing models with explanations). In this paper,
we propose a generative judge with 13B parameters, Auto-J, designed to address
these challenges. Our model is trained on user queries and LLM-generated
responses under massive real-world scenarios and accommodates diverse
evaluation protocols (e.g., pairwise response comparison and single-response
evaluation) with well-structured natural language critiques. To demonstrate the
efficacy of our approach, we construct a new testbed covering 58 different
scenarios. Experimentally, Auto-J outperforms a series of strong competitors,
including both open-source and closed-source models, by a large margin. We also
provide detailed analysis and case studies to further reveal the potential of
our method and make a variety of resources public at
https://github.com/GAIR-NLP/auto-j.
","2023-10-10","2310.05470v1.pdf"
"2310.05473","Chun-Mei Feng","Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fahad Khan, Wangmeng Zuo,
  Rick Siow Mong Goh, Chun-Mei Feng","Sentence-level Prompts Benefit Composed Image Retrieval","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Composed image retrieval (CIR) is the task of retrieving specific images by
using a query that involves both a reference image and a relative caption. Most
existing CIR models adopt the late-fusion strategy to combine visual and
language features. Besides, several approaches have also been suggested to
generate a pseudo-word token from the reference image, which is further
integrated into the relative caption for CIR. However, these pseudo-word-based
prompting methods have limitations when target image encompasses complex
changes on reference image, e.g., object removal and attribute modification. In
this work, we demonstrate that learning an appropriate sentence-level prompt
for the relative caption (SPRC) is sufficient for achieving effective composed
image retrieval. Instead of relying on pseudo-word-based prompts, we propose to
leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level
prompts. By concatenating the learned sentence-level prompt with the relative
caption, one can readily use existing text-based image retrieval models to
enhance CIR performance. Furthermore, we introduce both image-text contrastive
loss and text prompt alignment loss to enforce the learning of suitable
sentence-level prompts. Experiments show that our proposed method performs
favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR
datasets. The source code and pretrained model are publicly available at
https://github.com/chunmeifeng/SPRC
","2023-10-10","2310.05473v1.pdf"
"2310.05481","Steven Schockaert","Usashi Chatterjee, Amit Gajbhiye, Steven Schockaert","Cabbage Sweeter than Cake? Analysing the Potential of Large Language
  Models for Learning Conceptual Spaces","Accepted for EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The theory of Conceptual Spaces is an influential cognitive-linguistic
framework for representing the meaning of concepts. Conceptual spaces are
constructed from a set of quality dimensions, which essentially correspond to
primitive perceptual features (e.g. hue or size). These quality dimensions are
usually learned from human judgements, which means that applications of
conceptual spaces tend to be limited to narrow domains (e.g. modelling colour
or taste). Encouraged by recent findings about the ability of Large Language
Models (LLMs) to learn perceptually grounded representations, we explore the
potential of such models for learning conceptual spaces. Our experiments show
that LLMs can indeed be used for learning meaningful representations to some
extent. However, we also find that fine-tuned models of the BERT family are
able to match or even outperform the largest GPT-3 model, despite being 2 to 3
orders of magnitude smaller.
","2023-10-10","2310.05481v1.pdf"
"2310.05492","Guanting Dong","Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue,
  Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou","How Abilities in Large Language Models are Affected by Supervised
  Fine-tuning Data Composition","16 pages, 8 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) with enormous pre-training tokens and parameter
amounts emerge abilities, including math reasoning, code generation, and
instruction following. These abilities are further enhanced by supervised
fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each
ability, while proprietary LLMs are versatile for all abilities. It is
important to investigate how to unlock them with multiple abilities via SFT. In
this study, we specifically focus on the data composition between mathematical
reasoning, code generation, and general human-aligning abilities during SFT.
From a scaling perspective, we investigate the relationship between model
abilities and various factors including data amounts, data composition ratio,
model parameters, and SFT strategies. Our experiments reveal that different
abilities exhibit different scaling patterns, and larger models generally show
superior performance with the same amount of data. Mathematical reasoning and
code generation improve as data amounts increase consistently, while the
general ability is enhanced with about a thousand samples and improves slowly.
We find data composition results in various abilities improvements with low
data amounts, while conflicts of abilities with high data amounts. Our
experiments further show that composition data amount impacts performance,
while the influence of composition ratio is insignificant. Regarding the SFT
strategies, we evaluate sequential learning multiple abilities are prone to
catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)
strategy learns specialized abilities first and then learns general abilities
with a small amount of specialized data to prevent forgetting, offering a
promising solution to learn multiple abilities with different scaling patterns.
","2023-10-10","2310.05492v1.pdf"
"2310.05499","Yizhen Zheng","Shirui Pan, Yizhen Zheng, Yixin Liu","Integrating Graphs with Large Language Models: Methods and Prospects","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) such as GPT-4 have emerged as frontrunners,
showcasing unparalleled prowess in diverse applications, including answering
queries, code generation, and more. Parallelly, graph-structured data, an
intrinsic data type, is pervasive in real-world scenarios. Merging the
capabilities of LLMs with graph-structured data has been a topic of keen
interest. This paper bifurcates such integrations into two predominant
categories. The first leverages LLMs for graph learning, where LLMs can not
only augment existing graph algorithms but also stand as prediction models for
various graph tasks. Conversely, the second category underscores the pivotal
role of graphs in advancing LLMs. Mirroring human cognition, we solve complex
tasks by adopting graphs in either reasoning or collaboration. Integrating with
such structures can significantly boost the performance of LLMs in various
complicated tasks. We also discuss and propose open questions for integrating
LLMs with graph-structured data for the future direction of the field.
","2023-10-10","2310.05499v1.pdf"
"2310.05502","Yun Luo","Yun Luo and Zhen Yang and Fandong Meng and Yingjie Li and Fang Guo and
  Qinglin Qi and Jie Zhou and Yue Zhang","XAL: EXplainable Active Learning Makes Classifiers Better Low-resource
  Learners","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Active learning aims to construct an effective training set by iteratively
curating the most informative unlabeled data for annotation, which is practical
in low-resource tasks. Most active learning techniques in classification rely
on the model's uncertainty or disagreement to choose unlabeled data. However,
previous work indicates that existing models are poor at quantifying predictive
uncertainty, which can lead to over-confidence in superficial patterns and a
lack of exploration. Inspired by the cognitive processes in which humans deduce
and predict through causal information, we propose a novel Explainable Active
Learning framework (XAL) for low-resource text classification, which aims to
encourage classifiers to justify their inferences and delve into unlabeled data
for which they cannot provide reasonable explanations. Specifically, besides
using a pre-trained bi-directional encoder for classification, we employ a
pre-trained uni-directional decoder to generate and score the explanation. A
ranking loss is proposed to enhance the decoder's capability in scoring
explanations. During the selection of unlabeled data, we combine the predictive
uncertainty of the encoder and the explanation score of the decoder to acquire
informative data for annotation.
  As XAL is a general framework for text classification, we test our methods on
six different classification tasks. Extensive experiments show that XAL
achieves substantial improvement on all six tasks over previous AL methods.
Ablation studies demonstrate the effectiveness of each component, and human
evaluation shows that the model trained in XAL performs surprisingly well in
explaining its prediction.
","2023-10-10","2310.05502v1.pdf"
"2310.05506","Chengpeng Li","Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu,
  Chuanqi Tan, Xiang Wang, Chang Zhou","Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning
  Generalization","19 pages, 9 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In math reasoning with large language models (LLMs), fine-tuning data
augmentation by query evolution and diverse reasoning paths is empirically
verified effective, profoundly narrowing the gap between open-sourced LLMs and
cutting-edge proprietary LLMs. In this paper, we conduct an investigation for
such data augmentation in math reasoning and are intended to answer: (1) What
strategies of data augmentation are more effective; (2) What is the scaling
relationship between the amount of augmented data and model performance; and
(3) Can data augmentation incentivize generalization to out-of-domain
mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,
by complicating and diversifying the queries from GSM8K and sampling multiple
reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning
on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art
on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the
scale of 13B). A log-linear relationship is presented between MuggleMath's
performance and the amount of augmented data. We also find that MuggleMath is
weak in out-of-domain math reasoning generalization to MATH. This is attributed
to the differences in query distribution between AugGSM8K and MATH which
suggest that augmentation on a single benchmark could not help with overall
math reasoning performance. Codes and AugGSM8K will be uploaded to
https://github.com/OFA-Sys/gsm8k-ScRel.
","2023-10-10","2310.05506v1.pdf"
"2310.05553","Ilias Chalkidis","Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchordas,
  Gerasimos Spanakis","Regulation and NLP (RegNLP): Taming Large Language Models","9 pages, long paper at EMNLP 2023 proceedings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The scientific innovation in Natural Language Processing (NLP) and more
broadly in artificial intelligence (AI) is at its fastest pace to date. As
large language models (LLMs) unleash a new era of automation, important debates
emerge regarding the benefits and risks of their development, deployment and
use. Currently, these debates have been dominated by often polarized narratives
mainly led by the AI Safety and AI Ethics movements. This polarization, often
amplified by social media, is swaying political agendas on AI regulation and
governance and posing issues of regulatory capture. Capture occurs when the
regulator advances the interests of the industry it is supposed to regulate, or
of special interest groups rather than pursuing the general public interest.
Meanwhile in NLP research, attention has been increasingly paid to the
discussion of regulating risks and harms. This often happens without systematic
methodologies or sufficient rooting in the disciplines that inspire an extended
scope of NLP research, jeopardizing the scientific integrity of these
endeavors. Regulation studies are a rich source of knowledge on how to
systematically deal with risk and uncertainty, as well as with scientific
evidence, to evaluate and compare regulatory options. This resource has largely
remained untapped so far. In this paper, we argue how NLP research on these
topics can benefit from proximity to regulatory studies and adjacent fields. We
do so by discussing basic tenets of regulation, and risk and uncertainty, and
by highlighting the shortcomings of current NLP discussions dealing with risk
assessment. Finally, we advocate for the development of a new multidisciplinary
research space on regulation and NLP (RegNLP), focused on connecting scientific
knowledge to regulatory processes based on systematic methodologies.
","2023-10-10","2310.05553v1.pdf"
"2310.05563","Yuwei Wang","Yuwei Wang, Enmeng Lu, Zizhe Ruan, Yao Liang, Yi Zeng","STREAM: Social data and knowledge collective intelligence platform for
  TRaining Ethical AI Models","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents Social data and knowledge collective intelligence
platform for TRaining Ethical AI Models (STREAM) to address the challenge of
aligning AI models with human moral values, and to provide ethics datasets and
knowledge bases to help promote AI models ""follow good advice as naturally as a
stream follows its course"". By creating a comprehensive and representative
platform that accurately mirrors the moral judgments of diverse groups
including humans and AIs, we hope to effectively portray cultural and group
variations, and capture the dynamic evolution of moral judgments over time,
which in turn will facilitate the Establishment, Evaluation, Embedding,
Embodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI
models. Currently, STREAM has already furnished a comprehensive collection of
ethical scenarios, and amassed substantial moral judgment data annotated by
volunteers and various popular Large Language Models (LLMs), collectively
portraying the moral preferences and performances of both humans and AIs across
a range of moral contexts. This paper will outline the current structure and
construction of STREAM, explore its potential applications, and discuss its
future prospects.
","2023-10-10","2310.05563v1.pdf"
"2310.05592","Nils Feldhus","Nils Feldhus, Qianli Wang, Tatiana Anikina, Sahil Chopra, Cennet Oguz,
  Sebastian M\""oller","InterroLang: Exploring NLP Models and Datasets through Dialogue-based
  Explanations","EMNLP 2023 Findings. Camera-ready version","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  While recently developed NLP explainability methods let us open the black box
in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is
an interactive tool offering a conversational interface. Such a dialogue system
can help users explore datasets and models with explanations in a
contextualized manner, e.g. via clarification or follow-up questions, and
through a natural language interface. We adapt the conversational explanation
framework TalkToModel (Slack et al., 2022) to the NLP domain, add new
NLP-specific operations such as free-text rationalization, and illustrate its
generalizability on three NLP tasks (dialogue act classification, question
answering, hate speech detection). To recognize user queries for explanations,
we evaluate fine-tuned and few-shot prompting models and implement a novel
Adapter-based approach. We then conduct two user studies on (1) the perceived
correctness and helpfulness of the dialogues, and (2) the simulatability, i.e.
how objectively helpful dialogical explanations are for humans in figuring out
the model's predicted label when it's not shown. We found rationalization and
feature attribution were helpful in explaining the model behavior. Moreover,
users could more reliably predict the model outcome based on an explanation
dialogue rather than one-off explanations.
","2023-10-24","2310.05592v1.pdf"
"2310.05597","Molly Petersen","Molly R. Petersen, Lonneke van der Plas","Can language models learn analogical reasoning? Investigating training
  objectives and comparisons to human performance","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  While analogies are a common way to evaluate word embeddings in NLP, it is
also of interest to investigate whether or not analogical reasoning is a task
in itself that can be learned. In this paper, we test several ways to learn
basic analogical reasoning, specifically focusing on analogies that are more
typical of what is used to evaluate analogical reasoning in humans than those
in commonly used NLP benchmarks. Our experiments find that models are able to
learn analogical reasoning, even with a small amount of data. We additionally
compare our models to a dataset with a human baseline, and find that after
training, models approach human performance.
","2023-10-24","2310.05597v1.pdf"
"2310.05620","Duanyu Feng","Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie,
  Yifang Zhang, Weiguang Han, Wei Tian, Hao Wang","LAiW: A Chinese Legal Large Language Models Benchmark (A Technical
  Report)","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the emergence of numerous legal LLMs, there is currently a lack of a
comprehensive benchmark for evaluating their legal abilities. In this paper, we
propose the first Chinese Legal LLMs benchmark based on legal capabilities.
Through the collaborative efforts of legal and artificial intelligence experts,
we divide the legal capabilities of LLMs into three levels: basic legal NLP
capability, basic legal application capability, and complex legal application
capability. We have completed the first phase of evaluation, which mainly
focuses on the capability of basic legal NLP. The evaluation results show that
although some legal LLMs have better performance than their backbones, there is
still a gap compared to ChatGPT. Our benchmark can be found at URL.
","2023-10-10","2310.05620v1.pdf"
"2310.05627","Shuai Jia","Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, Xiuze Zhou, Liuliu Li
  and Dongming Han","Integrating Stock Features and Global Information via Large Language
  Models for Enhanced Stock Return Prediction","8 pages, International Joint Conferences on Artificial Intelligence","International Joint Conferences on Artificial Intelligence,2023","","","cs.CL cs.LG q-fin.ST","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The remarkable achievements and rapid advancements of Large Language Models
(LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in
quantitative investment. Traders can effectively leverage these LLMs to analyze
financial news and predict stock returns accurately. However, integrating LLMs
into existing quantitative models presents two primary challenges: the
insufficient utilization of semantic information embedded within LLMs and the
difficulties in aligning the latent information within LLMs with pre-existing
quantitative stock features. We propose a novel framework consisting of two
components to surmount these challenges. The first component, the Local-Global
(LG) model, introduces three distinct strategies for modeling global
information. These approaches are grounded respectively on stock features, the
capabilities of LLMs, and a hybrid method combining the two paradigms. The
second component, Self-Correlated Reinforcement Learning (SCRL), focuses on
aligning the embeddings of financial news generated by LLMs with stock features
within the same semantic space. By implementing our framework, we have
demonstrated superior performance in Rank Information Coefficient and returns,
particularly compared to models relying only on stock features in the China
A-share market.
","2023-10-11","2310.05627v1.pdf"
"2310.05628","Marco Bronzini","Marco Bronzini, Carlo Nicolini, Bruno Lepri, Andrea Passerini, Jacopo
  Staiano","Glitter or Gold? Deriving Structured Insights from Sustainability
  Reports via Large Language Models","","","","","cs.CL cs.CE cs.CY","http://creativecommons.org/licenses/by/4.0/","  Over the last decade, several regulatory bodies have started requiring the
disclosure of non-financial information from publicly listed companies, in
light of the investors' increasing attention to Environmental, Social, and
Governance (ESG) issues. Such information is publicly released in a variety of
non-structured and multi-modal documentation. Hence, it is not straightforward
to aggregate and consolidate such data in a cohesive framework to further
derive insights about sustainability practices across companies and markets.
Thus, it is natural to resort to Information Extraction (IE) techniques to
provide concise, informative and actionable data to the stakeholders. Moving
beyond traditional text processing techniques, in this work we leverage Large
Language Models (LLMs), along with prominent approaches such as Retrieved
Augmented Generation and in-context learning, to extract semantically
structured information from sustainability reports. We then adopt graph-based
representations to generate meaningful statistical, similarity and correlation
analyses concerning the obtained findings, highlighting the prominent
sustainability actions undertaken across industries and discussing emerging
similarity and disclosing patterns at company, sector and region levels.
Lastly, we investigate which factual aspects impact the most on companies' ESG
scores using our findings and other company information.
","2023-10-10","2310.05628v1.pdf"
"2310.05634","Xinze Li","Xinze Li, Yixin Cao2, Liangming Pan, Yubo Ma, Aixin Sun","Towards Verifiable Generation: A Benchmark for Knowledge-aware Language
  Model Attribution","8 pages","","","","cs.CL","http://creativecommons.org/publicdomain/zero/1.0/","  Although achieving great success, Large Language Models (LLMs) usually suffer
from unreliable hallucinations. In this paper, we define a new task of
Knowledge-aware Language Model Attribution (KaLMA) that improves upon three
core concerns on conventional attributed LMs. First, we extend attribution
source from unstructured texts to Knowledge Graph (KG), whose rich structures
benefit both the attribution performance and working scenarios. Second, we
propose a new ``Conscious Incompetence"" setting considering the incomplete
knowledge repository, where the model identifies the need for supporting
knowledge beyond the provided KG. Third, we propose a comprehensive automatic
evaluation metric encompassing text quality, citation quality, and text
citation alignment. To implement the above innovations, we build a dataset in
biography domain BioKaLMA via a well-designed evolutionary question generation
strategy, to control the question complexity and necessary knowledge to the
answer. For evaluation, we develop a baseline solution and demonstrate the room
for improvement in LLMs' citation generation, emphasizing the importance of
incorporating the ""Conscious Incompetence"" setting, and the critical role of
retrieval accuracy.
","2023-10-10","2310.05634v1.pdf"
"2310.05657","Cheng-Han Chiang","Cheng-Han Chiang and Hung-yi Lee","A Closer Look into Automatic Evaluation Using Large Language Models","EMNLP 2023 findings (short paper). Code:
  https://github.com/d223302/A-Closer-Look-To-LLM-Evaluation/","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Using large language models (LLMs) to evaluate text quality has recently
gained popularity. Some prior works explore the idea of using LLMs for
evaluation, while they differ in some details of the evaluation process. In
this paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et
al., 2023), and we discuss how those details in the evaluation process change
how well the ratings given by LLMs correlate with human ratings. We find that
the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more
aligned with human ratings. We also show that forcing the LLM to output only a
numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the
LLM to explain its own ratings consistently improves the correlation between
the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations
on two meta-evaluation datasets.
","2023-10-10","2310.05657v1.pdf"
"2310.05674","Sang Keun Choe","Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, Willie Neiswanger,
  Pengtao Xie, Emma Strubell, Eric Xing","Making Scalable Meta Learning Practical","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite its flexibility to learn diverse inductive biases in machine learning
programs, meta learning (i.e., learning to learn) has long been recognized to
suffer from poor scalability due to its tremendous compute/memory costs,
training instability, and a lack of efficient distributed training support. In
this work, we focus on making scalable meta learning practical by introducing
SAMA, which combines advances in both implicit differentiation algorithms and
systems. Specifically, SAMA is designed to flexibly support a broad range of
adaptive optimizers in the base level of meta learning programs, while reducing
computational burden by avoiding explicit computation of second-order gradient
information, and exploiting efficient distributed training techniques
implemented for first-order gradients. Evaluated on multiple large-scale meta
learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and
2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU
setups compared to other baseline meta learning algorithms. Furthermore, we
show that SAMA-based data optimization leads to consistent improvements in text
classification accuracy with BERT and RoBERTa large language models, and
achieves state-of-the-art results in both small- and large-scale data pruning
on image classification tasks, demonstrating the practical applicability of
scalable meta learning across language and vision domains.
","2023-10-24","2310.05674v1.pdf"
"2310.05680","Procheta Sen","Oscar Tuvey, Procheta Sen","Automated Argument Generation from Legal Facts","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The count of pending cases has shown an exponential rise across nations
(e.g., with more than 10 million pending cases in India alone). The main issue
lies in the fact that the number of cases submitted to the law system is far
greater than the available number of legal professionals present in a country.
Given this worldwide context, the utilization of AI technology has gained
paramount importance to enhance the efficiency and speed of legal procedures.
In this study we partcularly focus on helping legal professionals in the
process of analyzing a legal case. Our specific investigation delves into
harnessing the generative capabilities of open-sourced large language models to
create arguments derived from the facts present in legal cases. Experimental
results show that the generated arguments from the best performing method have
on average 63% overlap with the benchmark set gold standard annotations.
","2023-10-13","2310.05680v1.pdf"
"2310.05686","Angel Udias","Angel Udias, Antonio Alonso-Ayuso, Ignacio Sanchez, Sonia Hernandez,
  Maria Eugenia Castellanos, Raquel Montes Diez, Emilio Lopez Cano","The potential of large language models for improving probability
  learning: A study on ChatGPT3.5 and first-year computer engineering students","10 pages, 6 figures, 4 tables","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a
large-scale language model, in solving probability problems typically presented
in introductory computer engineering exams. Our study comprised a set of 23
probability exercises administered to students at Rey Juan Carlos University
(URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group
of five statistics professors, who assessed them qualitatively and assigned
grades based on the same criteria used for students. Our results indicate that
ChatGPT surpasses the average student in terms of phrasing, organization, and
logical reasoning. The model's performance remained consistent for both the
Spanish and English versions of the exercises. However, ChatGPT encountered
difficulties in executing basic numerical operations. Our experiments
demonstrate that requesting ChatGPT to provide the solution in the form of an R
script proved to be an effective approach for overcoming these limitations. In
summary, our results indicate that ChatGPT surpasses the average student in
solving probability problems commonly presented in introductory computer
engineering exams. Nonetheless, the model exhibits limitations in reasoning
around certain probability concepts. The model's ability to deliver
high-quality explanations and illustrate solutions in any programming language,
coupled with its performance in solving probability exercises, suggests that
large language models have the potential to serve as learning assistants.
","2023-10-10","2310.05686v1.pdf"
"2310.05690","Christopher Healey","Sengjie Liu, Christopher G. Healey","Abstractive Summarization of Large Document Collections Using GPT","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  This paper proposes a method of abstractive summarization designed to scale
to document collections instead of individual documents. Our approach applies a
combination of semantic clustering, document size reduction within topic
clusters, semantic chunking of a cluster's documents, GPT-based summarization
and concatenation, and a combined sentiment and text visualization of each
topic to support exploratory data analysis. Statistical comparison of our
results to existing state-of-the-art systems BART, BRIO, PEGASUS, and MoCa
using ROGUE summary scores showed statistically equivalent performance with
BART and PEGASUS on the CNN/Daily Mail test dataset, and with BART on the
Gigaword test dataset. This finding is promising since we view document
collection summarization as more challenging than individual document
summarization. We conclude with a discussion of how issues of scale are
","2023-10-10","2310.05690v1.pdf"
"2310.05694","Kai He","Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng,
  Erik Cambria","A Survey of Large Language Models for Healthcare: from Data, Technology,
  and Applications to Accountability and Ethics","","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  The utilization of large language models (LLMs) in the Healthcare domain has
generated both excitement and concern due to their ability to effectively
respond to freetext queries with certain professional knowledge. This survey
outlines the capabilities of the currently developed LLMs for Healthcare and
explicates their development process, with the aim of providing an overview of
the development roadmap from traditional Pretrained Language Models (PLMs) to
LLMs. Specifically, we first explore the potential of LLMs to enhance the
efficiency and effectiveness of various Healthcare applications highlighting
both the strengths and limitations. Secondly, we conduct a comparison between
the previous PLMs and the latest LLMs, as well as comparing various LLMs with
each other. Then we summarize related Healthcare training data, training
methods, optimization strategies, and usage. Finally, the unique concerns
associated with deploying LLMs in Healthcare settings are investigated,
particularly regarding fairness, accountability, transparency and ethics. Our
survey provide a comprehensive investigation from perspectives of both computer
science and Healthcare specialty. Besides the discussion about Healthcare
concerns, we supports the computer science community by compiling a collection
of open source resources, such as accessible datasets, the latest
methodologies, code implementations, and evaluation benchmarks in the Github.
Summarily, we contend that a significant paradigm shift is underway,
transitioning from PLMs to LLMs. This shift encompasses a move from
discriminative AI approaches to generative AI approaches, as well as a shift
from model-centered methodologies to datacentered methodologies.
","2023-10-10","2310.05694v1.pdf"
"2310.05707","Xinyi Wang","Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, Alessandro
  Sordoni","Guiding Language Model Reasoning with Planning Tokens","10 pages, 4 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
reasoning. However, most of the existing approaches to enhance this ability
rely heavily on data-driven methods, while neglecting the structural aspects of
the model's reasoning capacity. We find that while LLMs can manage individual
reasoning steps well, they struggle with maintaining consistency across an
entire reasoning chain. To solve this, we introduce 'planning tokens' at the
start of each reasoning step, serving as a guide for the model. These token
embeddings are then fine-tuned along with the rest of the model parameters. Our
approach requires a negligible increase in trainable parameters (just 0.001%)
and can be applied through either full fine-tuning or a more
parameter-efficient scheme. We demonstrate our method's effectiveness by
applying it to three different LLMs, showing notable accuracy improvements
across three math word problem datasets w.r.t. plain chain-of-thought
fine-tuning baselines.
","2023-10-10","2310.05707v1.pdf"
"2310.05719","Marco Giordano","Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris
  Anagnostidis, Sidak Pal Singh (ETH Zurich)","Transformer Fusion with Optimal Transport","M. Imfeld, J. Graldi, and M. Giordano are the first authors and
  contributed equally to this work","","","","cs.LG stat.ML","http://creativecommons.org/licenses/by-sa/4.0/","  Fusion is a technique for merging multiple independently-trained neural
networks in order to combine their capabilities. Past attempts have been
restricted to the case of fully-connected, convolutional, and residual
networks. In this paper, we present a systematic approach for fusing two or
more transformer-based networks exploiting Optimal Transport to (soft-)align
the various architectural components. We flesh out an abstraction for layer
alignment, that can generalize to arbitrary architectures -- in principle --
and we apply this to the key ingredients of Transformers such as multi-head
self-attention, layer-normalization, and residual connections, and we discuss
how to handle them via various ablation studies. Furthermore, our method allows
the fusion of models of different sizes (heterogeneous fusion), providing a new
and efficient way for compression of Transformers. The proposed approach is
evaluated on both image classification tasks via Vision Transformer and natural
language modeling tasks using BERT. Our approach consistently outperforms
vanilla fusion, and, after a surprisingly short finetuning, also outperforms
the individual converged parent models. In our analysis, we uncover intriguing
insights about the significant role of soft alignment in the case of
Transformers. Our results showcase the potential of fusing multiple
Transformers, thus compounding their expertise, in the budding paradigm of
model fusion and recombination.
","2023-10-17","2310.05719v1.pdf"
"2310.05720","Chen Yaosen","Yaosen Chen, Yu Yao, Zhiqiang Li, Wei Wang, Yanru Zhang, Han Yang,
  Xuming Wen","HyperLips: Hyper Control Lips with High Resolution Decoder for Talking
  Face Generation","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Talking face generation has a wide range of potential applications in the
field of virtual digital humans. However, rendering high-fidelity facial video
while ensuring lip synchronization is still a challenge for existing
audio-driven talking face generation approaches. To address this issue, we
propose HyperLips, a two-stage framework consisting of a hypernetwork for
controlling lips and a high-resolution decoder for rendering high-fidelity
faces. In the first stage, we construct a base face generation network that
uses the hypernetwork to control the encoding latent code of the visual face
information over audio. First, FaceEncoder is used to obtain latent code by
extracting features from the visual face information taken from the video
source containing the face frame.Then, HyperConv, which weighting parameters
are updated by HyperNet with the audio features as input, will modify the
latent code to synchronize the lip movement with the audio. Finally,
FaceDecoder will decode the modified and synchronized latent code into visual
face content. In the second stage, we obtain higher quality face videos through
a high-resolution decoder. To further improve the quality of face generation,
we trained a high-resolution decoder, HRDecoder, using face images and detected
sketches generated from the first stage as input.Extensive quantitative and
qualitative experiments show that our method outperforms state-of-the-art work
with more realistic, high-fidelity, and lip synchronization. Project page:
https://semchan.github.io/HyperLips Project/
","2023-10-17","2310.05720v1.pdf"
"2310.05727","Weimin Xiong","Weimin Xiong, Yiwen Guo, Hao Chen","The Program Testing Ability of Large Language Models for Code","","","","","cs.CL cs.AI cs.LG cs.SE","http://creativecommons.org/licenses/by/4.0/","  Recent development of large language models (LLMs) for code like CodeX and
CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their
ability of synthesizing code that completes a program for performing a
pre-defined task has been intensively tested and verified on benchmark datasets
including HumanEval and MBPP. Yet, evaluation of these LLMs from more
perspectives (than just program synthesis) is also anticipated, considering
their broad scope of applications in software engineering. In this paper, we
explore the ability of LLMs for testing programs/code. By performing thorough
analyses of recent LLMs for code in program testing, we show a series of
intriguing properties of these models and demonstrate how program testing
ability of LLMs can be improved. Following recent work which utilizes generated
test cases to enhance program synthesis, we further leverage our findings in
improving the quality of the synthesized programs and show +11.77% and +4.22%
higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline
and the recent state-of-the-art, respectively.
","2023-10-10","2310.05727v1.pdf"
"2310.05737","Lijun Yu","Lijun Yu, Jos\'e Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk
  Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G.
  Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang","Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","","","","","cs.CV cs.AI cs.MM","http://creativecommons.org/licenses/by/4.0/","  While Large Language Models (LLMs) are the dominant models for generative
tasks in language, they do not perform as well as diffusion models on image and
video generation. To effectively use LLMs for visual generation, one crucial
component is the visual tokenizer that maps pixel-space inputs to discrete
tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a
video tokenizer designed to generate concise and expressive tokens for both
videos and images using a common token vocabulary. Equipped with this new
tokenizer, we show that LLMs outperform diffusion models on standard image and
video generation benchmarks including ImageNet and Kinetics. In addition, we
demonstrate that our tokenizer surpasses the previously top-performing video
tokenizer on two more tasks: (1) video compression comparable to the
next-generation video codec (VCC) according to human evaluations, and (2)
learning effective representations for action recognition tasks.
","2023-10-10","2310.05737v1.pdf"
"2310.05746","Jiangjie Chen","Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, Kyle
  Richardson","Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and
  Execution of LLM Agents in an Auction Arena","Preprint","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Can Large Language Models (LLMs) simulate human behavior in complex
environments? LLMs have recently been shown to exhibit advanced reasoning
skills but much of NLP evaluation still relies on static benchmarks. Answering
this requires evaluation environments that probe strategic reasoning in
competitive, dynamic scenarios that involve long-term planning. We introduce
AucArena, a novel simulation environment for evaluating LLMs within auctions, a
setting chosen for being highly unpredictable and involving many skills related
to resource and risk management, while also being easy to evaluate. We conduct
several controlled simulations using state-of-the-art LLMs as bidding agents.
We find that through simple prompting, LLMs do indeed demonstrate many of the
skills needed for effectively engaging in auctions (e.g., managing budget,
adhering to long-term goals and priorities), skills that we find can be
sharpened by explicitly encouraging models to be adaptive and observe
strategies in past auctions. These results are significant as they show the
potential of using LLM agents to model intricate social dynamics, especially in
competitive settings. However, we also observe considerable variability in the
capabilities of individual LLMs. Notably, even our most advanced models (GPT-4)
are occasionally surpassed by heuristic baselines and human agents,
highlighting the potential for further improvements in the design of LLM agents
and the important role that our simulation environment can play in further
testing and refining agent architectures.
","2023-10-10","2310.05746v1.pdf"
"2310.05771","Weikai Yang","Weikai Yang, Mengchen Liu, Zheng Wang, and Shixia Liu","Foundation Models Meet Visualizations: Challenges and Opportunities","Submitted to Computational Visual Media","","","","cs.LG cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent studies have indicated that foundation models, such as BERT and GPT,
excel in adapting to a variety of downstream tasks. This adaptability has
established them as the dominant force in building artificial intelligence (AI)
systems. As visualization techniques intersect with these models, a new
research paradigm emerges. This paper divides these intersections into two main
areas: visualizations for foundation models (VIS4FM) and foundation models for
visualizations (FM4VIS). In VIS4FM, we explore the primary role of
visualizations in understanding, refining, and evaluating these intricate
models. This addresses the pressing need for transparency, explainability,
fairness, and robustness. Conversely, within FM4VIS, we highlight how
foundation models can be utilized to advance the visualization field itself.
The confluence of foundation models and visualizations holds great promise, but
it also comes with its own set of challenges. By highlighting these challenges
and the growing opportunities, this paper seeks to provide a starting point for
continued exploration in this promising avenue.
","2023-10-10","2310.05771v1.pdf"
"2310.05791","Dongbin Na","Juntae Kim, Eunjung Cho, Dongwoo Kim, Dongbin Na","Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for
  Competitive Programming Problems","8 pages","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent program development industries have required problem-solving
abilities for engineers, especially application developers. However, AI-based
education systems to help solve computer algorithm problems have not yet
attracted attention, while most big tech companies require the ability to solve
algorithm problems including Google, Meta, and Amazon. The most useful guide to
solving algorithm problems might be guessing the category (tag) of the facing
problems. Therefore, our study addresses the task of predicting the algorithm
tag as a useful tool for engineers and developers. Moreover, we also consider
predicting the difficulty levels of algorithm problems, which can be used as
useful guidance to calculate the required time to solve that problem. In this
paper, we present a real-world algorithm problem multi-task dataset, AMT, by
mainly collecting problem samples from the most famous and large competitive
programming website Codeforces. To the best of our knowledge, our proposed
dataset is the most large-scale dataset for predicting algorithm tags compared
to previous studies. Moreover, our work is the first to address predicting the
difficulty levels of algorithm problems. We present a deep learning-based novel
method for simultaneously predicting algorithm tags and the difficulty levels
of an algorithm problem given. All datasets and source codes are available at
https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.
","2023-10-10","2310.05791v1.pdf"
"2310.05818","Liang  Xu","Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue","SC-Safety: A Multi-round Open-ended Question Adversarial Safety
  Benchmark for Large Language Models in Chinese","20 pages, 8 tables, 16 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated
remarkable abilities in natural language understanding and generation. However,
alongside their positive impact on our daily tasks, they can also produce
harmful content that negatively affects societal perceptions. To systematically
assess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -
a multi-round adversarial benchmark with 4912 open-ended questions covering
more than 20 safety sub-dimensions. Adversarial human-model interactions and
conversations significantly increase the challenges compared to existing
methods. Experiments on 13 major LLMs supporting Chinese yield the following
insights: 1) Closed-source models outperform open-sourced ones in terms of
safety; 2) Models released from China demonstrate comparable safety levels to
LLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can
compete effectively in terms of safety. By introducing SC-Safety, we aim to
promote collaborative efforts to create safer and more trustworthy LLMs. The
benchmark and findings provide guidance on model selection. Our benchmark can
be found at https://www.CLUEbenchmarks.com
","2023-10-10","2310.05818v1.pdf"
"2310.05824","Pinzhen Chen","Nikolay Bogoychev and Pinzhen Chen","Terminology-Aware Translation with Constrained Decoding and Large
  Language Model Prompting","WMT 2023 Terminology Translation Task","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Terminology correctness is important in the downstream application of machine
translation, and a prevalent way to ensure this is to inject terminology
constraints into a translation system. In our submission to the WMT 2023
terminology translation task, we adopt a translate-then-refine approach which
can be domain-independent and requires minimal manual efforts. We annotate
random source words with pseudo-terminology translations obtained from word
alignment to first train a terminology-aware model. Further, we explore two
post-processing methods. First, we use an alignment process to discover whether
a terminology constraint has been violated, and if so, we re-decode with the
violating word negatively constrained. Alternatively, we leverage a large
language model to refine a hypothesis by providing it with terminology
constraints. Results show that our terminology-aware model learns to
incorporate terminologies effectively, and the large language model refinement
process can further improve terminology recall.
","2023-10-10","2310.05824v1.pdf"
"2310.05829","Cheng Tan","Cheng Tan, Jue Wang, Zhangyang Gao, Siyuan Li, Lirong Wu, Jun Xia,
  Stan Z. Li","Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning
  under A Unified View","Under review","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Spatio-temporal predictive learning plays a crucial role in self-supervised
learning, with wide-ranging applications across a diverse range of fields.
Previous approaches for temporal modeling fall into two categories:
recurrent-based and recurrent-free methods. The former, while meticulously
processing frames one by one, neglect short-term spatio-temporal information
redundancies, leading to inefficiencies. The latter naively stack frames
sequentially, overlooking the inherent temporal dependencies. In this paper, we
re-examine the two dominant temporal modeling approaches within the realm of
spatio-temporal predictive learning, offering a unified perspective. Building
upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive
learning), an innovative framework that reconciles the recurrent-based and
recurrent-free methods by integrating both micro-temporal and macro-temporal
scales. Extensive experiments on a wide range of spatio-temporal predictive
learning demonstrate that USTEP achieves significant improvements over existing
temporal modeling approaches, thereby establishing it as a robust solution for
a wide range of spatio-temporal applications.
","2023-10-10","2310.05829v1.pdf"
"2310.05833","Sebastian Gregor Gruber","Sebastian G. Gruber, Florian Buettner","A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative
  Models","Preprint","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative models, like large language models, are becoming increasingly
relevant in our daily lives, yet a theoretical framework to assess their
generalization behavior and uncertainty does not exist. Particularly, the
problem of uncertainty estimation is commonly solved in an ad-hoc manner and
task dependent. For example, natural language approaches cannot be transferred
to image generation. In this paper we introduce the first
bias-variance-covariance decomposition for kernel scores and their associated
entropy. We propose unbiased and consistent estimators for each quantity which
only require generated samples but not the underlying model itself. As an
application, we offer a generalization evaluation of diffusion models and
discover how mode collapse of minority groups is a contrary phenomenon to
overfitting. Further, we demonstrate that variance and predictive kernel
entropy are viable measures of uncertainty for image, audio, and language
generation. Specifically, our approach for uncertainty estimation is more
predictive of performance on CoQA and TriviaQA question answering datasets than
existing baselines and can also be applied to closed-source models.
","2023-10-10","2310.05833v1.pdf"
"2310.05845","Ziwei Chai","Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen
  Huang, Yang Yang","GraphLLM: Boosting Graph Reasoning Ability of Large Language Model","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The advancement of Large Language Models (LLMs) has remarkably pushed the
boundaries towards artificial general intelligence (AGI), with their
exceptional ability on understanding diverse types of information, including
but not limited to images and audio. Despite this progress, a critical gap
remains in empowering LLMs to proficiently understand and reason on graph data.
Recent studies underscore LLMs' underwhelming performance on fundamental graph
reasoning tasks. In this paper, we endeavor to unearth the obstacles that
impede LLMs in graph reasoning, pinpointing the common practice of converting
graphs into natural language descriptions (Graph2Text) as a fundamental
bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering
end-to-end approach that synergistically integrates graph learning models with
LLMs. This synergy equips LLMs with the ability to proficiently interpret and
reason on graph data, harnessing the superior expressive power of graph
learning models. Our empirical evaluations across four fundamental graph
reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a
substantial average accuracy enhancement of 54.44%, alongside a noteworthy
context reduction of 96.45% across various graph reasoning tasks.
","2023-10-10","2310.05845v1.pdf"
"2310.05861","Archiki Prasad","Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal","Rephrase, Augment, Reason: Visual Grounding of Questions for
  Vision-Language Models","22 pages, 4 figures, Code: https://github.com/archiki/RepARe","","","","cs.CL cs.AI cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An increasing number of vision-language tasks can be handled with little to
no training, i.e., in a zero and few-shot manner, by marrying large language
models (LLMs) to vision encoders, resulting in large vision-language models
(LVLMs). While this has huge upsides, such as not requiring training data or
custom architectures, how an input is presented to a LVLM can have a major
impact on zero-shot model performance. In particular, inputs phrased in an
underspecified way can result in incorrect answers due to factors like missing
visual information, complex implicit reasoning, or linguistic ambiguity.
Therefore, adding visually grounded information to the input as a preemptive
clarification should improve model performance by reducing underspecification,
e.g., by localizing objects and disambiguating references. Similarly, in the
VQA setting, changing the way questions are framed can make them easier for
models to answer. To this end, we present Rephrase, Augment and Reason
(RepARe), a gradient-free framework that extracts salient details about the
image using the underlying LVLM as a captioner and reasoner, in order to
propose modifications to the original question. We then use the LVLM's
confidence over a generated answer as an unsupervised scoring function to
select the rephrased question most likely to improve zero-shot performance.
Focusing on two visual question answering tasks, we show that RepARe can result
in a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%
point increase on A-OKVQA. Additionally, we find that using gold answers for
oracle question candidate selection achieves a substantial gain in VQA accuracy
by up to 14.41%. Through extensive analysis, we demonstrate that outputs from
RepARe increase syntactic complexity, and effectively utilize vision-language
interaction and the frozen language model in LVLMs.
","2023-10-10","2310.05861v1.pdf"
"2310.05863","Guangzhi Sun","Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li,
  Lu Lu, Zejun Ma, Chao Zhang","Fine-grained Audio-Visual Joint Representations for Multimodal Large
  Language Models","","","","","eess.AS cs.AI cs.CV cs.SD","http://creativecommons.org/licenses/by/4.0/","  Audio-visual large language models (LLM) have drawn significant attention,
yet the fine-grained combination of both input streams is rather
under-explored, which is challenging but necessary for LLMs to understand
general video inputs. To this end, a fine-grained audio-visual joint
representation (FAVOR) learning framework for multimodal LLMs is proposed in
this paper, which extends a text-based LLM to simultaneously perceive speech
and audio events in the audio input stream and images or videos in the visual
input stream, at the frame level. To fuse the audio and visual feature streams
into joint representations and to align the joint space with the LLM input
embedding space, we propose a causal Q-Former structure with a causal attention
module to enhance the capture of causal relations of the audio-visual frames
across time. An audio-visual evaluation benchmark (AVEB) is also proposed which
comprises six representative single-modal tasks with five cross-modal tasks
reflecting audio-visual co-reasoning abilities. While achieving competitive
single-modal performance on audio, speech and image tasks in AVEB, FAVOR
achieved over 20% accuracy improvements on the video question-answering task
when fine-grained information or temporal causal reasoning is required. FAVOR,
in addition, demonstrated remarkable video comprehension and reasoning
abilities on tasks that are unprecedented by other multimodal LLMs. An
interactive demo of FAVOR is available at
https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and
model checkpoints will be released soon.
","2023-10-11","2310.05863v1.pdf"
"2310.05866","Quntao Zhuang","Bingzhi Zhang, Peng Xu, Xiaohui Chen and Quntao Zhuang","Generative quantum machine learning via denoising diffusion
  probabilistic models","7+6 pages, 9 figures","","","","quant-ph cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep generative models are key-enabling technology to computer vision, text
generation and large language models. Denoising diffusion probabilistic models
(DDPMs) have recently gained much attention due to their ability to generate
diverse and high-quality samples in many computer vision tasks, as well as to
incorporate flexible model architectures and relatively simple training scheme.
Quantum generative models, empowered by entanglement and superposition, have
brought new insight to learning classical and quantum data. Inspired by the
classical counterpart, we propose the quantum denoising diffusion probabilistic
models (QuDDPM) to enable efficiently trainable generative learning of quantum
data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,
while introduces multiple intermediate training tasks as interpolation between
the target distribution and noise to avoid barren plateau and guarantee
efficient training. We demonstrate QuDDPM's capability in learning correlated
quantum noise model and learning topological structure of nontrivial
distribution of quantum data.
","2023-10-10","2310.05866v1.pdf"
"2310.05869","Insu Han","Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P.
  Woodruff, Amir Zandieh","HyperAttention: Long-context Attention in Near-Linear Time","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  We present an approximate attention mechanism named HyperAttention to address
the computational challenges posed by the growing complexity of long contexts
used in Large Language Models (LLMs). Recent work suggests that in the
worst-case scenario, quadratic time is necessary unless the entries of the
attention matrix are bounded or the matrix has low stable rank. We introduce
two parameters which measure: (1) the max column norm in the normalized
attention matrix, and (2) the ratio of row norms in the unnormalized attention
matrix after detecting and removing large entries. We use these fine-grained
parameters to capture the hardness of the problem. Despite previous lower
bounds, we are able to achieve a linear time sampling algorithm even when the
matrix has unbounded entries or a large stable rank, provided the above
parameters are small. HyperAttention features a modular design that easily
accommodates integration of other fast low-level implementations, particularly
FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to
identify large entries, HyperAttention outperforms existing methods, giving
significant speed improvements compared to state-of-the-art solutions like
FlashAttention. We validate the empirical performance of HyperAttention on a
variety of different long-context length datasets. For example, HyperAttention
makes the inference time of ChatGLM2 50\% faster on 32k context length while
perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,
with causal masking, HyperAttention offers 5-fold speedup on a single attention
layer.
","2023-10-12","2310.05869v1.pdf"
"2310.05872","Kaiwen Zhou","Kaiwen Zhou and Kwonjoon Lee and Teruhisa Misu and Xin Eric Wang","ViCor: Bridging Visual Understanding and Commonsense Reasoning with
  Large Language Models","","","","","cs.CV cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In our work, we explore the synergistic capabilities of pre-trained
vision-and-language models (VLMs) and large language models (LLMs) for visual
commonsense reasoning (VCR). We categorize the problem of VCR into visual
commonsense understanding (VCU) and visual commonsense inference (VCI). For
VCU, which involves perceiving the literal visual content, pre-trained VLMs
exhibit strong cross-dataset generalization. On the other hand, in VCI, where
the goal is to infer conclusions beyond image content, VLMs face difficulties.
We find that a baseline where VLMs provide perception results (image captions)
to LLMs leads to improved performance on VCI. However, we identify a challenge
with VLMs' passive perception, which often misses crucial context information,
leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we
suggest a collaborative approach where LLMs, when uncertain about their
reasoning, actively direct VLMs to concentrate on and gather relevant visual
elements to support potential commonsense inferences. In our method, named
ViCor, pre-trained LLMs serve as problem classifiers to analyze the problem
category, VLM commanders to leverage VLMs differently based on the problem
classification, and visual commonsense reasoners to answer the question. VLMs
will perform visual recognition and understanding. We evaluate our framework on
two VCR benchmark datasets and outperform all other methods that do not require
in-domain supervised fine-tuning.
","2023-10-10","2310.05872v1.pdf"
"2310.05876","Fazl Barez","Kayla Matteucci, Shahar Avin, Fazl Barez, Se\'an \'O h\'Eigeartaigh","AI Systems of Concern","9 pages, 1 figure, 2 tables","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Concerns around future dangers from advanced AI often centre on systems
hypothesised to have intrinsic characteristics such as agent-like behaviour,
strategic awareness, and long-range planning. We label this cluster of
characteristics as ""Property X"". Most present AI systems are low in ""Property
X""; however, in the absence of deliberate steering, current research directions
may rapidly lead to the emergence of highly capable AI systems that are also
high in ""Property X"". We argue that ""Property X"" characteristics are
intrinsically dangerous, and when combined with greater capabilities will
result in AI systems for which safety and control is difficult to guarantee.
Drawing on several scholars' alternative frameworks for possible AI research
trajectories, we argue that most of the proposed benefits of advanced AI can be
obtained by systems designed to minimise this property. We then propose
indicators and governance interventions to identify and limit the development
of systems with risky ""Property X"" characteristics.
","2023-10-10","2310.05876v1.pdf"
"2310.05884","Xinbo Wu","Xinbo Wu, Lav R. Varshney","A Meta-Learning Perspective on Transformers for Causal Language Modeling","","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Transformer architecture has become prominent in developing large causal
language models. However, mechanisms to explain its capabilities are not well
understood. Focused on the training process, here we establish a meta-learning
view of the Transformer architecture when trained for the causal language
modeling task, by explicating an inner optimization process that may happen
within the Transformer. Further, from within the inner optimization, we
discover and theoretically analyze a special characteristic of the norms of
learned token representations within Transformer-based causal language models.
Our analysis is supported by experiments conducted on pre-trained large
language models and real-world data.
","2023-10-10","2310.05884v1.pdf"
"2310.05910","Zhiqing Sun","Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen,
  David Cox, Yiming Yang, Chuang Gan","SALMON: Self-Alignment with Principle-Following Reward Models","Project page: https://github.com/IBM/SALMON","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Supervised Fine-Tuning (SFT) on response demonstrations combined with
Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful
paradigm for aligning LLM-based AI agents. However, a significant limitation of
such an approach is its dependency on high-quality human annotations, making
its application to intricate tasks challenging due to difficulties in obtaining
consistent response demonstrations and in-distribution response preferences.
This paper presents a novel approach, namely SALMON (Self-ALignMent with
principle-fOllowiNg reward models), to align base language models with minimal
human supervision, using only a small set of human-defined principles, yet
achieving superior performance. Central to our approach is a
principle-following reward model. Trained on synthetic preference data, this
model can generate reward scores based on arbitrary human-defined principles.
By merely adjusting these principles during the RL training phase, we gain full
control over the preferences with the reward model, subsequently influencing
the behavior of the RL-trained policies, and eliminating the reliance on the
collection of online human preferences. Applying our method to the LLaMA-2-70b
base language model, we developed an AI assistant named Dromedary-2. With only
6 exemplars for in-context learning and 31 human-defined principles,
Dromedary-2 significantly surpasses the performance of several state-of-the-art
AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have
open-sourced the code and model weights to encourage further research into
aligning LLM-based AI agents with enhanced supervision efficiency, improved
controllability, and scalable oversight.
","2023-10-10","2310.05910v1.pdf"
"2310.05914","Neel Jain","Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min
  Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi
  Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein","NEFTune: Noisy Embeddings Improve Instruction Finetuning","25 pages, Code is available on Github:
  https://github.com/neelsjain/NEFTune","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that language model finetuning can be improved, sometimes
dramatically, with a simple augmentation. NEFTune adds noise to the embedding
vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca
achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.
NEFTune also improves over strong baselines on modern instruction datasets.
Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%
improvement, and with OpenPlatypus an 8% improvement. Even powerful models
further refined with RLHF such as LLaMA-2-Chat benefit from additional training
with NEFTune.
","2023-10-11","2310.05914v1.pdf"
"2310.05915","Shunyu Yao","Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
  Narasimhan, Shunyu Yao","FireAct: Toward Language Agent Fine-tuning","Code, data, and models are available at
  https://fireact-agent.github.io","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recent efforts have augmented language models (LMs) with external tools or
environments, leading to the development of language agents that can reason and
act. However, most of these agents rely on few-shot prompting techniques with
off-the-shelf LMs. In this paper, we investigate and argue for the overlooked
direction of fine-tuning LMs to obtain language agents. Using a setup of
question answering (QA) with a Google search API, we explore a variety of base
LMs, prompting methods, fine-tuning data, and QA tasks, and find language
agents are consistently improved after fine-tuning their backbone LMs. For
example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4
leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,
a novel approach to fine-tuning LMs with trajectories from multiple tasks and
prompting methods, and show having more diverse fine-tuning data can further
improve agents. Along with other findings regarding scaling effects,
robustness, generalization, efficiency and cost, our work establishes
comprehensive benefits of fine-tuning LMs for agents, and provides an initial
set of experimental designs, insights, as well as open questions toward
language agent fine-tuning.
","2023-10-10","2310.05915v1.pdf"
"2310.05921","Jordan Lekeufack Sopze","Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael
  I. Jordan, Jitendra Malik","Conformal Decision Theory: Safe Autonomous Decisions from Imperfect
  Predictions","8 pages, 5 figures","","","","stat.ML cs.LG cs.RO stat.ME","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce Conformal Decision Theory, a framework for producing safe
autonomous decisions despite imperfect machine learning predictions. Examples
of such decisions are ubiquitous, from robot planning algorithms that rely on
pedestrian predictions, to calibrating autonomous manufacturing to exhibit high
throughput and low error, to the choice of trusting a nominal policy versus
switching to a safe backup policy at run-time. The decisions produced by our
algorithms are safe in the sense that they come with provable statistical
guarantees of having low risk without any assumptions on the world model
whatsoever; the observations need not be I.I.D. and can even be adversarial.
The theory extends results from conformal prediction to calibrate decisions
directly, without requiring the construction of prediction sets. Experiments
demonstrate the utility of our approach in robot motion planning around humans,
automated stock trading, and robot manufacturing.
","2023-10-11","2310.05921v1.pdf"
"2310.05976","Reiji Suzuki","Reiji Suzuki and Takaya Arita","An evolutionary model of personality traits related to cooperative
  behavior using a large language model","7 pages, 4 figures and 1 table","","","","physics.soc-ph cs.AI cs.CL cs.GT cs.MA cs.NE q-bio.PE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper aims to shed light on the evolutionary dynamics of diverse and
social populations by introducing the rich expressiveness of generative models
into the trait expression of social agent-based evolutionary models.
Specifically, we focus on the evolution of personality traits in the context of
a game-theoretic relationship as a situation in which inter-individual
interests exert strong selection pressures. We construct an agent model in
which linguistic descriptions of personality traits related to cooperative
behavior are used as genes. The deterministic strategies extracted from Large
Language Model (LLM) that make behavioral decisions based on these personality
traits are used as behavioral traits. The population is evolved according to
selection based on average payoff and mutation of genes by asking LLM to
slightly modify the parent gene toward cooperative or selfish. Through
preliminary experiments and analyses, we clarify that such a model can indeed
exhibit the evolution of cooperative behavior based on the diverse and
higher-order representation of personality traits. We also observed the
repeated intrusion of cooperative and selfish personality traits through
changes in the expression of personality traits, and found that the emerging
words in the evolved gene well reflected the behavioral tendency of its
personality in terms of their semantics.
","2023-10-11","2310.05976v1.pdf"
"2310.05984","Petter T\""ornberg","Petter T\""ornberg, Diliara Valeeva, Justus Uitermark, Christopher Bail","Simulating Social Media Using Large Language Models to Evaluate
  Alternative News Feed Algorithms","","","","","cs.SI cs.AI cs.MA","http://creativecommons.org/licenses/by/4.0/","  Social media is often criticized for amplifying toxic discourse and
discouraging constructive conversations. But designing social media platforms
to promote better conversations is inherently challenging. This paper asks
whether simulating social media through a combination of Large Language Models
(LLM) and Agent-Based Modeling can help researchers study how different news
feed algorithms shape the quality of online conversations. We create realistic
personas using data from the American National Election Study to populate
simulated social media platforms. Next, we prompt the agents to read and share
news articles - and like or comment upon each other's messages - within three
platforms that use different news feed algorithms. In the first platform, users
see the most liked and commented posts from users whom they follow. In the
second, they see posts from all users - even those outside their own network.
The third platform employs a novel ""bridging"" algorithm that highlights posts
that are liked by people with opposing political views. We find this bridging
algorithm promotes more constructive, non-toxic, conversation across political
divides than the other two models. Though further research is needed to
evaluate these findings, we argue that LLMs hold considerable potential to
improve simulation research on social media and many other complex social
settings.
","2023-10-11","2310.05984v1.pdf"
"2310.05993","Adrian Groza","Adrian Groza","Measuring reasoning capabilities of ChatGPT","","","","","cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  I shall quantify the logical faults generated by ChatGPT when applied to
reasoning tasks. For experiments, I use the 144 puzzles from the library
\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. The
library contains puzzles of various types, including arithmetic puzzles,
logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling
puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct
solutions for these puzzles were checked using the theorem prover
Prover9~\cite{mccune2005release} and the finite models finder
Mace4~\cite{mccune2003mace4} based on human-modelling in Equational First Order
Logic. A first output of this study is the benchmark of 100 logical puzzles.
For this dataset ChatGPT provided both correct answer and justification for 7\%
only. %, while BARD for 5\%. Since the dataset seems challenging, the
researchers are invited to test the dataset on more advanced or tuned models
than ChatGPT3.5 with more crafted prompts. A second output is the
classification of reasoning faults conveyed by ChatGPT. This classification
forms a basis for a taxonomy of reasoning faults generated by large language
models. I have identified 67 such logical faults, among which: inconsistencies,
implication does not hold, unsupported claim, lack of commonsense, wrong
justification. The 100 solutions generated by ChatGPT contain 698 logical
faults. That is on average, 7 fallacies for each reasoning task. A third ouput
is the annotated answers of the ChatGPT with the corresponding logical faults.
Each wrong statement within the ChatGPT answer was manually annotated, aiming
to quantify the amount of faulty text generated by the language model. On
average, 26.03\% from the generated text was a logical fault.
","2023-10-11","2310.05993v1.pdf"
"2310.06003","Chan Wu","Chan Wu, Hanxiao Zhang, Lin Ju, Jinjing Huang, Youshao Xiao, Zhaoxin
  Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang and Jun Zhou","Rethinking Memory and Communication Cost for Efficient Large Language
  Model Training","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As model sizes and training datasets continue to increase, large-scale model
training frameworks reduce memory consumption by various sharding techniques.
However, the huge communication overhead reduces the training efficiency,
especially in public cloud environments with varying network bandwidths. In
this paper, we rethink the impact of memory consumption and communication
overhead on the training speed of large language model, and propose a
memory-communication balanced \underline{Pa}rtial \underline{R}edundancy
\underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of
inter-group communication by grouping GPU clusters and introducing minor
intra-group memory redundancy, thereby improving the training efficiency of the
model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring)
communication topology to enhance communication efficiency between nodes or
across switches in large model training. Our experiments demonstrate that the
HO-Ring algorithm improves communication efficiency by 32.6\% compared to the
traditional Ring algorithm. Compared to the baseline ZeRO, PaRO significantly
improves training throughput by 1.2x-2.6x and achieves a near-linear
scalability. Therefore, the PaRO strategy provides more fine-grained options
for the trade-off between memory consumption and communication overhead in
different training scenarios.
","2023-10-11","2310.06003v1.pdf"
"2310.06009","Peter S. Park","Peter S. Park and Max Tegmark","Divide-and-Conquer Dynamics in AI-Driven Disempowerment","28 pages, nine visualizations (seven figures and two tables)","","","","cs.CY cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  AI companies are attempting to create AI systems that outperform humans at
most economically valuable work. Current AI models are already automating away
the livelihoods of some artists, actors, and writers. But there is infighting
between those who prioritize current harms and future harms. We construct a
game-theoretic model of conflict to study the causes and consequences of this
disunity. Our model also helps explain why throughout history, stakeholders
sharing a common threat have found it advantageous to unite against it, and why
the common threat has in turn found it advantageous to divide and conquer.
  Under realistic parameter assumptions, our model makes several predictions
that find preliminary corroboration in the historical-empirical record. First,
current victims of AI-driven disempowerment need the future victims to realize
that their interests are also under serious and imminent threat, so that future
victims are incentivized to support current victims in solidarity. Second, the
movement against AI-driven disempowerment can become more united, and thereby
more likely to prevail, if members believe that their efforts will be
successful as opposed to futile. Finally, the movement can better unite and
prevail if its members are less myopic. Myopic members prioritize their future
well-being less than their present well-being, and are thus disinclined to
solidarily support current victims today at personal cost, even if this is
necessary to counter the shared threat of AI-driven disempowerment.
","2023-10-11","2310.06009v1.pdf"
"2310.06046","Dipayan Saha","Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo
  Zhou, Mark Tehranipoor, Farimah Farahmandi","LLM for SoC Security: A Paradigm Shift","42 pages","","","","cs.CR cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  As the ubiquity and complexity of system-on-chip (SoC) designs increase
across electronic devices, the task of incorporating security into an SoC
design flow poses significant challenges. Existing security solutions are
inadequate to provide effective verification of modern SoC designs due to their
limitations in scalability, comprehensiveness, and adaptability. On the other
hand, Large Language Models (LLMs) are celebrated for their remarkable success
in natural language understanding, advanced reasoning, and program synthesis
tasks. Recognizing an opportunity, our research delves into leveraging the
emergent capabilities of Generative Pre-trained Transformers (GPTs) to address
the existing gaps in SoC security, aiming for a more efficient, scalable, and
adaptable methodology. By integrating LLMs into the SoC security verification
paradigm, we open a new frontier of possibilities and challenges to ensure the
security of increasingly complex SoCs. This paper offers an in-depth analysis
of existing works, showcases practical case studies, demonstrates comprehensive
experiments, and provides useful promoting guidelines. We also present the
achievements, prospects, and challenges of employing LLM in different SoC
security verification tasks.
","2023-10-11","2310.06046v1.pdf"
"2310.06061","Siddharth Jaiswal","Siddharth D Jaiswal, Ankit Kumar Verma, Animesh Mukherjee","Auditing Gender Analyzers on Text Data","This work has been accepted at IEEE/ACM ASONAM 2023. Please cite the
  version appearing in the ASONAM proceedings","","","","cs.CY cs.CL","http://creativecommons.org/licenses/by/4.0/","  AI models have become extremely popular and accessible to the general public.
However, they are continuously under the scanner due to their demonstrable
biases toward various sections of the society like people of color and
non-binary people. In this study, we audit three existing gender analyzers --
uClassify, Readable and HackerFactor, for biases against non-binary
individuals. These tools are designed to predict only the cisgender binary
labels, which leads to discrimination against non-binary members of the
society. We curate two datasets -- Reddit comments (660k) and, Tumblr posts
(2.05M) and our experimental evaluation shows that the tools are highly
inaccurate with the overall accuracy being ~50% on all platforms. Predictions
for non-binary comments on all platforms are mostly female, thus propagating
the societal bias that non-binary individuals are effeminate. To address this,
we fine-tune a BERT multi-label classifier on the two datasets in multiple
combinations, observe an overall performance of ~77% on the most realistically
deployable setting and a surprisingly higher performance of 90% for the
non-binary class. We also audit ChatGPT using zero-shot prompts on a small
dataset (due to high pricing) and observe an average accuracy of 58% for Reddit
and Tumblr combined (with overall better results for Reddit).
  Thus, we show that existing systems, including highly advanced ones like
ChatGPT are biased, and need better audits and moderation and, that such
societal biases can be addressed and alleviated through simple off-the-shelf
models like BERT trained on more gender inclusive datasets.
","2023-10-11","2310.06061v1.pdf"
"2310.06072","Detai Xin","Detai Xin, Junfeng Jiang, Shinnosuke Takamichi, Yuki Saito, Akiko
  Aizawa, Hiroshi Saruwatari","JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and
  Nonverbal Expressions","","","","","cs.SD eess.AS","http://creativecommons.org/licenses/by-sa/4.0/","  We present the JVNV, a Japanese emotional speech corpus with verbal content
and nonverbal vocalizations whose scripts are generated by a large-scale
language model. Existing emotional speech corpora lack not only proper
emotional scripts but also nonverbal vocalizations (NVs) that are essential
expressions in spoken language to express emotions. We propose an automatic
script generation method to produce emotional scripts by providing seed words
with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using
prompt engineering. We select 514 scripts with balanced phoneme coverage from
the generated candidate scripts with the assistance of emotion confidence
scores and language fluency scores. We demonstrate the effectiveness of JVNV by
showing that JVNV has better phoneme coverage and emotion recognizability than
previous Japanese emotional speech corpora. We then benchmark JVNV on emotional
text-to-speech synthesis using discrete codes to represent NVs. We show that
there still exists a gap between the performance of synthesizing read-aloud
speech and emotional speech, and adding NVs in the speech makes the task even
harder, which brings new challenges for this task and makes JVNV a valuable
resource for relevant works in the future. To our best knowledge, JVNV is the
first speech corpus that generates scripts automatically using large language
models.
","2023-10-11","2310.06072v1.pdf"
"2310.06083","Andres M Bran","Andres M Bran, Philippe Schwaller","Transformers and Large Language Models for Chemistry and Drug Discovery","","","","","cs.LG physics.chem-ph","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Language modeling has seen impressive progress over the last years, mainly
prompted by the invention of the Transformer architecture, sparking a
revolution in many fields of machine learning, with breakthroughs in chemistry
and biology. In this chapter, we explore how analogies between chemical and
natural language have inspired the use of Transformers to tackle important
bottlenecks in the drug discovery process, such as retrosynthetic planning and
chemical space exploration. The revolution started with models able to perform
particular tasks with a single type of data, like linearised molecular graphs,
which then evolved to include other types of data, like spectra from analytical
instruments, synthesis actions, and human language. A new trend leverages
recent developments in large language models, giving rise to a wave of models
capable of solving generic tasks in chemistry, all facilitated by the
flexibility of natural language. As we continue to explore and harness these
capabilities, we can look forward to a future where machine learning plays an
even more integral role in accelerating scientific discovery.
","2023-10-11","2310.06083v1.pdf"
"2310.06111","Arth Bohra","Arth Bohra, Govert Verkes, Artem Harutyunyan, Pascal Weinberger,
  Giovanni Campagna","BYOC: Personalized Few-Shot Classification with Co-Authored Class
  Descriptions","Accepted at EMNLP 2023 (Findings)","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Text classification is a well-studied and versatile building block for many
NLP applications. Yet, existing approaches require either large annotated
corpora to train a model with or, when using large language models as a base,
require carefully crafting the prompt as well as using a long context that can
fit many examples. As a result, it is not possible for end-users to build
classifiers for themselves. To address this issue, we propose a novel approach
to few-shot text classification using an LLM. Rather than few-shot examples,
the LLM is prompted with descriptions of the salient features of each class.
These descriptions are coauthored by the user and the LLM interactively: while
the user annotates each few-shot example, the LLM asks relevant questions that
the user answers. Examples, questions, and answers are summarized to form the
classification prompt. Our experiments show that our approach yields high
accuracy classifiers, within 82% of the performance of models trained with
significantly larger datasets while using only 1% of their training sets.
Additionally, in a study with 30 participants, we show that end-users are able
to build classifiers to suit their specific needs. The personalized classifiers
show an average accuracy of 90%, which is 15% higher than the state-of-the-art
approach.
","2023-10-11","2310.06111v1.pdf"
"2310.06116","Ali AhmadiTeshnizi","Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell","OptiMUS: Optimization Modeling Using mip Solvers and large language
  models","","","","","cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Optimization problems are pervasive across various sectors, from
manufacturing and distribution to healthcare. However, most such problems are
still solved heuristically by hand rather than optimally by state-of-the-art
solvers, as the expertise required to formulate and solve these problems limits
the widespread adoption of optimization tools and techniques. We introduce
OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and
solve MILP problems from their natural language descriptions. OptiMUS is
capable of developing mathematical models, writing and debugging solver code,
developing tests, and checking the validity of generated solutions. To
benchmark our agent, we present NLP4LP, a novel dataset of linear programming
(LP) and mixed integer linear programming (MILP) problems. Our experiments
demonstrate that OptiMUS is able to solve 67\% more problems compared to a
basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at
\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}
","2023-10-11","2310.06116v1.pdf"
"2310.06117","Swaroop Mishra","Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed
  H. Chi, Quoc V Le and Denny Zhou","Take a Step Back: Evoking Reasoning via Abstraction in Large Language
  Models","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present Step-Back Prompting, a simple prompting technique that enables
LLMs to do abstractions to derive high-level concepts and first principles from
instances containing specific details. Using the concepts and principles to
guide the reasoning steps, LLMs significantly improve their abilities in
following a correct reasoning path towards the solution. We conduct experiments
of Step-Back Prompting with PaLM-2L models and observe substantial performance
gains on a wide range of challenging reasoning-intensive tasks including STEM,
Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting
improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,
TimeQA by 27%, and MuSiQue by 7%.
","2023-10-11","2310.06117v1.pdf"
"2310.06122","Genoveva Vargas Solar","Genoveva Vargas-Solar, Mirian Halfeld Ferrari Alves, and Anne-Lyse
  Minard Forst","From Text to Knowledge with Graphs: modelling, querying and exploiting
  textual content","","","","","cs.DB","http://creativecommons.org/licenses/by/4.0/","  This paper highlights the challenges, current trends, and open issues related
to the representation, querying and analytics of content extracted from texts.
The internet contains vast text-based information on various subjects,
including commercial documents, medical records, scientific experiments,
engineering tests, and events that impact urban and natural environments.
Extracting knowledge from this text involves understanding the nuances of
natural language and accurately representing the content without losing
information. This allows knowledge to be accessed, inferred, or discovered. To
achieve this, combining results from various fields, such as linguistics,
natural language processing, knowledge representation, data storage, querying,
and analytics, is necessary. The vision in this paper is that graphs can be a
well-suited text content representation once annotated and the right querying
and analytics techniques are applied. This paper discusses this hypothesis from
the perspective of linguistics, natural language processing, graph models and
databases and artificial intelligence provided by the panellists of the DOING
session in the MADICS Symposium 2022.
","2023-10-11","2310.06122v1.pdf"
"2310.06124","Yash Garg","Yash Garg, Nebiyou Yismaw, Rakib Hyder, Ashley Prater-Bennette, M.
  Salman Asif","Factorized Tensor Networks for Multi-Task and Multi-Domain Learning","","","","","cs.LG cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multi-task and multi-domain learning methods seek to learn multiple
tasks/domains, jointly or one after another, using a single unified network.
The key challenge and opportunity is to exploit shared information across tasks
and domains to improve the efficiency of the unified network. The efficiency
can be in terms of accuracy, storage cost, computation, or sample complexity.
In this paper, we propose a factorized tensor network (FTN) that can achieve
accuracy comparable to independent single-task/domain networks with a small
number of additional parameters. FTN uses a frozen backbone network from a
source model and incrementally adds task/domain-specific low-rank tensor
factors to the shared frozen network. This approach can adapt to a large number
of target domains and tasks without catastrophic forgetting. Furthermore, FTN
requires a significantly smaller number of task-specific parameters compared to
existing methods. We performed experiments on widely used multi-domain and
multi-task datasets. We show the experiments on convolutional-based
architecture with different backbones and on transformer-based architecture. We
observed that FTN achieves similar accuracy as single-task/domain methods while
using only a fraction of additional parameters per task.
","2023-10-11","2310.06124v1.pdf"
"2310.06147","Hao Sun","Hao Sun","Reinforcement Learning in the Era of LLMs: What is Essential? What is
  needed? An RL Perspective on RLHF, Prompting, and Beyond","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in Large Language Models (LLMs) have garnered wide
attention and led to successful products such as ChatGPT and GPT-4. Their
proficiency in adhering to instructions and delivering harmless, helpful, and
honest (3H) responses can largely be attributed to the technique of
Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to
link the research in conventional RL to RL techniques used in LLM research.
Demystify this technique by discussing why, when, and how RL excels.
Furthermore, we explore potential future avenues that could either benefit from
or contribute to RLHF research.
  Highlighted Takeaways:
  1. RLHF is Online Inverse RL with Offline Demonstration Data.
  2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior
Cloning (BC) by alleviating the problem of compounding error.
  3. The RM step in RLHF generates a proxy of the expensive human feedback,
such an insight can be generalized to other LLM tasks such as prompting
evaluation and optimization where feedback is also expensive.
  4. The policy learning in RLHF is more challenging than conventional problems
studied in IRL due to their high action dimensionality and feedback sparsity.
  5. The main superiority of PPO over off-policy value-based methods is its
stability gained from (almost) on-policy data and conservative policy updates.
","2023-10-11","2310.06147v1.pdf"
"2310.06165","Karel D'Oosterlinck","Karel D'Oosterlinck, Semere Kiros Bitew, Brandon Papineau, Christopher
  Potts, Thomas Demeester, Chris Develder","CAW-coref: Conjunction-Aware Word-level Coreference Resolution","Accepted at CRAC 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  State-of-the-art coreference resolutions systems depend on multiple LLM calls
per document and are thus prohibitively expensive for many use cases (e.g.,
information extraction with large corpora). The leading word-level coreference
system (WL-coref) attains 96.6% of these SOTA systems' performance while being
much more efficient. In this work, we identify a routine yet important failure
case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We
offer a simple yet effective solution that improves the performance on the
OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level
coreference resolution and expensive SOTA approaches by 34.6%. Our
Conjunction-Aware Word-level coreference model (CAW-coref) and code is
available at https://github.com/KarelDO/wl-coref.
","2023-10-20","2310.06165v1.pdf"
"2310.06167","Lexin Zhou","Lexin Zhou, Pablo A. Moreno-Casares, Fernando Mart\'inez-Plumed, John
  Burden, Ryan Burnell, Lucy Cheke, C\`esar Ferri, Alexandru Marcoci, Behzad
  Mehrbakhsh, Yael Moros-Daval, Se\'an \'O h\'Eigeartaigh, Danaja Rutar, Wout
  Schellaert, Konstantinos Voudouris, Jos\'e Hern\'andez-Orallo","Predictable Artificial Intelligence","11 pages excluding references, 4 figures, and 2 tables. Paper Under
  Review","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  We introduce the fundamental ideas and challenges of Predictable AI, a
nascent research area that explores the ways in which we can anticipate key
indicators of present and future AI ecosystems. We argue that achieving
predictability is crucial for fostering trust, liability, control, alignment
and safety of AI ecosystems, and thus should be prioritised over performance.
While distinctive from other areas of technical and non-technical AI research,
the questions, hypotheses and challenges relevant to Predictable AI were yet to
be clearly described. This paper aims to elucidate them, calls for identifying
paths towards AI predictability and outlines the potential impact of this
emergent field.
","2023-10-11","2310.06167v1.pdf"
"2310.06176","Yinlam Chow","Jihwan Jeong, Yinlam Chow, Guy Tennenholtz, Chih-Wei Hsu, Azamat
  Tulepbergenov, Mohammad Ghavamzadeh, Craig Boutilier","Factual and Personalized Recommendations using Language Models and
  Reinforcement Learning","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recommender systems (RSs) play a central role in connecting users to content,
products, and services, matching candidate items to users based on their
preferences. While traditional RSs rely on implicit user feedback signals,
conversational RSs interact with users in natural language. In this work, we
develop a comPelling, Precise, Personalized, Preference-relevant language model
(P4LM) that recommends items to users while putting emphasis on explaining item
characteristics and their relevance. P4LM uses the embedding space
representation of a user's preferences to generate compelling responses that
are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we
develop a joint reward function that measures precision, appeal, and
personalization, which we use as AI-based feedback in a reinforcement
learning-based language model framework. Using the MovieLens 25M dataset, we
demonstrate that P4LM delivers compelling, personalized movie narratives to
users.
","2023-10-11","2310.06176v1.pdf"
"2310.06200","Tuomas Oikarinen","Justin Lee, Tuomas Oikarinen, Arjun Chatha, Keng-Chi Chang, Yilan
  Chen, Tsui-Wei Weng","The Importance of Prompt Tuning for Automated Neuron Explanations","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances have greatly increased the capabilities of large language
models (LLMs), but our understanding of the models and their safety has not
progressed as fast. In this paper we aim to understand LLMs deeper by studying
their individual neurons. We build upon previous work showing large language
models such as GPT-4 can be useful in explaining what each neuron in a language
model does. Specifically, we analyze the effect of the prompt used to generate
explanations and show that reformatting the explanation prompt in a more
natural way can significantly improve neuron explanation quality and greatly
reduce computational cost. We demonstrate the effects of our new prompts in
three different ways, incorporating both automated and human evaluations.
","2023-10-12","2310.06200v1.pdf"
"2310.06201","Yucheng Li","Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin","Compressing Context to Enhance Inference Efficiency of Large Language
  Models","EMNLP 2023. arXiv admin note: substantial text overlap with
  arXiv:2304.12102; text overlap with arXiv:2303.11076 by other authors","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) achieved remarkable performance across various
tasks. However, they face challenges in managing long documents and extended
conversations, due to significantly increased computational requirements, both
in memory and inference time, and potential context truncation when the input
exceeds the LLM's fixed context length. This paper proposes a method called
Selective Context that enhances the inference efficiency of LLMs by identifying
and pruning redundancy in the input context to make the input more compact. We
test our approach using common data sources requiring long context processing:
arXiv papers, news articles, and long conversations, on tasks of summarisation,
question answering, and response generation. Experimental results show that
Selective Context significantly reduces memory cost and decreases generation
latency while maintaining comparable performance compared to that achieved when
full context is used. Specifically, we achieve a 50\% reduction in context
cost, resulting in a 36\% reduction in inference memory usage and a 32\%
reduction in inference time, while observing only a minor drop of .023 in
BERTscore and .038 in faithfulness on four downstream applications, indicating
that our method strikes a good balance between efficiency and performance.
","2023-10-11","2310.06201v1.pdf"
"2310.06202","Saranya Venkatraman","Saranya Venkatraman, Adaku Uchendu, Dongwon Lee","GPT-who: An Information Density-based Machine-Generated Text Detector","8 pages","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The Uniform Information Density principle posits that humans prefer to spread
information evenly during language production. In this work, we examine if the
UID principle can help capture differences between Large Language Models (LLMs)
and human-generated text. We propose GPT-who, the first
psycholinguistically-aware multi-class domain-agnostic statistical-based
detector. This detector employs UID-based features to model the unique
statistical signature of each LLM and human author for accurate authorship
attribution. We evaluate our method using 4 large-scale benchmark datasets and
find that GPT-who outperforms state-of-the-art detectors (both statistical- &
non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by
over $20$% across domains. In addition to superior performance, it is
computationally inexpensive and utilizes an interpretable representation of
text articles. We present the largest analysis of the UID-based representations
of human and machine-generated texts (over 400k articles) to demonstrate how
authors distribute information differently, and in ways that enable their
detection using an off-the-shelf LM without any fine-tuning. We find that
GPT-who can distinguish texts generated by very sophisticated LLMs, even when
the overlying text is indiscernible.
","2023-10-11","2310.06202v1.pdf"
"2310.06213","Rohin Manvi","Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell,
  Stefano Ermon","GeoLLM: Extracting Geospatial Knowledge from Large Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  The application of machine learning (ML) in a range of geospatial tasks is
increasingly common but often relies on globally available covariates such as
satellite imagery that can either be expensive or lack predictive power. Here
we explore the question of whether the vast amounts of knowledge found in
Internet language corpora, now compressed within large language models (LLMs),
can be leveraged for geospatial prediction tasks. We first demonstrate that
LLMs embed remarkable spatial information about locations, but naively querying
LLMs using geographic coordinates alone is ineffective in predicting key
indicators like population density. We then present GeoLLM, a novel method that
can effectively extract geospatial knowledge from LLMs with auxiliary map data
from OpenStreetMap. We demonstrate the utility of our approach across multiple
tasks of central interest to the international community, including the
measurement of population density and economic livelihoods. Across these tasks,
our method demonstrates a 70% improvement in performance (measured using
Pearson's $r^2$) relative to baselines that use nearest neighbors or use
information directly from the prompt, and performance equal to or exceeding
satellite-based benchmarks in the literature. With GeoLLM, we observe that
GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting
that the performance of our method scales well with the size of the model and
its pretraining dataset. Our experiments reveal that LLMs are remarkably
sample-efficient, rich in geospatial information, and robust across the globe.
Crucially, GeoLLM shows promise in mitigating the limitations of existing
geospatial covariates and complementing them well.
","2023-10-11","2310.06213v1.pdf"
"2310.06214","Eslam Bakr","Eslam Mohamed Bakr, Mohamed Ayman, Mahmoud Ahmed, Habib Slim, Mohamed
  Elhoseiny","CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding","","","","","cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  3D visual grounding is the ability to localize objects in 3D scenes
conditioned by utterances. Most existing methods devote the referring head to
localize the referred object directly, causing failure in complex scenarios. In
addition, it does not illustrate how and why the network reaches the final
decision. In this paper, we address this question Can we design an
interpretable 3D visual grounding framework that has the potential to mimic the
human perception system?. To this end, we formulate the 3D visual grounding
problem as a sequence-to-sequence task by first predicting a chain of anchors
and then the final target. Interpretability not only improves the overall
performance but also helps us identify failure cases. Following the chain of
thoughts approach enables us to decompose the referring task into interpretable
intermediate steps, boosting the performance and making our framework extremely
data-efficient. Moreover, our proposed framework can be easily integrated into
any existing architecture. We validate our approach through comprehensive
experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent
performance gains compared to existing methods without requiring manually
annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is
significantly data-efficient, whereas on the Sr3D dataset, when trained only on
10% of the data, we match the SOTA performance that trained on the entire data.
","2023-10-11","2310.06214v1.pdf"
"2310.06225","Bruno Silva","Bruno Silva, Leonardo Nunes, Roberto Estev\~ao, Vijay Aski, Ranveer
  Chandra","GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using
  Large Language Models","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.
","2023-10-13","2310.06225v1.pdf"
"2310.06226","K Niranjan Kumar","K. Niranjan Kumar and Irfan Essa and Sehoon Ha","Words into Action: Learning Diverse Humanoid Robot Behaviors using
  Language Guided Iterative Motion Refinement","","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  Humanoid robots are well suited for human habitats due to their morphological
similarity, but developing controllers for them is a challenging task that
involves multiple sub-problems, such as control, planning and perception. In
this paper, we introduce a method to simplify controller design by enabling
users to train and fine-tune robot control policies using natural language
commands. We first learn a neural network policy that generates behaviors given
a natural language command, such as ""walk forward"", by combining Large Language
Models (LLMs), motion retargeting, and motion imitation. Based on the
synthesized motion, we iteratively fine-tune by updating the text prompt and
querying LLMs to find the best checkpoint associated with the closest motion in
history. We validate our approach using a simulated Digit humanoid robot and
demonstrate learning of diverse motions, such as walking, hopping, and kicking,
without the burden of complex reward engineering. In addition, we show that our
iterative refinement enables us to learn 3x times faster than a naive
formulation that learns from scratch.
","2023-10-11","2310.06226v1.pdf"
"2310.06228","Information-Technology Promotion Agency Japan","Masahiro Yamamoto","Evolution of Natural Language Processing Technology: Not Just Language
  Processing Towards General Purpose AI","40 pages","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Since the invention of computers, communication through natural language
(actual human language) has been a dream technology. However, natural language
is extremely difficult to mathematically formulate, making it difficult to
realize as an algorithm without considering programming. While there have been
numerous technological developments, one cannot say that any results allowing
free utilization have been achieved thus far. In the case of language learning
in humans, for instance when learning one's mother tongue or foreign language,
one must admit that this process is similar to the adage ""practice makes
perfect"" in principle, even though the learning method is significant up to a
point. Deep learning has played a central role in contemporary AI technology in
recent years. When applied to natural language processing (NLP), this produced
unprecedented results. Achievements exceeding the initial predictions have been
reported from the results of learning vast amounts of textual data using deep
learning. For instance, four arithmetic operations could be performed without
explicit learning, thereby enabling the explanation of complex images and the
generation of images from corresponding explanatory texts. It is an accurate
example of the learner embodying the concept of ""practice makes perfect"" by
using vast amounts of textual data. This report provides a technological
explanation of how cutting-edge NLP has made it possible to realize the
""practice makes perfect"" principle. Additionally, examples of how this can be
applied to business are provided. We reported in June 2022 in Japanese on the
NLP movement from late 2021 to early 2022. We would like to summarize this as a
memorandum since this is just the initial movement leading to the current large
language models (LLMs).
","2023-10-11","2310.06228v1.pdf"
"2310.06234","Wei Dong","Wei Dong, Dawei Yan, Zhijun Lin, Peng Wang","Efficient Adaptation of Large Vision Transformer via Adapter
  Re-Composing","Paper is accepted to NeurIPS 2023","","","","cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advent of high-capacity pre-trained models has revolutionized
problem-solving in computer vision, shifting the focus from training
task-specific models to adapting pre-trained models. Consequently, effectively
adapting large pre-trained models to downstream tasks in an efficient manner
has become a prominent research area. Existing solutions primarily concentrate
on designing lightweight adapters and their interaction with pre-trained
models, with the goal of minimizing the number of parameters requiring updates.
In this study, we propose a novel Adapter Re-Composing (ARC) strategy that
addresses efficient pre-trained model adaptation from a fresh perspective. Our
approach considers the reusability of adaptation parameters and introduces a
parameter-sharing scheme. Specifically, we leverage symmetric
down-/up-projections to construct bottleneck operations, which are shared
across layers. By learning low-dimensional re-scaling coefficients, we can
effectively re-compose layer-adaptive adapters. This parameter-sharing strategy
in adapter design allows us to significantly reduce the number of new
parameters while maintaining satisfactory performance, thereby offering a
promising approach to compress the adaptation cost. We conduct experiments on
24 downstream image classification tasks using various Vision Transformer
variants to evaluate our method. The results demonstrate that our approach
achieves compelling transfer learning performance with a reduced parameter
count. Our code is available at
\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.
","2023-10-11","2310.06234v1.pdf"
"2310.06235","Nebiyou Yismaw","Nebiyou Yismaw, Ulugbek S. Kamilov, M. Salman Asif","Domain Expansion via Network Adaptation for Solving Inverse Problems","","","","","eess.IV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep learning-based methods deliver state-of-the-art performance for solving
inverse problems that arise in computational imaging. These methods can be
broadly divided into two groups: (1) learn a network to map measurements to the
signal estimate, which is known to be fragile; (2) learn a prior for the signal
to use in an optimization-based recovery. Despite the impressive results from
the latter approach, many of these methods also lack robustness to shifts in
data distribution, measurements, and noise levels. Such domain shifts result in
a performance gap and in some cases introduce undesired artifacts in the
estimated signal. In this paper, we explore the qualitative and quantitative
effects of various domain shifts and propose a flexible and parameter efficient
framework that adapt pretrained networks to such shifts. We demonstrate the
effectiveness of our method for a number of natural image, MRI, and CT
reconstructions tasks under domain, measurement model, and noise-level shifts.
Our experiments demonstrate that our method provides significantly better
performance and parameter efficiency compared to existing domain adaptation
techniques.
","2023-10-11","2310.06235v1.pdf"
"2310.06239","Cheng Peng","Cheng Peng, Xi Yang, Kaleb E Smith, Zehao Yu, Aokun Chen, Jiang Bian,
  Yonghui Wu","Model Tuning or Prompt Tuning? A Study of Large Language Models for
  Clinical Concept and Relation Extraction","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Objective To develop soft prompt-based learning algorithms for large language
models (LLMs), examine the shape of prompts, prompt-tuning using
frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.
Methods We developed a soft prompt-based LLM model and compared 4 training
strategies including (1) fine-tuning without prompts; (2) hard-prompt with
unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with
frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for
clinical concept and relation extraction on two benchmark datasets. We
evaluated the transfer learning ability of the prompt-based learning algorithms
in a cross-institution setting. We also assessed the few-shot learning ability.
Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft
prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept
extraction, outperforming the traditional fine-tuning and hard prompt-based
models by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft
prompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end
relation extraction, outperforming the other two models by 0.2~2% and
0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million
parameters) LLMs have a big gap to be competitive with unfrozen models; scaling
LLMs up to billions of parameters makes frozen LLMs competitive with unfrozen
LLMs. For cross-institute evaluation, soft prompting with a frozen
GatorTron-8.9B model achieved the best performance. This study demonstrates
that (1) machines can learn soft prompts better than humans, (2) frozen LLMs
have better few-shot learning ability and transfer learning ability to
facilitate muti-institution applications, and (3) frozen LLMs require large
models.
","2023-10-11","2310.06239v1.pdf"
"2310.06245","Benjamin Kane","Benjamin Kane and Lenhart Schubert","We are what we repeatedly do: Inducing and deploying habitual schemas in
  persona-based responses","","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Many practical applications of dialogue technology require the generation of
responses according to a particular developer-specified persona. While a
variety of personas can be elicited from recent large language models, the
opaqueness and unpredictability of these models make it desirable to be able to
specify personas in an explicit form. In previous work, personas have typically
been represented as sets of one-off pieces of self-knowledge that are retrieved
by the dialogue system for use in generation. However, in realistic human
conversations, personas are often revealed through story-like narratives that
involve rich habitual knowledge -- knowledge about kinds of events that an
agent often participates in (e.g., work activities, hobbies, sporting
activities, favorite entertainments, etc.), including typical goals,
sub-events, preconditions, and postconditions of those events. We capture such
habitual knowledge using an explicit schema representation, and propose an
approach to dialogue generation that retrieves relevant schemas to condition a
large language model to generate persona-based responses. Furthermore, we
demonstrate a method for bootstrapping the creation of such schemas by first
generating generic passages from a set of simple facts, and then inducing
schemas from the generated passages.
","2023-10-11","2310.06245v1.pdf"
"2310.06248","Prashanth Gurunath Shivakumar","Prashanth Gurunath Shivakumar, Jari Kolehmainen, Yile Gu, Ankur
  Gandhe, Ariya Rastrow, Ivan Bulyko","Discriminative Speech Recognition Rescoring with Pre-trained Language
  Models","ASRU 2023","","","","eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Second pass rescoring is a critical component of competitive automatic speech
recognition (ASR) systems. Large language models have demonstrated their
ability in using pre-trained information for better rescoring of ASR
hypothesis. Discriminative training, directly optimizing the minimum
word-error-rate (MWER) criterion typically improves rescoring. In this study,
we propose and explore several discriminative fine-tuning schemes for
pre-trained LMs. We propose two architectures based on different pooling
strategies of output embeddings and compare with probability based MWER. We
conduct detailed comparisons between pre-trained causal and bidirectional LMs
in discriminative settings. Experiments on LibriSpeech demonstrate that all
MWER training schemes are beneficial, giving additional gains upto 8.5\% WER.
Proposed pooling variants achieve lower latency while retaining most
improvements. Finally, our study concludes that bidirectionality is better
utilized with discriminative training.
","2023-10-11","2310.06248v1.pdf"
"2310.06254","Benjamin Kane","Benjamin Kane and Lenhart Schubert","Get the gist? Using large language models for few-shot
  decontextualization","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In many NLP applications that involve interpreting sentences within a rich
context -- for instance, information retrieval systems or dialogue systems --
it is desirable to be able to preserve the sentence in a form that can be
readily understood without context, for later reuse -- a process known as
``decontextualization''. While previous work demonstrated that generative
Seq2Seq models could effectively perform decontextualization after being
fine-tuned on a specific dataset, this approach requires expensive human
annotations and may not transfer to other domains. We propose a few-shot method
of decontextualization using a large language model, and present preliminary
results showing that this method achieves viable performance on multiple
domains using only a small set of examples.
","2023-10-11","2310.06254v1.pdf"
"2310.06257","Amisha Srivastava","Amisha Srivastava, Sanjay Das, Navnil Choudhury, Rafail Psiakis, Pedro
  Henrique Silva, Debjit Pal, Kanad Basu","SCAR: Power Side-Channel Analysis at RTL-Level","","","","","cs.CR cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Power side-channel attacks exploit the dynamic power consumption of
cryptographic operations to leak sensitive information of encryption hardware.
Therefore, it is necessary to conduct power side-channel analysis for assessing
the susceptibility of cryptographic systems and mitigating potential risks.
Existing power side-channel analysis primarily focuses on post-silicon
implementations, which are inflexible in addressing design flaws, leading to
costly and time-consuming post-fabrication design re-spins. Hence, pre-silicon
power side-channel analysis is required for early detection of vulnerabilities
to improve design robustness. In this paper, we introduce SCAR, a novel
pre-silicon power side-channel analysis framework based on Graph Neural
Networks (GNN). SCAR converts register-transfer level (RTL) designs of
encryption hardware into control-data flow graphs and use that to detect the
design modules susceptible to side-channel leakage. Furthermore, we incorporate
a deep learning-based explainer in SCAR to generate quantifiable and
human-accessible explanation of our detection and localization decisions. We
have also developed a fortification component as a part of SCAR that uses
large-language models (LLM) to automatically generate and insert additional
design code at the localized zone to shore up the side-channel leakage. When
evaluated on popular encryption algorithms like AES, RSA, and PRESENT, and
postquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR,
achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall.
Additionally, through explainability analysis, SCAR reduces features for GNN
model training by 57% while maintaining comparable accuracy. We believe that
SCAR will transform the security-critical hardware design cycle, resulting in
faster design closure at a reduced design cost.
","2023-10-11","2310.06257v1.pdf"
"2310.06260","Arthur dos Santos","Arthur dos Santos, Jayr Pereira, Rodrigo Nogueira, Bruno Masiero,
  Shiva Sander-Tavallaey, Elias Zea","An experiment on an automated literature survey of data-driven speech
  enhancement methods","","","","","cs.SD cs.CL eess.AS","http://creativecommons.org/licenses/by/4.0/","  The increasing number of scientific publications in acoustics, in general,
presents difficulties in conducting traditional literature surveys. This work
explores the use of a generative pre-trained transformer (GPT) model to
automate a literature survey of 116 articles on data-driven speech enhancement
methods. The main objective is to evaluate the capabilities and limitations of
the model in providing accurate responses to specific queries about the papers
selected from a reference human-based survey. While we see great potential to
automate literature surveys in acoustics, improvements are needed to address
technical questions more clearly and accurately.
","2023-10-11","2310.06260v1.pdf"
"2310.06269","Michael Feffer","Michael Feffer, Nikolas Martelaro, and Hoda Heidari","The AI Incident Database as an Educational Tool to Raise Awareness of AI
  Harms: A Classroom Exploration of Efficacy, Limitations, & Future
  Improvements","37 pages, 11 figures; To appear in the proceedings of EAAMO 2023","","","","cs.CY cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Prior work has established the importance of integrating AI ethics topics
into computer and data sciences curricula. We provide evidence suggesting that
one of the critical objectives of AI Ethics education must be to raise
awareness of AI harms. While there are various sources to learn about such
harms, The AI Incident Database (AIID) is one of the few attempts at offering a
relatively comprehensive database indexing prior instances of harms or near
harms stemming from the deployment of AI technologies in the real world. This
study assesses the effectiveness of AIID as an educational tool to raise
awareness regarding the prevalence and severity of AI harms in socially
high-stakes domains. We present findings obtained through a classroom study
conducted at an R1 institution as part of a course focused on the societal and
ethical considerations around AI and ML. Our qualitative findings characterize
students' initial perceptions of core topics in AI ethics and their desire to
close the educational gap between their technical skills and their ability to
think systematically about ethical and societal aspects of their work. We find
that interacting with the database helps students better understand the
magnitude and severity of AI harms and instills in them a sense of urgency
around (a) designing functional and safe AI and (b) strengthening governance
and accountability mechanisms. Finally, we compile students' feedback about the
tool and our class activity into actionable recommendations for the database
development team and the broader community to improve awareness of AI harms in
AI ethics education.
","2023-10-11","2310.06269v1.pdf"
"2310.06271","Ziwei Ji","Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung","Towards Mitigating Hallucination in Large Language Models via
  Self-Reflection","Accepted by the findings of EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have shown promise for generative and
knowledge-intensive tasks including question-answering (QA) tasks. However, the
practical deployment still faces challenges, notably the issue of
""hallucination"", where models generate plausible-sounding but unfaithful or
nonsensical information. This issue becomes particularly critical in the
medical domain due to the uncommon professional concepts and potential social
risks involved. This paper analyses the phenomenon of hallucination in medical
generative QA systems using widely adopted LLMs and datasets. Our investigation
centers on the identification and comprehension of common problematic answers,
with a specific emphasis on hallucination. To tackle this challenge, we present
an interactive self-reflection methodology that incorporates knowledge
acquisition and answer generation. Through this feedback process, our approach
steadily enhances the factuality, consistency, and entailment of the generated
answers. Consequently, we harness the interactivity and multitasking ability of
LLMs and produce progressively more precise and accurate answers. Experimental
results on both automatic and human evaluation demonstrate the superiority of
our approach in hallucination reduction compared to baselines.
","2023-10-11","2310.06271v1.pdf"
"2310.06272","Chau Pham","Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo
  Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang","Let Models Speak Ciphers: Multiagent Debate through Embeddings","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Discussion and debate among Large Language Models (LLMs) have gained
considerable attention due to their potential to enhance the reasoning ability
of LLMs. Although natural language is an obvious choice for communication due
to LLM's language understanding capability, the token sampling step needed when
generating natural language poses a potential risk of information loss, as it
uses only one token to represent the model's belief across the entire
vocabulary. In this paper, we introduce a communication regime named CIPHER
(Communicative Inter-Model Protocol Through Embedding Representation) to
address this issue. Specifically, we remove the token sampling step from LLMs
and let them communicate their beliefs across the vocabulary through the
expectation of the raw transformer output embeddings. Remarkably, by deviating
from natural language, CIPHER offers an advantage of encoding a broader
spectrum of information without any modification to the model weights. While
the state-of-the-art LLM debate methods using natural language outperforms
traditional inference by a margin of 1.5-8%, our experiment results show that
CIPHER debate further extends this lead by 1-3.5% across five reasoning tasks
and multiple open-source LLMs of varying sizes. This showcases the superiority
and robustness of embeddings as an alternative ""language"" for communication
among LLMs.
","2023-10-11","2310.06272v1.pdf"
"2310.06278","Haoxiang Luo","Haoxiang Luo, Jian Luo, Athanasios V. Vasilakos","BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large
  Language Models","","","","","cs.NI cs.AI cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  In recent years, artificial intelligence (AI) and machine learning (ML) are
reshaping society's production methods and productivity, and also changing the
paradigm of scientific research. Among them, the AI language model represented
by ChatGPT has made great progress. Such large language models (LLMs) serve
people in the form of AI-generated content (AIGC) and are widely used in
consulting, healthcare, and education. However, it is difficult to guarantee
the authenticity and reliability of AIGC learning data. In addition, there are
also hidden dangers of privacy disclosure in distributed AI training. Moreover,
the content generated by LLMs is difficult to identify and trace, and it is
difficult to cross-platform mutual recognition. The above information security
issues in the coming era of AI powered by LLMs will be infinitely amplified and
affect everyone's life. Therefore, we consider empowering LLMs using blockchain
technology with superior security features to propose a vision for trusted AI.
This paper mainly introduces the motivation and technical route of blockchain
for LLM (BC4LLM), including reliable learning corpus, secure training process,
and identifiable generated content. Meanwhile, this paper also reviews the
potential applications and future challenges, especially in the frontier
communication networks field, including network resource allocation, dynamic
spectrum sharing, and semantic communication. Based on the above work combined
and the prospect of blockchain and LLMs, it is expected to help the early
realization of trusted AI and provide guidance for the academic community.
","2023-10-11","2310.06278v1.pdf"
"2310.06282","Zhikang Dong","Zhikang Dong, Bin Chen, Xiulong Liu, Pawel Polak, Peng Zhang","MuseChat: A Conversational Music Recommendation System for Videos","","","","","cs.LG cs.CV cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce MuseChat, an innovative dialog-based music recommendation
system. This unique platform not only offers interactive user engagement but
also suggests music tailored for input videos, so that users can refine and
personalize their music selections. In contrast, previous systems predominantly
emphasized content compatibility, often overlooking the nuances of users'
individual preferences. For example, all the datasets only provide basic
music-video pairings or such pairings with textual music descriptions. To
address this gap, our research offers three contributions. First, we devise a
conversation-synthesis method that simulates a two-turn interaction between a
user and a recommendation system, which leverages pre-trained music tags and
artist information. In this interaction, users submit a video to the system,
which then suggests a suitable music piece with a rationale. Afterwards, users
communicate their musical preferences, and the system presents a refined music
recommendation with reasoning. Second, we introduce a multi-modal
recommendation engine that matches music either by aligning it with visual cues
from the video or by harmonizing visual information, feedback from previously
recommended music, and the user's textual input. Third, we bridge music
representations and textual data with a Large Language Model(Vicuna-7B). This
alignment equips MuseChat to deliver music recommendations and their underlying
reasoning in a manner resembling human communication. Our evaluations show that
MuseChat surpasses existing state-of-the-art models in music retrieval tasks
and pioneers the integration of the recommendation process within a natural
language framework.
","2023-10-12","2310.06282v1.pdf"
"2310.06290","Xueyang Li","Tian Yan, Xueyang Li, Sifat Ut Taki, Saeid Mehrdad","Gem5Pred: Predictive Approaches For Gem5 Simulation Time","","","","","cs.AR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Gem5, an open-source, flexible, and cost-effective simulator, is widely
recognized and utilized in both academic and industry fields for hardware
simulation. However, the typically time-consuming nature of simulating programs
on Gem5 underscores the need for a predictive model that can estimate
simulation time. As of now, no such dataset or model exists. In response to
this gap, this paper makes a novel contribution by introducing a unique dataset
specifically created for this purpose. We also conducted analysis of the
effects of different instruction types on the simulation time in Gem5. After
this, we employ three distinct models leveraging CodeBERT to execute the
prediction task based on the developed dataset. Our superior regression model
achieves a Mean Absolute Error (MAE) of 0.546, while our top-performing
classification model records an Accuracy of 0.696. Our models establish a
foundation for future investigations on this topic, serving as benchmarks
against which subsequent models can be compared. We hope that our contribution
can simulate further research in this field. The dataset we used is available
at https://github.com/XueyangLiOSU/Gem5Pred.
","2023-10-11","2310.06290v1.pdf"
"2310.06301","Daniel Murfet","Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, Daniel Murfet","Dynamical versus Bayesian Phase Transitions in a Toy Model of
  Superposition","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  We investigate phase transitions in a Toy Model of Superposition (TMS) using
Singular Learning Theory (SLT). We derive a closed formula for the theoretical
loss and, in the case of two hidden dimensions, discover that regular $k$-gons
are critical points. We present supporting theory indicating that the local
learning coefficient (a geometric invariant) of these $k$-gons determines phase
transitions in the Bayesian posterior as a function of training sample size. We
then show empirically that the same $k$-gon critical points also determine the
behavior of SGD training. The picture that emerges adds evidence to the
conjecture that the SGD learning trajectory is subject to a sequential learning
mechanism. Specifically, we find that the learning process in TMS, be it
through SGD or Bayesian learning, can be characterized by a journey through
parameter space from regions of high loss and low complexity to regions of low
loss and high complexity.
","2023-10-11","2310.06301v1.pdf"
"2310.06302","Shuaichen Chang","Shuaichen Chang, Eric Fosler-Lussier","Selective Demonstrations for Cross-domain Text-to-SQL","EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) with in-context learning have demonstrated
impressive generalization capabilities in the cross-domain text-to-SQL task,
without the use of in-domain annotations. However, incorporating in-domain
demonstration examples has been found to greatly enhance LLMs' performance. In
this paper, we delve into the key factors within in-domain examples that
contribute to the improvement and explore whether we can harness these benefits
without relying on in-domain annotations. Based on our findings, we propose a
demonstration selection framework ODIS which utilizes both out-of-domain
examples and synthetically generated in-domain examples to construct
demonstrations. By retrieving demonstrations from hybrid sources, ODIS
leverages the advantages of both, showcasing its effectiveness compared to
baseline methods that rely on a single data source. Furthermore, ODIS
outperforms state-of-the-art approaches on two cross-domain text-to-SQL
datasets, with improvements of 1.1 and 11.8 points in execution accuracy,
respectively.
","2023-10-11","2310.06302v1.pdf"
"2310.06303","Carson Stark","Carson Stark, Bohkyung Chun, Casey Charleston, Varsha Ravi, Luis
  Pabon, Surya Sunkari, Tarun Mohan, Peter Stone, and Justin Hart","Dobby: A Conversational Service Robot Driven by GPT-4","","","","","cs.RO cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work introduces a robotics platform which embeds a conversational AI
agent in an embodied system for natural language understanding and intelligent
decision-making for service tasks; integrating task planning and human-like
conversation. The agent is derived from a large language model, which has
learned from a vast corpus of general knowledge. In addition to generating
dialogue, this agent can interface with the physical world by invoking commands
on the robot; seamlessly merging communication and behavior. This system is
demonstrated in a free-form tour-guide scenario, in an HRI study combining
robots with and without conversational AI capabilities. Performance is measured
along five dimensions: overall effectiveness, exploration abilities,
scrutinization abilities, receptiveness to personification, and adaptability.
","2023-10-11","2310.06303v1.pdf"
"2310.06310","Laura Plein","Laura Plein, Tegawend\'e F. Bissyand\'e","Can LLMs Demystify Bug Reports?","","","","","cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Bugs are notoriously challenging: they slow down software users and result in
time-consuming investigations for developers. These challenges are exacerbated
when bugs must be reported in natural language by users. Indeed, we lack
reliable tools to automatically address reported bugs (i.e., enabling their
analysis, reproduction, and bug fixing). With the recent promises created by
LLMs such as ChatGPT for various tasks, including in software engineering, we
ask ourselves: What if ChatGPT could understand bug reports and reproduce them?
This question will be the main focus of this study. To evaluate whether ChatGPT
is capable of catching the semantics of bug reports, we used the popular
Defects4J benchmark with its bug reports. Our study has shown that ChatGPT was
able to demystify and reproduce 50% of the reported bugs. ChatGPT being able to
automatically address half of the reported bugs shows promising potential in
the direction of applying machine learning to address bugs with only a
human-in-the-loop to report the bug.
","2023-10-11","2310.06310v1.pdf"
"2310.06311","Song Wen","Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao Dong, Dimitris
  Metaxas","Improving Compositional Text-to-image Generation with Large
  Vision-Language Models","","","","","cs.CV cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in text-to-image models, particularly diffusion models,
have shown significant promise. However, compositional text-to-image models
frequently encounter difficulties in generating high-quality images that
accurately align with input texts describing multiple objects, variable
attributes, and intricate spatial relationships. To address this limitation, we
employ large vision-language models (LVLMs) for multi-dimensional assessment of
the alignment between generated images and their corresponding input texts.
Utilizing this assessment, we fine-tune the diffusion model to enhance its
alignment capabilities. During the inference phase, an initial image is
produced using the fine-tuned diffusion model. The LVLM is then employed to
pinpoint areas of misalignment in the initial image, which are subsequently
corrected using the image editing algorithm until no further misalignments are
detected by the LVLM. The resultant image is consequently more closely aligned
with the input text. Our experimental results validate that the proposed
methodology significantly improves text-image alignment in compositional image
generation, particularly with respect to object number, attribute binding,
spatial relationships, and aesthetic quality.
","2023-10-11","2310.06311v1.pdf"
"2310.06320","Laura Plein","Laura Plein, Wendk\^uuni C. Ou\'edraogo, Jacques Klein, Tegawend\'e F.
  Bissyand\'e","Automatic Generation of Test Cases based on Bug Reports: a Feasibility
  Study with Large Language Models","","","","","cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Software testing is a core discipline in software engineering where a large
array of research results has been produced, notably in the area of automatic
test generation. Because existing approaches produce test cases that either can
be qualified as simple (e.g. unit tests) or that require precise
specifications, most testing procedures still rely on test cases written by
humans to form test suites. Such test suites, however, are incomplete: they
only cover parts of the project or they are produced after the bug is fixed.
Yet, several research challenges, such as automatic program repair, and
practitioner processes, build on the assumption that available test suites are
sufficient. There is thus a need to break existing barriers in automatic test
case generation. While prior work largely focused on random unit testing
inputs, we propose to consider generating test cases that realistically
represent complex user execution scenarios, which reveal buggy behaviour. Such
scenarios are informally described in bug reports, which should therefore be
considered as natural inputs for specifying bug-triggering test cases. In this
work, we investigate the feasibility of performing this generation by
leveraging large language models (LLMs) and using bug reports as inputs. Our
experiments include the use of ChatGPT, as an online service, as well as
CodeGPT, a code-related pre-trained LLM that was fine-tuned for our task.
Overall, we experimentally show that bug reports associated to up to 50% of
Defects4J bugs can prompt ChatGPT to generate an executable test case. We show
that even new bug reports can indeed be used as input for generating executable
test cases. Finally, we report experimental results which confirm that
LLM-generated test cases are immediately useful in software engineering tasks
such as fault localization as well as patch validation in automated program
repair.
","2023-10-11","2310.06320v1.pdf"
"2310.06347","Jingyang Zhang","Jingyang Zhang, Shiwei Li, Yuanxun Lu, Tian Fang, David McKinnon,
  Yanghai Tsin, Long Quan, Yao Yao","JointNet: Extending Text-to-Image Diffusion for Dense Distribution
  Modeling","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce JointNet, a novel neural network architecture for modeling the
joint distribution of images and an additional dense modality (e.g., depth
maps). JointNet is extended from a pre-trained text-to-image diffusion model,
where a copy of the original network is created for the new dense modality
branch and is densely connected with the RGB branch. The RGB branch is locked
during network fine-tuning, which enables efficient learning of the new
modality distribution while maintaining the strong generalization ability of
the large-scale pre-trained diffusion model. We demonstrate the effectiveness
of JointNet by using RGBD diffusion as an example and through extensive
experiments, showcasing its applicability in a variety of applications,
including joint RGBD generation, dense depth prediction, depth-conditioned
image generation, and coherent tile-based 3D panorama generation.
","2023-10-11","2310.06347v1.pdf"
"2310.06356","Aiwei Liu","Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng and Lijie Wen","A Semantic Invariant Robust Watermark for Large Language Models","16 pages, 9 figures, 2 tables","","","","cs.CR cs.CL","http://creativecommons.org/licenses/by/4.0/","  Watermark algorithms for large language models (LLMs) have achieved extremely
high accuracy in detecting text generated by LLMs. Such algorithms typically
involve adding extra watermark logits to the LLM's logits at each generation
step. However, prior algorithms face a trade-off between attack robustness and
security robustness. This is because the watermark logits for a token are
determined by a certain number of preceding tokens; a small number leads to low
security robustness, while a large number results in insufficient attack
robustness. In this work, we propose a semantic invariant watermarking method
for LLMs that provides both attack robustness and security robustness. The
watermark logits in our work are determined by the semantics of all preceding
tokens. Specifically, we utilize another embedding LLM to generate semantic
embeddings for all preceding tokens, and then these semantic embeddings are
transformed into the watermark logits through our trained watermark model.
Subsequent analyses and experiments demonstrated the attack robustness of our
method in semantically invariant settings: synonym substitution and text
paraphrasing settings. Finally, we also show that our watermark possesses
adequate security robustness. Our code and data are available at
https://github.com/THU-BPM/Robust_Watermark.
","2023-10-11","2310.06356v1.pdf"
"2310.06361","Kimon Kieslich","Kimon Kieslich, Nicholas Diakopoulos, Natali Helberger","Anticipating Impacts: Using Large-Scale Scenario Writing to Explore
  Diverse Implications of Generative AI in the News Environment","","","","","cs.CY cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The tremendous rise of generative AI has reached every part of society -
including the news environment. There are many concerns about the individual
and societal impact of the increasing use of generative AI, including issues
such as disinformation and misinformation, discrimination, and the promotion of
social tensions. However, research on anticipating the impact of generative AI
is still in its infancy and mostly limited to the views of technology
developers and/or researchers. In this paper, we aim to broaden the perspective
and capture the expectations of three stakeholder groups (news consumers;
technology developers; content creators) about the potential negative impacts
of generative AI, as well as mitigation strategies to address these.
Methodologically, we apply scenario writing and use participatory foresight in
the context of a survey (n=119) to delve into cognitively diverse imaginations
of the future. We qualitatively analyze the scenarios using thematic analysis
to systematically map potential impacts of generative AI on the news
environment, potential mitigation strategies, and the role of stakeholders in
causing and mitigating these impacts. In addition, we measure respondents'
opinions on a specific mitigation strategy, namely transparency obligations as
suggested in Article 52 of the draft EU AI Act. We compare the results across
different stakeholder groups and elaborate on the (non-) presence of different
expected impacts across these groups. We conclude by discussing the usefulness
of scenario-writing and participatory foresight as a toolbox for generative AI
impact assessment.
","2023-10-11","2310.06361v1.pdf"
"2310.06374","Di Wu","Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang","Rethinking Model Selection and Decoding for Keyphrase Generation with
  Pre-trained Sequence-to-Sequence Models","EMNLP 2023 camera ready","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Keyphrase Generation (KPG) is a longstanding task in NLP with widespread
applications. The advent of sequence-to-sequence (seq2seq) pre-trained language
models (PLMs) has ushered in a transformative era for KPG, yielding promising
performance improvements. However, many design decisions remain unexplored and
are often made arbitrarily. This paper undertakes a systematic analysis of the
influence of model selection and decoding strategies on PLM-based KPG. We begin
by elucidating why seq2seq PLMs are apt for KPG, anchored by an
attention-driven hypothesis. We then establish that conventional wisdom for
selecting seq2seq PLMs lacks depth: (1) merely increasing model size or
performing task-specific adaptation is not parameter-efficient; (2) although
combining in-domain pre-training with task adaptation benefits KPG, it does
partially hinder generalization. Regarding decoding, we demonstrate that while
greedy search achieves strong F1 scores, it lags in recall compared with
sampling-based methods. Based on these insights, we propose DeSel, a
likelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves
greedy search by an average of 4.7% semantic F1 across five datasets. Our
collective findings pave the way for deeper future investigations into
PLM-based KPG.
","2023-10-24","2310.06374v1.pdf"
"2310.06387","Zeming Wei","Zeming Wei, Yifei Wang, Yisen Wang","Jailbreak and Guard Aligned Language Models with Only Few In-Context
  Demonstrations","","","","","cs.LG cs.AI cs.CL cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have shown remarkable success in various tasks,
but concerns about their safety and the potential for generating malicious
content have emerged. In this paper, we explore the power of In-Context
Learning (ICL) in manipulating the alignment ability of LLMs. We find that by
providing just few in-context demonstrations without fine-tuning, LLMs can be
manipulated to increase or decrease the probability of jailbreaking, i.e.
answering malicious prompts. Based on these observations, we propose In-Context
Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding
aligned language model purposes. ICA crafts malicious contexts to guide models
in generating harmful outputs, while ICD enhances model robustness by
demonstrations of rejecting to answer harmful prompts. Our experiments show the
effectiveness of ICA and ICD in increasing or reducing the success rate of
adversarial jailbreaking attacks. Overall, we shed light on the potential of
ICL to influence LLM behavior and provide a new perspective for enhancing the
safety and alignment of LLMs.
","2023-10-11","2310.06387v1.pdf"
"2310.06391","Stefano De Paoli Prof","Stefano De Paoli","Improved prompting and process for writing user personas with LLMs,
  using qualitative interviews: Capturing behaviour and personality traits of
  users","","","","","cs.HC cs.CL cs.CY","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This draft paper presents a workflow for creating User Personas with Large
Language Models, using the results of a Thematic Analysis of qualitative
interviews. The proposed workflow uses improved prompting and a larger pool of
Themes, compared to previous work conducted by the author for the same task.
This is possible due to the capabilities of a recently released LLM which
allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to
the possibility to offer a refined prompting for the creation of Personas. The
paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then
discusses the improved workflow for creating Personas. The paper also offers
some reflections on the relationship between the proposed process and existing
approaches to Personas such as the data-driven and qualitative Personas.
Moreover, the paper offers reflections on the capacity of LLMs to capture user
behaviours and personality traits, from the underlying dataset of qualitative
interviews used for the analysis.
","2023-10-11","2310.06391v1.pdf"
"2310.06397","Kaiming Huang","Kaiming Huang, Mathias Payer, Zhiyun Qian, Jack Sampson, Gang Tan,
  Trent Jaeger","Top of the Heap: Efficient Memory Error Protection for Many Heap Objects","","","","","cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Exploits against heap memory errors continue to be a major concern. Although
many defenses have been proposed, heap data are not protected from attacks that
exploit memory errors systematically. Research defenses focus on complete
coverage of heap objects, often giving up on comprehensive memory safety
protection and/or incurring high costs in performance overhead and memory
usage. In this paper, we propose a solution for heap memory safety enforcement
that aims to provide comprehensive protection from memory errors efficiently by
protecting those heap objects whose accesses are provably safe from memory
errors. Specifically, we present the Uriah system that statically validates
spatial and type memory safety for heap objects, isolating compliant objects on
a safe heap that enforces temporal type safety to prevent attacks on memory
reuse. Using Uriah, 71.9% of heap allocation sites can be shown to produce
objects (73% of allocations are found safe) that satisfy spatial and type
safety, which are then isolated using Uriah's heap allocator from memory
accesses via unsafe heap objects. Uriah only incurs 2.9% overhead and only uses
9.3% more memory on SPEC CPU2006 (C/C++) benchmarks, showing that many heap
objects can be protected from all classes of memory errors efficiently.
","2023-10-11","2310.06397v1.pdf"
"2310.06404","Deajin Jo","Daejin Jo, Daniel Wontae Nam, Gunsoo Han, Kyoung-Woon On, Taehwan
  Kwon, Seungeun Rho, Sungwoong Kim","Hexa: Self-Improving for Knowledge-Grounded Dialogue System","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  A common practice in knowledge-grounded dialogue generation is to explicitly
utilize intermediate steps (e.g., web-search, memory retrieval) with modular
approaches. However, data for such steps are often inaccessible compared to
those of dialogue responses as they are unobservable in an ordinary dialogue.
To fill in the absence of these data, we develop a self-improving method to
improve the generative performances of intermediate steps without the ground
truth data. In particular, we propose a novel bootstrapping scheme with a
guided prompt and a modified loss function to enhance the diversity of
appropriate self-generated responses. Through experiments on various benchmark
datasets, we empirically demonstrate that our method successfully leverages a
self-improving mechanism in generating intermediate and final responses and
improves the performances on the task of knowledge-grounded dialogue
generation.
","2023-10-24","2310.06404v1.pdf"
"2310.06422","Kilian Sprenkamp","Kilian Sprenkamp, Daniel Gordon Jones, Liudmila Zavolokina","Large Language Models for Propaganda Detection","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.
","2023-10-11","2310.06422v1.pdf"
"2310.06430","Jianguo Huang","Jianguo Huang, Huajun Xi, Linjun Zhang, Huaxiu Yao, Yue Qiu, Hongxin
  Wei","Conformal Prediction for Deep Classifier via Label Ranking","","","","","cs.LG cs.CV math.ST stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Conformal prediction is a statistical framework that generates prediction
sets containing ground-truth labels with a desired coverage guarantee. The
predicted probabilities produced by machine learning models are generally
miscalibrated, leading to large prediction sets in conformal prediction. In
this paper, we empirically and theoretically show that disregarding the
probabilities' value will mitigate the undesirable effect of miscalibrated
probability values. Then, we propose a novel algorithm named $\textit{Sorted
Adaptive prediction sets}$ (SAPS), which discards all the probability values
except for the maximum softmax probability. The key idea behind SAPS is to
minimize the dependence of the non-conformity score on the probability values
while retaining the uncertainty information. In this manner, SAPS can produce
sets of small size and communicate instance-wise uncertainty. Theoretically, we
provide a finite-sample coverage guarantee of SAPS and show that the expected
value of set size from SAPS is always smaller than APS. Extensive experiments
validate that SAPS not only lessens the prediction sets but also broadly
enhances the conditional coverage rate and adaptation of prediction sets.
","2023-10-11","2310.06430v1.pdf"
"2310.06434","Huck Yang","Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit
  Kumar, Narsis A. Kiani, David Gomez-Cabrero, Jesper N. Tegner","Whispering LLaMA: A Cross-Modal Generative Error Correction Framework
  for Speech Recognition","Accepted to EMNLP 2023 as main paper. 10 pages. Revised math
  notations. GitHub: https://github.com/Srijith-rkr/Whispering-LLaMA","","","","cs.CL cs.AI cs.MM cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.
","2023-10-18","2310.06434v1.pdf"
"2310.06440","Xiangyu Wu","Xiangyu Wu, Yang Yang, Shengdong Xu, Yifeng Wu, Qingguo Chen, Jianfeng
  Lu","Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic
  Reasoning Task 2023","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we present our solution to a Multi-modal Algorithmic Reasoning
Task: SMART-101 Challenge. Different from the traditional visual
question-answering datasets, this challenge evaluates the abstraction,
deduction, and generalization abilities of neural networks in solving
visuolinguistic puzzles designed specifically for children in the 6-8 age
group. We employed a divide-and-conquer approach. At the data level, inspired
by the challenge paper, we categorized the whole questions into eight types and
utilized the llama-2-chat model to directly generate the type for each question
in a zero-shot manner. Additionally, we trained a yolov7 model on the icon45
dataset for object detection and combined it with the OCR method to recognize
and locate objects and text within the images. At the model level, we utilized
the BLIP-2 model and added eight adapters to the image encoder VIT-G to
adaptively extract visual features for different question types. We fed the
pre-constructed question templates as input and generated answers using the
flan-t5-xxl decoder. Under the puzzle splits configuration, we achieved an
accuracy score of 26.5 on the validation set and 24.30 on the private test set.
","2023-10-11","2310.06440v1.pdf"
"2310.06450","Tianshu Yu","Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li","Constructive Large Language Models Alignment with Diverse Feedback","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In recent research on large language models (LLMs), there has been a growing
emphasis on aligning these models with human values to reduce the impact of
harmful content. However, current alignment methods often rely solely on
singular forms of human feedback, such as preferences, annotated labels, or
natural language critiques, overlooking the potential advantages of combining
these feedback types. This limitation leads to suboptimal performance, even
when ample training data is available. In this paper, we introduce Constructive
and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired
by constructivist learning theory. Our approach involves collecting three
distinct types of feedback tailored to problems of varying difficulty levels
within the training dataset. Specifically, we exploit critique feedback for
easy problems, refinement feedback for medium problems, and preference feedback
for hard problems. By training our model with this diversified feedback, we
achieve enhanced alignment performance while using less training data. To
assess the effectiveness of CDF, we evaluate it against previous methods in
three downstream tasks: question answering, dialog generation, and text
summarization. Experimental results demonstrate that CDF achieves superior
performance even with a smaller training dataset.
","2023-10-12","2310.06450v1.pdf"
"2310.06452","Robert Kirk","Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
  Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu","Understanding the Effects of RLHF on LLM Generalisation and Diversity","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the trade-off between generalisation
and diversity.
","2023-10-11","2310.06452v1.pdf"
"2310.06474","Yue Deng","Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing","Multilingual Jailbreak Challenges in Large Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models (LLMs) exhibit remarkable capabilities across a
wide range of tasks, they pose potential safety concerns, such as the
``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to
exhibit undesirable behavior. Although several preventive measures have been
developed to mitigate the potential risks associated with LLMs, they have
primarily focused on English data. In this study, we reveal the presence of
multilingual jailbreak challenges within LLMs and consider two potential risk
scenarios: unintentional and intentional. The unintentional scenario involves
users querying LLMs using non-English prompts and inadvertently bypassing the
safety mechanisms, while the intentional scenario concerns malicious users
combining malicious instructions with multilingual prompts to deliberately
attack LLMs. The experimental results reveal that in the unintentional
scenario, the rate of unsafe content increases as the availability of languages
decreases. Specifically, low-resource languages exhibit three times the
likelihood of encountering harmful content compared to high-resource languages,
with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts
can exacerbate the negative impact of malicious instructions, with
astonishingly high rates of unsafe output: 80.92\% for ChatGPT and 40.71\% for
GPT-4. To handle such a challenge in the multilingual context, we propose a
novel \textsc{Self-Defense} framework that automatically generates multilingual
training data for safety fine-tuning. Experimental results show that ChatGPT
fine-tuned with such data can achieve a substantial reduction in unsafe content
generation. Data is available at
https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This
paper contains examples with potentially harmful content.
","2023-10-11","2310.06474v1.pdf"
"2310.06498","Shiping Yang","Shiping Yang, Renliang Sun, Xiaojun Wan","A New Benchmark and Reverse Validation Method for Passage-level
  Hallucination Detection","EMNLP2023 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have shown their ability to collaborate
effectively with humans in real-world scenarios. However, LLMs are apt to
generate hallucinations, i.e., makeup incorrect text and unverified
information, which can cause significant damage when deployed for
mission-critical tasks. In this paper, we propose a self-check approach based
on reverse validation to detect factual errors automatically in a zero-resource
fashion. To facilitate future studies and assess different methods, we
construct a hallucination detection benchmark named PHD, which is generated by
ChatGPT and annotated by human annotators. Contrasting previous studies of
zero-resource hallucination detection, our method and benchmark concentrate on
passage-level detection instead of sentence-level. We empirically evaluate our
method and existing zero-resource detection methods on two datasets. The
experimental results demonstrate that the proposed method considerably
outperforms the baselines while costing fewer tokens and less time.
Furthermore, we manually analyze some hallucination cases that LLM failed to
capture, revealing the shared limitation of zero-resource methods.
","2023-10-25","2310.06498v1.pdf"
"2310.06500","Yuan Li","Yuan Li, Yixuan Zhang, and Lichao Sun","MetaAgents: Simulating Interactions of Human Behaviors for LLM-based
  Task-oriented Coordination via Collaborative Generative Agents","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Significant advancements have occurred in the application of Large Language
Models (LLMs) for various tasks and social simulations. Despite this, their
capacities to coordinate within task-oriented social contexts are
under-explored. Such capabilities are crucial if LLMs are to effectively mimic
human-like social behavior and produce meaningful results. To bridge this gap,
we introduce collaborative generative agents, endowing LLM-based Agents with
consistent behavior patterns and task-solving abilities. We situate these
agents in a simulated job fair environment as a case study to scrutinize their
coordination skills. We propose a novel framework that equips collaborative
generative agents with human-like reasoning abilities and specialized skills.
Our evaluation demonstrates that these agents show promising performance.
However, we also uncover limitations that hinder their effectiveness in more
complex coordination tasks. Our work provides valuable insights into the role
and evolution of LLMs in task-oriented social simulations.
","2023-10-11","2310.06500v1.pdf"
"2310.06502","Jia-Dong Zhang","Xiancai Xu, Jia-Dong Zhang, Rongchang Xiao, Lei Xiong","The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment
  Quadruples: A Comparative Analysis","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, ChatGPT has attracted great attention from both industry and
academia due to its surprising abilities in natural language understanding and
generation. We are particularly curious about whether it can achieve promising
performance on one of the most complex tasks in aspect-based sentiment
analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from
texts. To this end, in this paper we develop a specialized prompt template that
enables ChatGPT to effectively tackle this complex quadruple extraction task.
Further, we propose a selection method on few-shot examples to fully exploit
the in-context learning ability of ChatGPT and uplift its effectiveness on this
complex task. Finally, we provide a comparative evaluation on ChatGPT against
existing state-of-the-art quadruple extraction models based on four public
datasets and highlight some important findings regarding the capability
boundaries of ChatGPT in the quadruple extraction.
","2023-10-11","2310.06502v1.pdf"
"2310.06504","Guanting Dong","Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi
  Feng, Yueyan Qiu, Zhuoma Gongque, Keqing He, Zechen Wang, Weiran Xu","Revisit Input Perturbation Problems for LLMs: A Unified Robustness
  Evaluation Framework for Noisy Slot Filling Task","Accepted at NLPCC 2023 (Oral Presentation)","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the increasing capabilities of large language models (LLMs), these
high-performance models have achieved state-of-the-art results on a wide range
of natural language processing (NLP) tasks. However, the models' performance on
commonly-used benchmark datasets often fails to accurately reflect their
reliability and robustness when applied to real-world noisy data. To address
these challenges, we propose a unified robustness evaluation framework based on
the slot-filling task to systematically evaluate the dialogue understanding
capability of LLMs in diverse input perturbation scenarios. Specifically, we
construct a input perturbation evaluation dataset, Noise-LLM, which contains
five types of single perturbation and four types of mixed perturbation data.
Furthermore, we utilize a multi-level data augmentation method (character,
word, and sentence levels) to construct a candidate data pool, and carefully
design two ways of automatic task demonstration construction strategies
(instance-level and entity-level) with various prompt templates. Our aim is to
assess how well various robustness methods of LLMs perform in real-world noisy
scenarios. The experiments have demonstrated that the current open-source LLMs
generally achieve limited perturbation robustness performance. Based on these
experimental observations, we make some forward-looking suggestions to fuel the
research in this direction.
","2023-10-11","2310.06504v1.pdf"
"2310.06505","Su Youn Yoon Ms","Su-Youn Yoon, Eva Miszoglad, Lisa R. Pierce","Evaluation of ChatGPT Feedback on ELL Writers' Coherence and Cohesion","24 pages, 1 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Since its launch in November 2022, ChatGPT has had a transformative effect on
education where students are using it to help with homework assignments and
teachers are actively employing it in their teaching practices. This includes
using ChatGPT as a tool for writing teachers to grade and generate feedback on
students' essays. In this study, we evaluated the quality of the feedback
generated by ChatGPT regarding the coherence and cohesion of the essays written
by English Language Learners (ELLs) students. We selected 50 argumentative
essays and generated feedback on coherence and cohesion using the ELLIPSE
rubric. During the feedback evaluation, we used a two-step approach: first,
each sentence in the feedback was classified into subtypes based on its
function (e.g., positive reinforcement, problem statement). Next, we evaluated
its accuracy and usability according to these types. Both the analysis of
feedback types and the evaluation of accuracy and usability revealed that most
feedback sentences were highly abstract and generic, failing to provide
concrete suggestions for improvement. The accuracy in detecting major problems,
such as repetitive ideas and the inaccurate use of cohesive devices, depended
on superficial linguistic features and was often incorrect. In conclusion,
ChatGPT, without specific training for the feedback generation task, does not
offer effective feedback on ELL students' coherence and cohesion.
","2023-10-11","2310.06505v1.pdf"
"2310.06522","Shreyank N Gowda","Shreyank N Gowda, Xinyue Hao, Gen Li, Laura Sevilla-Lara, Shashank
  Narayana Gowda","Watt For What: Rethinking Deep Learning's Energy-Performance
  Relationship","","","","","cs.LG cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Deep learning models have revolutionized various fields, from image
recognition to natural language processing, by achieving unprecedented levels
of accuracy. However, their increasing energy consumption has raised concerns
about their environmental impact, disadvantaging smaller entities in research
and exacerbating global energy consumption. In this paper, we explore the
trade-off between model accuracy and electricity consumption, proposing a
metric that penalizes large consumption of electricity. We conduct a
comprehensive study on the electricity consumption of various deep learning
models across different GPUs, presenting a detailed analysis of their
accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity
consumed, we demonstrate how smaller, more energy-efficient models can
significantly expedite research while mitigating environmental concerns. Our
results highlight the potential for a more sustainable approach to deep
learning, emphasizing the importance of optimizing models for efficiency. This
research also contributes to a more equitable research landscape, where smaller
entities can compete effectively with larger counterparts. This advocates for
the adoption of efficient deep learning practices to reduce electricity
consumption, safeguarding the environment for future generations whilst also
helping ensure a fairer competitive landscape.
","2023-10-11","2310.06522v1.pdf"
"2310.06547","Weimin Xiong","Weimin Xiong, Yifan Song, Peiyi Wang, Sujian Li","Rationale-Enhanced Language Models are Better Continual Relation
  Learners","Accepted at EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Continual relation extraction (CRE) aims to solve the problem of catastrophic
forgetting when learning a sequence of newly emerging relations. Recent CRE
studies have found that catastrophic forgetting arises from the model's lack of
robustness against future analogous relations. To address the issue, we
introduce rationale, i.e., the explanations of relation classification results
generated by large language models (LLM), into CRE task. Specifically, we
design the multi-task rationale tuning strategy to help the model learn current
relations robustly. We also conduct contrastive rationale replay to further
distinguish analogous relations. Experimental results on two standard
benchmarks demonstrate that our method outperforms the state-of-the-art CRE
models.
","2023-10-11","2310.06547v1.pdf"
"2310.06552","Joseph Spartacus Boyle","Joseph S. Boyle, Antanas Kascenas, Pat Lok, Maria Liakata, Alison Q.
  O'Neil","Automated clinical coding using off-the-shelf large language models","9 pages, 4 figures","","","","cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment.
Unsupervised pre-training alone does not guarantee precise knowledge of the ICD
ontology and specialist clinical coding task, therefore we frame the task as
information extraction, providing a description of each coded concept and
asking the model to retrieve related mentions. For efficiency, rather than
iterating over all codes, we leverage the hierarchical nature of the ICD
ontology to sparsely search for relevant codes. Then, in a second stage, which
we term 'meta-refinement', we utilise GPT-4 to select a subset of the relevant
labels as predictions. We validate our method using Llama-2, GPT-3.5 and GPT-4
on the CodiEsp dataset of ICD-coded clinical case documents. Our tree-search
method achieves state-of-the-art performance on rarer classes, achieving the
best macro-F1 of 0.225, whilst achieving slightly lower micro-F1 of 0.157,
compared to 0.216 and 0.219 respectively from PLM-ICD. To the best of our
knowledge, this is the first method for automated ICD coding requiring no
task-specific learning.
","2023-10-11","2310.06552v1.pdf"
"2310.06588","Yupei Du","Yupei Du, Albert Gatt, Dong Nguyen","FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics","15 pages, 3 figures","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Despite the massive success of fine-tuning large Pre-trained Language Models
(PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain
susceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM)
is a simple yet effective dual-model approach that enhances the robustness of
fine-tuned PLMs, which involves fine-tuning a model on the original training
set (i.e. reference model), selecting a specified fraction of important
training examples according to the training dynamics of the reference model,
and fine-tuning the same model on these selected examples (i.e. main model).
However, it suffers from the drawback of requiring fine-tuning the same model
twice, which is computationally expensive for large models. In this paper, we
first show that 1) training dynamics are highly transferable across different
model sizes and different pre-training methods, and that 2) main models
fine-tuned using DM learn faster than when using conventional Empirical Risk
Minimization (ERM). Building on these observations, we propose a novel
fine-tuning approach based on the DM method: Fine-Tuning by transFerring
Training dynamics (FTFT). Compared with DM, FTFT uses more efficient reference
models and then fine-tunes more capable main models for fewer steps. Our
experiments show that FTFT achieves better generalization robustness than ERM
while spending less than half of the training cost.
","2023-10-11","2310.06588v1.pdf"
"2310.06594","Ning Liao","Ning Liao, Shaofeng Zhang, Renqiu Xia, Bo Zhang, Min Cao, Yu Qiao,
  Junchi Yan","REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning
  Datasets","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There is an emerging line of research on multimodal instruction tuning, and a
line of benchmarks have been proposed for evaluating these models recently.
Instead of evaluating the models directly, in this paper we try to evaluate the
Vision-Language Instruction-Tuning (VLIT) datasets themselves and further seek
the way of building a dataset for developing an all-powerful VLIT model, which
we believe could also be of utility for establishing a grounded protocol for
benchmarking VLIT models. For effective analysis of VLIT datasets that remains
an open question, we propose a tune-cross-evaluation paradigm: tuning on one
dataset and evaluating on the others in turn. For each single tune-evaluation
experiment set, we define the Meta Quality (MQ) as the mean score measured by a
series of caption metrics including BLEU, METEOR, and ROUGE-L to quantify the
quality of a certain dataset or a sample. On this basis, to evaluate the
comprehensiveness of a dataset, we develop the Dataset Quality (DQ) covering
all tune-evaluation sets. To lay the foundation for building a comprehensive
dataset and developing an all-powerful model for practical applications, we
further define the Sample Quality (SQ) to quantify the all-sided quality of
each sample. Extensive experiments validate the rationality of the proposed
evaluation paradigm. Based on the holistic evaluation, we build a new dataset,
REVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting samples
with higher SQ from each dataset. With only half of the full data, the model
trained on REVO-LION can achieve performance comparable to simply adding all
VLIT datasets up. In addition to developing an all-powerful model, REVO-LION
also includes an evaluation set, which is expected to serve as a convenient
evaluation benchmark for future research.
","2023-10-11","2310.06594v1.pdf"
"2310.06627","Bingchen Zhao","Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, Yongshuo Zong,
  Bingchen Zhao","What If the TV Was Off? Examining Counterfactual Reasoning Abilities of
  Multi-modal Language Models","Short paper accepted at ICCV 2023 VLAR workshop","","","","cs.CL cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Counterfactual reasoning ability is one of the core abilities of human
intelligence. This reasoning process involves the processing of alternatives to
observed states or past events, and this process can improve our ability for
planning and decision-making. In this work, we focus on benchmarking the
counterfactual reasoning ability of multi-modal large language models. We take
the question and answer pairs from the VQAv2 dataset and add one counterfactual
presupposition to the questions, with the answer being modified accordingly.
After generating counterfactual questions and answers using ChatGPT, we
manually examine all generated questions and answers to ensure correctness.
Over 2k counterfactual question and answer pairs are collected this way. We
evaluate recent vision language models on our newly collected test dataset and
found that all models exhibit a large performance drop compared to the results
tested on questions without the counterfactual presupposition. This result
indicates that there still exists space for developing vision language models.
Apart from the vision language models, our proposed dataset can also serves as
a benchmark for evaluating the ability of code generation LLMs, results
demonstrate a large gap between GPT-4 and current open-source models. Our code
and dataset are available at \url{https://github.com/Letian2003/C-VQA}.
","2023-10-11","2310.06627v1.pdf"
"2310.06641","Thomas Mensink","Lisa Alazraki, Lluis Castrejon, Mostafa Dehghani, Fantine Huot, Jasper
  Uijlings, Thomas Mensink","How (not) to ensemble LVLMs for VQA","Under submission","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper studies ensembling in the era of Large Vision-Language Models
(LVLMs). Ensembling is a classical method to combine different models to get
increased performance. In the recent work on Encyclopedic-VQA the authors
examine a wide variety of models to solve their task: from vanilla LVLMs, to
models including the caption as extra context, to models augmented with
Lens-based retrieval of Wikipedia pages. Intuitively these models are highly
complementary, which should make them ideal for ensembling. Indeed, an oracle
experiment shows potential gains from 48.8% accuracy (the best single model)
all the way up to 67% (best possible ensemble). So it is a trivial exercise to
create an ensemble with substantial real gains. Or is it?
","2023-10-11","2310.06641v1.pdf"
"2310.06646","Juo-Tung Chen","Juo-Tung Chen, Chien-Ming Huang","Forgetful Large Language Models: Lessons Learned from Using LLMs in
  Robot Programming","9 pages ,8 figures, accepted by the AAAI 2023 Fall Symposium Series","","","","cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models offer new ways of empowering people to program robot
applications-namely, code generation via prompting. However, the code generated
by LLMs is susceptible to errors. This work reports a preliminary exploration
that empirically characterizes common errors produced by LLMs in robot
programming. We categorize these errors into two phases: interpretation and
execution. In this work, we focus on errors in execution and observe that they
are caused by LLMs being ""forgetful"" of key information provided in user
prompts. Based on this observation, we propose prompt engineering tactics
designed to reduce errors in execution. We then demonstrate the effectiveness
of these tactics with three language models: ChatGPT, Bard, and LLaMA-2.
Finally, we discuss lessons learned from using LLMs in robot programming and
call for the benchmarking of LLM-powered end-user development of robot
applications.
","2023-10-11","2310.06646v1.pdf"
"2310.06648","Chao Qian","Ren-Jian Wang, Ke Xue, Yutong Wang, Peng Yang, Haobo Fu, Qiang Fu,
  Chao Qian","Diversity from Human Feedback","","","","","cs.LG cs.AI cs.NE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diversity plays a significant role in many problems, such as ensemble
learning, reinforcement learning, and combinatorial optimization. How to define
the diversity measure is a longstanding problem. Many methods rely on expert
experience to define a proper behavior space and then obtain the diversity
measure, which is, however, challenging in many scenarios. In this paper, we
propose the problem of learning a behavior space from human feedback and
present a general method called Diversity from Human Feedback (DivHF) to solve
it. DivHF learns a behavior descriptor consistent with human preference by
querying human feedback. The learned behavior descriptor can be combined with
any distance measure to define a diversity measure. We demonstrate the
effectiveness of DivHF by integrating it with the Quality-Diversity
optimization algorithm MAP-Elites and conducting experiments on the QDax suite.
The results show that DivHF learns a behavior space that aligns better with
human requirements compared to direct data-driven approaches and leads to more
diverse solutions under human preference. Our contributions include formulating
the problem, proposing the DivHF method, and demonstrating its effectiveness
through experiments.
","2023-10-11","2310.06648v1.pdf"
"2310.06666","Caoyun Fan","Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin","Unlock the Potential of Counterfactually-Augmented Data in
  Out-Of-Distribution Generalization","Expert Systems With Applications 2023. arXiv admin note: text overlap
  with arXiv:2302.09345","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip
the corresponding labels -- has the potential to improve the
Out-Of-Distribution (OOD) generalization capability of language models, as CAD
induces language models to exploit domain-independent causal features and
exclude spurious correlations. However, the empirical results of CAD's OOD
generalization are not as efficient as anticipated. In this study, we attribute
the inefficiency to the myopia phenomenon caused by CAD: language models only
focus on causal features that are edited in the augmentation operation and
exclude other non-edited causal features. Therefore, the potential of CAD is
not fully exploited. To address this issue, we analyze the myopia phenomenon in
feature space from the perspective of Fisher's Linear Discriminant, then we
introduce two additional constraints based on CAD's structural properties
(dataset-level and sentence-level) to help language models extract more
complete causal features in CAD, thereby mitigating the myopia phenomenon and
improving OOD generalization capability. We evaluate our method on two tasks:
Sentiment Analysis and Natural Language Inference, and the experimental results
demonstrate that our method could unlock the potential of CAD and improve the
OOD generalization performance of language models by 1.0% to 5.9%.
","2023-10-11","2310.06666v1.pdf"
"2310.06675","Jonathan Tonglet","Jonathan Tonglet, Manon Reusens, Philipp Borchert, Bart Baesens","SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA","Camera ready revision for EMNLP 2023 main conference. Code available
  at https://github.com/jtonglet/SEER","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Question answering over hybrid contexts is a complex task, which requires the
combination of information extracted from unstructured texts and structured
tables in various ways. Recently, In-Context Learning demonstrated significant
performance advances for reasoning tasks. In this paradigm, a large language
model performs predictions based on a small set of supporting exemplars. The
performance of In-Context Learning depends heavily on the selection procedure
of the supporting exemplars, particularly in the case of HybridQA, where
considering the diversity of reasoning chains and the large size of the hybrid
contexts becomes crucial. In this work, we present Selection of ExEmplars for
hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that
is both representative and diverse. The key novelty of SEER is that it
formulates exemplar selection as a Knapsack Integer Linear Program. The
Knapsack framework provides the flexibility to incorporate diversity
constraints that prioritize exemplars with desirable attributes, and capacity
constraints that ensure that the prompt size respects the provided capacity
budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two
real-world benchmarks for HybridQA, where it outperforms previous exemplar
selection methods.
","2023-10-23","2310.06675v1.pdf"
"2310.06680","Zhenlan Ji","Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang","Benchmarking and Explaining Large Language Model-based Code Generation:
  A Causality-Centric Approach","","","","","cs.SE cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While code generation has been widely used in various software development
scenarios, the quality of the generated code is not guaranteed. This has been a
particular concern in the era of large language models (LLMs)- based code
generation, where LLMs, deemed a complex and powerful black-box model, is
instructed by a high-level natural language specification, namely a prompt, to
generate code. Nevertheless, effectively evaluating and explaining the code
generation capability of LLMs is inherently challenging, given the complexity
of LLMs and the lack of transparency.
  Inspired by the recent progress in causality analysis and its application in
software engineering, this paper launches a causality analysis-based approach
to systematically analyze the causal relations between the LLM input prompts
and the generated code. To handle various technical challenges in this study,
we first propose a novel causal graph-based representation of the prompt and
the generated code, which is established over the fine-grained,
human-understandable concepts in the input prompts. The formed causal graph is
then used to identify the causal relations between the prompt and the derived
code. We illustrate the insights that our framework can provide by studying
over 3 popular LLMs with over 12 prompt adjustment strategies. The results of
these studies illustrate the potential of our technique to provide insights
into LLM effectiveness, and aid end-users in understanding predictions.
Additionally, we demonstrate that our approach provides actionable insights to
improve the quality of the LLM-generated code by properly calibrating the
prompt.
","2023-10-11","2310.06680v1.pdf"
"2310.06684","Bowen Jin","Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Han Zhao, Jiawei Han","Learning Multiplex Embeddings on Text-rich Networks with One Text
  Encoder","9 pages, 11 appendix pages","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In real-world scenarios, texts in a network are often linked by multiple
semantic relations (e.g., papers in an academic network are referenced by other
publications, written by the same author, or published in the same venue),
where text documents and their relations form a multiplex text-rich network.
Mainstream text representation learning methods use pretrained language models
(PLMs) to generate one embedding for each text unit, expecting that all types
of relations between texts can be captured by these single-view embeddings.
However, this presumption does not hold particularly in multiplex text-rich
networks. Along another line of work, multiplex graph neural networks (GNNs)
directly initialize node attributes as a feature vector for node representation
learning, but they cannot fully capture the semantics of the nodes' associated
texts. To bridge these gaps, we propose METERN, a new framework for learning
Multiplex Embeddings on TExt-Rich Networks. In contrast to existing methods,
METERN uses one text encoder to model the shared knowledge across relations and
leverages a small number of parameters per relation to derive relation-specific
representations. This allows the encoder to effectively capture the multiplex
structures in the network while also preserving parameter efficiency. We
conduct experiments on nine downstream tasks in five networks from both
academic and e-commerce domains, where METERN outperforms baselines
significantly and consistently. The code is available at
https://github.com/PeterGriffinJin/METERN-submit.
","2023-10-11","2310.06684v1.pdf"
"2310.06692","Anni Zou","Anni Zou, Zhuosheng Zhang, Hai Zhao, Xiangru Tang","Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task
  Scenarios with Large Language Models","17 pages, 7 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have unveiled remarkable reasoning capabilities
by exploiting chain-of-thought (CoT) prompting, which generates intermediate
reasoning chains to serve as the rationale for deriving the answer. However,
current CoT methods either simply employ general prompts such as Let's think
step by step, or heavily rely on handcrafted task-specific demonstrations to
attain preferable performances, thereby engendering an inescapable gap between
performance and generalization. To bridge this gap, we propose Meta-CoT, a
generalizable CoT prompting method in mixed-task scenarios where the type of
input questions is unknown. Meta-CoT firstly categorizes the scenario based on
the input question and subsequently constructs diverse demonstrations from the
corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys
remarkable performances on ten public benchmark reasoning tasks and superior
generalization capabilities. Notably, Meta-CoT achieves the state-of-the-art
result on SVAMP (93.7%) without any additional program-aided methods. Our
further experiments on five out-of-distribution datasets verify the stability
and generality of Meta-CoT.
","2023-10-12","2310.06692v1.pdf"
"2310.06694","Mengzhou Xia","Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen","Sheared LLaMA: Accelerating Language Model Pre-training via Structured
  Pruning","The code and models are available at
  https://github.com/princeton-nlp/LLM-Shearing","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged
moderate-sized large language models (LLMs) highlights the potential of
building smaller yet powerful LLMs. Regardless, the cost of training such
models from scratch on trillions of tokens remains high. In this work, we study
structured pruning as an effective means to develop smaller LLMs from
pre-trained, larger models. Our approach employs two key techniques: (1)
targeted structured pruning, which prunes a larger model to a specified target
shape by removing layers, heads, and intermediate and hidden dimensions in an
end-to-end manner, and (2) dynamic batch loading, which dynamically updates the
composition of sampled data in each training batch based on varying losses
across different domains. We demonstrate the efficacy of our approach by
presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B
and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art
open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA
models, on a wide range of downstream and instruction tuning evaluations, while
requiring only 3% of compute compared to training such models from scratch.
This work provides compelling evidence that leveraging existing LLMs with
structured pruning is a far more cost-effective approach for building smaller
LLMs.
","2023-10-11","2310.06694v1.pdf"
"2310.06714","Shenglai Zeng","Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue
  Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin","Exploring Memorization in Fine-tuned Language Models","","","","","cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  LLMs have shown great capabilities in various tasks but also exhibited
memorization of training data, thus raising tremendous privacy and copyright
concerns. While prior work has studied memorization during pre-training, the
exploration of memorization during fine-tuning is rather limited. Compared with
pre-training, fine-tuning typically involves sensitive data and diverse
objectives, thus may bring unique memorization behaviors and distinct privacy
risks. In this work, we conduct the first comprehensive analysis to explore
LMs' memorization during fine-tuning across tasks. Our studies with
open-sourced and our own fine-tuned LMs across various tasks indicate that
fine-tuned memorization presents a strong disparity among tasks. We provide an
understanding of this task disparity via sparse coding theory and unveil a
strong correlation between memorization and attention score distribution. By
investigating its memorization behavior, multi-task fine-tuning paves a
potential strategy to mitigate fine-tuned memorization.
","2023-10-11","2310.06714v1.pdf"
"2310.06762","Xiao Wang","Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin,
  Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xuanjing
  Huang","TRACE: A Comprehensive Benchmark for Continual Learning in Large
  Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Aligned large language models (LLMs) demonstrate exceptional capabilities in
task-solving, following instructions, and ensuring safety. However, the
continual learning aspect of these aligned LLMs has been largely overlooked.
Existing continual learning benchmarks lack sufficient challenge for leading
aligned LLMs, owing to both their simplicity and the models' potential exposure
during instruction tuning. In this paper, we introduce TRACE, a novel benchmark
designed to evaluate continual learning in LLMs. TRACE consists of 8 distinct
datasets spanning challenging tasks including domain-specific tasks,
multilingual capabilities, code generation, and mathematical reasoning. All
datasets are standardized into a unified format, allowing for effortless
automatic evaluation of LLMs. Our experiments show that after training on
TRACE, aligned LLMs exhibit significant declines in both general ability and
instruction-following capabilities. For example, the accuracy of llama2-chat
13B on gsm8k dataset declined precipitously from 28.8\% to 2\% after training
on our datasets. This highlights the challenge of finding a suitable tradeoff
between achieving performance on specific tasks while preserving the original
prowess of LLMs. Empirical findings suggest that tasks inherently equipped with
reasoning paths contribute significantly to preserving certain capabilities of
LLMs against potential declines. Motivated by this, we introduce the
Reasoning-augmented Continual Learning (RCL) approach. RCL integrates
task-specific cues with meta-rationales, effectively reducing catastrophic
forgetting in LLMs while expediting convergence on novel tasks.
","2023-10-11","2310.06762v1.pdf"
"2310.06770","Carlos E. Jimenez","Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei,
  Ofir Press, Karthik Narasimhan","SWE-bench: Can Language Models Resolve Real-World GitHub Issues?","Data, code, and leaderboard are available at https://www.swebench.com","","","","cs.CL cs.AI cs.SE","http://creativecommons.org/licenses/by/4.0/","  Language models have outpaced our ability to evaluate them effectively, but
for their future development it is essential to study the frontier of their
capabilities. We consider real-world software engineering to be a rich,
sustainable, and challenging testbed for evaluating the next generation of
language models. We therefore introduce SWE-bench, an evaluation framework
including $2,294$ software engineering problems drawn from real GitHub issues
and corresponding pull requests across $12$ popular Python repositories. Given
a codebase along with a description of an issue to be resolved, a language
model is tasked with editing the codebase to address the issue. Resolving
issues in SWE-bench frequently requires understanding and coordinating changes
across multiple functions, classes, and even files simultaneously, calling for
models to interact with execution environments, process extremely long contexts
and perform complex reasoning that goes far beyond traditional code generation.
Our evaluations show that both state-of-the-art proprietary models and our
fine-tuned model SWE-Llama can resolve only the simplest issues. Claude 2 and
GPT-4 solve a mere $4.8$% and $1.7$% of instances respectively, even when
provided with an oracle retriever. Advances on SWE-bench represent steps
towards LMs that are more practical, intelligent, and autonomous.
","2023-10-11","2310.06770v1.pdf"
"2310.06771","Krishna Pillutla","Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, Krishna
  Pillutla, Arun Ganesh, Thomas Steinke, Abhradeep Thakurta","Correlated Noise Provably Beats Independent Noise for Differentially
  Private Learning","Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, and Krishna
  Pillutla contributed equally","","","","cs.LG cs.AI cs.CR math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Differentially private learning algorithms inject noise into the learning
process. While the most common private learning algorithm, DP-SGD, adds
independent Gaussian noise in each iteration, recent work on matrix
factorization mechanisms has shown empirically that introducing correlations in
the noise can greatly improve their utility. We characterize the asymptotic
learning utility for any choice of the correlation function, giving precise
analytical bounds for linear regression and as the solution to a convex program
for general convex functions. We show, using these bounds, how correlated noise
provably improves upon vanilla DP-SGD as a function of problem parameters such
as the effective dimension and condition number. Moreover, our analytical
expression for the near-optimal correlation function circumvents the cubic
complexity of the semi-definite program used to optimize the noise correlation
matrix in previous work. We validate our theory with experiments on private
deep learning. Our work matches or outperforms prior work while being efficient
both in terms of compute and memory.
","2023-10-11","2310.06771v1.pdf"
"2310.06775","Carlos Toxtli","David Shapiro, Wangfan Li, Manuel Delaflor, Carlos Toxtli","Conceptual Framework for Autonomous Cognitive Entities","34 pages, 12 figures","","10.13140/RG.2.2.14161.30569","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  The rapid development and adoption of Generative AI (GAI) technology in the
form of chatbots such as ChatGPT and Claude has greatly increased interest in
agentic machines. This paper introduces the Autonomous Cognitive Entity (ACE)
model, a novel framework for a cognitive architecture, enabling machines and
software agents to operate more independently. Drawing inspiration from the OSI
model, the ACE framework presents layers of abstraction to conceptualize
artificial cognitive architectures. The model is designed to harness the
capabilities of the latest generative AI technologies, including large language
models (LLMs) and multimodal generative models (MMMs), to build autonomous,
agentic systems. The ACE framework comprises six layers: the Aspirational
Layer, Global Strategy, Agent Model, Executive Function, Cognitive Control, and
Task Prosecution. Each layer plays a distinct role, ranging from setting the
moral compass and strategic thinking to task selection and execution. The ACE
framework also incorporates mechanisms for handling failures and adapting
actions, thereby enhancing the robustness and flexibility of autonomous agents.
This paper introduces the conceptual framework and proposes implementation
strategies that have been tested and observed in industry. The goal of this
paper is to formalize this framework so as to be more accessible.
","2023-10-16","2310.06775v1.pdf"
"2310.06786","Keiran Paster","Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba","OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text","","","","","cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There is growing evidence that pretraining on high quality, carefully
thought-out tokens such as code or mathematics plays an important role in
improving the reasoning abilities of large language models. For example,
Minerva, a PaLM model finetuned on billions of tokens of mathematical documents
from arXiv and the web, reported dramatically improved performance on problems
that require quantitative reasoning. However, because all known open source web
datasets employ preprocessing that does not faithfully preserve mathematical
notation, the benefits of large scale training on quantitive web documents are
unavailable to the research community. We introduce OpenWebMath, an open
dataset inspired by these works containing 14.7B tokens of mathematical
webpages from Common Crawl. We describe in detail our method for extracting
text and LaTeX content and removing boilerplate from HTML documents, as well as
our methods for quality filtering and deduplication. Additionally, we run
small-scale experiments by training 1.4B parameter language models on
OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass
the performance of models trained on over 20x the amount of general language
data. We hope that our dataset, openly released on the Hugging Face Hub, will
help spur advances in the reasoning abilities of large language models.
","2023-10-11","2310.06786v1.pdf"
"2310.06816","John Morris","John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M.
  Rush","Text Embeddings Reveal (Almost) As Much As Text","Accepted at EMNLP 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  How much private information do text embeddings reveal about the original
text? We investigate the problem of embedding \textit{inversion},
reconstructing the full text represented in dense text embeddings. We frame the
problem as controlled generation: generating text that, when reembedded, is
close to a fixed point in latent space. We find that although a na\""ive model
conditioned on the embedding performs poorly, a multi-step method that
iteratively corrects and re-embeds text is able to recover $92\%$ of
$32\text{-token}$ text inputs exactly. We train our model to decode text
embeddings from two state-of-the-art embedding models, and also show that our
model can recover important personal information (full names) from a dataset of
clinical notes. Our code is available on Github:
\href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.
","2023-10-11","2310.06816v1.pdf"
"2310.06824","Samuel Marks","Samuel Marks and Max Tegmark","The Geometry of Truth: Emergent Linear Structure in Large Language Model
  Representations of True/False Datasets","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have impressive capabilities, but are also prone
to outputting falsehoods. Recent work has developed techniques for inferring
whether a LLM is telling the truth by training probes on the LLM's internal
activations. However, this line of work is controversial, with some authors
pointing out failures of these probes to generalize in basic ways, among other
conceptual issues. In this work, we curate high-quality datasets of true/false
statements and use them to study in detail the structure of LLM representations
of truth, drawing on three lines of evidence: 1. Visualizations of LLM
true/false statement representations, which reveal clear linear structure. 2.
Transfer experiments in which probes trained on one dataset generalize to
different datasets. 3. Causal evidence obtained by surgically intervening in a
LLM's forward pass, causing it to treat false statements as true and vice
versa. Overall, we present evidence that language models linearly represent the
truth or falsehood of factual statements. We also introduce a novel technique,
mass-mean probing, which generalizes better and is more causally implicated in
model outputs than other probing techniques.
","2023-10-11","2310.06824v1.pdf"
"2310.06825","Devendra Singh Chaplot","Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, L\'elio Renard Lavaud, Marie-Anne Lachaux,
  Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\'ee Lacroix,
  William El Sayed","Mistral 7B","Models and code are available at
  https://mistral.ai/news/announcing-mistral-7b/","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered
for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B
across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and
code generation. Our model leverages grouped-query attention (GQA) for faster
inference, coupled with sliding window attention (SWA) to effectively handle
sequences of arbitrary length with a reduced inference cost. We also provide a
model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses
the Llama 2 13B -- Chat model both on human and automated benchmarks. Our
models are released under the Apache 2.0 license.
","2023-10-11","2310.06825v1.pdf"
"2310.06827","Erik Jones","Erik Jones, Hamid Palangi, Clarisse Sim\~oes, Varun Chandrasekaran,
  Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, Ece Kamar","Teaching Language Models to Hallucinate Less with Synthetic Tasks","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) frequently hallucinate on abstractive
summarization tasks such as document-based question-answering, meeting
summarization, and clinical report generation, even though all necessary
information is included in context. However, optimizing LLMs to hallucinate
less on these tasks is challenging, as hallucination is hard to efficiently
evaluate at each optimization step. In this work, we show that reducing
hallucination on a synthetic task can also reduce hallucination on real-world
downstream tasks. Our method, SynTra, first designs a synthetic task where
hallucinations are easy to elicit and measure. It next optimizes the LLM's
system message via prefix-tuning on the synthetic task, and finally transfers
the system message to realistic, hard-to-optimize tasks. Across three realistic
abstractive summarization tasks, SynTra reduces hallucination for two
13B-parameter LLMs using only a synthetic retrieval task for supervision. We
also find that optimizing the system message rather than the model weights can
be critical; fine-tuning the entire model on the synthetic task can
counterintuitively increase hallucination. Overall, SynTra demonstrates that
the extra flexibility of working with synthetic data can help mitigate
undesired behaviors in practice.
","2023-10-11","2310.06827v1.pdf"
"2310.06830","Yiheng Xu","Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi,
  Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao,
  Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu","Lemur: Harmonizing Natural Language and Code for Language Agents","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce Lemur and Lemur-Chat, openly accessible language models
optimized for both natural language and coding capabilities to serve as the
backbone of versatile language agents. The evolution from language chat models
to functional language agents demands that models not only master human
interaction, reasoning, and planning but also ensure grounding in the relevant
environments. This calls for a harmonious blend of language and coding
capabilities in the models. Lemur and Lemur-Chat are proposed to address this
necessity, demonstrating balanced proficiencies in both domains, unlike
existing open-source models that tend to specialize in either. Through
meticulous pre-training using a code-intensive corpus and instruction
fine-tuning on text and code data, our models achieve state-of-the-art averaged
performance across diverse text and coding benchmarks among open-source models.
Comprehensive experiments demonstrate Lemur's superiority over existing
open-source models and its proficiency across various agent tasks involving
human communication, tool usage, and interaction under fully- and partially-
observable environments. The harmonization between natural and programming
languages enables Lemur-Chat to significantly narrow the gap with proprietary
models on agent abilities, providing key insights into developing advanced
open-source agents adept at reasoning, planning, and operating seamlessly
across environments. https://github.com/OpenLemur/Lemur
","2023-10-11","2310.06830v1.pdf"
"2310.06835","Kaustuv Mukherji","Kaustuv Mukherji, Devendra Parkar, Lahari Pokala, Dyuman Aditya, Paulo
  Shakarian, Clark Dorman","Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement
  Learning","Submitted to 2024 IEEE International Conference on Semantic Computing","","","","cs.LG cs.AI cs.LO","http://creativecommons.org/licenses/by/4.0/","  Recent advances in reinforcement learning (RL) have shown much promise across
a variety of applications. However, issues such as scalability, explainability,
and Markovian assumptions limit its applicability in certain domains. We
observe that many of these shortcomings emanate from the simulator as opposed
to the RL training algorithms themselves. As such, we propose a semantic proxy
for simulation based on a temporal extension to annotated logic. In comparison
with two high-fidelity simulators, we show up to three orders of magnitude
speed-up while preserving the quality of policy learned. In addition, we show
the ability to model and leverage non-Markovian dynamics and instantaneous
actions while providing an explainable trace describing the outcomes of the
agent actions.
","2023-10-17","2310.06835v1.pdf"
"2310.06837","Eric Zelikman","Eric Zelikman, Wanjing Anya Ma, Jasmine E. Tran, Diyi Yang, Jason D.
  Yeatman, Nick Haber","Generating and Evaluating Tests for K-12 Students with Language Model
  Simulations: A Case Study on Sentence Reading Efficiency","Accepted to EMNLP 2023 (Main)","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Developing an educational test can be expensive and time-consuming, as each
item must be written by experts and then evaluated by collecting hundreds of
student responses. Moreover, many tests require multiple distinct sets of
questions administered throughout the school year to closely monitor students'
progress, known as parallel tests. In this study, we focus on tests of silent
sentence reading efficiency, used to assess students' reading ability over
time. To generate high-quality parallel tests, we propose to fine-tune large
language models (LLMs) to simulate how previous students would have responded
to unseen items. With these simulated responses, we can estimate each item's
difficulty and ambiguity. We first use GPT-4 to generate new test items
following a list of expert-developed rules and then apply a fine-tuned LLM to
filter the items based on criteria from psychological measurements. We also
propose an optimal-transport-inspired technique for generating parallel tests
and show the generated tests closely correspond to the original test's
difficulty and reliability based on crowdworker responses. Our evaluation of a
generated test with 234 students from grades 2 to 8 produces test scores highly
correlated (r=0.93) to those of a standard test form written by human experts
and evaluated across thousands of K-12 students.
","2023-10-11","2310.06837v1.pdf"
"2310.06839","Huiqiang Jiang","Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin,
  Yuqing Yang, Lili Qiu","LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios
  via Prompt Compression","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In long context scenarios, large language models (LLMs) face three main
challenges: higher computational/financial cost, longer latency, and inferior
performance. Some studies reveal that the performance of LLMs depends on both
the density and the position of the key information (question relevant) in the
input prompt. Inspired by these findings, we propose LongLLMLingua for prompt
compression towards improving LLMs' perception of the key information to
simultaneously address the three challenges. We conduct evaluation on a wide
range of long context scenarios including single-/multi-document QA, few-shot
learning, summarization, synthetic tasks, and code completion. The experimental
results show that LongLLMLingua compressed prompt can derive higher performance
with much less cost. The latency of the end-to-end system is also reduced. For
example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost
of up to 17.1% over the original prompt with ~4x fewer tokens as input to
GPT-3.5-Turbo. It can derive cost savings of \$28.5 and \$27.4 per 1,000
samples from the LongBench and ZeroScrolls benchmark, respectively.
Additionally, when compressing prompts of ~10k tokens at a compression rate of
2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our
code is available at https://aka.ms/LLMLingua.
","2023-10-11","2310.06839v1.pdf"
"2310.06846","Robert Wray","James R. Kirk, Robert E. Wray, John E. Laird","Exploiting Language Models as a Source of Knowledge for Cognitive Agents","9 pages, 4 figures, 2 tables. AAAI FSS on Integrating Cognitive
  Architecture and Generative Models","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large language models (LLMs) provide capabilities far beyond sentence
completion, including question answering, summarization, and natural-language
inference. While many of these capabilities have potential application to
cognitive systems, our research is exploiting language models as a source of
task knowledge for cognitive agents, that is, agents realized via a cognitive
architecture. We identify challenges and opportunities for using language
models as an external knowledge source for cognitive systems and possible ways
to improve the effectiveness of knowledge extraction by integrating extraction
with cognitive architecture capabilities, highlighting with examples from our
recent work in this area.
","2023-10-12","2310.06846v1.pdf"
"2310.06856","Adrian Groza","Adrian Groza and Anca Marginean","Brave new world: Artificial Intelligence in teaching and learning","","","","","cs.CY cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We exemplify how Large Language Models are used in both teaching and
learning. We also discuss the AI incidents that have already occurred in the
education domain, and we argue for the urgent need to introduce AI policies in
universities and for the ongoing strategies to regulate AI. Regarding policy
for AI, our view is that each institution should have a policy for AI in
teaching and learning. This is important from at least twofolds: (i) to raise
awareness on the numerous educational tools that can both positively and
negatively affect education; (ii) to minimise the risk of AI incidents in
education.
","2023-10-12","2310.06856v1.pdf"
"2310.06879","Xiangyu Wu","Xiangyu Wu, Yi Gao, Hailiang Zhang, Yang Yang, Weili Guo, Jianfeng Lu","The Solution for the CVPR2023 NICE Image Captioning Challenge","","","","","cs.CV eess.IV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we present our solution to the New frontiers for Zero-shot
Image Captioning Challenge. Different from the traditional image captioning
datasets, this challenge includes a larger new variety of visual concepts from
many domains (such as COVID-19) as well as various image types (photographs,
illustrations, graphics). For the data level, we collect external training data
from Laion-5B, a large-scale CLIP-filtered image-text dataset. For the model
level, we use OFA, a large-scale visual-language pre-training model based on
handcrafted templates, to perform the image captioning task. In addition, we
introduce contrastive learning to align image-text pairs to learn new visual
concepts in the pre-training stage. Then, we propose a similarity-bucket
strategy and incorporate this strategy into the template to force the model to
generate higher quality and more matching captions. Finally, by
retrieval-augmented strategy, we construct a content-rich template, containing
the most relevant top-k captions from other image-text pairs, to guide the
model in generating semantic-rich captions. Our method ranks first on the
leaderboard, achieving 105.17 and 325.72 Cider-Score in the validation and test
phase, respectively.
","2023-10-12","2310.06879v1.pdf"
"2310.06904","Deepti Ghadiyaram","Piero Esposito, Parmida Atighehchian, Anastasis Germanidis and Deepti
  Ghadiyaram","Mitigating stereotypical biases in text to image generative systems","4 figures, 8 pages","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  State-of-the-art generative text-to-image models are known to exhibit social
biases and over-represent certain groups like people of perceived lighter skin
tones and men in their outcomes. In this work, we propose a method to mitigate
such biases and ensure that the outcomes are fair across different groups of
people. We do this by finetuning text-to-image models on synthetic data that
varies in perceived skin tones and genders constructed from diverse text
prompts. These text prompts are constructed from multiplicative combinations of
ethnicities, genders, professions, age groups, and so on, resulting in diverse
synthetic data. Our diversity finetuned (DFT) model improves the group fairness
metric by 150% for perceived skin tone and 97.7% for perceived gender. Compared
to baselines, DFT models generate more people with perceived darker skin tone
and more women. To foster open research, we will release all text prompts and
code to generate training images.
","2023-10-12","2310.06904v1.pdf"
"2310.06913","Atish Kumar Dipongkor","Atish Kumar Dipongkor, Kevin Moran","A Comparative Study of Transformer-based Neural Text Representation
  Techniques on Bug Triaging","12 pages, to appear in the Proceedings of 38th IEEE/ACM International
  Conference on Automated Software Engineering (ASE'23)","","","","cs.SE cs.CL cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Often, the first step in managing bug reports is related to triaging a bug to
the appropriate developer who is best suited to understand, localize, and fix
the target bug. Additionally, assigning a given bug to a particular part of a
software project can help to expedite the fixing process. However, despite the
importance of these activities, they are quite challenging, where days can be
spent on the manual triaging process. Past studies have attempted to leverage
the limited textual data of bug reports to train text classification models
that automate this process -- to varying degrees of success. However, the
textual representations and machine learning models used in prior work are
limited by their expressiveness, often failing to capture nuanced textual
patterns that might otherwise aid in the triaging process. Recently, large,
transformer-based, pre-trained neural text representation techniques such as
BERT have achieved greater performance in several natural language processing
tasks. However, the potential for using these techniques to improve upon prior
approaches for automated bug triaging is not well studied or understood.
  Therefore, in this paper we offer one of the first investigations that
fine-tunes transformer-based language models for the task of bug triaging on
four open source datasets, spanning a collective 53 years of development
history with over 400 developers and over 150 software project components. Our
study includes both a quantitative and qualitative analysis of effectiveness.
Our findings illustrate that DeBERTa is the most effective technique across the
triaging tasks of developer and component assignment, and the measured
performance delta is statistically significant compared to other techniques.
However, through our qualitative analysis, we also observe that each technique
possesses unique abilities best suited to certain types of bug reports.
","2023-10-12","2310.06913v1.pdf"
"2310.06927","Eldar Kurtic","Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan
  Alistarh","Sparse Fine-tuning for Inference Acceleration of Large Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  We consider the problem of accurate sparse fine-tuning of large language
models (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while
inducing sparsity in their weights. On the accuracy side, we observe that
standard loss-based fine-tuning may fail to recover accuracy, especially at
high sparsities. To address this, we perform a detailed study of
distillation-type losses, determining an L2-based distillation approach we term
SquareHead which enables accurate recovery even at higher sparsities, across
all model types. On the practical efficiency side, we show that sparse LLMs can
be executed with speedups by taking advantage of sparsity, for both CPU and GPU
runtimes. While the standard approach is to leverage sparsity for computational
reduction, we observe that in the case of memory-bound LLMs sparsity can also
be leveraged for reducing memory bandwidth. We exhibit end-to-end results
showing speedups due to sparsity, while recovering accuracy, on T5 (language
translation), Whisper (speech translation), and open GPT-type (MPT for text
generation). For MPT text generation, we show for the first time that sparse
fine-tuning can reach 75% sparsity without accuracy drops, provide notable
end-to-end speedups for both CPU and GPU inference, and highlight that sparsity
is also compatible with quantization approaches. Models and software for
reproducing our results are provided in Section 6.
","2023-10-16","2310.06927v1.pdf"
"2310.06936","Stephen Moskal","Stephen Moskal, Sam Laney, Erik Hemberg, Una-May O'Reilly","LLMs Killed the Script Kiddie: How Agents Supported by Large Language
  Models Change the Landscape of Network Threat Testing","","","","","cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we explore the potential of Large Language Models (LLMs) to
reason about threats, generate information about tools, and automate cyber
campaigns. We begin with a manual exploration of LLMs in supporting specific
threat-related actions and decisions. We proceed by automating the decision
process in a cyber campaign. We present prompt engineering approaches for a
plan-act-report loop for one action of a threat campaign and and a prompt
chaining design that directs the sequential decision process of a multi-action
campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the
short campaign we demonstrate and provide insights into prompt design for
eliciting actionable responses. We discuss the potential impact of LLMs on the
threat landscape and the ethical considerations of using LLMs for accelerating
threat actor capabilities. We report a promising, yet concerning, application
of generative AI to cyber threats. However, the LLM's capabilities to deal with
more complex networks, sophisticated vulnerabilities, and the sensitivity of
prompts are open questions. This research should spur deliberations over the
inevitable advancements in LLM-supported cyber adversarial landscape.
","2023-10-12","2310.06936v1.pdf"
"2310.06983","Vincent Trost","Courtland Leer, Vincent Trost, Vineeth Voruganti","Violation of Expectation via Metacognitive Prompting Reduces Theory of
  Mind Prediction Error in Large Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent research shows that Large Language Models (LLMs) exhibit a compelling
level of proficiency in Theory of Mind (ToM) tasks. This ability to impute
unobservable mental states to others is vital to human social cognition and may
prove equally important in principal-agent relations between individual humans
and Artificial Intelligences (AIs). In this paper, we explore how a mechanism
studied in developmental psychology known as Violation of Expectation (VoE) can
be implemented to reduce errors in LLM prediction about users by leveraging
emergent ToM affordances. And we introduce a \textit{metacognitive prompting}
framework to apply VoE in the context of an AI tutor. By storing and retrieving
facts derived in cases where LLM expectation about the user was violated, we
find that LLMs are able to learn about users in ways that echo theories of
human learning. Finally, we discuss latent hazards and augmentative
opportunities associated with modeling user psychology and propose ways to
mitigate risk along with possible directions for future inquiry.
","2023-10-12","2310.06983v1.pdf"
"2310.06987","Yangsibo Huang","Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen","Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation","","","","","cs.CL cs.AI cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The rapid progress in open-source large language models (LLMs) is
significantly advancing AI development. Extensive efforts have been made before
model release to align their behavior with human values, with the primary goal
of ensuring their helpfulness and harmlessness. However, even carefully aligned
models can be manipulated maliciously, leading to unintended behaviors, known
as ""jailbreaks"". These jailbreaks are typically triggered by specific text
inputs, often referred to as adversarial prompts. In this work, we propose the
generation exploitation attack, an extremely simple approach that disrupts
model alignment by only manipulating variations of decoding methods. By
exploiting different generation strategies, including varying decoding
hyper-parameters and sampling methods, we increase the misalignment rate from
0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,
and MPT families, outperforming state-of-the-art attacks with $30\times$ lower
computational cost. Finally, we propose an effective alignment method that
explores diverse generation strategies, which can reasonably reduce the
misalignment rate under our attack. Altogether, our study underscores a major
failure in current safety evaluation and alignment procedures for open-source
LLMs, strongly advocating for more comprehensive red teaming and better
alignment before releasing such models. Our code is available at
https://github.com/Princeton-SysML/Jailbreak_LLM.
","2023-10-12","2310.06987v1.pdf"
"2310.07018","Yi Ru Wang","Yi Ru Wang, Jiafei Duan, Dieter Fox, Siddhartha Srinivasa","NEWTON: Are Large Language Models Capable of Physical Reasoning?","EMNLP 2023 Findings; 8 pages, 3 figures, 7 tables; Project page:
  https://newtonreasoning.github.io","","","","cs.CL cs.AI cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs), through their contextualized representations,
have been empirically proven to encapsulate syntactic, semantic, word sense,
and common-sense knowledge. However, there has been limited exploration of
their physical reasoning abilities, specifically concerning the crucial
attributes for comprehending everyday objects. To address this gap, we
introduce NEWTON, a repository and benchmark for evaluating the physics
reasoning skills of LLMs. Further, to enable domain-specific adaptation of this
benchmark, we present a pipeline to enable researchers to generate a variant of
this benchmark that has been customized to the objects and attributes relevant
for their application. The NEWTON repository comprises a collection of 2800
object-attribute pairs, providing the foundation for generating infinite-scale
assessment templates. The NEWTON benchmark consists of 160K QA questions,
curated using the NEWTON repository to investigate the physical reasoning
capabilities of several mainstream language models across foundational,
explicit, and implicit reasoning tasks. Through extensive empirical analysis,
our results highlight the capabilities of LLMs for physical reasoning. We find
that LLMs like GPT-4 demonstrate strong reasoning capabilities in
scenario-based tasks but exhibit less consistency in object-attribute reasoning
compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates
its potential for evaluating and enhancing language models, paving the way for
their integration into physically grounded settings, such as robotic
manipulation. Project site: https://newtonreasoning.github.io
","2023-10-12","2310.07018v1.pdf"
"2310.07033","Gabriele Campanella","Gabriele Campanella, Ricky Kwan, Eugene Fluder, Jennifer Zeng, Aryeh
  Stock, Brandon Veremis, Alexandros D. Polydorides, Cyrus Hedvat, Adam
  Schoenfeld, Chad Vanderbilt, Patricia Kovatch, Carlos Cordon-Cardo, Thomas J.
  Fuchs","Computational Pathology at Health System Scale -- Self-Supervised
  Foundation Models from Three Billion Images","","","","","cs.CV cs.AI cs.LG eess.IV","http://creativecommons.org/licenses/by/4.0/","  Recent breakthroughs in self-supervised learning have enabled the use of
large unlabeled datasets to train visual foundation models that can generalize
to a variety of downstream tasks. While this training paradigm is well suited
for the medical domain where annotations are scarce, large-scale pre-training
in the medical domain, and in particular pathology, has not been extensively
studied. Previous work in self-supervised learning in pathology has leveraged
smaller datasets for both pre-training and evaluating downstream performance.
The aim of this project is to train the largest academic foundation model and
benchmark the most prominent self-supervised learning algorithms by
pre-training and evaluating downstream performance on large clinical pathology
datasets. We collected the largest pathology dataset to date, consisting of
over 3 billion images from over 423 thousand microscopy slides. We compared
pre-training of visual transformer models using the masked autoencoder (MAE)
and DINO algorithms. We evaluated performance on six clinically relevant tasks
from three anatomic sites and two institutions: breast cancer detection,
inflammatory bowel disease detection, breast cancer estrogen receptor
prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer
immunotherapy response prediction. Our results demonstrate that pre-training on
pathology data is beneficial for downstream performance compared to
pre-training on natural images. Additionally, the DINO algorithm achieved
better generalization performance across all tasks tested. The presented
results signify a phase change in computational pathology research, paving the
way into a new era of more performant models based on large-scale, parallel
pre-training at the billion-image scale.
","2023-10-12","2310.07033v1.pdf"
"2310.07059","Xueren Ge","Xueren Ge, Ronald Dean Williams, John A. Stankovic, Homa Alemzadeh","DKEC: Domain Knowledge Enhanced Multi-Label Classification for
  Electronic Health Records","Submitted to AAAI 2024","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multi-label text classification (MLTC) tasks in the medical domain often face
long-tail label distribution, where rare classes have fewer training samples
than frequent classes. Although previous works have explored different model
architectures and hierarchical label structures to find important features,
most of them neglect to incorporate the domain knowledge from medical
guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced
Classifier for medical diagnosis prediction with two innovations: (1) a
label-wise attention mechanism that incorporates a heterogeneous graph and
domain ontologies to capture the semantic relationships between medical
entities, (2) a simple yet effective group-wise training method based on
similarity of labels to increase samples of rare classes. We evaluate DKEC on
two real-world medical datasets: the RAA dataset, a collection of 4,417 patient
care reports from emergency medical services (EMS) incidents, and a subset of
53,898 reports from the MIMIC-III dataset. Experimental results show that our
method outperforms the state-of-the-art, particularly for the few-shot (tail)
classes. More importantly, we study the applicability of DKEC to different
language models and show that DKEC can help the smaller language models achieve
comparable performance to large language models.
","2023-10-12","2310.07059v1.pdf"
"2310.07061","He Zhang","He Zhang, Chuhao Wu, Jingyi Xie, ChanMin Kim, John M. Carroll","QualiGPT: GPT as an easy-to-use tool for qualitative coding","25 pages, 7 figures, 1 table, under review","","","","cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Qualitative research delves deeply into individual complex perspectives on
technology and various phenomena. However, a meticulous analysis of qualitative
data often requires a significant amount of time, especially during the crucial
coding stage. Although there is software specifically designed for qualitative
evaluation, many of these platforms fall short in terms of automatic coding,
intuitive usability, and cost-effectiveness. With the rise of Large Language
Models (LLMs) such as GPT-3 and its successors, we are at the forefront of a
transformative era for enhancing qualitative analysis. In this paper, we
introduce QualiGPT, a specialized tool designed after considering challenges
associated with ChatGPT and qualitative analysis. It harnesses the capabilities
of the Generative Pretrained Transformer (GPT) and its API for thematic
analysis of qualitative data. By comparing traditional manual coding with
QualiGPT's analysis on both simulated and actual datasets, we verify that
QualiGPT not only refines the qualitative analysis process but also elevates
its transparency, credibility, and accessibility. Notably, compared to existing
analytical platforms, QualiGPT stands out with its intuitive design,
significantly reducing the learning curve and operational barriers for users.
","2023-10-12","2310.07061v1.pdf"
"2310.07064","Zhaocheng Zhu","Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale
  Schuurmans, Hanjun Dai","Large Language Models can Learn Rules","","","","","cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  When prompted with a few examples and intermediate steps, large language
models (LLMs) have demonstrated impressive performance in various reasoning
tasks. However, prompting methods that rely on implicit knowledge in an LLM
often hallucinate incorrect answers when the implicit knowledge is wrong or
inconsistent with the task. To tackle this problem, we present
Hypotheses-to-Theories (HtT), a framework that learns a rule library for
reasoning with LLMs. HtT contains two stages, an induction stage and a
deduction stage. In the induction stage, an LLM is first asked to generate and
verify rules over a set of training examples. Rules that appear and lead to
correct answers sufficiently often are collected to form a rule library. In the
deduction stage, the LLM is then prompted to employ the learned rule library to
perform reasoning to answer test questions. Experiments on both numerical
reasoning and relational reasoning problems show that HtT improves existing
prompting methods, with an absolute gain of 11-27% in accuracy. The learned
rules are also transferable to different models and to different forms of the
same problem.
","2023-10-12","2310.07064v1.pdf"
"2310.07075","Kexun Zhang","Kexun Zhang, Hongqiao Chen, Lei Li, William Wang","Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State
  Decoding","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have shown promising capabilities in using
external tools to solve complex problems. However, existing approaches either
involve fine-tuning on tool demonstrations, which do not generalize to new
tools without additional training, or providing tool documentation in context,
limiting the number of tools. Both approaches often generate syntactically
invalid tool calls. In this paper, we propose ToolDec, a finite-state
machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates
tool-related errors for any tool-augmented LLMs by ensuring valid tool names
and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively
select tools using only the information contained in their names, with no need
for fine-tuning or in-context documentation. We evaluated multiple prior
methods and their ToolDec-enhanced versions on a variety of tasks involving
tools like math functions, knowledge graph relations, and complex real-world
RESTful APIs. Our experiments show that ToolDec reduces syntactic errors to
zero, consequently achieving significantly better performance and as much as a
2x speedup. We also show that ToolDec achieves superior generalization
performance on unseen tools, performing up to 8x better than the baselines.
","2023-10-12","2310.07075v1.pdf"
"2310.07086","Adway Das","Adway Das, Abhishek Kumar Prajapati, Pengxiang Zhang, Mukund Srinath,
  Andisheh Ranjbari","Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback:
  An NLP Framework","Word Count: 5515 words + 3 table (250 words per table) = 6265 words","","","","cs.AI cs.SI","http://creativecommons.org/licenses/by/4.0/","  Traditional methods of collecting user feedback through transit surveys are
often time-consuming, resource intensive, and costly. In this paper, we propose
a novel NLP-based framework that harnesses the vast, abundant, and inexpensive
data available on social media platforms like Twitter to understand users'
perceptions of various service issues. Twitter, being a microblogging platform,
hosts a wealth of real-time user-generated content that often includes valuable
feedback and opinions on various products, services, and experiences. The
proposed framework streamlines the process of gathering and analyzing user
feedback without the need for costly and time-consuming user feedback surveys
using two techniques. First, it utilizes few-shot learning for tweet
classification within predefined categories, allowing effective identification
of the issues described in tweets. It then employs a lexicon-based sentiment
analysis model to assess the intensity and polarity of the tweet sentiments,
distinguishing between positive, negative, and neutral tweets. The
effectiveness of the framework was validated on a subset of manually labeled
Twitter data and was applied to the NYC subway system as a case study. The
framework accurately classifies tweets into predefined categories related to
safety, reliability, and maintenance of the subway system and effectively
measured sentiment intensities within each category. The general findings were
corroborated through a comparison with an agency-run customer survey conducted
in the same year. The findings highlight the effectiveness of the proposed
framework in gauging user feedback through inexpensive social media data to
understand the pain points of the transit system and plan for targeted
improvements.
","2023-10-12","2310.07086v1.pdf"
"2310.07088","Ranjita Naik","Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi,
  Besmira Nushi","Diversity of Thought Improves Reasoning Abilities of Large Language
  Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are documented to struggle in settings that
require complex reasoning. Nevertheless, instructing the model to break down
the problem into smaller reasoning steps (Wei et al., 2022), or ensembling
various generations through modifying decoding steps (Wang et al., 2023) boosts
performance. Current methods assume that the input prompt is fixed and expect
the decoding strategies to introduce the diversity needed for ensembling. In
this work, we relax this assumption and discuss how one can create and leverage
variations of the input prompt as a means to diversity of thought to improve
model performance. We propose a method that automatically improves prompt
diversity by soliciting feedback from the LLM to ideate approaches that fit for
the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse
reasoning path Self-Ensemble) across multiple inference calls. We also propose
a cost-effective alternative where diverse prompts are used within a single
inference call; we call this IDIV-SE (In-call DIVerse reasoning path
Self-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform
the previously discussed baselines using both GPT-3.5 and GPT-4 on several
reasoning benchmarks, without modifying the decoding process. Additionally,
DIV-SE advances state-of-the-art performance on recent planning benchmarks
(Valmeekam et al., 2023), exceeding the highest previously reported accuracy by
at least 29.6 percentage points on the most challenging 4/5 Blocksworld task.
Our results shed light on how to enforce prompt diversity toward LLM reasoning
and thereby improve the pareto frontier of the accuracy-cost trade-off.
","2023-10-12","2310.07088v1.pdf"
"2310.07091","Jieting Long","Jieting Long, Zewei Shi, Penghao Jiang, Yidong Gan","Jaeger: A Concatenation-Based Multi-Transformer VQA Model","This paper is the technical research paper of CIKM 2023 DocIU
  challenges. The authors received the CIKM 2023 DocIU Winner Award, sponsored
  by Google, Microsoft, and the Centre for data-driven geoscience","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Document-based Visual Question Answering poses a challenging task between
linguistic sense disambiguation and fine-grained multimodal retrieval. Although
there has been encouraging progress in document-based question answering due to
the utilization of large language and open-world prior models\cite{1}, several
challenges persist, including prolonged response times, extended inference
durations, and imprecision in matching. In order to overcome these challenges,
we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive
question features, we leverage the exceptional capabilities of RoBERTa
large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we
subject the outputs from both models to a concatenation process. This operation
allows the model to consider information from diverse sources concurrently,
strengthening its representational capability. By leveraging pre-trained models
for feature extraction, our approach has the potential to amplify the
performance of these models through concatenation. After concatenation, we
apply dimensionality reduction to the output features, reducing the model's
computational effectiveness and inference time. Empirical results demonstrate
that our proposed model achieves competitive performance on Task C of the
PDF-VQA Dataset. If the user adds any new data, they should make sure to style
it as per the instructions provided in previous sections.
","2023-10-20","2310.07091v1.pdf"
"2310.07093","Abhibha Gupta","Arushi Sharma, Abhibha Gupta, Maneesh Bilalpur","Argumentative Stance Prediction: An Exploratory Study on Multimodality
  and Few-Shot Learning","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  To advance argumentative stance prediction as a multimodal problem, the First
Shared Task in Multimodal Argument Mining hosted stance prediction in crucial
social topics of gun control and abortion. Our exploratory study attempts to
evaluate the necessity of images for stance prediction in tweets and compare
out-of-the-box text-based large-language models (LLM) in few-shot settings
against fine-tuned unimodal and multimodal models. Our work suggests an
ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms
both the multimodal (0.677 F1-score) and text-based few-shot prediction using a
recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in
performance, our findings suggest that the multimodal models tend to perform
better when image content is summarized as natural language over their native
pixel structure and, using in-context examples improves few-shot performance of
LLMs.
","2023-10-12","2310.07093v1.pdf"
"2310.07096","Shawn Tan","Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan","Sparse Universal Transformer","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Universal Transformer (UT) is a variant of the Transformer that shares
parameters across its layers. Empirical evidence shows that UTs have better
compositional generalization than Vanilla Transformers (VTs) in formal language
tasks. The parameter-sharing also affords it better parameter efficiency than
VTs. Despite its many advantages, scaling UT parameters is much more compute
and memory intensive than scaling up a VT. This paper proposes the Sparse
Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE)
and a new stick-breaking-based dynamic halting mechanism to reduce UT's
computation complexity while retaining its parameter efficiency and
generalization ability. Experiments show that SUT achieves the same performance
as strong baseline models while only using half computation and parameters on
WMT'14 and strong generalization results on formal language tasks (Logical
inference and CFQ). The new halting mechanism also enables around 50\%
reduction in computation during inference with very little performance decrease
on formal language tasks.
","2023-10-12","2310.07096v1.pdf"
"2310.07099","Benjamin Kereopa-Yorke Mr","Benjamin Kereopa-Yorke","ClausewitzGPT Framework: A New Frontier in Theoretical Large Language
  Model Enhanced Information Operations","14 pages, 14 figures","","","","cs.CY cs.AI cs.CR cs.SI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  In a digital epoch where cyberspace is the emerging nexus of geopolitical
contention, the melding of information operations and Large Language Models
(LLMs) heralds a paradigm shift, replete with immense opportunities and
intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023)
democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of
actors, from sovereign nations to rogue entities (Howard et al., 2023), find
themselves equipped with potent narrative-shaping instruments (Goldstein et
al., 2023). This paper puts forth a framework for navigating this brave new
world in the ""ClausewitzGPT"" equation. This novel formulation not only seeks to
quantify the risks inherent in machine-speed LLM-augmented operations but also
underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023).
These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge
as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race
forward, we do not lose sight of moral compasses and societal imperatives.
  Mathematically underpinned and inspired by the timeless tenets of
Clausewitz's military strategy (Clausewitz, 1832), this thesis delves into the
intricate dynamics of AI-augmented information operations. With references to
recent findings and research (Department of State, 2023), it highlights the
staggering year-on-year growth of AI information campaigns (Evgeny Pashentsev,
2023), stressing the urgency of our current juncture. The synthesis of
Enlightenment thinking, and Clausewitz's principles provides a foundational
lens, emphasising the imperative of clear strategic vision, ethical
considerations, and holistic understanding in the face of rapid technological
advancement.
","2023-10-12","2310.07099v1.pdf"
"2310.07100","Yixin Liu","Yixin Liu, Chenrui Fan, Xun Chen, Pan Zhou and Lichao Sun","GraphCloak: Safeguarding Task-specific Knowledge within Graph-structured
  Data from Unauthorized Exploitation","","","","","cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As Graph Neural Networks (GNNs) become increasingly prevalent in a variety of
fields, from social network analysis to protein-protein interaction studies,
growing concerns have emerged regarding the unauthorized utilization of
personal data. Recent studies have shown that imperceptible poisoning attacks
are an effective method of protecting image data from such misuse. However, the
efficacy of this approach in the graph domain remains unexplored. To bridge
this gap, this paper introduces GraphCloak to safeguard against the
unauthorized usage of graph data. Compared with prior work, GraphCloak offers
unique significant innovations: (1) graph-oriented, the perturbations are
applied to both topological structures and descriptive features of the graph;
(2) effective and stealthy, our cloaking method can bypass various inspections
while causing a significant performance drop in GNNs trained on the cloaked
graphs; and (3) stable across settings, our methods consistently perform
effectively under a range of practical settings with limited knowledge. To
address the intractable bi-level optimization problem, we propose two
error-minimizing-based poisoning methods that target perturbations on the
structural and feature space, along with a subgraph injection poisoning method.
Our comprehensive evaluation of these methods underscores their effectiveness,
stealthiness, and stability. We also delve into potential countermeasures and
provide analytical justification for their effectiveness, paving the way for
intriguing future research.
","2023-10-12","2310.07100v1.pdf"
"2310.07116","Nan Zhang","Nan Zhang, Rami Bahsoon, Nikos Tziritas, Georgios Theodoropoulos","A Digital Twin Approach for Adaptive Compliance in Cyber-Physical
  Systems: Case of Smart Warehouse Logistics","10 pages, 10 figures, submitted to IEEE Transactions on Systems, Man,
  and Cybernetics: Systems. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible","","","","eess.SY cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Engineering regulatory compliance in complex Cyber-Physical Systems (CPS),
such as smart warehouse logistics, is challenging due to the open and dynamic
nature of these systems, scales, and unpredictable modes of human-robot
interactions that can be best learnt at runtime. Traditional offline approaches
for engineering compliance often involve modelling at a higher, more abstract
level (e.g. using languages like SysML). These abstract models only support
analysis in offline-designed and simplified scenarios. However, open and
complex systems may be unpredictable, and their behaviours are difficult to be
fully captured by abstract models. These systems may also involve other
business goals, possibly conflicting with regulatory compliance. To overcome
these challenges, fine-grained simulation models are promising to complement
abstract models and support accurate runtime predictions and performance
evaluation with trade-off analysis. The novel contribution of this work is a
Digital Twin-oriented architecture for adaptive compliance leveraging abstract
goal modelling, fine-grained agent-based modelling and runtime simulation for
managing compliance trade-offs. A case study from smart warehouse logistics is
used to demonstrate the approach considering safety and productivity
trade-offs.
","2023-10-12","2310.07116v1.pdf"
"2310.07132","Youssef Mroueh","Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald,
  Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, and Jerret
  Ross","Risk Assessment and Statistical Significance in the Age of Foundation
  Models","","","","","cs.LG math.ST q-fin.RM stat.ML stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose a distributional framework for assessing socio-technical risks of
foundation models with quantified statistical significance. Our approach hinges
on a new statistical relative testing based on first and second order
stochastic dominance of real random variables. We show that the second order
statistics in this test are linked to mean-risk models commonly used in
econometrics and mathematical finance to balance risk and utility when choosing
between alternatives. Using this framework, we formally develop a risk-aware
approach for foundation model selection given guardrails quantified by
specified metrics. Inspired by portfolio optimization and selection theory in
mathematical finance, we define a \emph{metrics portfolio} for each model as a
means to aggregate a collection of metrics, and perform model selection based
on the stochastic dominance of these portfolios. The statistical significance
of our tests is backed theoretically by an asymptotic analysis via central
limit theorems instantiated in practice via a bootstrap variance estimate. We
use our framework to compare various large language models regarding risks
related to drifting from instructions and outputting toxic content.
","2023-10-12","2310.07132v1.pdf"
"2310.07136","Dar Gilboa","Dar Gilboa and Jarrod R. McClean","Exponential Quantum Communication Advantage in Distributed Learning","","","","","quant-ph stat.ML","http://creativecommons.org/licenses/by/4.0/","  Training and inference with large machine learning models that far exceed the
memory capacity of individual devices necessitates the design of distributed
architectures, forcing one to contend with communication constraints. We
present a framework for distributed computation over a quantum network in which
data is encoded into specialized quantum states. We prove that for certain
models within this framework, inference and training using gradient descent can
be performed with exponentially less communication compared to their classical
analogs, and with relatively modest time and space complexity overheads
relative to standard gradient-based methods. To our knowledge, this is the
first example of exponential quantum advantage for a generic class of machine
learning problems with dense classical data that holds regardless of the data
encoding cost. Moreover, we show that models in this class can encode highly
nonlinear features of their inputs, and their expressivity increases
exponentially with model depth. We also find that, interestingly, the
communication advantage nearly vanishes for simpler linear classifiers. These
results can be combined with natural privacy advantages in the communicated
quantum states that limit the amount of information that can be extracted from
them about the data and model parameters. Taken as a whole, these findings form
a promising foundation for distributed machine learning over quantum networks.
","2023-10-12","2310.07136v1.pdf"
"2310.07146","Zhiyu Chen","Zhiyu Chen, Yujie Lu, William Yang Wang","Empowering Psychotherapy with Large Language Models: Cognitive
  Distortion Detection through Diagnosis of Thought Prompting","EMNLP 2023 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Mental illness remains one of the most critical public health issues of our
time, due to the severe scarcity and accessibility limit of professionals.
Psychotherapy requires high-level expertise to conduct deep, complex reasoning
and analysis on the cognition modeling of the patients. In the era of Large
Language Models, we believe it is the right time to develop AI assistance for
computational psychotherapy. We study the task of cognitive distortion
detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs
diagnosis on the patient's speech via three stages: subjectivity assessment to
separate the facts and the thoughts; contrastive reasoning to elicit the
reasoning processes supporting and contradicting the thoughts; and schema
analysis to summarize the cognition schemas. The generated diagnosis rationales
through the three stages are essential for assisting the professionals.
Experiments demonstrate that DoT obtains significant improvements over ChatGPT
for cognitive distortion detection, while generating high-quality rationales
approved by human experts.
","2023-10-12","2310.07146v1.pdf"
"2310.07147","Zhikai Li","Zhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt
  Keutzer","QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have showcased remarkable impacts across a wide
spectrum of natural language processing tasks. Fine-tuning these pre-trained
models on downstream datasets provides further significant performance gains,
but this process has been challenging due to its extraordinary resource
requirements. To this end, existing efforts focus on parameter-efficient
fine-tuning, which, unfortunately, fail to capitalize on the powerful potential
of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized
Full-parameter Tuning framework for LLMs that enables memory-efficient
fine-tuning without harming performance. Our framework incorporates two novel
ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the
momentum and has consistent update magnitudes for each parameter, an inherent
advantage for robust quantization; and (ii) we quantize all model states and
store them as integer values, and present a gradient flow and parameter update
scheme for the quantized weights. As a result, QFT reduces the model state
memory to 21% of the standard solution while achieving comparable performance,
e.g., tuning a LLaMA-7B model requires only <30GB of memory, satisfied by a
single A6000 GPU.
","2023-10-12","2310.07147v1.pdf"
"2310.07152","Ziqi Zhang","Ziqi Zhang, Chen Gong, Yifeng Cai, Yuanyuan Yuan, Bingyan Liu, Ding
  Li, Yao Guo, Xiangqun Chen","No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN
  Partition for On-Device ML","Accepted by S&P'24","","","","cs.CR cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  On-device ML introduces new security challenges: DNN models become white-box
accessible to device users. Based on white-box information, adversaries can
conduct effective model stealing (MS) and membership inference attack (MIA).
Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims
to downgrade (easy) white-box attacks to (harder) black-box attacks. However,
one major shortcoming is the sharply increased latency (up to 50X). To
accelerate TEE-shield DNN computation with GPUs, researchers proposed several
model partition techniques. These solutions, referred to as TEE-Shielded DNN
Partition (TSDP), partition a DNN model into two parts, offloading the
privacy-insensitive part to the GPU while shielding the privacy-sensitive part
within the TEE. This paper benchmarks existing TSDP solutions using both MS and
MIA across a variety of DNN models, datasets, and metrics. We show important
findings that existing TSDP solutions are vulnerable to privacy-stealing
attacks and are not as safe as commonly believed. We also unveil the inherent
difficulty in deciding optimal DNN partition configurations (i.e., the highest
security with minimal utility cost) for present TSDP solutions. The experiments
show that such ``sweet spot'' configurations vary across datasets and models.
Based on lessons harvested from the experiments, we present TEESlice, a novel
TSDP method that defends against MS and MIA during DNN inference. TEESlice
follows a partition-before-training strategy, which allows for accurate
separation between privacy-related weights from public weights. TEESlice
delivers the same security protection as shielding the entire DNN model inside
TEE (the ``upper-bound'' security guarantees) with over 10X less overhead (in
both experimental and real-world environments) than prior TSDP solutions and no
accuracy loss.
","2023-10-12","2310.07152v1.pdf"
"2310.07160","Joshua Gardner","Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner","LLark: A Multimodal Foundation Model for Music","","","","","cs.SD cs.LG eess.AS","http://creativecommons.org/licenses/by/4.0/","  Music has a unique and complex structure which is challenging for both expert
humans and existing AI systems to understand, and presents unique challenges
relative to other forms of audio. We present LLark, an instruction-tuned
multimodal model for music understanding. We detail our process for dataset
creation, which involves augmenting the annotations of diverse open-source
music datasets and converting them to a unified instruction-tuning format. We
propose a multimodal architecture for LLark, integrating a pretrained
generative model for music with a pretrained language model. In evaluations on
three types of tasks (music understanding, captioning, and reasoning), we show
that our model matches or outperforms existing baselines in zero-shot
generalization for music understanding, and that humans show a high degree of
agreement with the model's responses in captioning and reasoning tasks. LLark
is trained entirely from open-source music data and models, and we make our
training code available along with the release of this paper. Additional
results and audio examples are at https://bit.ly/llark, and our source code is
available at https://github.com/spotify-research/llark .
","2023-10-12","2310.07160v1.pdf"
"2310.07170","Eiki Murata","Tatsuya Ide, Eiki Murata, Daisuke Kawahara, Takato Yamazaki, Shengzhe
  Li, Kenta Shinzato, Toshinori Sato","PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a
  Language Model","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Despite the remarkable progress in natural language understanding with
pretrained Transformers, neural language models often do not handle commonsense
knowledge well. Toward commonsense-aware models, there have been attempts to
obtain knowledge, ranging from automatic acquisition to crowdsourcing. However,
it is difficult to obtain a high-quality knowledge base at a low cost,
especially from scratch. In this paper, we propose PHALM, a method of building
a knowledge graph from scratch, by prompting both crowdworkers and a large
language model (LLM). We used this method to build a Japanese event knowledge
graph and trained Japanese commonsense generation models. Experimental results
revealed the acceptability of the built graph and inferences generated by the
trained models. We also report the difference in prompting humans and an LLM.
Our code, data, and models are available at
github.com/nlp-waseda/comet-atomic-ja.
","2023-10-12","2310.07170v1.pdf"
"2310.07177","Xiaoxuan Liu","Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng,
  Alvin Cheung, Hao Zhang","Online Speculative Decoding","","","","","cs.AI cs.CL cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  Speculative decoding is a pivotal technique to accelerate the inference of
large language models (LLMs) by employing a smaller draft model to predict the
target model's outputs. However, its efficacy can be limited due to the low
predictive accuracy of the draft model, particularly when faced with diverse
text inputs and a significant capability gap between the draft and target
models. We introduce online speculative decoding (OSD) to address this
challenge. The main idea is to continually update (multiple) draft model(s) on
observed user query data using the abundant excess computational power in an
LLM serving cluster. Given that LLM inference is memory-bounded, the surplus
computational power in a typical LLM serving cluster can be repurposed for
online retraining of draft models, thereby making the training cost-neutral.
Since the query distribution of an LLM service is relatively simple, retraining
on query distribution enables the draft model to more accurately predict the
target model's outputs, particularly on data originating from query
distributions. As the draft model evolves online, it aligns with the query
distribution in real time, mitigating distribution shifts. We develop a
prototype of online speculative decoding based on online knowledge distillation
and evaluate it using both synthetic and real query data on several popular
LLMs. The results show a substantial increase in the token acceptance rate by
0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.
","2023-10-19","2310.07177v1.pdf"
"2310.07188","Yitao Yang","Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong Xu","Adaptive Gating in Mixture-of-Experts based Language Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models, such as OpenAI's ChatGPT, have demonstrated
exceptional language understanding capabilities in various NLP tasks. Sparsely
activated mixture-of-experts (MoE) has emerged as a promising solution for
scaling models while maintaining a constant number of computational operations.
Existing MoE model adopts a fixed gating network where each token is computed
by the same number of experts. However, this approach contradicts our intuition
that the tokens in each sequence vary in terms of their linguistic complexity
and, consequently, require different computational costs. Little is discussed
in prior research on the trade-off between computation per token and model
performance. This paper introduces adaptive gating in MoE, a flexible training
strategy that allows tokens to be processed by a variable number of experts
based on expert probability distribution. The proposed framework preserves
sparsity while improving training efficiency. Additionally, curriculum learning
is leveraged to further reduce training time. Extensive experiments on diverse
NLP tasks show that adaptive gating reduces at most 22.5% training time while
maintaining inference quality. Moreover, we conduct a comprehensive analysis of
the routing decisions and present our insights when adaptive gating is used.
","2023-10-12","2310.07188v1.pdf"
"2310.07197","Zongguo Wang","Ziyi Chen, Fankai Xie, Meng Wan, Yang Yuan, Miao Liu, Zongguo Wang,
  Sheng Meng, Yangang Wang","MatChat: A Large Language Model and Application Service Platform for
  Materials Science","","","","","cond-mat.mtrl-sci cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The prediction of chemical synthesis pathways plays a pivotal role in
materials science research. Challenges, such as the complexity of synthesis
pathways and the lack of comprehensive datasets, currently hinder our ability
to predict these chemical processes accurately. However, recent advancements in
generative artificial intelligence (GAI), including automated text generation
and question-answering systems, coupled with fine-tuning techniques, have
facilitated the deployment of large-scale AI models tailored to specific
domains. In this study, we harness the power of the LLaMA2-7B model and enhance
it through a learning process that incorporates 13,878 pieces of structured
material knowledge data. This specialized AI model, named MatChat, focuses on
predicting inorganic material synthesis pathways. MatChat exhibits remarkable
proficiency in generating and reasoning with knowledge in materials science.
Although MatChat requires further refinement to meet the diverse material
design needs, this research undeniably highlights its impressive reasoning
capabilities and innovative potential in the field of materials science.
MatChat is now accessible online and open for use, with both the model and its
application framework available as open source. This study establishes a robust
foundation for collaborative innovation in the integration of generative AI in
materials science.
","2023-10-12","2310.07197v1.pdf"
"2310.07204","Ryan Po","Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T.
  Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski,
  Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias
  Nie{\ss}ner, Bj\""orn Ommer, Christian Theobalt, Peter Wonka, Gordon Wetzstein","State of the Art on Diffusion Models for Visual Computing","","","","","cs.AI cs.CV cs.GR cs.LG","http://creativecommons.org/licenses/by/4.0/","  The field of visual computing is rapidly advancing due to the emergence of
generative artificial intelligence (AI), which unlocks unprecedented
capabilities for the generation, editing, and reconstruction of images, videos,
and 3D scenes. In these domains, diffusion models are the generative AI
architecture of choice. Within the last year alone, the literature on
diffusion-based tools and applications has seen exponential growth and relevant
papers are published across the computer graphics, computer vision, and AI
communities with new works appearing daily on arXiv. This rapid growth of the
field makes it difficult to keep up with all recent developments. The goal of
this state-of-the-art report (STAR) is to introduce the basic mathematical
concepts of diffusion models, implementation details and design choices of the
popular Stable Diffusion model, as well as overview important aspects of these
generative AI tools, including personalization, conditioning, inversion, among
others. Moreover, we give a comprehensive overview of the rapidly growing
literature on diffusion-based generation and editing, categorized by the type
of generated medium, including 2D images, videos, 3D objects, locomotion, and
4D scenes. Finally, we discuss available datasets, metrics, open challenges,
and social implications. This STAR provides an intuitive starting point to
explore this exciting topic for researchers, artists, and practitioners alike.
","2023-10-16","2310.07204v1.pdf"
"2310.07219","Abigail Goldsteen","Shlomit Shachor, Natalia Razinkov, Abigail Goldsteen","Improved Membership Inference Attacks Against Language Classification
  Models","","","","","cs.LG cs.AI cs.CR","http://creativecommons.org/licenses/by/4.0/","  Artificial intelligence systems are prevalent in everyday life, with use
cases in retail, manufacturing, health, and many other fields. With the rise in
AI adoption, associated risks have been identified, including privacy risks to
the people whose data was used to train models. Assessing the privacy risks of
machine learning models is crucial to enabling knowledgeable decisions on
whether to use, deploy, or share a model. A common approach to privacy risk
assessment is to run one or more known attacks against the model and measure
their success rate. We present a novel framework for running membership
inference attacks against classification models. Our framework takes advantage
of the ensemble method, generating many specialized attack models for different
subsets of the data. We show that this approach achieves higher accuracy than
either a single attack model or an attack model per class label, both on
classical and language classification tasks.
","2023-10-12","2310.07219v1.pdf"
"2310.07225","Adam Mahdi","Karolina Korgul, Andrew M. Bean, Felix Krones, Robert McCraith, Adam
  Mahdi","Exploring the Landscape of Large Language Models In Medical Question
  Answering: Observations and Open Questions","11 pages, 8 figures","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have shown promise in medical question answering
by achieving passing scores in standardised exams and have been suggested as
tools for supporting healthcare workers. Deploying LLMs into such a high-risk
context requires a clear understanding of the limitations of these models. With
the rapid development and release of new LLMs, it is especially valuable to
identify patterns which exist across models and may, therefore, continue to
appear in newer versions. In this paper, we evaluate a wide range of popular
LLMs on their knowledge of medical questions in order to better understand
their properties as a group. From this comparison, we provide preliminary
observations and raise open questions for further research.
","2023-10-12","2310.07225v1.pdf"
"2310.07235","Nimrah Mustafa","Nimrah Mustafa, Aleksandar Bojchevski, Rebekka Burkholz","Are GATs Out of Balance?","25 pages. To be published in Advances in Neural Information
  Processing Systems (NeurIPS), 2023","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While the expressive power and computational capabilities of graph neural
networks (GNNs) have been theoretically studied, their optimization and
learning dynamics, in general, remain largely unexplored. Our study undertakes
the Graph Attention Network (GAT), a popular GNN architecture in which a node's
neighborhood aggregation is weighted by parameterized attention coefficients.
We derive a conservation law of GAT gradient flow dynamics, which explains why
a high portion of parameters in GATs with standard initialization struggle to
change during training. This effect is amplified in deeper GATs, which perform
significantly worse than their shallow counterparts. To alleviate this problem,
we devise an initialization scheme that balances the GAT network. Our approach
i) allows more effective propagation of gradients and in turn enables
trainability of deeper networks, and ii) attains a considerable speedup in
training and convergence time in comparison to the standard initialization. Our
main theorem serves as a stepping stone to studying the learning dynamics of
positive homogeneous models with attention mechanisms.
","2023-10-26","2310.07235v1.pdf"
"2310.07236","Liyang Chen","Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin
  Kang, Haozhi Huang","AdaMesh: Personalized Facial Expressions and Head Poses for
  Speech-Driven 3D Facial Animation","Project Page: https://adamesh.github.io","","","","cs.CV cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Speech-driven 3D facial animation aims at generating facial movements that
are synchronized with the driving speech, which has been widely explored
recently. Existing works mostly neglect the person-specific talking style in
generation, including facial expression and head pose styles. Several works
intend to capture the personalities by fine-tuning modules. However, limited
training data leads to the lack of vividness. In this work, we propose AdaMesh,
a novel adaptive speech-driven facial animation approach, which learns the
personalized talking style from a reference video of about 10 seconds and
generates vivid facial expressions and head poses. Specifically, we propose
mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,
which efficiently captures the facial expression style. For the personalized
pose style, we propose a pose adapter by building a discrete pose prior and
retrieving the appropriate style embedding with a semantic-aware pose style
matrix without fine-tuning. Extensive experimental results show that our
approach outperforms state-of-the-art methods, preserves the talking style in
the reference video, and generates vivid facial animation. The supplementary
video and code will be available at https://adamesh.github.io.
","2023-10-12","2310.07236v1.pdf"
"2310.07240","Yuhan Liu","Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang
  Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh
  Ananthanarayanan, Junchen Jiang","CacheGen: Fast Context Loading for Language Model Applications","","","","","cs.NI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As large language models (LLMs) take on more complex tasks, their inputs
incorporate longer contexts to respond to questions that require domain
knowledge or user-specific conversational histories. Yet, using long contexts
poses a challenge for responsive LLM systems, as nothing can be generated until
all the contexts are fetched to and processed by the LLM. Existing systems
optimize only the computation delay in context processing (e.g., by caching
intermediate key-value features of the text context) but often cause longer
network delays in context fetching (e.g., key-value features consume orders of
magnitude larger bandwidth than the text context).
  This paper presents CacheGen to minimize the delays in fetching and
processing contexts for LLMs. CacheGen reduces the bandwidth needed for
transmitting long contexts' key-value (KV) features through a novel encoder
that compresses KV features into more compact bitstream representations. The
encoder combines adaptive quantization with a tailored arithmetic coder, taking
advantage of the KV features' distributional properties, such as locality
across tokens. Furthermore, CacheGen minimizes the total delay in fetching and
processing a context by using a controller that determines when to load the
context as compressed KV features or raw text and picks the appropriate
compression level if loaded as KV features. We test CacheGen on three models of
various sizes and three datasets of different context lengths. Compared to
recent methods that handle long contexts, CacheGen reduces bandwidth usage by
3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x
while maintaining similar LLM performance on various tasks as loading the text
contexts.
","2023-10-12","2310.07240v1.pdf"
"2310.07246","Yuanjun Lv","Xinfa Zhu, Yuanjun Lv, Yi Lei, Tao Li, Wendi He, Hongbin Zhou, Heng
  Lu, Lei Xie","Vec-Tok Speech: speech vectorization and tokenization for neural speech
  generation","15 pages, 2 figures","","","","cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models (LMs) have recently flourished in natural language processing
and computer vision, generating high-fidelity texts or images in various tasks.
In contrast, the current speech generative models are still struggling
regarding speech quality and task generalization. This paper presents Vec-Tok
Speech, an extensible framework that resembles multiple speech generation
tasks, generating expressive and high-fidelity speech. Specifically, we propose
a novel speech codec based on speech vectors and semantic tokens. Speech
vectors contain acoustic details contributing to high-fidelity speech
reconstruction, while semantic tokens focus on the linguistic content of
speech, facilitating language modeling. Based on the proposed speech codec,
Vec-Tok Speech leverages an LM to undertake the core of speech generation.
Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and
bit rate for lower exposure bias and longer context coverage, improving the
performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual
zero-shot voice conversion (VC), zero-shot speaking style transfer
text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising,
and speaker de-identification and anonymization. Experiments show that Vec-Tok
Speech, built on 50k hours of speech, performs better than other SOTA models.
Code will be available at https://github.com/BakerBunker/VecTok .
","2023-10-13","2310.07246v1.pdf"
"2310.07251","Utkarsh Agarwal","Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit
  Choudhury","Ethical Reasoning over Moral Alignment: A Case and Framework for
  In-Context Ethical Policies in LLMs","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  In this position paper, we argue that instead of morally aligning LLMs to
specific set of ethical principles, we should infuse generic ethical reasoning
capabilities into them so that they can handle value pluralism at a global
scale. When provided with an ethical policy, an LLM should be capable of making
decisions that are ethically consistent to the policy. We develop a framework
that integrates moral dilemmas with moral principles pertaining to different
foramlisms of normative ethics, and at different levels of abstractions.
Initial experiments with GPT-x models shows that while GPT-4 is a nearly
perfect ethical reasoner, the models still have bias towards the moral values
of Western and English speaking societies.
","2023-10-12","2310.07251v1.pdf"
"2310.07263","Daniel Tanneberg","Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg
  Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg,
  Michael Gienger","CoPAL: Corrective Planning of Robot Actions with Large Language Models","","","","","cs.RO cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the pursuit of fully autonomous robotic systems capable of taking over
tasks traditionally performed by humans, the complexity of open-world
environments poses a considerable challenge. Addressing this imperative, this
study contributes to the field of Large Language Models (LLMs) applied to task
and motion planning for robots. We propose a system architecture that
orchestrates a seamless interplay between multiple cognitive levels,
encompassing reasoning, planning, and motion generation. At its core lies a
novel replanning strategy that handles physically grounded, logical, and
semantic errors in the generated plans. We demonstrate the efficacy of the
proposed feedback architecture, particularly its impact on executability,
correctness, and time complexity via empirical evaluation in the context of a
simulation and two intricate real-world scenarios: blocks world, barman and
pizza preparation.
","2023-10-12","2310.07263v1.pdf"
"2310.07276","Qizhi Pei","Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu,
  Yingce Xia, Rui Yan","BioT5: Enriching Cross-modal Integration in Biology with Chemical
  Knowledge and Natural Language Associations","Accepted by Empirical Methods in Natural Language Processing 2023
  (EMNLP 2023)","","","","cs.CL cs.AI cs.LG q-bio.BM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in biological research leverage the integration of
molecules, proteins, and natural language to enhance drug discovery. However,
current models exhibit several limitations, such as the generation of invalid
molecular SMILES, underutilization of contextual information, and equal
treatment of structured and unstructured knowledge. To address these issues, we
propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches
cross-modal integration in biology with chemical knowledge and natural language
associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular
representations and extracts knowledge from the surrounding context of
bio-entities in unstructured biological literature. Furthermore,
$\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,
leading to more effective utilization of information. After fine-tuning, BioT5
shows superior performance across a wide range of tasks, demonstrating its
strong capability of capturing underlying relations and properties of
bio-entities. Our code is available at
$\href{https://github.com/QizhiPei/BioT5}{Github}$.
","2023-10-18","2310.07276v1.pdf"
"2310.07282","Anoop V. S.","Shyni Sharaf and V. S. Anoop","An Analysis on Large Language Models in Healthcare: A Case Study of
  BioBERT","","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper conducts a comprehensive investigation into applying large
language models, particularly on BioBERT, in healthcare. It begins with
thoroughly examining previous natural language processing (NLP) approaches in
healthcare, shedding light on the limitations and challenges these methods
face. Following that, this research explores the path that led to the
incorporation of BioBERT into healthcare applications, highlighting its
suitability for addressing the specific requirements of tasks related to
biomedical text mining. The analysis outlines a systematic methodology for
fine-tuning BioBERT to meet the unique needs of the healthcare domain. This
approach includes various components, including the gathering of data from a
wide range of healthcare sources, data annotation for tasks like identifying
medical entities and categorizing them, and the application of specialized
preprocessing techniques tailored to handle the complexities found in
biomedical texts. Additionally, the paper covers aspects related to model
evaluation, with a focus on healthcare benchmarks and functions like processing
of natural language in biomedical, question-answering, clinical document
classification, and medical entity recognition. It explores techniques to
improve the model's interpretability and validates its performance compared to
existing healthcare-focused language models. The paper thoroughly examines
ethical considerations, particularly patient privacy and data security. It
highlights the benefits of incorporating BioBERT into healthcare contexts,
including enhanced clinical decision support and more efficient information
retrieval. Nevertheless, it acknowledges the impediments and complexities of
this integration, encompassing concerns regarding data privacy, transparency,
resource-intensive requirements, and the necessity for model customization to
align with diverse healthcare domains.
","2023-10-13","2310.07282v1.pdf"
"2310.07284","Xiang Hao","Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan","Typing to Listen at the Cocktail Party: Text-Guided Target Speaker
  Extraction","Under review, https://github.com/haoxiangsnr/llm-tse","","","","eess.AS cs.CL","http://creativecommons.org/licenses/by/4.0/","  Humans possess an extraordinary ability to selectively focus on the sound
source of interest amidst complex acoustic environments, commonly referred to
as cocktail party scenarios. In an attempt to replicate this remarkable
auditory attention capability in machines, target speaker extraction (TSE)
models have been developed. These models leverage the pre-registered cues of
the target speaker to extract the sound source of interest. However, the
effectiveness of these models is hindered in real-world scenarios due to the
unreliable or even absence of pre-registered cues. To address this limitation,
this study investigates the integration of natural language description to
enhance the feasibility, controllability, and performance of existing TSE
models. Specifically, we propose a model named LLM-TSE, wherein a large
language model (LLM) extracts useful semantic cues from the user's typed text
input. These cues can serve as independent extraction cues, task selectors to
control the TSE process or complement the pre-registered cues. Our experimental
results demonstrate competitive performance when only text-based cues are
presented, the effectiveness of using input text as a task selector, and a new
state-of-the-art when combining text-based cues with pre-registered cues. To
our knowledge, this is the first study to successfully incorporate LLMs to
guide target speaker extraction, which can be a cornerstone for cocktail party
problem research.
","2023-10-17","2310.07284v1.pdf"
"2310.07289","Liang Chen","Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, Tat-Seng
  Chua, Kam-Fai Wong","Beyond Factuality: A Comprehensive Evaluation of Large Language Models
  as Knowledge Generators","Accepted to EMNLP 2023 main conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) outperform information retrieval techniques for
downstream knowledge-intensive tasks when being prompted to generate world
knowledge. However, community concerns abound regarding the factuality and
potential implications of using this uncensored knowledge. In light of this, we
introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to
systematically and automatically evaluate generated knowledge from six
important perspectives -- Factuality, Relevance, Coherence, Informativeness,
Helpfulness and Validity. We conduct an extensive empirical analysis of the
generated knowledge from three different types of LLMs on two widely studied
knowledge-intensive tasks, i.e., open-domain question answering and
knowledge-grounded dialogue. Surprisingly, our study reveals that the
factuality of generated knowledge, even if lower, does not significantly hinder
downstream tasks. Instead, the relevance and coherence of the outputs are more
important than small factual mistakes. Further, we show how to use CONNER to
improve knowledge-intensive tasks by designing two strategies: Prompt
Engineering and Knowledge Selection. Our evaluation code and LLM-generated
knowledge with human annotations will be released to facilitate future
research.
","2023-10-12","2310.07289v1.pdf"
"2310.07298","Robin Staab","Robin Staab, Mark Vero, Mislav Balunovi\'c, Martin Vechev","Beyond Memorization: Violating Privacy Via Inference with Large Language
  Models","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Current privacy research on large language models (LLMs) primarily focuses on
the issue of extracting memorized training data. At the same time, models'
inference capabilities have increased drastically. This raises the key question
of whether current LLMs could violate individuals' privacy by inferring
personal attributes from text given at inference time. In this work, we present
the first comprehensive study on the capabilities of pretrained LLMs to infer
personal attributes from text. We construct a dataset consisting of real Reddit
profiles, and show that current LLMs can infer a wide range of personal
attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and
$95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time
($240\times$) required by humans. As people increasingly interact with
LLM-powered chatbots across all aspects of life, we also explore the emerging
threat of privacy-invasive chatbots trying to extract personal information
through seemingly benign questions. Finally, we show that common mitigations,
i.e., text anonymization and model alignment, are currently ineffective at
protecting user privacy against LLM inference. Our findings highlight that
current LLMs can infer personal data at a previously unattainable scale. In the
absence of working defenses, we advocate for a broader discussion around LLM
privacy implications beyond memorization, striving for a wider privacy
protection.
","2023-10-12","2310.07298v1.pdf"
"2310.07299","Yue Zhang","Yue Zhang, Leyang Cui, Enbo Zhao, Wei Bi, Shuming Shi","RobustGEC: Robust Grammatical Error Correction Against Subtle Context
  Perturbation","Accepted to EMNLP 2023 (main conference, long paper)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Grammatical Error Correction (GEC) systems play a vital role in assisting
people with their daily writing tasks. However, users may sometimes come across
a GEC system that initially performs well but fails to correct errors when the
inputs are slightly modified. To ensure an ideal user experience, a reliable
GEC system should have the ability to provide consistent and accurate
suggestions when encountering irrelevant context perturbations, which we refer
to as context robustness. In this paper, we introduce RobustGEC, a benchmark
designed to evaluate the context robustness of GEC systems. RobustGEC comprises
5,000 GEC cases, each with one original error-correct sentence pair and five
variants carefully devised by human annotators. Utilizing RobustGEC, we reveal
that state-of-the-art GEC systems still lack sufficient robustness against
context perturbations. In addition, we propose a simple yet effective method
for remitting this issue.
","2023-10-12","2310.07299v1.pdf"
"2310.07301","Yuchong Sun","Yuchong Sun, Che Liu, Jinwen Huang, Ruihua Song, Fuzheng Zhang, Di
  Zhang, Zhongyuan Wang, Kun Gai","Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Impressive progress has been made on chat models based on Large Language
Models (LLMs) recently; however, there is a noticeable lag in multi-turn
conversations between open-source chat models (e.g., Alpaca and Vicuna) and the
leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we
attribute the lag to the lack of enough high-quality multi-turn
instruction-tuning data. The available instruction-tuning data for the
community are either single-turn conversations or multi-turn ones with certain
issues, such as non-human-like instructions, less detailed responses, or rare
topic shifts. In this paper, we address these challenges by introducing Parrot,
a highly scalable solution designed to automatically generate high-quality
instruction-tuning data, which are then used to enhance the effectiveness of
chat models in multi-turn conversations. Specifically, we start by training the
Parrot-Ask model, which is designed to emulate real users in generating
instructions. We then utilize Parrot-Ask to engage in multi-turn conversations
with ChatGPT across a diverse range of topics, resulting in a collection of 40K
high-quality multi-turn dialogues (Parrot-40K). These data are subsequently
employed to train a chat model that we have named Parrot-Chat. We demonstrate
that the dialogues gathered from Parrot-Ask markedly outperform existing
multi-turn instruction-following datasets in critical metrics, including topic
diversity, number of turns, and resemblance to human conversation. With only
40K training examples, Parrot-Chat achieves strong performance against other
13B open-source models across a range of instruction-following benchmarks, and
particularly excels in evaluations of multi-turn capabilities. We make all
codes, datasets, and two versions of the Parrot-Ask model based on LLaMA2-13B
and KuaiYii-13B available at https://github.com/kwai/KwaiYii/Parrot.
","2023-10-12","2310.07301v1.pdf"
"2310.07321","Amin Dada","Amin Dada, Aokun Chen, Cheng Peng, Kaleb E Smith, Ahmad
  Idrissi-Yaghir, Constantin Marc Seibold, Jianning Li, Lars Heiliger, Xi Yang,
  Christoph M. Friedrich, Daniel Truhn, Jan Egger, Jiang Bian, Jens Kleesiek,
  Yonghui Wu","On the Impact of Cross-Domain Data on German Language Models","13 pages, 1 figure, accepted at Findings of the Association for
  Computational Linguistics: EMNLP 2023","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Traditionally, large language models have been either trained on general web
crawls or domain-specific data. However, recent successes of generative large
language models, have shed light on the benefits of cross-domain datasets. To
examine the significance of prioritizing data diversity over quality, we
present a German dataset comprising texts from five domains, along with another
dataset aimed at containing high-quality data. Through training a series of
models ranging between 122M and 750M parameters on both datasets, we conduct a
comprehensive benchmark on multiple downstream tasks. Our findings demonstrate
that the models trained on the cross-domain dataset outperform those trained on
quality data alone, leading to improvements up to $4.45\%$ over the previous
state-of-the-art. The models are available at
https://huggingface.co/ikim-uk-essen
","2023-10-16","2310.07321v1.pdf"
"2310.07328","Qingyi Si","Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, Weiping Wang","An Empirical Study of Instruction-tuning Large Language Models in
  Chinese","EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The success of ChatGPT validates the potential of large language models
(LLMs) in artificial general intelligence (AGI). Subsequently, the release of
LLMs has sparked the open-source community's interest in instruction-tuning,
which is deemed to accelerate ChatGPT's replication process. However, research
on instruction-tuning LLMs in Chinese, the world's most spoken language, is
still in its early stages. Therefore, this paper makes an in-depth empirical
study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that
provides valuable findings for effectively customizing LLMs that can better
respond to Chinese instructions. Specifically, we systematically explore the
impact of LLM bases, parameter-efficient methods, instruction data types, which
are the three most important elements for instruction-tuning. Besides, we also
conduct experiment to study the impact of other factors, e.g., chain-of-thought
data and human-value alignment. We hope that this empirical study can make a
modest contribution to the open Chinese version of ChatGPT. This paper will
release a powerful Chinese LLMs that is comparable to ChatGLM. The code and
data are available at https://github.com/PhoebusSi/Alpaca-CoT.
","2023-10-23","2310.07328v1.pdf"
"2310.07338","Han Zhang","Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, Jiang Bian","Towards Foundation Models for Learning on Tabular Data","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Learning on tabular data underpins numerous real-world applications. Despite
considerable efforts in developing effective learning models for tabular data,
current transferable tabular models remain in their infancy, limited by either
the lack of support for direct instruction following in new tasks or the
neglect of acquiring foundational knowledge and capabilities from diverse
tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs)
to overcome these limitations. TabFMs harness the potential of generative
tabular learning, employing a pre-trained large language model (LLM) as the
base model and fine-tuning it using purpose-designed objectives on an extensive
range of tabular datasets. This approach endows TabFMs with a profound
understanding and universal capabilities essential for learning on tabular
data. Our evaluations underscore TabFM's effectiveness: not only does it
significantly excel in instruction-following tasks like zero-shot and
in-context inference, but it also showcases performance that approaches, and in
instances, even transcends, the renowned yet mysterious closed-source LLMs like
GPT-4. Furthermore, when fine-tuning with scarce data, our model achieves
remarkable efficiency and maintains competitive performance with abundant
training data. Finally, while our results are promising, we also delve into
TabFM's limitations and potential opportunities, aiming to stimulate and
expedite future research on developing more potent TabFMs.
","2023-10-24","2310.07338v1.pdf"
"2310.07343","Zihan Zhang","Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, Jun Wang","How Do Large Language Models Capture the Ever-changing World Knowledge?
  A Review of Recent Advances","EMNLP 2023 main conference, paper link at
  https://github.com/hyintell/awesome-refreshing-llms","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Although large language models (LLMs) are impressive in solving various
tasks, they can quickly be outdated after deployment. Maintaining their
up-to-date status is a pressing concern in the current era. This paper provides
a comprehensive review of recent advances in aligning LLMs with the
ever-changing world knowledge without re-training from scratch. We categorize
research works systemically and provide in-depth comparisons and discussion. We
also discuss existing challenges and highlight future directions to facilitate
research in this field. We release the paper list at
https://github.com/hyintell/awesome-refreshing-llms
","2023-10-12","2310.07343v1.pdf"
"2310.07347","Chengyu Dong","Chengyu Dong, Liyuan Liu, Hao Cheng, Jingbo Shang, Jianfeng Gao,
  Xiaodong Liu","Fast-ELECTRA for Efficient Pre-training","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  ELECTRA pre-trains language models by detecting tokens in a sequence that
have been replaced by an auxiliary model. Although ELECTRA offers a significant
boost in efficiency, its potential is constrained by the training cost brought
by the auxiliary model. Notably, this model, which is jointly trained with the
main model, only serves to assist the training of the main model and is
discarded post-training. This results in a substantial amount of training cost
being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which
leverages an existing language model as the auxiliary model. To construct a
learning curriculum for the main model, we smooth its output distribution via
temperature scaling following a descending schedule. Our approach rivals the
performance of state-of-the-art ELECTRA-style pre-training methods, while
significantly eliminating the computation and memory cost brought by the joint
training of the auxiliary model. Our method also reduces the sensitivity to
hyper-parameters and enhances the pre-training stability.
","2023-10-12","2310.07347v1.pdf"
"2310.07417","Pedro Cotovio","Pedro Giesteira Cotovio, Ernesto Jimenez-Ruiz, Catia Pesquita","What can knowledge graph alignment gain with Neuro-Symbolic learning
  approaches?","","","","","cs.AI cs.LG cs.SC","http://creativecommons.org/licenses/by/4.0/","  Knowledge Graphs (KG) are the backbone of many data-intensive applications
since they can represent data coupled with its meaning and context. Aligning
KGs across different domains and providers is necessary to afford a fuller and
integrated representation. A severe limitation of current KG alignment (KGA)
algorithms is that they fail to articulate logical thinking and reasoning with
lexical, structural, and semantic data learning. Deep learning models are
increasingly popular for KGA inspired by their good performance in other tasks,
but they suffer from limitations in explainability, reasoning, and data
efficiency. Hybrid neurosymbolic learning models hold the promise of
integrating logical and data perspectives to produce high-quality alignments
that are explainable and support validation through human-centric approaches.
This paper examines the current state of the art in KGA and explores the
potential for neurosymbolic integration, highlighting promising research
directions for combining these fields.
","2023-10-12","2310.07417v1.pdf"
"2310.07419","Hazarapet Tunanyan","Hazarapet Tunanyan, Dejia Xu, Shant Navasardyan, Zhangyang Wang,
  Humphrey Shi","Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing
  Else","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances in text-to-image diffusion models have enabled the
photorealistic generation of images from text prompts. Despite the great
progress, existing models still struggle to generate compositional
multi-concept images naturally, limiting their ability to visualize human
imagination. While several recent works have attempted to address this issue,
they either introduce additional training or adopt guidance at inference time.
In this work, we consider a more ambitious goal: natural multi-concept
generation using a pre-trained diffusion model, and with almost no extra cost.
To achieve this goal, we identify the limitations in the text embeddings used
for the pre-trained text-to-image diffusion models. Specifically, we observe
concept dominance and non-localized contribution that severely degrade
multi-concept generation performance. We further design a minimal low-cost
solution that overcomes the above issues by tweaking (not re-training) the text
embeddings for more realistic multi-concept text-to-image generation. Our
Correction by Similarities method tweaks the embedding of concepts by
collecting semantic features from most similar tokens to localize the
contribution. To avoid mixing features of concepts, we also apply Cross-Token
Non-Maximum Suppression, which excludes the overlap of contributions from
different concepts. Experiments show that our approach outperforms previous
methods in text-to-image, image manipulation, and personalization tasks,
despite not introducing additional training or inference costs to the diffusion
steps.
","2023-10-12","2310.07419v1.pdf"
"2310.07455","Laia Domingo Colomer","Laia Domingo","Classical and quantum reservoir computing: development and applications
  in machine learning","Doctoral thesis","","","","quant-ph math.DS","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Reservoir computing is a novel machine learning algorithm that uses a
nonlinear dynamical system to efficiently learn complex temporal patterns from
data. The objective of this thesis is to investigate the principles of
reservoir computing and develop state-of-the-art variants capable of addressing
diverse applications in machine learning. The research demonstrates the
algorithm's robustness and adaptability across very different domains,
including agricultural time series forecasting and the time propagation of
quantum systems. The first contribution of this thesis consists in developing a
reservoir computing-based methodology to predict future agricultural product
prices, which is crucial for ensuring the sustainability of the food market.
The next contribution of the thesis is devoted to solving the Schr\""odinger
equation for complex quantum systems. A novel reservoir computing framework is
proposed to efficiently propagate quantum wavefunctions in time, enabling the
computation of all eigenstates of a quantum system within a specific energy
range. This approach is used to study prominent systems in the field of quantum
chemistry and quantum chaos. The last contribution of this thesis focuses on
optimizing algorithm designs for quantum reservoir computing. The results
demonstrate that families of quantum circuits with higher complexity, according
to the majorization criterion, yield superior performance in quantum machine
learning. Moreover, the impact of quantum noise on the algorithm performance is
evaluated, revealing that the amplitude damping noise can actually be
beneficial for the performance of quantum reservoir computing, while the
depolarizing and phase damping noise should be prioritized for correction.
Furthermore, the optimal design of quantum reservoirs is employed to construct
a hybrid quantum-classical neural network that tackles a fundamental problem in
drug design.
","2023-10-12","2310.07455v1.pdf"
"2310.07472","Zhiyu Lin","Zhiyu Lin, Mark Riedl","An Ontology of Co-Creative AI Systems","Submitted to NeurIPS Workshop on ML for Creativity and Design 2023","","","","cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The term co-creativity has been used to describe a wide variety of human-AI
assemblages in which human and AI are both involved in a creative endeavor. In
order to assist with disambiguating research efforts, we present an ontology of
co-creative systems, focusing on how responsibilities are divided between human
and AI system and the information exchanged between them. We extend Lubart's
original ontology of creativity support tools with three new categories
emphasizing artificial intelligence: computer-as-subcontractor,
computer-as-critic, and computer-as-teammate, some of which have
sub-categorizations.
","2023-10-12","2310.07472v1.pdf"
"2310.07478","Minji Yoon","Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov","Multimodal Graph Learning for Generative Tasks","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Multimodal learning combines multiple data modalities, broadening the types
and complexity of data our models can utilize: for example, from plain text to
image-caption pairs. Most multimodal learning algorithms focus on modeling
simple one-to-one pairs of data from two modalities, such as image-caption
pairs, or audio-text pairs. However, in most real-world settings, entities of
different modalities interact with each other in more complex and multifaceted
ways, going beyond one-to-one mappings. We propose to represent these complex
relationships as graphs, allowing us to capture data with any number of
modalities, and with complex relationships between modalities that can flexibly
vary from one sample to another. Toward this goal, we propose Multimodal Graph
Learning (MMGL), a general and systematic framework for capturing information
from multiple multimodal neighbors with relational structures among them. In
particular, we focus on MMGL for generative tasks, building upon pretrained
Language Models (LMs), aiming to augment their text generation with multimodal
neighbor contexts. We study three research questions raised by MMGL: (1) how
can we infuse multiple neighbor information into the pretrained LMs, while
avoiding scalability issues? (2) how can we infuse the graph structure
information among multimodal neighbors into the LMs? and (3) how can we
finetune the pretrained LMs to learn from the neighbor context in a
parameter-efficient manner? We conduct extensive experiments to answer these
three questions on MMGL and analyze the empirical results to pave the way for
future MMGL research.
","2023-10-13","2310.07478v1.pdf"
"2310.07488","Jiayi Fu","Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui
  Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao
  Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng
  Zhang, Zhongyuan Wang, Di Zhang, Kun Gai","KwaiYiiMath: Technical Report","technical report. arXiv admin note: text overlap with
  arXiv:2306.16636 by other authors","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in large language models (LLMs) have demonstrated
remarkable abilities in handling a variety of natural language processing (NLP)
downstream tasks, even on mathematical tasks requiring multi-step reasoning. In
this report, we introduce the KwaiYiiMath which enhances the mathematical
reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)
and Reinforced Learning from Human Feedback (RLHF), including on both English
and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale
Chinese primary school mathematics test set (named KMath), consisting of 188
examples to evaluate the correctness of the problem-solving process generated
by the models. Empirical studies demonstrate that KwaiYiiMath can achieve
state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with
the similar size models, respectively.
","2023-10-20","2310.07488v1.pdf"
"2310.07516","Sadasivan Shankar","Sadasivan Shankar","Energy Estimates Across Layers of Computing: From Devices to Large-Scale
  Applications in Machine Learning for Natural Language Processing, Scientific
  Computing, and Cryptocurrency Mining","6 pages, 5 figures","","","","cs.CY cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Estimates of energy usage in layers of computing from devices to algorithms
have been determined and analyzed. Building on the previous analysis [3],
energy needed from single devices and systems including three large-scale
computing applications such as Artificial Intelligence (AI)/Machine Learning
for Natural Language Processing, Scientific Simulations, and Cryptocurrency
Mining have been estimated. In contrast to the bit-level switching, in which
transistors achieved energy efficiency due to geometrical scaling, higher
energy is expended both at the at the instructions and simulations levels of an
application. Additionally, the analysis based on AI/ML Accelerators indicate
that changes in architectures using an older semiconductor technology node have
comparable energy efficiency with a different architecture using a newer
technology. Further comparisons of the energy in computing systems with the
thermodynamic and biological limits, indicate that there is a 27-36 orders of
magnitude higher energy requirements for total simulation of an application.
These energy estimates underscore the need for serious considerations of energy
efficiency in computing by including energy as a design parameter, enabling
growing needs of compute-intensive applications in a digital world.
","2023-10-12","2310.07516v1.pdf"
"2310.07521","Cunxiang Wang","Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang,
  Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang,
  Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang","Survey on Factuality in Large Language Models: Knowledge, Retrieval and
  Domain-Specificity","44 pages; 300+ references","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This survey addresses the crucial issue of factuality in Large Language
Models (LLMs). As LLMs find applications across diverse domains, the
reliability and accuracy of their outputs become vital. We define the
Factuality Issue as the probability of LLMs to produce content inconsistent
with established facts. We first delve into the implications of these
inaccuracies, highlighting the potential consequences and challenges posed by
factual errors in LLM outputs. Subsequently, we analyze the mechanisms through
which LLMs store and process facts, seeking the primary causes of factual
errors. Our discussion then transitions to methodologies for evaluating LLM
factuality, emphasizing key metrics, benchmarks, and studies. We further
explore strategies for enhancing LLM factuality, including approaches tailored
for specific domains. We focus two primary LLM configurations standalone LLMs
and Retrieval-Augmented LLMs that utilizes external data, we detail their
unique challenges and potential enhancements. Our survey offers a structured
guide for researchers aiming to fortify the factual reliability of LLMs.
","2023-10-19","2310.07521v1.pdf"
"2310.07545","Javier Pastor-Galindo","Javier Pastor-Galindo, Pantaleone Nespoli and Jos\'e A.
  Ruip\'erez-Valiente","Generative Agent-Based Social Networks for Disinformation: Research
  Opportunities and Open Challenges","","","","","cs.SI cs.MA","http://creativecommons.org/licenses/by/4.0/","  This article presents the affordances that Generative Artificial Intelligence
can have in disinformation context, one of the major threats to our digitalized
society. We present a research framework to generate customized agent-based
social networks for disinformation simulations that would enable understanding
and evaluation of the phenomena whilst discussing open challenges.
","2023-10-12","2310.07545v1.pdf"
"2310.07579","Martin Pawelczyk","Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju","In-Context Unlearning: Language Models as Few Shot Unlearners","","","","","cs.LG cs.AI cs.CR","http://creativecommons.org/licenses/by/4.0/","  Machine unlearning, the study of efficiently removing the impact of specific
training points on the trained model, has garnered increased attention of late,
driven by the need to comply with privacy regulations like the Right to be
Forgotten. Although unlearning is particularly relevant for LLMs in light of
the copyright issues they raise, achieving precise unlearning is
computationally infeasible for very large models. To this end, recent work has
proposed several algorithms which approximate the removal of training data
without retraining the model. These algorithms crucially rely on access to the
model parameters in order to update them, an assumption that may not hold in
practice due to computational constraints or when the LLM is accessed via API.
In this work, we propose a new class of unlearning methods for LLMs we call
''In-Context Unlearning'', providing inputs in context and without having to
update model parameters. To unlearn a particular training instance, we provide
the instance alongside a flipped label and additional correctly labelled
instances which are prepended as inputs to the LLM at inference time. Our
experimental results demonstrate that these contexts effectively remove
specific information from the training set while maintaining performance levels
that are competitive with (or in some cases exceed) state-of-the-art unlearning
methods that require access to the LLM parameters.
","2023-10-13","2310.07579v1.pdf"
"2310.07581","Raymond Fok","Raymond Fok, Joseph Chee Chang, Tal August, Amy X. Zhang, Daniel S.
  Weld","Qlarify: Bridging Scholarly Abstracts and Papers with Recursively
  Expandable Summaries","19 pages, 7 figures, 4 tables. Under review","","","","cs.HC","http://creativecommons.org/licenses/by-nc-nd/4.0/","  As scientific literature has grown exponentially, researchers often rely on
paper triaging strategies such as browsing abstracts before deciding to delve
into a paper's full text. However, when an abstract is insufficient,
researchers are required to navigate an informational chasm between 150-word
abstracts and 10,000-word papers. To bridge that gap, we introduce the idea of
recursively expandable summaries and present Qlarify, an interactive system
that allows users to recursively expand an abstract by progressively
incorporating additional information from a paper's full text. Starting from an
abstract, users can brush over summary text to specify targeted information
needs or select AI-suggested entities in the text. Responses are then generated
on-demand by an LLM and appear in the form of a fluid, threaded expansion of
the existing text. Each generated summary can be efficiently verified through
attribution to a relevant source-passage in the paper. Through an interview
study (n=9) and a field deployment (n=275) at a research conference, we use
Qlarify as a technology probe to elaborate upon the expandable summaries design
space, highlight how scholars benefit from Qlarify's expandable abstracts, and
identify future opportunities to support low-effort and just-in-time
exploration of scientific documents $\unicode{x2013}$ and other information
spaces $\unicode{x2013}$ through LLM-powered interactions.
","2023-10-12","2310.07581v1.pdf"
"2310.07589","Luiza Pozzobon","Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker","Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented
  Models","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Considerable effort has been dedicated to mitigating toxicity, but existing
methods often require drastic modifications to model parameters or the use of
computationally intensive auxiliary models. Furthermore, previous approaches
have often neglected the crucial factor of language's evolving nature over
time. In this work, we present a comprehensive perspective on toxicity
mitigation that takes into account its changing nature. We introduce
Goodtriever, a flexible methodology that matches the current state-of-the-art
toxicity mitigation while achieving 43% relative latency reduction during
inference and being more computationally efficient. By incorporating a
retrieval-based approach at decoding time, Goodtriever enables
toxicity-controlled text generation. Our research advocates for an increased
focus on adaptable mitigation techniques, which better reflect the data drift
models face when deployed in the wild. Code and data are available at
https://github.com/for-ai/goodtriever.
","2023-10-12","2310.07589v1.pdf"
"2310.07609","Xinyuan Lu","Liangming Pan, Xinyuan Lu, Min-Yen Kan, Preslav Nakov","QACHECK: A Demonstration System for Question-Guided Multi-Hop
  Fact-Checking","Accepted at EMNLP 2023 System Demonstrations Track","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Fact-checking real-world claims often requires complex, multi-step reasoning
due to the absence of direct evidence to support or refute them. However,
existing fact-checking systems often lack transparency in their
decision-making, making it challenging for users to comprehend their reasoning
process. To address this, we propose the Question-guided Multi-hop
Fact-Checking (QACHECK) system, which guides the model's reasoning process by
asking a series of questions critical for verifying a claim. QACHECK has five
key modules: a claim verifier, a question generator, a question-answering
module, a QA validator, and a reasoner. Users can input a claim into QACHECK,
which then predicts its veracity and provides a comprehensive report detailing
its reasoning process, guided by a sequence of (question, answer) pairs.
QACHECK also provides the source of evidence supporting each question,
fostering a transparent, explainable, and user-friendly fact-checking process.
A recorded video of QACHECK is at https://www.youtube.com/watch?v=ju8kxSldM64
","2023-10-12","2310.07609v1.pdf"
"2310.07611","Sumuk Shashidhar","Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang,
  Heng Ji","Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in
  Self-Refined Open-Source Models","Accepted to Findings of EMNLP 2023","","","","cs.CL cs.AI cs.PF","http://creativecommons.org/licenses/by/4.0/","  The dominance of proprietary LLMs has led to restricted access and raised
information privacy concerns. High-performing open-source alternatives are
crucial for information-sensitive and high-volume applications but often lag
behind in performance. To address this gap, we propose (1) A untargeted variant
of iterative self-critique and self-refinement devoid of external influence.
(2) A novel ranking metric - Performance, Refinement, and Inference Cost Score
(PeRFICS) - to find the optimal model for a given task considering refined
performance and cost. Our experiments show that SoTA open source models of
varying sizes from 7B - 65B, on average, improve 8.2% from their baseline
performance. Strikingly, even models with extremely small memory footprints,
such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39%
improvement in high-creativity, open ended tasks on the Vicuna benchmark.
Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement.
This work has profound implications for resource-constrained and
information-sensitive environments seeking to leverage LLMs without incurring
prohibitive costs, compromising on performance and privacy. The domain-agnostic
self-refinement process coupled with our novel ranking metric facilitates
informed decision-making in model selection, thereby reducing costs and
democratizing access to high-performing language models, as evidenced by case
studies.
","2023-10-24","2310.07611v1.pdf"
"2310.07629","Hannah Rose Kirk Miss","Hannah Rose Kirk, Andrew M. Bean, Bertie Vidgen, Paul R\""ottger, Scott
  A. Hale","The Past, Present and Better Future of Feedback Learning in Large
  Language Models for Subjective Human Preferences and Values","Accepted for the 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP, Main)","","","","cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Human feedback is increasingly used to steer the behaviours of Large Language
Models (LLMs). However, it is unclear how to collect and incorporate feedback
in a way that is efficient, effective and unbiased, especially for highly
subjective human preferences and values. In this paper, we survey existing
approaches for learning from human feedback, drawing on 95 papers primarily
from the ACL and arXiv repositories.First, we summarise the past, pre-LLM
trends for integrating human feedback into language models. Second, we give an
overview of present techniques and practices, as well as the motivations for
using feedback; conceptual frameworks for defining values and preferences; and
how feedback is collected and from whom. Finally, we encourage a better future
of feedback learning in LLMs by raising five unresolved conceptual and
practical challenges.
","2023-10-12","2310.07629v1.pdf"
"2310.07632","Hai Huang","Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang","Prompt Backdoors in Visual Prompt Learning","","","","","cs.CR cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Fine-tuning large pre-trained computer vision models is infeasible for
resource-limited users. Visual prompt learning (VPL) has thus emerged to
provide an efficient and flexible alternative to model fine-tuning through
Visual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider
optimizes a visual prompt given downstream data, and downstream users can use
this prompt together with the large pre-trained model for prediction. However,
this new learning paradigm may also pose security risks when the VPPTaaS
provider instead provides a malicious visual prompt. In this paper, we take the
first step to explore such risks through the lens of backdoor attacks.
Specifically, we propose BadVisualPrompt, a simple yet effective backdoor
attack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads to
above $99\%$ attack success rates with only negligible model accuracy drop by
$1.5\%$. In particular, we identify and then address a new technical challenge
related to interactions between the backdoor trigger and visual prompt, which
does not exist in conventional, model-level backdoors. Moreover, we provide
in-depth analyses of seven backdoor defenses from model, prompt, and input
levels. Overall, all these defenses are either ineffective or impractical to
mitigate our BadVisualPrompt, implying the critical vulnerability of VPL.
","2023-10-12","2310.07632v1.pdf"
"2310.07641","Zhiyuan Zeng","Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen","Evaluating Large Language Models at Evaluating Instruction Following","Under review","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  As research in large language models (LLMs) continues to accelerate,
LLM-based evaluation has emerged as a scalable and cost-effective alternative
to human evaluations for comparing the ever increasing list of models. This
paper investigates the efficacy of these ""LLM evaluators"", particularly in
using them to assess instruction following, a metric that gauges how closely
generated text adheres to the given instruction. We introduce a challenging
meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM
evaluator in discerning instruction-following outputs. The authors manually
curated 419 pairs of outputs, one adhering to instructions while the other
diverging, yet may possess deceptive qualities that mislead an LLM evaluator,
e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover
that different evaluators (i.e., combinations of LLMs and prompts) exhibit
distinct performance on LLMBar and even the highest-scoring ones have
substantial room for improvement. We also present a novel suite of prompting
strategies that further close the gap between LLM and human evaluators. With
LLMBar, we hope to offer more insight into LLM evaluators and foster future
research in developing better instruction-following models.
","2023-10-12","2310.07641v1.pdf"
"2310.07644","Chaoqi Liang","Chaoqi Liang, Weiqiang Bai, Lifeng Qiao, Yuchen Ren, Jianle Sun, Peng
  Ye, Hongliang Yan, Xinzhu Ma, Wangmeng Zuo, and Wanli Ouyang","Rethinking the BERT-like Pretraining for DNA Sequences","","","","","cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  With the success of large-scale pretraining in NLP, there is an increasing
trend of applying it to the domain of life sciences. In particular, pretraining
methods based on DNA sequences have garnered growing attention due to their
potential to capture generic information about genes. However, existing
pretraining methods for DNA sequences largely rely on direct adoptions of BERT
pretraining from NLP, lacking a comprehensive understanding and a specifically
tailored approach. To address this research gap, we first conducted a series of
exploratory experiments and gained several insightful observations: 1) In the
fine-tuning phase of downstream tasks, when using K-mer overlapping
tokenization instead of K-mer non-overlapping tokenization, both overlapping
and non-overlapping pretraining weights show consistent performance
improvement.2) During the pre-training process, using K-mer overlapping
tokenization quickly produces clear K-mer embeddings and reduces the loss to a
very low level, while using K-mer non-overlapping tokenization results in less
distinct embeddings and continuously decreases the loss. 3) Using overlapping
tokenization causes the self-attention in the intermediate layers of
pre-trained models to tend to overly focus on certain tokens, reflecting that
these layers are not adequately optimized. In summary, overlapping tokenization
can benefit the fine-tuning of downstream tasks but leads to inadequate
pretraining with fast convergence. To unleash the pretraining potential, we
introduce a novel approach called RandomMask, which gradually increases the
task difficulty of BERT-like pretraining by continuously expanding its mask
boundary, forcing the model to learn more knowledge. RandomMask is simple but
effective, achieving top-tier performance across 26 datasets of 28 datasets
spanning 7 downstream tasks.
","2023-10-13","2310.07644v1.pdf"
"2310.07652","Lei Wang","Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, Yong Wang","LLM4Vis: Explainable Visualization Recommendation using ChatGPT","EMNLP 2023 (Industry Track)","","","","cs.HC cs.CL","http://creativecommons.org/licenses/by/4.0/","  Data visualization is a powerful tool for exploring and communicating
insights in various domains. To automate visualization choice for datasets, a
task known as visualization recommendation has been proposed. Various
machine-learning-based approaches have been developed for this purpose, but
they often require a large corpus of dataset-visualization pairs for training
and lack natural explanations for their results. To address this research gap,
we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform
visualization recommendation and return human-like explanations using very few
demonstration examples. Our approach involves feature description,
demonstration example selection, explanation generation, demonstration example
construction, and inference steps. To obtain demonstration examples with
high-quality explanations, we propose a new explanation generation
bootstrapping to iteratively refine generated explanations by considering the
previous generation and template-based hint. Evaluations on the VizML dataset
show that LLM4Vis outperforms or performs similarly to supervised learning
models like Random Forest, Decision Tree, and MLP in both few-shot and
zero-shot settings. The qualitative evaluation also shows the effectiveness of
explanations generated by LLM4Vis. We make our code publicly available at
\href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.
","2023-10-17","2310.07652v1.pdf"
"2310.07653","Zeqiang Lai","Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang","Mini-DALLE3: Interactive Text to Image by Prompting Large Language
  Models","Technical report. Project page at https://minidalle3.github.io/","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The revolution of artificial intelligence content generation has been rapidly
accelerated with the booming text-to-image (T2I) diffusion models. Within just
two years of development, it was unprecedentedly of high-quality, diversity,
and creativity that the state-of-the-art models could generate. However, a
prevalent limitation persists in the effective communication with these popular
T2I models, such as Stable Diffusion, using natural language descriptions. This
typically makes an engaging image hard to obtain without expertise in prompt
engineering with complex word compositions, magic tags, and annotations.
Inspired by the recently released DALLE3 - a T2I model directly built-in
ChatGPT that talks human language, we revisit the existing T2I systems
endeavoring to align human intent and introduce a new task - interactive text
to image (iT2I), where people can interact with LLM for interleaved
high-quality image generation/edit/refinement and question answering with
stronger images and text correspondences using natural language. In addressing
the iT2I problem, we present a simple approach that augments LLMs for iT2I with
prompting techniques and off-the-shelf T2I models. We evaluate our approach for
iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT,
LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a
convenient and low-cost way to introduce the iT2I ability for any existing LLMs
and any text-to-image models without any training while bringing little
degradation on LLMs' inherent capabilities in, e.g., question answering and
code generation. We hope this work could draw broader attention and provide
inspiration for boosting user experience in human-machine interactions
alongside the image quality of the next-generation T2I systems.
","2023-10-16","2310.07653v1.pdf"
"2310.07659","Lang Qin","Lang Qin, Yao Zhang, Hongru Liang, Jun Wang, Zhenglu Yang","Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for
  Knowledge-Grounded Dialogue","Accepted by EMNLP2023 main conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Accurate knowledge selection is critical in knowledge-grounded dialogue
systems. Towards a closer look at it, we offer a novel perspective to organize
existing literature, i.e., knowledge selection coupled with, after, and before
generation. We focus on the third under-explored category of study, which can
not only select knowledge accurately in advance, but has the advantage to
reduce the learning, adjustment, and interpretation burden of subsequent
response generation models, especially LLMs. We propose GATE, a
generator-agnostic knowledge selection method, to prepare knowledge for
subsequent response generation models by selecting context-related knowledge
among different knowledge structures and variable knowledge requirements.
Experimental results demonstrate the superiority of GATE, and indicate that
knowledge selection before generation is a lightweight yet effective way to
facilitate LLMs (e.g., ChatGPT) to generate more informative responses.
","2023-10-23","2310.07659v1.pdf"
"2310.07676","Hai Huang","Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang","Composite Backdoor Attacks Against Large Language Models","","","","","cs.CR cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have demonstrated superior performance compared
to previous methods on various tasks, and often serve as the foundation models
for many researches and services. However, the untrustworthy third-party LLMs
may covertly introduce vulnerabilities for downstream tasks. In this paper, we
explore the vulnerability of LLMs through the lens of backdoor attacks.
Different from existing backdoor attacks against LLMs, ours scatters multiple
trigger keys in different prompt components. Such a Composite Backdoor Attack
(CBA) is shown to be stealthier than implanting the same multiple trigger keys
in only a single component. CBA ensures that the backdoor is activated only
when all trigger keys appear. Our experiments demonstrate that CBA is effective
in both natural language processing (NLP) and multimodal tasks. For instance,
with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,
our attack achieves a $100\%$ Attack Success Rate (ASR) with a False Triggered
Rate (FTR) below $2.06\%$ and negligible model accuracy degradation. The unique
characteristics of our CBA can be tailored for various practical scenarios,
e.g., targeting specific user groups. Our work highlights the necessity of
increased security research on the trustworthiness of foundation LLMs.
","2023-10-12","2310.07676v1.pdf"
"2310.07678","Ioannis Livieris","Ioannis E. Livieris, Emmanuel Pintelas, Niki Kiriakidou, Panagiotis
  Pintelas","Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM","The manuscript has been accepted for publication in ""Journal of
  Imaging""","Journal of Imaging. 2023; 9(10):224","10.3390/jimaging9100224","","cs.CV cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  With the proliferation of image-based applications in various domains, the
need for accurate and interpretable image similarity measures has become
increasingly critical. Existing image similarity models often lack
transparency, making it challenging to understand the reasons why two images
are considered similar. In this paper, we propose the concept of explainable
image similarity, where the goal is the development of an approach, which is
capable of providing similarity scores along with visual factual and
counterfactual explanations. Along this line, we present a new framework, which
integrates Siamese Networks and Grad-CAM for providing explainable image
similarity and discuss the potential benefits and challenges of adopting this
approach. In addition, we provide a comprehensive discussion about factual and
counterfactual explanations provided by the proposed framework for assisting
decision making. The proposed approach has the potential to enhance the
interpretability, trustworthiness and user acceptance of image-based systems in
real-world image similarity applications. The implementation code can be found
in https://github.com/ioannislivieris/Grad_CAM_Siamese.git.
","2023-10-18","2310.07678v1.pdf"
"2310.07690","Hao Wang","Hao Wang, Jianqi Hu, Andrea Morandi, Alfonso Nardi, Fei Xia, Xuanchen
  Li, Romolo Savo, Qiang Liu, Rachel Grange, Sylvain Gigan","Large-scale photonic computing with nonlinear disordered media","","","","","physics.optics cs.ET","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Neural networks find widespread use in scientific and technological
applications, yet their implementations in conventional computers have
encountered bottlenecks due to ever-expanding computational needs. Photonic
neuromorphic hardware, which manipulates information and represents data
continuously in the optical domain, is one of the promising platforms with
potential advantages of massive parallelism, ultralow latency, and reduced
energy consumption. While linear photonic neural networks are within reach,
photonic computing with large-scale optical nonlinear nodes remains largely
unexplored. Here, we demonstrate a large-scale, high-performance nonlinear
photonic neural system based on a disordered polycrystalline slab composed of
lithium niobate nanocrystals. Mediated by random quasi-phase-matching and
multiple scattering, linear and nonlinear optical speckle features are
generated as the interplay between the simultaneous linear random scattering
and the second-harmonic generation, defining a complex neural network in which
the second-order nonlinearity acts as internal nonlinear activation functions.
Benchmarked against linear random projection, such nonlinear mapping embedded
with rich physical computational operations shows improved performance across a
large collection of machine learning tasks in image classification, regression,
and graph classification with varying complexity. Demonstrating up to 27,648
input and 3,500 nonlinear output nodes, the combination of optical nonlinearity
and random scattering serves as a scalable computing engine for diverse
applications.
","2023-10-12","2310.07690v1.pdf"
"2310.07697","Bo Peng","Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao","ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Recent works have successfully extended large-scale text-to-image models to
the video domain, producing promising results but at a high computational cost
and requiring a large amount of video data. In this work, we introduce
ConditionVideo, a training-free approach to text-to-video generation based on
the provided condition, video, and input text, by leveraging the power of
off-the-shelf text-to-image generation methods (e.g., Stable Diffusion).
ConditionVideo generates realistic dynamic videos from random noise or given
scene videos. Our method explicitly disentangles the motion representation into
condition-guided and scenery motion components. To this end, the ConditionVideo
model is designed with a UNet branch and a control branch. To improve temporal
coherence, we introduce sparse bi-directional spatial-temporal attention
(sBiST-Attn). The 3D control network extends the conventional 2D controlnet
model, aiming to strengthen conditional generation accuracy by additionally
leveraging the bi-directional frames in the temporal domain. Our method
exhibits superior performance in terms of frame consistency, clip score, and
conditional accuracy, outperforming other compared methods.
","2023-10-12","2310.07697v1.pdf"
"2310.07699","Zhengfeng Lai","Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei
  Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang,
  Meng Cao","From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched
  Captions","CV/ML","","","","cs.CV cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Web-crawled datasets are pivotal to the success of pre-training
vision-language models, exemplified by CLIP. However, web-crawled AltTexts can
be noisy and potentially irrelevant to images, thereby undermining the crucial
image-text alignment. Existing methods for rewriting captions using large
language models (LLMs) have shown promise on small, curated datasets like CC3M
and CC12M. Nevertheless, their efficacy on massive web-captured captions is
constrained by the inherent noise and randomness in such data. In this study,
we address this limitation by focusing on two key aspects: data quality and
data variety. Unlike recent LLM rewriting techniques, we emphasize exploiting
visual concepts and their integration into the captions to improve data
quality. For data variety, we propose a novel mixed training scheme that
optimally leverages AltTexts alongside newly generated Visual-enriched Captions
(VeC). We use CLIP as one example and adapt the method for CLIP training on
large-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive
evaluation of VeCLIP across small, medium, and large scales of raw data. Our
results show significant advantages in image-text alignment and overall model
performance, underscoring the effectiveness of VeCLIP in improving CLIP
training. For example, VeCLIP achieves a remarkable over 20% improvement in
COCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,
we also achieve a notable over 3% improvement while using only 14% of the data
employed in the vanilla CLIP and 11% in ALIGN.
","2023-10-12","2310.07699v1.pdf"
"2310.07704","Haoxuan You","Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui
  Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang","Ferret: Refer and Ground Anything Anywhere at Any Granularity","30 pages, 10 figures. Code/Project Website:
  https://github.com/apple/ml-ferret","","","","cs.CV cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of
understanding spatial referring of any shape or granularity within an image and
accurately grounding open-vocabulary descriptions. To unify referring and
grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid
region representation that integrates discrete coordinates and continuous
features jointly to represent a region in the image. To extract the continuous
features of versatile regions, we propose a spatial-aware visual sampler, adept
at handling varying sparsity across different shapes. Consequently, Ferret can
accept diverse region inputs, such as points, bounding boxes, and free-form
shapes. To bolster the desired capability of Ferret, we curate GRIT, a
comprehensive refer-and-ground instruction tuning dataset including 1.1M
samples that contain rich hierarchical spatial knowledge, with 95K hard
negative data to promote model robustness. The resulting model not only
achieves superior performance in classical referring and grounding tasks, but
also greatly outperforms existing MLLMs in region-based and
localization-demanded multimodal chatting. Our evaluations also reveal a
significantly improved capability of describing image details and a remarkable
alleviation in object hallucination. Code and data will be available at
https://github.com/apple/ml-ferret
","2023-10-12","2310.07704v1.pdf"
"2310.07707","Aditya Kusupati","Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen,
  Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali
  Farhadi, Prateek Jain","MatFormer: Nested Transformer for Elastic Inference","31 pages, 12 figures, first three authors contributed equally","","","","cs.LG cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformer models are deployed in a wide range of settings, from
multi-accelerator clusters to standalone mobile phones. The diverse inference
constraints in these scenarios necessitate practitioners to train foundation
models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes.
Due to significant training costs, only a select few model sizes are trained
and supported, limiting more fine-grained control over relevant tradeoffs,
including latency, cost, and accuracy. This work introduces MatFormer, a nested
Transformer architecture designed to offer elasticity in a variety of
deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer
model is jointly optimized with a few nested smaller FFN blocks. This training
procedure allows for the Mix'n'Match of model granularities across layers --
i.e., a trained universal MatFormer model enables extraction of hundreds of
accurate smaller models, which were never explicitly optimized. We empirically
demonstrate MatFormer's effectiveness across different model classes (decoders
& encoders), modalities (language & vision), and scales (up to 2.6B
parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)
allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting
comparable validation loss and one-shot downstream evaluations to their
independently trained counterparts. Furthermore, we observe that smaller
encoders extracted from a universal MatFormer-based ViT (MatViT) encoder
preserve the metric-space structure for adaptive large-scale retrieval.
Finally, we showcase that speculative decoding with the accurate and consistent
submodels extracted from MatFormer can further reduce inference latency.
","2023-10-12","2310.07707v1.pdf"
"2310.07710","Yihan Wu","Yihan Wu, Zhengmian Hu, Hongyang Zhang, Heng Huang","DiPmark: A Stealthy, Efficient and Resilient Watermark for Large
  Language Models","","","","","cs.CR cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Watermarking techniques offer a promising way to secure data via embedding
covert information into the data. A paramount challenge in the domain lies in
preserving the distribution of original data during watermarking. Our research
extends and refines existing watermarking framework, placing emphasis on the
importance of a distribution-preserving (DiP) watermark. Contrary to the
current strategies, our proposed DiPmark preserves the original token
distribution during watermarking (stealthy), is detectable without access to
the language model API or weights (efficient), and is robust to moderate
changes of tokens (resilient). This is achieved by incorporating a novel
reweight strategy, combined with a hash function that assigns unique
\textit{i.i.d.} ciphers based on the context. The empirical benchmarks of our
approach underscore its stealthiness, efficiency, and resilience, making it a
robust solution for watermarking tasks that demand impeccable quality
preservation.
","2023-10-12","2310.07710v1.pdf"
"2310.07712","Raphael Tang","Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture","Found in the Middle: Permutation Self-Consistency Improves Listwise
  Ranking in Large Language Models","First two authors contributed equally; 10 pages, 6 figures","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) exhibit positional bias in how they use context,
which especially complicates listwise ranking. To address this, we propose
permutation self-consistency, a form of self-consistency over ranking list
outputs of black-box LLMs. Our key idea is to marginalize out different list
orders in the prompt to produce an order-independent ranking with less
positional bias. First, given some input prompt, we repeatedly shuffle the list
in the prompt and pass it through the LLM while holding the instructions the
same. Next, we aggregate the resulting sample of rankings by computing the
central ranking closest in distance to all of them, marginalizing out prompt
order biases in the process. Theoretically, we prove the robustness of our
method, showing convergence to the true ranking in the presence of random
perturbations. Empirically, on five list-ranking datasets in sorting and
passage reranking, our approach improves scores from conventional inference by
up to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous
state of the art in passage reranking. Our code is at
https://github.com/castorini/perm-sc.
","2023-10-12","2310.07712v1.pdf"
"2310.07713","Wei Ping","Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad
  Shoeybi, Bryan Catanzaro","InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining","","","","","cs.CL cs.AI cs.IR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Pretraining auto-regressive large language models (LLMs) with retrieval
demonstrates better perplexity and factual accuracy by leveraging external
databases. However, the size of existing pretrained retrieval-augmented LLM is
still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness
of instruction tuning and zero-shot generalization. In this work, we introduce
Retro 48B, the largest LLM pretrained with retrieval before instruction tuning.
Specifically, we continue to pretrain the 43B GPT model on additional 100
billion tokens using the Retro augmentation method by retrieving from 1.2
trillion tokens. The obtained foundation model, Retro 48B, largely outperforms
the original 43B GPT in terms of perplexity. After instruction tuning on Retro,
InstructRetro demonstrates significant improvement over the instruction tuned
GPT on zero-shot question answering (QA) tasks. Specifically, the average
improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form
QA tasks, and 10% over GPT across 4 challenging long-form QA tasks.
Surprisingly, we find that one can ablate the encoder from InstructRetro
architecture and directly use its decoder backbone, while achieving comparable
results. We hypothesize that pretraining with retrieval makes its decoder good
at incorporating context for QA. Our results highlights the promising direction
to obtain a better GPT decoder for QA through continued pretraining with
retrieval before instruction tuning.
","2023-10-12","2310.07713v1.pdf"
"2310.07715","Sireesh Gururaja","Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma
  Strubell","To Build Our Future, We Must Know Our Past: Contextualizing Paradigm
  Shifts in Natural Language Processing","Accepted to EMNLP 2023","","","","cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  NLP is in a period of disruptive change that is impacting our methodologies,
funding sources, and public perception. In this work, we seek to understand how
to shape our future by better understanding our past. We study factors that
shape NLP as a field, including culture, incentives, and infrastructure by
conducting long-form interviews with 26 NLP researchers of varying seniority,
research area, institution, and social identity. Our interviewees identify
cyclical patterns in the field, as well as new shifts without historical
parallel, including changes in benchmark culture and software infrastructure.
We complement this discussion with quantitative analysis of citation,
authorship, and language use in the ACL Anthology over time. We conclude by
discussing shared visions, concerns, and hopes for the future of NLP. We hope
that this study of our field's past and present can prompt informed discussion
of our community's implicit norms and more deliberate action to consciously
shape the future.
","2023-10-12","2310.07715v1.pdf"
"2310.07726","GuanLin Li","Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei
  Zhang","Towards the Vulnerability of Watermarking Artificial Intelligence
  Generated Content","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Artificial Intelligence Generated Content (AIGC) is gaining great popularity
in social media, with many commercial services available. These services
leverage advanced generative models, such as latent diffusion models and large
language models, to generate creative content (e.g., realistic images, fluent
sentences) for users. The usage of such generated content needs to be highly
regulated, as the service providers need to ensure the users do not violate the
usage policies (e.g., abuse for commercialization, generating and distributing
unsafe content).
  Numerous watermarking approaches have been proposed recently. However, in
this paper, we show that an adversary can easily break these watermarking
mechanisms. Specifically, we consider two possible attacks. (1) Watermark
removal: the adversary can easily erase the embedded watermark from the
generated content and then use it freely without the regulation of the service
provider. (2) Watermark forge: the adversary can create illegal content with
forged watermarks from another user, causing the service provider to make wrong
attributions. We propose WMaGi, a unified framework to achieve both attacks in
a holistic way. The key idea is to leverage a pre-trained diffusion model for
content processing, and a generative adversarial network for watermark removing
or forging. We evaluate WMaGi on different datasets and embedding setups. The
results prove that it can achieve high success rates while maintaining the
quality of the generated content. Compared with existing diffusion model-based
attacks, WMaGi is 5,050$\sim$11,000$\times$ faster.
","2023-10-13","2310.07726v1.pdf"
"2310.07749","Jie An","Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng
  Liu, Lijuan Wang, Jiebo Luo","OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work investigates a challenging task named open-domain interleaved
image-text generation, which generates interleaved texts and images following
an input query. We propose a new interleaved generation framework based on
prompting large-language models (LLMs) and pre-trained text-to-image (T2I)
models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions,
coordinates T2I models, creates visual prompts for generating images, and
incorporates global contexts into the T2I models. This global context improves
the entity and style consistencies of images in the interleaved generation. For
model assessment, we first propose to use large multi-modal models (LMMs) to
evaluate the entity and style consistencies of open-domain interleaved
image-text sequences. According to the LMM evaluation on our constructed
evaluation set, the proposed interleaved generation framework can generate
high-quality image-text content for various domains and applications, such as
how-to question answering, storytelling, graphical story rewriting, and
webpage/poster generation tasks. Moreover, we validate the effectiveness of the
proposed LMM evaluation technique with human assessment. We hope our proposed
framework, benchmark, and LMM evaluation could help establish the intriguing
interleaved image-text generation task.
","2023-10-13","2310.07749v1.pdf"
"2310.07793","Ruotong Liao","Ruotong Liao, Xu Jia, Yunpu Ma, Volker Tresp","GenTKG: Generative Forecasting on Temporal Knowledge Graph","8 pages","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
","2023-10-13","2310.07793v1.pdf"
"2310.07795","Siru Ouyang","Siru Ouyang, Jiaxin Huang, Pranav Pillai, Yunyi Zhang, Yu Zhang,
  Jiawei Han","Ontology Enrichment for Effective Fine-grained Entity Typing","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Fine-grained entity typing (FET) is the task of identifying specific entity
types at a fine-grained level for entity mentions based on their contextual
information. Conventional methods for FET require extensive human annotation,
which is time-consuming and costly. Recent studies have been developing weakly
supervised or zero-shot approaches. We study the setting of zero-shot FET where
only an ontology is provided. However, most existing ontology structures lack
rich supporting information and even contain ambiguous relations, making them
ineffective in guiding FET. Recently developed language models, though
promising in various few-shot and zero-shot NLP tasks, may face challenges in
zero-shot FET due to their lack of interaction with task-specific ontology. In
this study, we propose OnEFET, where we (1) enrich each node in the ontology
structure with two types of extra information: instance information for
training sample augmentation and topic information to relate types to contexts,
and (2) develop a coarse-to-fine typing algorithm that exploits the enriched
information by training an entailment model with contrasting topics and
instance-based augmented training samples. Our experiments show that OnEFET
achieves high-quality fine-grained entity typing without human annotation,
outperforming existing zero-shot methods by a large margin and rivaling
supervised methods.
","2023-10-13","2310.07795v1.pdf"
"2310.07818","Thilini Wijesiriwardene","Thilini Wijesiriwardene, Ruwan Wickramarachchi, Aishwarya Naresh
  Reganti, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das","Exploring the Relationship between Analogy Identification and Sentence
  Structure Encoding in Large Language Models","This paper has been withdrawn due to a mistake in submission","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Identifying analogies plays a pivotal role in human cognition and language
proficiency. In the last decade, there has been extensive research on word
analogies in the form of ``A is to B as C is to D.'' However, there is a
growing interest in analogies that involve longer text, such as sentences and
collections of sentences, which convey analogous meanings. While the current
NLP research community evaluates the ability of Large Language Models (LLMs) to
identify such analogies, the underlying reasons behind these abilities warrant
deeper investigation. Furthermore, the capability of LLMs to encode both
syntactic and semantic structures of language within their embeddings has
garnered significant attention with the surge in their utilization. In this
work, we examine the relationship between the abilities of multiple LLMs to
identify sentence analogies, and their capacity to encode syntactic and
semantic structures. Through our analysis, we find that analogy identification
ability of LLMs is positively correlated with their ability to encode syntactic
and semantic structures of sentences. Specifically, we find that the LLMs which
capture syntactic structures better, also have higher abilities in identifying
sentence analogies.
","2023-10-17","2310.07818v1.pdf"
"2310.07820","Nate Gruver","Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson","Large Language Models Are Zero-Shot Time Series Forecasters","NeurIPS 2023. Code available at: https://github.com/ngruver/llmtime","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  By encoding time series as a string of numerical digits, we can frame time
series forecasting as next-token prediction in text. Developing this approach,
we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can
surprisingly zero-shot extrapolate time series at a level comparable to or
exceeding the performance of purpose-built time series models trained on the
downstream tasks. To facilitate this performance, we propose procedures for
effectively tokenizing time series data and converting discrete distributions
over tokens into highly flexible densities over continuous values. We argue the
success of LLMs for time series stems from their ability to naturally represent
multimodal distributions, in conjunction with biases for simplicity, and
repetition, which align with the salient features in many time series, such as
repeated seasonal trends. We also show how LLMs can naturally handle missing
data without imputation through non-numerical text, accommodate textual side
information, and answer questions to help explain predictions. While we find
that increasing model size generally improves performance on time series, we
show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,
and poor uncertainty calibration, which is likely the result of alignment
interventions such as RLHF.
","2023-10-13","2310.07820v1.pdf"
"2310.07830","Sia Gholami","Sia Gholami, Marwan Omar","Does Synthetic Data Make Large Language Models More Efficient?","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data's potential, ensuring
optimal model performance in diverse applications.
","2023-10-13","2310.07830v1.pdf"
"2310.07831","Aaron Defazio","Aaron Defazio and Ashok Cutkosky and Harsh Mehta and Konstantin
  Mishchenko","When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement","","","","","cs.LG cs.AI stat.ML","http://creativecommons.org/licenses/by/4.0/","  Learning rate schedules used in practice bear little resemblance to those
recommended by theory. We close much of this theory/practice gap, and as a
consequence are able to derive new problem-adaptive learning rate schedules.
Our key technical contribution is a refined analysis of learning rate schedules
for a wide class of optimization algorithms (including SGD). In contrast to
most prior works that study the convergence of the average iterate, we study
the last iterate, which is what most people use in practice. When considering
only worst-case analysis, our theory predicts that the best choice is the
linear decay schedule: a popular choice in practice that sets the stepsize
proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the
total number of steps. To go beyond this worst-case analysis, we use the
observed gradient norms to derive schedules refined for any particular task.
These refined schedules exhibit learning rate warm-up and rapid learning rate
annealing near the end of training. Ours is the first systematic approach to
automatically yield both of these properties. We perform the most comprehensive
evaluation of learning rate schedules to date, evaluating across 10 diverse
deep learning problems, a series of LLMs, and a suite of logistic regression
problems. We validate that overall, the linear-decay schedule matches or
outperforms all commonly used default schedules including cosine annealing, and
that our schedule refinement method gives further improvements.
","2023-10-13","2310.07831v1.pdf"
"2310.07838","Qingyue Zhao","Qingyue Zhao and Banghua Zhu","Towards the Fundamental Limits of Knowledge Transfer over Finite Domains","38 pages, 2 figures; related works polished","","","","cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
","2023-10-18","2310.07838v1.pdf"
"2310.07849","Zhuoyan Li","Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin","Synthetic Data Generation with Large Language Models for Text
  Classification: Potential and Limitations","EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The collection and curation of high-quality training data is crucial for
developing text classification models with superior performance, but it is
often associated with significant costs and time investment. Researchers have
recently explored using large language models (LLMs) to generate synthetic
datasets as an alternative approach. However, the effectiveness of the
LLM-generated synthetic data in supporting model training is inconsistent
across different classification tasks. To better understand factors that
moderate the effectiveness of the LLM-generated synthetic data, in this study,
we look into how the performance of models trained on these synthetic data may
vary with the subjectivity of classification. Our results indicate that
subjectivity, at both the task level and instance level, is negatively
associated with the performance of the model trained on synthetic data. We
conclude by discussing the implications of our work on the potential and
limitations of leveraging LLM for synthetic data generation.
","2023-10-16","2310.07849v1.pdf"
"2310.07856","Jiho Shin","Jiho Shin, Hadi Hemmati, Moshi Wei, Song Wang","Assessing Evaluation Metrics for Neural Test Oracle Generation","10 pages + reference","","","","cs.CL cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this work, we revisit existing oracle generation studies plus ChatGPT to
empirically investigate the current standing of their performance in both
NLG-based and test adequacy metrics. Specifically, we train and run four
state-of-the-art test oracle generation models on five NLG-based and two test
adequacy metrics for our analysis. We apply two different correlation analyses
between these two different sets of metrics. Surprisingly, we found no
significant correlation between the NLG-based metrics and test adequacy
metrics. For instance, oracles generated from ChatGPT on the project
activemq-artemis had the highest performance on all the NLG-based metrics among
the studied NOGs, however, it had the most number of projects with a decrease
in test adequacy metrics compared to all the studied NOGs. We further conduct a
qualitative analysis to explore the reasons behind our observations, we found
that oracles with high NLG-based metrics but low test adequacy metrics tend to
have complex or multiple chained method invocations within the oracle's
parameters, making it hard for the model to generate completely, affecting the
test adequacy metrics. On the other hand, oracles with low NLG-based metrics
but high test adequacy metrics tend to have to call different assertion types
or a different method that functions similarly to the ones in the ground truth.
Overall, this work complements prior studies on test oracle generation with an
extensive performance evaluation with both NLG and test adequacy metrics and
provides guidelines for better assessment of deep learning applications in
software test generation in the future.
","2023-10-13","2310.07856v1.pdf"
"2310.07867","Massimiliano Furlan","Daniele Condorelli, Massimiliano Furlan","Cheap Talking Algorithms","","","","","econ.TH cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We simulate behaviour of independent reinforcement learning algorithms
playing the Crawford and Sobel (1982) game of strategic information
transmission. We show that a sender and a receiver training together converge
to strategies close to the exante optimal equilibrium of the game. Hence,
communication takes place to the largest extent predicted by Nash equilibrium
given the degree of conflict of interest between agents. The conclusion is
shown to be robust to alternative specifications of the hyperparameters and of
the game. We discuss implications for theories of equilibrium selection in
information transmission games, for work on emerging communication among
algorithms in computer science and for the economics of collusions in markets
populated by artificially intelligent agents.
","2023-10-13","2310.07867v1.pdf"
"2310.07871","Xiaochen Wang","Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin, Suhan Cui, Yuan Zhong,
  Yaqing Wang, Fenglong Ma","Hierarchical Pretraining on Multimodal Electronic Health Records","Accepted by EMNLP 2023","","","","cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Pretraining has proven to be a powerful technique in natural language
processing (NLP), exhibiting remarkable success in various NLP downstream
tasks. However, in the medical domain, existing pretrained models on electronic
health records (EHR) fail to capture the hierarchical nature of EHR data,
limiting their generalization capability across diverse downstream tasks using
a single pretrained model. To tackle this challenge, this paper introduces a
novel, general, and unified pretraining framework called MEDHMP, specifically
designed for hierarchically multimodal EHR data. The effectiveness of the
proposed MEDHMP is demonstrated through experimental results on eight
downstream tasks spanning three levels. Comparisons against eighteen baselines
further highlight the efficacy of our approach.
","2023-10-23","2310.07871v1.pdf"
"2310.07875","Justin Waugh","Gus Eggert, Kevin Huo, Mike Biven, and Justin Waugh","TabLib: A Dataset of 627M Tables with Context","","","","","cs.CL cs.AI cs.DB cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present ""TabLib'', a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.
","2023-10-13","2310.07875v1.pdf"
"2310.07879","Hao-Ping Lee","Hao-Ping Lee, Yu-Ju Yang, Thomas Serban von Davier, Jodi Forlizzi,
  Sauvik Das","Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy
  Risks","","","","","cs.HC","http://creativecommons.org/licenses/by/4.0/","  Privacy is a key principle for developing ethical AI technologies, but how
does including AI technologies in products and services change privacy risks?
We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI
privacy incidents. We codified how the unique capabilities and requirements of
AI technologies described in those incidents generated new privacy risks,
exacerbated known ones, or otherwise did not meaningfully alter the risk. We
present 12 high-level privacy risks that AI technologies either newly created
(e.g., exposure risks from deepfake pornography) or exacerbated (e.g.,
surveillance risks from collecting training data). One upshot of our work is
that incorporating AI technologies into a product can alter the privacy risks
it entails. Yet, current privacy-preserving AI/ML methods (e.g., federated
learning, differential privacy) only address a subset of the privacy risks
arising from the capabilities and data requirements of AI.
","2023-10-13","2310.07879v1.pdf"
"2310.07889","Bowen Pan","Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva,
  Phillip Isola, Yoon Kim","LangNav: Language as a Perceptual Representation for Navigation","","","","","cs.CV cs.AI cs.CL cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We explore the use of language as a perceptual representation for
vision-and-language navigation. Our approach uses off-the-shelf vision systems
(for image captioning and object detection) to convert an agent's egocentric
panoramic view at each time step into natural language descriptions. We then
finetune a pretrained language model to select an action, based on the current
view and the trajectory history, that would best fulfill the navigation
instructions. In contrast to the standard setup which adapts a pretrained
language model to work directly with continuous visual features from pretrained
vision models, our approach instead uses (discrete) language as the perceptual
representation. We explore two use cases of our language-based navigation
(LangNav) approach on the R2R vision-and-language navigation benchmark:
generating synthetic trajectories from a prompted large language model (GPT-4)
with which to finetune a smaller language model; and sim-to-real transfer where
we transfer a policy learned on a simulated environment (ALFRED) to a
real-world environment (R2R). Our approach is found to improve upon strong
baselines that rely on visual features in settings where only a few gold
trajectories (10-100) are available, demonstrating the potential of using
language as a perceptual representation for navigation tasks.
","2023-10-13","2310.07889v1.pdf"
"2310.07899","Sumedh Sontakke","Sumedh A Sontakke, Jesse Zhang, S\'ebastien M. R. Arnold, Karl
  Pertsch, Erdem B{\i}y{\i}k, Dorsa Sadigh, Chelsea Finn, Laurent Itti","RoboCLIP: One Demonstration is Enough to Learn Robot Policies","","","","","cs.AI cs.RO","http://creativecommons.org/licenses/by/4.0/","  Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.
","2023-10-13","2310.07899v1.pdf"
"2310.07911","Huiyin Xue","Huiyin Xue and Nikolaos Aletras","Pit One Against Many: Leveraging Attention-head Embeddings for
  Parameter-efficient Multi-head Attention","Accepted at EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Scaling pre-trained language models has resulted in large performance gains
in various natural language processing tasks but comes with a large cost in
memory requirements. Inspired by the position embeddings in transformers, we
aim to simplify and reduce the memory footprint of the multi-head attention
(MHA) mechanism. We propose an alternative module that uses only a single
shared projection matrix and multiple head embeddings (MHE), i.e. one per head.
We empirically demonstrate that our MHE attention is substantially more memory
efficient compared to alternative attention mechanisms while achieving high
predictive performance retention ratio to vanilla MHA on several downstream
tasks. MHE attention only requires a negligible fraction of additional
parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size
of the head embeddings) compared to a single-head attention, while MHA requires
$(3n^2-3n)d^2-3nd$ additional parameters.
","2023-10-13","2310.07911v1.pdf"
"2310.07915","Dawen Zhang","Dawen Zhang, Boming Xia, Yue Liu, Xiwei Xu, Thong Hoang, Zhenchang
  Xing, Mark Staples, Qinghua Lu, Liming Zhu","Tag Your Fish in the Broken Net: A Responsible Web Framework for
  Protecting Online Privacy and Copyright","","","","","cs.NI cs.CY cs.SI","http://creativecommons.org/licenses/by/4.0/","  The World Wide Web, a ubiquitous source of information, serves as a primary
resource for countless individuals, amassing a vast amount of data from global
internet users. However, this online data, when scraped, indexed, and utilized
for activities like web crawling, search engine indexing, and, notably, AI
model training, often diverges from the original intent of its contributors.
The ascent of Generative AI has accentuated concerns surrounding data privacy
and copyright infringement. Regrettably, the web's current framework falls
short in facilitating pivotal actions like consent withdrawal or data copyright
claims. While some companies offer voluntary measures, such as crawler access
restrictions, these often remain inaccessible to individual users. To empower
online users to exercise their rights and enable companies to adhere to
regulations, this paper introduces a user-controlled consent tagging framework
for online data. It leverages the extensibility of HTTP and HTML in conjunction
with the decentralized nature of distributed ledger technology. With this
framework, users have the ability to tag their online data at the time of
transmission, and subsequently, they can track and request the withdrawal of
consent for their data from the data holders. A proof-of-concept system is
implemented, demonstrating the feasibility of the framework. This work holds
significant potential for contributing to the reinforcement of user consent,
privacy, and copyright on the modern internet and lays the groundwork for
future insights into creating a more responsible and user-centric web
ecosystem.
","2023-10-13","2310.07915v1.pdf"
"2310.07923","William Merrill","William Merrill and Ashish Sabharwal","The Expressive Power of Transformers with Chain of Thought","9-page preprint","","","","cs.LG cs.CC cs.CL cs.LO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers' reasoning can be improved by allowing them to use a ""chain of
thought"" or ""scratchpad"", i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps adds a
clear new ability (under standard complexity conjectures): recognizing all
regular languages. Our results also imply that linear steps keep transformer
decoders within context-sensitive languages, and polynomial steps make them
recognize exactly the class of polynomial-time solvable problems -- the first
exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer's chain of thought or scratchpad
impacts its reasoning power.
","2023-10-19","2310.07923v1.pdf"
"2310.07937","Bangguo Yu","Bangguo Yu, Hamidreza Kasaei, Ming Cao","Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using
  Large Language Models","7 pages, 4 figures, conference","","","","cs.RO cs.AI","http://creativecommons.org/licenses/by/4.0/","  In advanced human-robot interaction tasks, visual target navigation is
crucial for autonomous robots navigating unknown environments. While numerous
approaches have been developed in the past, most are designed for single-robot
operations, which often suffer from reduced efficiency and robustness due to
environmental complexities. Furthermore, learning policies for multi-robot
collaboration are resource-intensive. To address these challenges, we propose
Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs)
as a global planner for multi-robot cooperative visual target navigation.
Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs'
scene comprehension. It then assigns exploration frontiers to each robot for
efficient target search. Experimental results on Habitat-Matterport 3D (HM3D)
demonstrate that Co-NavGPT surpasses existing models in success rates and
efficiency without any learning process, demonstrating the vast potential of
LLMs in multi-robot collaboration domains. The supplementary video, prompts,
and code can be accessed via the following link:
\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
","2023-10-13","2310.07937v1.pdf"
"2310.07944","Hongxu Pu","Hongxu Pu, Xincong Yang, Jing Li, Runhao Guo, Heng Li","AutoRepo: A general framework for multi-modal LLM-based automated
  construction reporting","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ensuring the safety, quality, and timely completion of construction projects
is paramount, with construction inspections serving as a vital instrument
towards these goals. Nevertheless, the predominantly manual approach of
present-day inspections frequently results in inefficiencies and inadequate
information management. Such methods often fall short of providing holistic,
exhaustive assessments, consequently engendering regulatory oversights and
potential safety hazards. To address this issue, this paper presents a novel
framework named AutoRepo for automated generation of construction inspection
reports. The unmanned vehicles efficiently perform construction inspections and
collect scene information, while the multimodal large language models (LLMs)
are leveraged to automatically generate the inspection reports. The framework
was applied and tested on a real-world construction site, demonstrating its
potential to expedite the inspection process, significantly reduce resource
allocation, and produce high-quality, regulatory standard-compliant inspection
reports. This research thus underscores the immense potential of multimodal
large language models in revolutionizing construction inspection practices,
signaling a significant leap forward towards a more efficient and safer
construction management paradigm.
","2023-10-13","2310.07944v1.pdf"
"2310.07957","Nilay Patel","Nilay Patel and Rahul Saha and Jeffrey Flanigan","A New Approach Towards Autoformalization","Under review at MATHAI 2023 @ NeurIPS 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Verifying mathematical proofs is difficult, but can be automated with the
assistance of a computer. Autoformalization is the task of automatically
translating natural language mathematics into a formal language that can be
verified by a program. This is a challenging task, and especially for
higher-level mathematics found in research papers. Research paper mathematics
requires large amounts of background and context. In this paper, we propose an
avenue towards tackling autoformalization for research-level mathematics, by
breaking the task into easier and more approachable subtasks: unlinked
formalization (formalization with unlinked definitions and theorems), entity
linking (linking to the proper theorems and definitions), and finally adjusting
types so it passes the type checker. In addition, we present arXiv2Formal, a
benchmark dataset for unlinked formalization consisting of 50 theorems
formalized for the Lean theorem prover sampled from papers on arXiv.org. We
welcome any contributions from the community to future versions of this
dataset.
","2023-10-23","2310.07957v1.pdf"
"2310.07984","Yizhen Zheng","Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T.N. Nguyen, Lauren T. May,
  Geoffrey I. Webb, Shirui Pan","Large Language Models for Scientific Synthesis, Inference and
  Explanation","Supplementary Information:
  https://drive.google.com/file/d/1KrpUpzuFTeMx6a6zl18lqdo8vV-UUa1Z/view?usp=sharing
  Github Repo: https://github.com/zyzisastudyreallyhardguy/LLM4SD","","","","cs.AI cs.CE","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large language models are a form of artificial intelligence systems whose
primary knowledge consists of the statistical patterns, semantic relationships,
and syntactical structures of language1. Despite their limited forms of
""knowledge"", these systems are adept at numerous complex tasks including
creative writing, storytelling, translation, question-answering, summarization,
and computer code generation. However, they have yet to demonstrate advanced
applications in natural science. Here we show how large language models can
perform scientific synthesis, inference, and explanation. We present a method
for using general-purpose large language models to make inferences from
scientific datasets of the form usually associated with special-purpose machine
learning algorithms. We show that the large language model can augment this
""knowledge"" by synthesizing from the scientific literature. When a conventional
machine learning system is augmented with this synthesized and inferred
knowledge it can outperform the current state of the art across a range of
benchmark tasks for predicting molecular properties. This approach has the
further advantage that the large language model can explain the machine
learning system's predictions. We anticipate that our framework will open new
avenues for AI to accelerate the pace of scientific discovery.
","2023-10-13","2310.07984v1.pdf"
"2310.07999","Yite Wang","Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan,
  Haibin Lin, Ruoyu Sun, Hongxia Yang","LEMON: Lossless model expansion","Preprint","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Scaling of deep neural networks, especially Transformers, is pivotal for
their surging performance and has further led to the emergence of sophisticated
reasoning capabilities in foundation models. Such scaling generally requires
training large models from scratch with random initialization, failing to
leverage the knowledge acquired by their smaller counterparts, which are
already resource-intensive to obtain. To tackle this inefficiency, we present
$\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a
recipe to initialize scaled models using the weights of their smaller but
pre-trained counterparts. This is followed by model training with an optimized
learning rate scheduler tailored explicitly for the scaled models,
substantially reducing the training time compared to training from scratch.
Notably, LEMON is versatile, ensuring compatibility with various network
structures, including models like Vision Transformers and BERT. Our empirical
results demonstrate that LEMON reduces computational costs by 56.7% for Vision
Transformers and 33.2% for BERT when compared to training from scratch.
","2023-10-13","2310.07999v1.pdf"
"2310.08008","Aparna Elangovan","Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor","Effects of Human Adversarial and Affable Samples on BERT
  Generalizability","To appear at EMNLP Findings 2023","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  BERT-based models have had strong performance on leaderboards, yet have been
demonstrably worse in real-world settings requiring generalization. Limited
quantities of training data is considered a key impediment to achieving
generalizability in machine learning. In this paper, we examine the impact of
training data quality, not quantity, on a model's generalizability. We consider
two characteristics of training data: the portion of human-adversarial
(h-adversarial), i.e., sample pairs with seemingly minor differences but
different ground-truth labels, and human-affable (h-affable) training samples,
i.e., sample pairs with minor differences but the same ground-truth label. We
find that for a fixed size of training samples, as a rule of thumb, having
10-30% h-adversarial instances improves the precision, and therefore F1, by up
to 20 points in the tasks of text classification and relation extraction.
Increasing h-adversarials beyond this range can result in performance plateaus
or even degradation. In contrast, h-affables may not contribute to a model's
generalizability and may even degrade generalization performance.
","2023-10-18","2310.08008v1.pdf"
"2310.08017","Aravind Sesagiri Raamkumar","Siyuan Brandon Loh, Aravind Sesagiri Raamkumar","Harnessing Large Language Models' Empathetic Response Generation
  Capabilities for Online Mental Health Counselling Support","7 pages, 1 figure","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Large Language Models (LLMs) have demonstrated remarkable performance across
various information-seeking and reasoning tasks. These computational systems
drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also
carry substantial promise in meeting the growing demands of mental health care,
albeit relatively unexplored. As such, this study sought to examine LLMs'
capability to generate empathetic responses in conversations that emulate those
in a mental health counselling setting. We selected five LLMs: version 3.5 and
version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways
Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple
instructional prompt, these models responded to utterances derived from the
EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we
compared their responses to those from traditional response generation dialogue
systems, which were fine-tuned on the ED dataset, along with human-generated
responses. Notably, we discovered that responses from the LLMs were remarkably
more empathetic in most scenarios. We position our findings in light of
catapulting advancements in creating empathetic conversational systems.
","2023-10-13","2310.08017v1.pdf"
"2310.08027","Hao Lang","Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, Yongbin Li","Exploring Large Language Models for Multi-Modal Out-of-Distribution
  Detection","EMNLP2023 Findings Long Paper","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.
","2023-10-13","2310.08027v1.pdf"
"2310.08034","Can Cui","Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye and Ziran Wang","Receive, Reason, and React: Drive as You Say with Large Language Models
  in Autonomous Vehicles","arXiv admin note: text overlap with arXiv:2309.10228","","","","cs.HC cs.AI cs.RO","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The fusion of human-centric design and artificial intelligence (AI)
capabilities has opened up new possibilities for next-generation autonomous
vehicles that go beyond transportation. These vehicles can dynamically interact
with passengers and adapt to their preferences. This paper proposes a novel
framework that leverages Large Language Models (LLMs) to enhance the
decision-making process in autonomous vehicles. By utilizing LLMs' linguistic
and contextual understanding abilities with specialized tools, we aim to
integrate the language and reasoning capabilities of LLMs into autonomous
vehicles. Our research includes experiments in HighwayEnv, a collection of
environments for autonomous driving and tactical decision-making tasks, to
explore LLMs' interpretation, interaction, and reasoning in various scenarios.
We also examine real-time personalization, demonstrating how LLMs can influence
driving behaviors based on verbal commands. Our empirical results highlight the
substantial advantages of utilizing chain-of-thought prompting, leading to
improved driving decisions, and showing the potential for LLMs to enhance
personalized driving experiences through ongoing verbal feedback. The proposed
framework aims to transform autonomous vehicle operations, offering
personalized support, transparent decision-making, and continuous learning to
enhance safety and effectiveness. We achieve user-centric, transparent, and
adaptive autonomous driving ecosystems supported by the integration of LLMs
into autonomous vehicles.
","2023-10-13","2310.08034v1.pdf"
"2310.08041","Bohan Zhuang","Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan
  Zhuang","QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large
  Language Models","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) excel in NLP, but their demands hinder their
widespread deployment. While Quantization-Aware Training (QAT) offers a
solution, its extensive training costs make Post-Training Quantization (PTQ) a
more practical approach for LLMs. In existing studies, activation outliers in
particular channels are identified as the bottleneck to PTQ accuracy. They
propose to transform the magnitudes from activations to weights, which however
offers limited alleviation or suffers from unstable gradients, resulting in a
severe performance drop at low-bitwidth. In this paper, we propose QLLM, an
accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM
introduces an adaptive channel reassembly technique that reallocates the
magnitude of outliers to other channels, thereby mitigating their impact on the
quantization range. This is achieved by channel disassembly and channel
assembly, which first breaks down the outlier channels into several
sub-channels to ensure a more balanced distribution of activation magnitudes.
Then similar channels are merged to maintain the original channel number for
efficiency. Additionally, an adaptive strategy is designed to autonomously
determine the optimal number of sub-channels for channel disassembly. To
further compensate for the performance loss caused by quantization, we propose
an efficient tuning method that only learns a small number of low-rank weights
while freezing the pre-trained quantized model. After training, these low-rank
parameters can be fused into the frozen weights without affecting inference.
Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate
quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B
within 10 hours on a single A100-80G GPU, outperforming the previous
state-of-the-art method by 7.89% on the average accuracy across five zero-shot
tasks.
","2023-10-13","2310.08041v1.pdf"
"2310.08049","Ivan Lee","Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick","Exploring the Relationship Between Model Architecture and In-Context
  Learning Ability","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  What is the relationship between model architecture and the ability to
perform in-context learning? In this empirical study, we take the first steps
towards answering this question. In particular, we evaluate fifteen model
architectures across a suite of synthetic in-context learning tasks. The
selected architectures represent a broad range of paradigms, including
recurrent and convolution-based neural networks, transformers, and emerging
attention alternatives. We discover that all considered architectures can
perform in-context learning under certain conditions. However, contemporary
architectures are found to be the best performing, especially as task
complexity grows. Additionally, our follow-up experiments delve into various
factors that influence in-context learning. We observe varied sensitivities
among architectures with respect to hyperparameter settings. Our study of
training dynamics reveals that certain architectures exhibit a smooth,
progressive learning trajectory, while others demonstrate periods of stagnation
followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we
find that several emerging attention alternatives are more robust in-context
learners than transformers; since such approaches have constant-sized memory
footprints at inference time, this result opens the future possibility of
scaling up in-context learning to vastly larger numbers of in-context examples.
","2023-10-13","2310.08049v1.pdf"
"2310.08067","Hanbin Wang","Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, Haoyang Zhang","GameGPT: Multi-agent Collaborative Framework for Game Development","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The large language model (LLM) based agents have demonstrated their capacity
to automate and expedite software development processes. In this paper, we
focus on game development and propose a multi-agent collaborative framework,
dubbed GameGPT, to automate game development. While many studies have
pinpointed hallucination as a primary roadblock for deploying LLMs in
production, we identify another concern: redundancy. Our framework presents a
series of methods to mitigate both concerns. These methods include dual
collaboration and layered approaches with several in-house lexicons, to
mitigate the hallucination and redundancy in the planning, task identification,
and implementation phases. Furthermore, a decoupling approach is also
introduced to achieve code generation with better precision.
","2023-10-13","2310.08067v1.pdf"
"2310.08072","Kosuke Takahashi","Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki","Training Generative Question-Answering on Synthetic Data Obtained from
  an Instruct-tuned Model","PACLIC 2023 short paper, 4 pages (6 pages including references), 4
  figures","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This paper presents a simple and cost-effective method for synthesizing data
to train question-answering systems. For training, fine-tuning GPT models is a
common practice in resource-rich languages like English, however, it becomes
challenging for non-English languages due to the scarcity of sufficient
question-answer (QA) pairs. Existing approaches use question and answer
generators trained on human-authored QA pairs, which involves substantial human
expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a
zero-shot or few-shot manner. We conduct experiments to compare various
strategies for obtaining QA pairs from the instruct-tuned model. The results
demonstrate that a model trained on our proposed synthetic data achieves
comparable performance to a model trained on manually curated datasets, without
incurring human costs.
","2023-10-16","2310.08072v1.pdf"
"2310.08094","Zijie Wu","Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, Xiang Bai","SingleInsert: Inserting New Concepts from a Single Image into
  Text-to-Image Models for Flexible Editing","Project page: https://jarrentwu1031.github.io/SingleInsert-web/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent progress in text-to-image (T2I) models enables high-quality image
generation with flexible textual control. To utilize the abundant visual priors
in the off-the-shelf T2I models, a series of methods try to invert an image to
proper embedding that aligns with the semantic space of the T2I model. However,
these image-to-text (I2T) inversion methods typically need multiple source
images containing the same concept or struggle with the imbalance between
editing flexibility and visual fidelity. In this work, we point out that the
critical problem lies in the foreground-background entanglement when learning
an intended concept, and propose a simple and effective baseline for
single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage
scheme. In the first stage, we regulate the learned embedding to concentrate on
the foreground area without being associated with the irrelevant background. In
the second stage, we finetune the T2I model for better visual resemblance and
devise a semantic loss to prevent the language drift problem. With the proposed
techniques, SingleInsert excels in single concept generation with high visual
fidelity while allowing flexible editing. Additionally, SingleInsert can
perform single-image novel view synthesis and multiple concepts composition
without requiring joint training. To facilitate evaluation, we design an
editing prompt list and introduce a metric named Editing Success Rate (ESR) for
quantitative assessment of editing flexibility. Our project page is:
https://jarrentwu1031.github.io/SingleInsert-web/
","2023-10-13","2310.08094v1.pdf"
"2310.08096","Tobias Schimanski","Tobias Schimanski, Julia Bingler, Camilla Hyslop, Mathias Kraus,
  Markus Leippold","ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction
  Targets","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Public and private actors struggle to assess the vast amounts of information
about sustainability commitments made by various institutions. To address this
problem, we create a novel tool for automatically detecting corporate,
national, and regional net zero and reduction targets in three steps. First, we
introduce an expert-annotated data set with 3.5K text samples. Second, we train
and release ClimateBERT-NetZero, a natural language classifier to detect
whether a text contains a net zero or reduction target. Third, we showcase its
analysis potential with two use cases: We first demonstrate how
ClimateBERT-NetZero can be combined with conventional question-answering (Q&A)
models to analyze the ambitions displayed in net zero and reduction targets.
Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call
transcripts and outline how communication patterns evolve over time. Our
experiments demonstrate promising pathways for extracting and analyzing net
zero and emission reduction targets at scale.
","2023-10-13","2310.08096v1.pdf"
"2310.08099","Anoop V. S.","Ajay Krishnan, V. S. Anoop","ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using
  Natural Language Processing","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Climate change's impact on human health poses unprecedented and diverse
challenges. Unless proactive measures based on solid evidence are implemented,
these threats will likely escalate and continue to endanger human well-being.
The escalating advancements in information and communication technologies have
facilitated the widespread availability and utilization of social media
platforms. Individuals utilize platforms such as Twitter and Facebook to
express their opinions, thoughts, and critiques on diverse subjects,
encompassing the pressing issue of climate change. The proliferation of climate
change-related content on social media necessitates comprehensive analysis to
glean meaningful insights. This paper employs natural language processing (NLP)
techniques to analyze climate change discourse and quantify the sentiment of
climate change-related tweets. We use ClimateBERT, a pretrained model
fine-tuned specifically for the climate change domain. The objective is to
discern the sentiment individuals express and uncover patterns in public
opinion concerning climate change. Analyzing tweet sentiments allows a deeper
comprehension of public perceptions, concerns, and emotions about this critical
global challenge. The findings from this experiment unearth valuable insights
into public sentiment and the entities associated with climate change
discourse. Policymakers, researchers, and organizations can leverage such
analyses to understand public perceptions, identify influential actors, and
devise informed strategies to address climate change challenges.
","2023-10-20","2310.08099v1.pdf"
"2310.08101","Junxiao Shen Mr","Junxiao Shen, John J. Dudley, Jingyao Zheng, Bill Byrne, Per Ola
  Kristensson","Promptor: A Conversational and Autonomous Prompt Generation Agent for
  Intelligent Text Entry Techniques","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text entry is an essential task in our day-to-day digital interactions.
Numerous intelligent features have been developed to streamline this process,
making text entry more effective, efficient, and fluid. These improvements
include sentence prediction and user personalization. However, as deep
learning-based language models become the norm for these advanced features, the
necessity for data collection and model fine-tuning increases. These challenges
can be mitigated by harnessing the in-context learning capability of large
language models such as GPT-3.5. This unique feature allows the language model
to acquire new skills through prompts, eliminating the need for data collection
and fine-tuning. Consequently, large language models can learn various text
prediction techniques. We initially showed that, for a sentence prediction
task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is
comparable with a fine-tuned GPT-3.5 model, with the latter two methods
requiring costly data collection, fine-tuning and post-processing. However, the
task of prompting large language models to specialize in specific text
prediction tasks can be challenging, particularly for designers without
expertise in prompt engineering. To address this, we introduce Promptor, a
conversational prompt generation agent designed to engage proactively with
designers. Promptor can automatically generate complex prompts tailored to meet
specific needs, thus offering a solution to this challenge. We conducted a user
study involving 24 participants creating prompts for three intelligent text
entry tasks, half of the participants used Promptor while the other half
designed prompts themselves. The results show that Promptor-designed prompts
result in a 35% increase in similarity and 22% in coherence over those by
designers.
","2023-10-17","2310.08101v1.pdf"
"2310.08102","Muhammad Razif Rizqullah","Muhammad Razif Rizqullah (1), Ayu Purwarianti (1) and Alham Fikri Aji
  (2) ((1) Bandung Institute of Technology, (2) Mohamed bin Zayed University of
  Artificial Intelligence)","QASiNa: Religious Domain Question Answering using Sirah Nabawiyah","6 Pages. In Proceeding of 10th International Conference on Advanced
  Informatics: Concepts, Theory and Applications (ICAICTA 2023)","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Nowadays, Question Answering (QA) tasks receive significant research focus,
particularly with the development of Large Language Model (LLM) such as Chat
GPT [1]. LLM can be applied to various domains, but it contradicts the
principles of information transmission when applied to the Islamic domain. In
Islam we strictly regulates the sources of information and who can give
interpretations or tafseer for that sources [2]. The approach used by LLM to
generate answers based on its own interpretation is similar to the concept of
tafseer, LLM is neither an Islamic expert nor a human which is not permitted in
Islam. Indonesia is the country with the largest Islamic believer population in
the world [3]. With the high influence of LLM, we need to make evaluation of
LLM in religious domain. Currently, there is only few religious QA dataset
available and none of them using Sirah Nabawiyah especially in Indonesian
Language. In this paper, we propose the Question Answering Sirah Nabawiyah
(QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in
Indonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5],
and IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0
[7]. XLM-R model returned the best performance on QASiNa with EM of 61.20,
F1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance
with Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and
F1-Score with higher Substring Match, the gap of EM and Substring Match get
wider in GPT-4. The experiment indicate that Chat GPT tends to give excessive
interpretations as evidenced by its higher Substring Match scores compared to
EM and F1-Score, even after providing instruction and context. This concludes
Chat GPT is unsuitable for question answering task in religious domain
especially for Islamic religion.
","2023-10-13","2310.08102v1.pdf"
"2310.08103","O. Ivy Wong","Dawei Chen, Vinay Kerai, Matthew J. Alger, O. Ivy Wong and Cheng Soon
  Ong","Radio Galaxy Zoo: tagging radio subjects using text","14 pages, 9 figures, accepted for publication in PASA","","","","astro-ph.GA","http://creativecommons.org/licenses/by-nc-sa/4.0/","  RadioTalk is a communication platform that enabled members of the Radio
Galaxy Zoo (RGZ) citizen science project to engage in discussion threads and
provide further descriptions of the radio subjects they were observing in the
form of tags and comments. It contains a wealth of auxiliary information which
is useful for the morphology identification of complex and extended radio
sources. In this paper, we present this new dataset, and for the first time in
radio astronomy, we combine text and images to automatically classify radio
galaxies using a multi-modal learning approach. We found incorporating text
features improved classification performance which demonstrates that text
annotations are rare but valuable sources of information for classifying
astronomical sources, and suggests the importance of exploiting multi-modal
information in future citizen science projects. We also discovered over 10,000
new radio sources beyond the RGZ-DR1 catalogue in this dataset.
","2023-10-13","2310.08103v1.pdf"
"2310.08109","Shihang Feng","Gerard T. Schuster and Shihang Feng","Overview of Physics-Informed Machine Learning Inversion of Geophysical
  Data","37 pages, 16 figures","","","","physics.geo-ph cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  We review four types of algorithms for physics-informed machine learning
(PIML) inversion of geophysical data. The unifying equation is given by the
joint objective function $\epsilon$:
  \begin{eqnarray} \epsilon^{||-PIML}&=&\lambda_1 \overbrace{||{\bf
W}^{ML}({\bf H}_{{\bf w}} {\bf d}^{obs}-{\bf m})||^2}^{NN} + \lambda_2
\overbrace{{||{\bf W}^{FWI}({\bf L} {\bf m}-{\bf d}^{obs})||^2}}^{FWI} ~+
\nonumber\\ \nonumber\\ && + ~~Regularizer, \label{PIML.eq120}
\end{eqnarray}where the optimal model ${\bf m}^*$ and weights $\bf w^*$
minimize $\epsilon$. Here, The matrix weights are given by the boldface symbol
$\bf W$, and full waveform inversion (FWI) is typically computed using a
finite-difference solution of the wave equation, where $\bf L$ represents the
forward modeling operation of the wave equation as a function of the model $\bf
m$. Also, a fully-connected neural network (NN) is used to compute the model
${\bf H_w}{\bf d}^{obs} \approx \bf m$ from the observed input data ${\bf
d}^{obs}$. The selection of weights $\lambda_i$ and the NN operations determine
one of four different PIML algorithms.
  PIML offers potential advantages over standard FWI through its enhanced
ability to avoid local minima and the option to locally train the inversion
operator, minimizing the requirement for extensive training data for global
applicability. However, the effectiveness of PIML relies on the similarity
between the test and trained data. Nevertheless, a possible strategy to
overcome this limitation involves initial pretraining of a PIML architecture
with data from a broader region, followed by fine-tuning for specific data-a
method reminiscent of the way large language models are pretrained and adapted
for various tasks.
","2023-10-13","2310.08109v1.pdf"
"2310.08118","Karthik Valmeekam","Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati","Can Large Language Models Really Improve by Self-critiquing Their Own
  Plans?","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There have been widespread claims about Large Language Models (LLMs) being
able to successfully verify or self-critique their candidate solutions in
reasoning problems in an iterative mode. Intrigued by those claims, in this
paper we set out to investigate the verification/self-critiquing abilities of
large language models in the context of planning. We evaluate a planning system
that employs LLMs for both plan generation and verification. We assess the
verifier LLM's performance against ground-truth verification, the impact of
self-critiquing on plan generation, and the influence of varying feedback
levels on system performance. Using GPT-4, a state-of-the-art LLM, for both
generation and verification, our findings reveal that self-critiquing appears
to diminish plan generation performance, especially when compared to systems
with external, sound verifiers and the LLM verifiers in that system produce a
notable number of false positives, compromising the system's reliability.
Additionally, the nature of feedback, whether binary or detailed, showed
minimal impact on plan generation. Collectively, our results cast doubt on the
effectiveness of LLMs in a self-critiquing, iterative framework for planning
tasks.
","2023-10-13","2310.08118v1.pdf"
"2310.08122","Stojan Trajanovski","Sepideh Mahabadi and Stojan Trajanovski","Core-sets for Fair and Diverse Data Summarization","NeurIPS 2023","","","","cs.DS cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study core-set construction algorithms for the task of Diversity
Maximization under fairness/partition constraint. Given a set of points $P$ in
a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the
goal of this problem is to pick $k_i$ points from each group $i$ such that the
overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider
two natural diversity measures: sum-of-pairwise distances and
sum-of-nearest-neighbor distances, and show improved core-set construction
algorithms with respect to these measures. More precisely, we show the first
constant factor core-set w.r.t. sum-of-pairwise distances whose size is
independent of the size of the dataset and the aspect ratio. Second, we show
the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we
run several experiments showing the effectiveness of our core-set approach. In
particular, we apply constrained diversity maximization to summarize a set of
timed messages that takes into account the messages' recency. Specifically, the
summary should include more recent messages compared to older ones. This is a
real task in one of the largest communication platforms, affecting the
experience of hundreds of millions daily active users. By utilizing our
core-set method for this task, we achieve a 100x speed-up while losing the
diversity by only a few percent. Moreover, our approach allows us to improve
the space usage of the algorithm in the streaming setting.
","2023-10-13","2310.08122v1.pdf"
"2310.08123","Chia-Yu Hung","Chia-Yu Hung, Zhiqiang Hu, Yujia Hu, Roy Ka-Wei Lee","Who Wrote it and Why? Prompting Large-Language Models for Authorship
  Verification","7 pages,1 figure","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Authorship verification (AV) is a fundamental task in natural language
processing (NLP) and computational linguistics, with applications in forensic
analysis, plagiarism detection, and identification of deceptive content.
Existing AV techniques, including traditional stylometric and deep learning
approaches, face limitations in terms of data requirements and lack of
explainability. To address these limitations, this paper proposes PromptAV, a
novel technique that leverages Large-Language Models (LLMs) for AV by providing
step-by-step stylometric explanation prompts. PromptAV outperforms
state-of-the-art baselines, operates effectively with limited training data,
and enhances interpretability through intuitive explanations, showcasing its
potential as an effective and interpretable solution for the AV task.
","2023-10-13","2310.08123v1.pdf"
"2310.08129","Zijie Chen","Zijie Chen, Lichao Zhang, Fangsheng Weng, Lili Pan, Zhenzhong Lan","Tailored Visions: Enhancing Text-to-Image Generation with Personalized
  Prompt Rewriting","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  We propose a novel perspective of viewing large pretrained models as search
engines, thereby enabling the repurposing of techniques previously used to
enhance search engine performance. As an illustration, we employ a personalized
query rewriting technique in the realm of text-to-image generation. Despite
significant progress in the field, it is still challenging to create
personalized visual representations that align closely with the desires and
preferences of individual users. This process requires users to articulate
their ideas in words that are both comprehensible to the models and accurately
capture their vision, posing difficulties for many users. In this paper, we
tackle this challenge by leveraging historical user interactions with the
system to enhance user prompts. We propose a novel approach that involves
rewriting user prompts based a new large-scale text-to-image dataset with over
300k prompts from 3115 users. Our rewriting model enhances the expressiveness
and alignment of user prompts with their intended visual outputs. Experimental
results demonstrate the superiority of our methods over baseline approaches, as
evidenced in our new offline evaluation method and online tests. Our approach
opens up exciting possibilities of applying more search engine techniques to
build truly personalized large pretrained models.
","2023-10-13","2310.08129v1.pdf"
"2310.08130","Yuxuan Yao","Yuxuan Yao, Han Wu, Qiling Xu, Linqi Song","Fine-grained Conversational Decoding via Isotropic and Proximal Search","To appear in EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  General-purpose text decoding approaches are usually adopted for dialogue
response generation. Although the quality of the generated responses can be
improved with dialogue-specific encoding methods, conversational decoding
methods are still under-explored. Inspired by \citet{wu2023learning} that a
good dialogue feature space should follow the rules of locality and isotropy,
we present a fine-grained conversational decoding method, termed
\textit{isotropic and proximal search (IPS)}. Our method is designed to
generate the semantic-concentrated response, while still maintaining
informativeness and discrimination against the context. Experiments show that
our approach outperforms existing decoding strategies in the dialogue field
across both automatic and human evaluation metrics. More in-depth analyses
further confirm the effectiveness of our approach.
","2023-10-24","2310.08130v1.pdf"
"2310.08152","Siyu Ren","Siyu Ren, Qi Jia, Kenny Q. Zhu","Context Compression for Auto-regressive Transformers with Sentinel
  Tokens","To appear at EMNLP 2023 main conference","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The quadratic complexity of the attention module makes it gradually become
the bulk of compute in Transformer-based LLMs during generation. Moreover, the
excessive key-value cache that arises when dealing with long inputs also brings
severe issues on memory footprint and inference latency. In this work, we
propose a plug-and-play approach that is able to incrementally compress the
intermediate activation of a specified span of tokens into compact ones,
thereby reducing both memory and computational cost when processing subsequent
context. Experiments on both in-domain language modeling and zero-shot
open-ended document generation demonstrate the advantage of our approach over
sparse attention baselines in terms of fluency, n-gram matching, and semantic
similarity. At last, we comprehensively profile the benefit of context
compression on improving the system throughout. Code is available at
https://github.com/DRSY/KV_Compression.
","2023-10-17","2310.08152v1.pdf"
"2310.08164","Fazl Barez","Luke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, Fazl
  Barez","Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse
  Autoencoders","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) aligned to human preferences via reinforcement
learning from human feedback (RLHF) underpin many commercial applications.
However, how RLHF impacts LLM internals remains opaque. We propose a novel
method to interpret learned reward functions in RLHF-tuned LLMs using sparse
autoencoders. Our approach trains autoencoder sets on activations from a base
LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we
identify unique features that reflect the accuracy of the learned reward model.
To quantify this, we construct a scenario where the tuned LLM learns
token-reward mappings to maximize reward. This is the first application of
sparse autoencoders for interpreting learned rewards and broadly inspecting
reward learning in LLMs. Our method provides an abstract approximation of
reward integrity. This presents a promising technique for ensuring alignment
between specified objectives and model behaviors.
","2023-10-13","2310.08164v1.pdf"
"2310.08166","JunYu Lu","Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing
  Zhang, Yan Song, Pingjian Zhang","Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task
  Instruction Tuning","","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Recent advancements enlarge the capabilities of large language models (LLMs)
in zero-shot image-to-text generation and understanding by integrating
multi-modal inputs. However, such success is typically limited to English
scenarios due to the lack of large-scale and high-quality non-English
multi-modal resources, making it extremely difficult to establish competitive
counterparts in other languages. In this paper, we introduce the Ziya-VL
series, a set of bilingual large-scale vision-language models (LVLMs) designed
to incorporate visual semantics into LLM for multi-modal dialogue. Composed of
Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from
BLIP-2, further exploring the assistance of optimization schemes such as
instruction tuning, multi-stage training and low-rank adaptation module for
visual-language alignment. In addition, we stimulate the understanding ability
of GPT-4 in multi-modal scenarios, translating our gathered English image-text
datasets into Chinese and generating instruction-response through the
in-context learning method. The experiment results demonstrate that compared to
the existing LVLMs, Ziya-VL achieves competitive performance across a wide
range of English-only tasks including zero-shot image-text retrieval, image
captioning, and visual question answering. The evaluation leaderboard accessed
by GPT-4 also indicates that our models possess satisfactory image-text
understanding and generation capabilities in Chinese multi-modal scenario
dialogues. Code, demo and models are available at
~\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.
","2023-10-13","2310.08166v1.pdf"
"2310.08167","Erkan Gunes","Erkan Gunes, Christoffer Koch Florczak","Multiclass Classification of Policy Documents with Large Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Classifying policy documents into policy issue topics has been a long-time
effort in political science and communication disciplines. Efforts to automate
text classification processes for social science research purposes have so far
achieved remarkable results, but there is still a large room for progress. In
this work, we test the prediction performance of an alternative strategy, which
requires human involvement much less than full manual coding. We use the GPT
3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned
Large Language Models (LLM), to classify congressional bills and congressional
hearings into Comparative Agendas Project's 21 major policy issue topics. We
propose three use-case scenarios and estimate overall accuracies ranging from
%58-83 depending on scenario and GPT model employed. The three scenarios aims
at minimal, moderate, and major human interference, respectively. Overall, our
results point towards the insufficiency of complete reliance on GPT with
minimal human intervention, an increasing accuracy along with the human effort
exerted, and a surprisingly high accuracy achieved in the most humanly
demanding use-case. However, the superior use-case achieved the %83 accuracy on
the %65 of the data in which the two models agreed, suggesting that a similar
approach to ours can be relatively easily implemented and allow for mostly
automated coding of a majority of a given dataset. This could free up resources
allowing manual human coding of the remaining %35 of the data to achieve an
overall higher level of accuracy while reducing costs significantly.
","2023-10-13","2310.08167v1.pdf"
"2310.08172","Zheyuan Zhang","Zheyuan Zhang, Jifan Yu, Juanzi Li, Lei Hou","Exploring the Cognitive Knowledge Structure of Large Language Models: An
  Educational Diagnostic Assessment Approach","Findings of EMNLP 2023 (Short Paper)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have not only exhibited exceptional performance
across various tasks, but also demonstrated sparks of intelligence. Recent
studies have focused on assessing their capabilities on human exams and
revealed their impressive competence in different domains. However, cognitive
research on the overall knowledge structure of LLMs is still lacking. In this
paper, based on educational diagnostic assessment method, we conduct an
evaluation using MoocRadar, a meticulously annotated human test dataset based
on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain
insights of their cognitive capabilities. This research emphasizes the
significance of investigating LLMs' knowledge and understanding the disparate
cognitive patterns of LLMs. By shedding light on models' knowledge, researchers
can advance development and utilization of LLMs in a more informed and
effective manner.
","2023-10-19","2310.08172v1.pdf"
"2310.08184","Hongling Zheng","Hongling Zheng, Li Shen, Anke Tang, Yong Luo, Han Hu, Bo Du, Dacheng
  Tao","Learn From Model Beyond Fine-Tuning: A Survey","20 pages, 9 figures","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Foundation models (FM) have demonstrated remarkable performance across a wide
range of tasks (especially in the fields of natural language processing and
computer vision), primarily attributed to their ability to comprehend
instructions and access extensive, high-quality data. This not only showcases
their current effectiveness but also sets a promising trajectory towards the
development of artificial general intelligence. Unfortunately, due to multiple
constraints, the raw data of the model used for large model training are often
inaccessible, so the use of end-to-end models for downstream tasks has become a
new research trend, which we call Learn From Model (LFM) in this article. LFM
focuses on the research, modification, and design of FM based on the model
interface, so as to better understand the model structure and weights (in a
black box environment), and to generalize the model to downstream tasks. The
study of LFM techniques can be broadly categorized into five major areas: model
tuning, model distillation, model reuse, meta learning and model editing. Each
category encompasses a repertoire of methods and strategies that aim to enhance
the capabilities and performance of FM. This paper gives a comprehensive review
of the current methods based on FM from the perspective of LFM, in order to
help readers better understand the current research status and ideas. To
conclude, we summarize the survey by highlighting several critical areas for
future exploration and addressing open issues that require further attention
from the research community. The relevant papers we investigated in this
article can be accessed at
<https://github.com/ruthless-man/Awesome-Learn-from-Model>.
","2023-10-13","2310.08184v1.pdf"
"2310.08185","Wenshan Wu","Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong
  Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, Nan Duan","EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form
  Narrative Text Generation","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Plan-and-Write is a common hierarchical approach in long-form narrative text
generation, which first creates a plan to guide the narrative writing.
Following this approach, several studies rely on simply prompting large
language models for planning, which often yields suboptimal results. In this
paper, we propose a new framework called Evaluation-guided Iterative Plan
Extraction for long-form narrative text generation (EIPE-text), which extracts
plans from the corpus of narratives and utilizes the extracted plans to
construct a better planner. EIPE-text has three stages: plan extraction,
learning, and inference. In the plan extraction stage, it iteratively extracts
and improves plans from the narrative corpus and constructs a plan corpus. We
propose a question answer (QA) based evaluation mechanism to automatically
evaluate the plans and generate detailed plan refinement instructions to guide
the iterative improvement. In the learning stage, we build a better planner by
fine-tuning with the plan corpus or in-context learning with examples in the
plan corpus. Finally, we leverage a hierarchical approach to generate long-form
narratives. We evaluate the effectiveness of EIPE-text in the domains of novels
and storytelling. Both GPT-4-based evaluations and human evaluations
demonstrate that our method can generate more coherent and relevant long-form
narratives. Our code will be released in the future.
","2023-10-13","2310.08185v1.pdf"
"2310.08207","Piotr Makowski Prof.","Piotr Tomasz Makowski","(Re)conceptualizations: Intentional concept development in the social
  sciences","22 pages. Chapter for the volume on conceptual engineering (Brill)","","","","physics.soc-ph","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Can intentional concept development in the social sciences be understood in
terms of conceptual engineering (CE)? To answer this question, I analyze
various types of conceptual changes in the social changes-with a special
attention to organizational research and the so-called
(re)conceptualizations-and distinguish between CE as a theoretical practice and
CE as a research program. I show that social scientists, from the point of view
of their scientific practice, exercise CE in two versions: CE de novo is
employed as new conceptualizations and moderately progressive CE-as
reconceptualizations. Importantly, the second type of CE-rather neglected in
philosophy of the social sciences-appears to be highly important for the
incremental progress of inquiry. Still, both types appear to be equally
significant also for CE understood as a research program and for its prospects
in the social sciences. Here, I point to three possible paths that help
bridging the gap between actual practices of concept development in the social
sciences and normative, programmatic approaches to CE: best practice
recommendations, institutional actions and uses of AI-agents.
","2023-10-13","2310.08207v1.pdf"
"2310.08215","B\'alint Mucs\'anyi","B\'alint Mucs\'anyi and Michael Kirchhof and Elisa Nguyen and
  Alexander Rubinstein and Seong Joon Oh","Trustworthy Machine Learning","373 pages, textbook at the University of T\""ubingen","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  As machine learning technology gets applied to actual products and solutions,
new challenges have emerged. Models unexpectedly fail to generalize to small
changes in the distribution, tend to be confident on novel data they have never
seen, or cannot communicate the rationale behind their decisions effectively
with the end users. Collectively, we face a trustworthiness issue with the
current machine learning technology. This textbook on Trustworthy Machine
Learning (TML) covers a theoretical and technical background of four key topics
in TML: Out-of-Distribution Generalization, Explainability, Uncertainty
Quantification, and Evaluation of Trustworthiness. We discuss important
classical and contemporary research papers of the aforementioned fields and
uncover and connect their underlying intuitions. The book evolved from the
homonymous course at the University of T\""ubingen, first offered in the Winter
Semester of 2022/23. It is meant to be a stand-alone product accompanied by
code snippets and various pointers to further sources on topics of TML. The
dedicated website of the book is https://trustworthyml.io/.
","2023-10-13","2310.08215v1.pdf"
"2310.08232","Xin Zhang","Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan
  Zhang, Min Zhang","Language Models are Universal Embedders","13 pages, in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the large language model (LLM) revolution, embedding is a key component of
various systems. For example, it is used to retrieve knowledge or memories for
LLMs, to build content moderation filters, etc. As such cases span from English
to other natural or programming languages, from retrieval to classification and
beyond, it is desirable to build a unified embedding model rather than
dedicated ones for each scenario. In this work, we make an initial step towards
this goal, demonstrating that multiple languages (both natural and programming)
pre-trained transformer decoders can embed universally when finetuned on
limited English data. We provide a comprehensive practice with thorough
evaluations. On English MTEB, our models achieve competitive performance on
different embedding tasks by minimal training data. On other benchmarks, such
as multilingual classification and code search, our models (without any
supervision) perform comparably to, or even surpass heavily supervised
baselines and/or APIs. These results provide evidence of a promising path
towards building powerful unified embedders that can be applied across tasks
and languages.
","2023-10-13","2310.08232v1.pdf"
"2310.08235","Shaofei Cai","Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao
  Liang","GROOT: Learning to Follow Instructions by Watching Gameplay Videos","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. Code and video can be
found on the website https://craftjarvis-groot.github.io.
","2023-10-13","2310.08235v1.pdf"
"2310.08255","Ashish Ramayee Asokan","Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, R.
  Venkatesh Babu","Distilling from Vision-Language Models for Improved OOD Generalization
  in Vision Tasks","Code is available at https://github.com/val-iisc/VL2V-ADiP.git","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Vision-Language Models (VLMs) such as CLIP are trained on large amounts of
image-text pairs, resulting in remarkable generalization across several data
distributions. The prohibitively expensive training and data
collection/curation costs of these models make them valuable Intellectual
Property (IP) for organizations. This motivates a vendor-client paradigm, where
a vendor trains a large-scale VLM and grants only input-output access to
clients on a pay-per-query basis in a black-box setting. The client aims to
minimize inference cost by distilling the VLM to a student model using the
limited available task-specific data, and further deploying this student model
in the downstream application. While naive distillation largely improves the
In-Domain (ID) accuracy of the student, it fails to transfer the superior
out-of-distribution (OOD) generalization of the VLM teacher using the limited
available labeled images. To mitigate this, we propose Vision-Language to
Vision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and
language modalities of the teacher model with the vision modality of a
pre-trained student model, and further distills the aligned VLM embeddings to
the student. This maximally retains the pre-trained features of the student,
while also incorporating the rich representations of the VLM image encoder and
the superior generalization of the text embeddings. The proposed approach
achieves state-of-the-art results on the standard Domain Generalization
benchmarks in a black-box teacher setting, and also when weights of the VLM are
accessible.
","2023-10-13","2310.08255v1.pdf"
"2310.08256","Cheongwoong Kang","Cheongwoong Kang and Jaesik Choi","Impact of Co-occurrence on Factual Knowledge of Large Language Models","EMNLP 2023 Findings","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large language models (LLMs) often make factually incorrect responses despite
their success in various applications. In this paper, we hypothesize that
relying heavily on simple co-occurrence statistics of the pre-training corpora
is one of the main factors that cause factual errors. Our results reveal that
LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently
co-occurred words over the correct answer. Consequently, LLMs struggle to
recall facts whose subject and object rarely co-occur in the pre-training
dataset although they are seen during finetuning. We show that co-occurrence
bias remains despite scaling up model sizes or finetuning. Therefore, we
suggest finetuning on a debiased dataset to mitigate the bias by filtering out
biased samples whose subject-object co-occurrence count is high. Although
debiased finetuning allows LLMs to memorize rare facts in the training set, it
is not effective in recalling rare facts unseen during finetuning. Further
research in mitigation will help build reliable language models by preventing
potential errors. The code is available at
\url{https://github.com/CheongWoong/impact_of_cooccurrence}.
","2023-10-13","2310.08256v1.pdf"
"2310.08278","Arjun Ashok","Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani,
  George Adamopoulos, Rishika Bhagwatkar, Marin Bilo\v{s}, Hena Ghonia, Nadhir
  Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas
  Chapados, Yuriy Nevmyvaka, Irina Rish","Lag-Llama: Towards Foundation Models for Time Series Forecasting","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen ""out-of-distribution"" time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
","2023-10-13","2310.08278v1.pdf"
"2310.08304","Arpit Mittal","Arpit Mittal, Harshil Jhaveri, Swapnil Mallick, Abhishek Ajmera","CHIP: Contrastive Hierarchical Image Pretraining","","","","","cs.CV cs.AI cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  Few-shot object classification is the task of classifying objects in an image
with limited number of examples as supervision. We propose a one-shot/few-shot
classification model that can classify an object of any unseen class into a
relatively general category in an hierarchically based classification. Our
model uses a three-level hierarchical contrastive loss based ResNet152
classifier for classifying an object based on its features extracted from Image
embedding, not used during the training phase. For our experimentation, we have
used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal
classes for training our model and created our own dataset of unseen classes
for evaluating our trained model. Our model provides satisfactory results in
classifying the unknown objects into a generic category which has been later
discussed in greater detail.
","2023-10-13","2310.08304v1.pdf"
"2310.08309","Zhe Yang","Zhe Yang, Damai Dai, Peiyi Wang, Zhifang Sui","Not All Demonstration Examples are Equally Beneficial: Reweighting
  Demonstration Examples for In-Context Learning","Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have recently gained the In-Context Learning
(ICL) ability with the models scaling up, allowing them to quickly adapt to
downstream tasks with only a few demonstration examples prepended in the input
sequence. Nonetheless, the current practice of ICL treats all demonstration
examples equally, which still warrants improvement, as the quality of examples
is usually uneven. In this paper, we investigate how to determine approximately
optimal weights for demonstration examples and how to apply them during ICL. To
assess the quality of weights in the absence of additional validation data, we
design a masked self-prediction (MSP) score that exhibits a strong correlation
with the final ICL performance. To expedite the weight-searching process, we
discretize the continuous weight space and adopt beam search. With
approximately optimal weights obtained, we further propose two strategies to
apply them to demonstrations at different model positions. Experimental results
on 8 text classification tasks show that our approach outperforms conventional
ICL by a large margin. Our code are publicly available at
https:github.com/Zhe-Young/WICL.
","2023-10-13","2310.08309v1.pdf"
"2310.08319","Xueguang Ma","Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin","Fine-Tuning LLaMA for Multi-Stage Text Retrieval","","","","","cs.IR","http://creativecommons.org/licenses/by/4.0/","  The effectiveness of multi-stage text retrieval has been solidly demonstrated
since before the era of pre-trained language models. However, most existing
studies utilize models that predate recent advances in large language models
(LLMs). This study seeks to explore potential improvements that
state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning
the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise
reranker (RankLLaMA) for both passage retrieval and document retrieval using
the MS MARCO datasets. Our findings demonstrate that the effectiveness of large
language models indeed surpasses that of smaller models. Additionally, since
LLMs can inherently handle longer contexts, they can represent entire documents
holistically, obviating the need for traditional segmenting and pooling
strategies. Furthermore, evaluations on BEIR demonstrate that our
RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model
checkpoints from this study are available on HuggingFace.
","2023-10-13","2310.08319v1.pdf"
"2310.08320","Dominik Hintersdorf","Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting","Defending Our Privacy With Backdoors","14 pages, 4 figures","","","","cs.LG cs.CL cs.CR cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-""a person"" instead of the person's name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new ""dual-use""
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
","2023-10-13","2310.08320v1.pdf"
"2310.08338","Charles Onu","Charles C. Onu, Samantha Latremouille, Arsenii Gorin, Junhao Wang,
  Uchenna Ekwochi, Peter O. Ubuane, Omolara A. Kehinde, Muhammad A. Salisu,
  Datonye Briggs, Yoshua Bengio, Doina Precup","A cry for help: Early detection of brain injury in newborns","","","","","eess.AS cs.SD q-bio.NC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Since the 1960s, neonatal clinicians have known that newborns suffering from
certain neurological conditions exhibit altered crying patterns such as the
high-pitched cry in birth asphyxia. Despite an annual burden of over 1.5
million infant deaths and disabilities, early detection of neonatal brain
injuries due to asphyxia remains a challenge, particularly in developing
countries where the majority of births are not attended by a trained physician.
Here we report on the first inter-continental clinical study to demonstrate
that neonatal brain injury can be reliably determined from recorded infant
cries using an AI algorithm we call Roseline. Previous and recent work has been
limited by the lack of a large, high-quality clinical database of cry
recordings, constraining the application of state-of-the-art machine learning.
We develop a new training methodology for audio-based pathology detection
models and evaluate this system on a large database of newborn cry sounds
acquired from geographically diverse settings -- 5 hospitals across 3
continents. Our system extracts interpretable acoustic biomarkers that support
clinical decisions and is able to accurately detect neurological injury from
newborns' cries with an AUC of 92.5% (88.7% sensitivity at 80% specificity).
Cry-based neurological monitoring opens the door for low-cost, easy-to-use,
non-invasive and contact-free screening of at-risk babies, especially when
integrated into simple devices like smartphones or neonatal ICU monitors. This
would provide a reliable tool where there are no alternatives, but also curtail
the need to regularly exert newborns to physically-exhausting or
radiation-exposing assessments such as brain CT scans. This work sets the stage
for embracing the infant cry as a vital sign and indicates the potential of
AI-driven sound monitoring for the future of affordable healthcare.
","2023-10-16","2310.08338v1.pdf"
"2310.08348","Yazhe Niu","Yazhe Niu, Yuan Pu, Zhenjie Yang, Xueyan Li, Tong Zhou, Jiyuan Ren,
  Shuai Hu, Hongsheng Li, Yu Liu","LightZero: A Unified Benchmark for Monte Carlo Tree Search in General
  Sequential Decision Scenarios","NeurIPS 2023 Spotlight","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Building agents based on tree-search planning capabilities with learned
models has achieved remarkable success in classic decision-making problems,
such as Go and Atari. However, it has been deemed challenging or even
infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse
real-world applications, especially when these environments involve complex
action spaces and significant simulation costs, or inherent stochasticity. In
this work, we introduce LightZero, the first unified benchmark for deploying
MCTS/MuZero in general sequential decision scenarios. Specificially, we
summarize the most critical challenges in designing a general MCTS-style
decision-making solver, then decompose the tightly-coupled algorithm and system
design of tree-search RL methods into distinct sub-modules. By incorporating
more appropriate exploration and optimization strategies, we can significantly
enhance these sub-modules and construct powerful LightZero agents to tackle
tasks across a wide range of domains, such as board games, Atari, MuJoCo,
MiniGrid and GoBigger. Detailed benchmark results reveal the significant
potential of such methods in building scalable and efficient decision
intelligence. The code is available as part of OpenDILab at
https://github.com/opendilab/LightZero.
","2023-10-13","2310.08348v1.pdf"
"2310.08365","Md. Rezaul Karim","Md. Rezaul Karim and Lina Molinas Comet and Md Shajalal and Oya Beyan
  and Dietrich Rebholz-Schuhmann and Stefan Decker","From Large Language Models to Knowledge Graphs for Biomarker Discovery
  in Cancer","arXiv admin note: substantial text overlap with arXiv:2302.04737","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Domain experts often rely on up-to-date knowledge for apprehending and
disseminating specific biological processes that help them design strategies to
develop prevention and therapeutic decision-making. A challenging scenario for
artificial intelligence (AI) is using biomedical data (e.g., texts, imaging,
omics, and clinical) to provide diagnosis and treatment recommendations for
cancerous conditions. Data and knowledge about cancer, drugs, genes, proteins,
and their mechanism is spread across structured (knowledge bases (KBs)) and
unstructured (e.g., scientific articles) sources. A large-scale knowledge graph
(KG) can be constructed by integrating these data, followed by extracting facts
about semantically interrelated entities and relations. Such KGs not only allow
exploration and question answering (QA) but also allow domain experts to deduce
new knowledge. However, exploring and querying large-scale KGs is tedious for
non-domain users due to a lack of understanding of the underlying data assets
and semantic technologies. In this paper, we develop a domain KG to leverage
cancer-specific biomarker discovery and interactive QA. For this, a domain
ontology called OncoNet Ontology (ONO) is developed to enable semantic
reasoning for validating gene-disease relations. The KG is then enriched by
harmonizing the ONO, controlled vocabularies, and additional biomedical
concepts from scientific articles by employing BioBERT- and SciBERT-based
information extraction (IE) methods. Further, since the biomedical domain is
evolving, where new findings often replace old ones, without employing
up-to-date findings, there is a high chance an AI system exhibits concept drift
while providing diagnosis and treatment. Therefore, we finetuned the KG using
large language models (LLMs) based on more recent articles and KBs that might
not have been seen by the named entity recognition models.
","2023-10-13","2310.08365v1.pdf"
"2310.08367","Haowei Lin","Haowei Lin, Zihao Wang, Jianzhu Ma, Yitao Liang","MCU: A Task-centric Framework for Open-ended Agent Evaluation in
  Minecraft","","","","","cs.AI cs.CL cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  To pursue the goal of creating an open-ended agent in Minecraft, an
open-ended game environment with unlimited possibilities, this paper introduces
a task-centric framework named MCU for Minecraft agent evaluation. The MCU
framework leverages the concept of atom tasks as fundamental building blocks,
enabling the generation of diverse or even arbitrary tasks. Within the MCU
framework, each task is measured with six distinct difficulty scores (time
consumption, operational effort, planning complexity, intricacy, creativity,
novelty). These scores offer a multi-dimensional assessment of a task from
different angles, and thus can reveal an agent's capability on specific facets.
The difficulty scores also serve as the feature of each task, which creates a
meaningful task space and unveils the relationship between tasks. For efficient
evaluation of Minecraft agents employing the MCU framework, we maintain a
unified benchmark, namely SkillForge, which comprises representative tasks with
diverse categories and difficulty distribution. We also provide convenient
filters for users to select tasks to assess specific capabilities of agents. We
show that MCU has the high expressivity to cover all tasks used in recent
literature on Minecraft agent, and underscores the need for advancements in
areas such as creativity, precise control, and out-of-distribution
generalization under the goal of open-ended Minecraft agent development.
","2023-10-13","2310.08367v1.pdf"
"2310.08372","Boyang Xue","Boyang Xue and Weichao Wang and Hongru Wang and Fei Mi and Rui Wang
  and Yasheng Wang and Lifeng Shang and Xin Jiang and Qun Liu and Kam-Fai Wong","Improving Factual Consistency for Knowledge-Grounded Dialogue Systems
  via Knowledge Enhancement and Alignment","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pretrained language models (PLMs) based knowledge-grounded dialogue systems
are prone to generate responses that are factually inconsistent with the
provided knowledge source. In such inconsistent responses, the dialogue models
fail to accurately express the external knowledge they rely upon. Inspired by
previous work which identified that feed-forward networks (FFNs) within
Transformers are responsible for factual knowledge expressions, we investigate
two methods to efficiently improve the factual expression capability {of FFNs}
by knowledge enhancement and alignment respectively. We first propose
\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers
to enhance factual knowledge expressions} given the specific patterns of
knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement
learning for factual consistency (RLFC) method to implicitly adjust FFNs'
expressions in responses by aligning with gold knowledge for the factual
consistency preference. To comprehensively assess the factual consistency and
dialogue quality of responses, we employ extensive automatic measures and human
evaluations including sophisticated fine-grained NLI-based metrics.
Experimental results on WoW and CMU\_DoG datasets demonstrate that our methods
efficiently enhance the ability of the FFN module to convey factual knowledge,
validating the efficacy of improving factual consistency for knowledge-grounded
dialogue systems.
","2023-10-17","2310.08372v1.pdf"
"2310.08391","Jingfeng Wu","Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu,
  Peter L. Bartlett","How Many Pretraining Tasks Are Needed for In-Context Learning of Linear
  Regression?","","","","","stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformers pretrained on diverse tasks exhibit remarkable in-context
learning (ICL) capabilities, enabling them to solve unseen tasks solely based
on input contexts without adjusting model parameters. In this paper, we study
ICL in one of its simplest setups: pretraining a linearly parameterized
single-layer linear attention model for linear regression with a Gaussian
prior. We establish a statistical task complexity bound for the attention model
pretraining, showing that effective pretraining only requires a small number of
independent tasks. Furthermore, we prove that the pretrained model closely
matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by
achieving nearly Bayes optimal risk on unseen tasks under a fixed context
length. These theoretical findings complement prior experimental research and
shed light on the statistical foundations of ICL.
","2023-10-13","2310.08391v1.pdf"
"2310.08394","Ondrej Skopek","Ondrej Skopek, Rahul Aralikatte, Sian Gooding, Victor Carbune","Towards Better Evaluation of Instruction-Following: A Case-Study in
  Summarization","CoNLL 2023 camera-ready version","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Despite recent advances, evaluating how well large language models (LLMs)
follow user instructions remains an open problem. While evaluation methods of
language models have seen a rise in prompt-based approaches, limited work on
the correctness of these methods has been conducted. In this work, we perform a
meta-evaluation of a variety of metrics to quantify how accurately they measure
the instruction-following abilities of LLMs. Our investigation is performed on
grounded query-based summarization by collecting a new short-form, real-world
dataset riSum, containing 300 document-instruction pairs with 3 answers each.
All 900 answers are rated by 3 human annotators. Using riSum, we analyze the
agreement between evaluation methods and human judgment. Finally, we propose
new LLM-based reference-free evaluation methods that improve upon established
baselines and perform on par with costly reference-based metrics that require
high-quality summaries.
","2023-10-23","2310.08394v1.pdf"
"2310.08395","Yuanyuan Liang","Yuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Weining Qian,
  Yunshi Lan","Prompting Large Language Models with Chain-of-Thought for Few-Shot
  Knowledge Base Question Generation","Accepted by EMNLP 2023 main conference","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The task of Question Generation over Knowledge Bases (KBQG) aims to convert a
logical form into a natural language question. For the sake of expensive cost
of large-scale question annotation, the methods of KBQG under low-resource
scenarios urgently need to be developed. However, current methods heavily rely
on annotated data for fine-tuning, which is not well-suited for few-shot
question generation. The emergence of Large Language Models (LLMs) has shown
their impressive generalization ability in few-shot tasks. Inspired by
Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for
reasoning, we formulate KBQG task as a reasoning problem, where the generation
of a complete question is splitted into a series of sub-question generation.
Our proposed prompting method KQG-CoT first retrieves supportive logical forms
from the unlabeled data pool taking account of the characteristics of the
logical form. Then, we write a prompt to explicit the reasoning chain of
generating complicated questions based on the selected demonstrations. To
further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the
logical forms by their complexity. We conduct extensive experiments over three
public KBQG datasets. The results demonstrate that our prompting method
consistently outperforms other prompting baselines on the evaluated datasets.
Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of
the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,
METEOR, and ROUGE-L, respectively.
","2023-10-24","2310.08395v1.pdf"
"2310.08410","Qiuhong Wei","Qiuhong Wei, Zhengxiong Yao, Ying Cui, Bo Wei, Zhezhen Jin, and Ximing
  Xu","Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review
  and Meta-Analysis","","","","","stat.ME stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models such as ChatGPT are increasingly explored in medical
domains. However, the absence of standard guidelines for performance evaluation
has led to methodological inconsistencies. This study aims to summarize the
available evidence on evaluating ChatGPT's performance in medicine and provide
direction for future research. We searched ten medical literature databases on
June 15, 2023, using the keyword ""ChatGPT"". A total of 3520 articles were
identified, of which 60 were reviewed and summarized in this paper and 17 were
included in the meta-analysis. The analysis showed that ChatGPT displayed an
overall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing
medical queries. However, the studies varied in question resource,
question-asking process, and evaluation metrics. Moreover, many studies failed
to report methodological details, including the version of ChatGPT and whether
each question was used independently or repeatedly. Our findings revealed that
although ChatGPT demonstrated considerable potential for application in
healthcare, the heterogeneity of the studies and insufficient reporting may
affect the reliability of these results. Further well-designed studies with
comprehensive and transparent reporting are needed to evaluate ChatGPT's
performance in medicine.
","2023-10-13","2310.08410v1.pdf"
"2310.08419","Patrick Chao","Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George
  J. Pappas, Eric Wong","Jailbreaking Black Box Large Language Models in Twenty Queries","21 pages, 10 figures","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  There is growing interest in ensuring that large language models (LLMs) align
with human values. However, the alignment of such models is vulnerable to
adversarial jailbreaks, which coax LLMs into overriding their safety
guardrails. The identification of these vulnerabilities is therefore
instrumental in understanding inherent weaknesses and preventing future misuse.
To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an
algorithm that generates semantic jailbreaks with only black-box access to an
LLM. PAIR -- which is inspired by social engineering attacks -- uses an
attacker LLM to automatically generate jailbreaks for a separate targeted LLM
without human intervention. In this way, the attacker LLM iteratively queries
the target LLM to update and refine a candidate jailbreak. Empirically, PAIR
often requires fewer than twenty queries to produce a jailbreak, which is
orders of magnitude more efficient than existing algorithms. PAIR also achieves
competitive jailbreaking success rates and transferability on open and
closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.
","2023-10-17","2310.08419v1.pdf"
"2310.08420","Yifei Zhang","Yifei Zhang, Siyi Gu, Bo Pan, Guangji Bai, Xiaofeng Yang, Liang Zhao","Visual Attention-Prompted Prediction and Learning","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Explanation(attention)-guided learning is a method that enhances a model's
predictive power by incorporating human understanding during the training
phase. While attention-guided learning has shown promising results, it often
involves time-consuming and computationally expensive model retraining. To
address this issue, we introduce the attention-prompted prediction technique,
which enables direct prediction guided by the attention prompt without the need
for model retraining. However, this approach presents several challenges,
including: 1) How to incorporate the visual attention prompt into the model's
decision-making process and leverage it for future predictions even in the
absence of a prompt? and 2) How to handle the incomplete information from the
visual attention prompt? To tackle these challenges, we propose a novel
framework called Visual Attention-Prompted Prediction and Learning, which
seamlessly integrates visual attention prompts into the model's decision-making
process and adapts to images both with and without attention prompts for
prediction. To address the incomplete information of the visual attention
prompt, we introduce a perturbation-based attention map modification method.
Additionally, we propose an optimization-based mask aggregation method with a
new weight learning function for adaptive perturbed annotation aggregation in
the attention map modification process. Our overall framework is designed to
learn in an attention-prompt guided multi-task manner to enhance future
predictions even for samples without attention prompts and trained in an
alternating manner for better convergence. Extensive experiments conducted on
two datasets demonstrate the effectiveness of our proposed framework in
enhancing predictions for samples, both with and without provided prompts.
","2023-10-13","2310.08420v1.pdf"
"2310.08421","Shervin Halat","Shervin Halat, Mohammad Rahmati, Ehsan Nazerfard","SegLoc: Visual Self-supervised Learning Scheme for Dense Prediction
  Tasks of Security Inspection X-ray Images","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Lately, remarkable advancements of artificial intelligence have been
attributed to the integration of self-supervised learning (SSL) scheme. Despite
impressive achievements within natural language processing (NLP), SSL in
computer vision has not been able to stay on track comparatively. Recently,
integration of contrastive learning on top of existing visual SSL models has
established considerable progress, thereby being able to outperform supervised
counterparts. Nevertheless, the improvements were mostly limited to
classification tasks; moreover, few studies have evaluated visual SSL models in
real-world scenarios, while the majority considered datasets containing
class-wise portrait images, notably ImageNet. Thus, here, we have considered
dense prediction tasks on security inspection x-ray images to evaluate our
proposed model Segmentation Localization (SegLoc). Based upon the model
Instance Localization (InsLoc), our model has managed to address one of the
most challenging downsides of contrastive learning, i.e., false negative pairs
of query embeddings. To do so, our pre-training dataset is synthesized by
cutting, transforming, then pasting labeled segments, as foregrounds, from an
already existing labeled dataset (PIDray) onto instances, as backgrounds, of an
unlabeled dataset (SIXray;) further, we fully harness the labels through
integration of the notion, one queue per class, into MoCo-v2 memory bank,
avoiding false negative pairs. Regarding the task in question, our approach has
outperformed random initialization method by 3% to 6%, while having
underperformed supervised initialization, in AR and AP metrics at different IoU
values for 20 to 30 pre-training epochs.
","2023-10-24","2310.08421v3.pdf"
"2310.08433","Carlos G\'omez-Rodr\'iguez","Carlos G\'omez-Rodr\'iguez and Paul Williams","A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative
  Writing","Accepted for publication in Findings of EMNLP 2023","","","","cs.CL cs.CY","http://creativecommons.org/licenses/by/4.0/","  We evaluate a range of recent LLMs on English creative writing, a challenging
and complex task that requires imagination, coherence, and style. We use a
difficult, open-ended scenario chosen to avoid training data reuse: an epic
narration of a single combat between Ignatius J. Reilly, the protagonist of the
Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl,
a prehistoric flying reptile. We ask several LLMs and humans to write such a
story and conduct a human evalution involving various criteria such as fluency,
coherence, originality, humor, and style. Our results show that some
state-of-the-art commercial LLMs match or slightly outperform our writers in
most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in
creativity, while humor shows a binary divide between LLMs that can handle it
comparably to humans and those that fail at it. We discuss the implications and
limitations of our study and suggest directions for future research.
","2023-10-13","2310.08433v1.pdf"
"2310.08446","Xiangyan Liu","Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin","Towards Robust Multi-Modal Reasoning via Model Selection","10 pages, 5 figures","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The reasoning capabilities of LLM (Large Language Model) are widely
acknowledged in recent research, inspiring studies on tool learning and
autonomous agents. LLM serves as the ""brain"" of agent, orchestrating multiple
tools for collaborative multi-step task solving. Unlike methods invoking tools
like calculators or weather APIs for straightforward tasks, multi-modal agents
excel by integrating diverse AI models for complex challenges. However, current
multi-modal agents neglect the significance of model selection: they primarily
focus on the planning and execution phases, and will only invoke predefined
task-specific models for each subtask, making the execution fragile. Meanwhile,
other traditional model selection methods are either incompatible with or
suboptimal for the multi-modal agent scenarios, due to ignorance of
dependencies among subtasks arising by multi-step reasoning.
  To this end, we identify the key challenges therein and propose the
$\textit{M}^3$ framework as a plug-in with negligible runtime overhead at
test-time. This framework improves model selection and bolsters the robustness
of multi-modal agents in multi-step reasoning. In the absence of suitable
benchmarks, we create MS-GQA, a new dataset specifically designed to
investigate the model selection challenge in multi-modal agents. Our
experiments reveal that our framework enables dynamic model selection,
considering both user inputs and subtask dependencies, thereby robustifying the
overall reasoning process. Our code and benchmark:
https://github.com/LINs-lab/M3.
","2023-10-13","2310.08446v1.pdf"
"2310.08459","Runxue Bao","Runxue Bao, Yiming Sun, Yuhe Gao, Jindong Wang, Qiang Yang, Haifeng
  Chen, Zhi-Hong Mao, Ye Ye","A Survey of Heterogeneous Transfer Learning","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The application of transfer learning, an approach utilizing knowledge from a
source domain to enhance model performance in a target domain, has seen a
tremendous rise in recent years, underpinning many real-world scenarios. The
key to its success lies in the shared common knowledge between the domains, a
prerequisite in most transfer learning methodologies. These methods typically
presuppose identical feature spaces and label spaces in both domains, known as
homogeneous transfer learning, which, however, is not always a practical
assumption. Oftentimes, the source and target domains vary in feature spaces,
data distributions, and label spaces, making it challenging or costly to secure
source domain data with identical feature and label spaces as the target
domain. Arbitrary elimination of these differences is not always feasible or
optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with
such disparities, has emerged as a promising approach for a variety of tasks.
Despite the existence of a survey in 2017 on this topic, the fast-paced
advances post-2017 necessitate an updated, in-depth review. We therefore
present a comprehensive survey of recent developments in heterogeneous transfer
learning methods, offering a systematic guide for future research. Our paper
reviews methodologies for diverse learning scenarios, discusses the limitations
of current studies, and covers various application contexts, including Natural
Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster
a deeper understanding and spur future research.
","2023-10-17","2310.08459v1.pdf"
"2310.08461","Yongchao Zhou","Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon,
  Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran\c{c}ois Kagy, Rishabh Agarwal","DistillSpec: Improving Speculative Decoding via Knowledge Distillation","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Speculative decoding (SD) accelerates large language model inference by
employing a faster draft model for generating multiple tokens, which are then
verified in parallel by the larger target model, resulting in the text
generated according to the target model distribution. However, identifying a
compact draft model that is well-aligned with the target model is challenging.
To tackle this issue, we propose DistillSpec that uses knowledge distillation
to better align the draft model with the target model, before applying SD.
DistillSpec makes two key design choices, which we demonstrate via systematic
study to be crucial to improving the draft and target alignment: utilizing
on-policy data generation from the draft model, and tailoring the divergence
function to the task and decoding strategy. Notably, DistillSpec yields
impressive 10 - 45% speedups over standard SD on a range of standard
benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine
DistillSpec with lossy SD to achieve fine-grained control over the latency vs.
task performance trade-off. Finally, in practical scenarios with models of
varying sizes, first using distillation to boost the performance of the target
model and then applying DistillSpec to train a well-aligned draft model can
reduce decoding latency by 6-10x with minimal performance drop, compared to
standard decoding without distillation.
","2023-10-13","2310.08461v1.pdf"
"2310.08465","Rui Zhao","Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu,
  Weijia Wu, Jussi Keppo, Mike Zheng Shou","MotionDirector: Motion Customization of Text-to-Video Diffusion Models","Project Page: https://showlab.github.io/MotionDirector/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large-scale pre-trained diffusion models have exhibited remarkable
capabilities in diverse video generations. Given a set of video clips of the
same motion concept, the task of Motion Customization is to adapt existing
text-to-video diffusion models to generate videos with this motion. For
example, generating a video with a car moving in a prescribed manner under
specific camera movements to make a movie, or a video illustrating how a bear
would lift weights to inspire creators. Adaptation methods have been developed
for customizing appearance like subject or style, yet unexplored for motion. It
is straightforward to extend mainstream adaption methods for motion
customization, including full model tuning, parameter-efficient tuning of
additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept
learned by these methods is often coupled with the limited appearances in the
training videos, making it difficult to generalize the customized motion to
other appearances. To overcome this challenge, we propose MotionDirector, with
a dual-path LoRAs architecture to decouple the learning of appearance and
motion. Further, we design a novel appearance-debiased temporal loss to
mitigate the influence of appearance on the temporal training objective.
Experimental results show the proposed method can generate videos of diverse
appearances for the customized motions. Our method also supports various
downstream applications, such as the mixing of different videos with their
appearance and motion respectively, and animating a single image with
customized motions. Our code and model weights will be released.
","2023-10-13","2310.08465v1.pdf"
"2310.08487","Yuanchun Shen","Yuanchun Shen, Ruotong Liao, Zhen Han, Yunpu Ma, Volker Tresp","GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language
  Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  While multi-modal models have successfully integrated information from image,
video, and audio modalities, integrating graph modality into large language
models (LLMs) remains unexplored. This discrepancy largely stems from the
inherent divergence between structured graph data and unstructured text data.
Incorporating graph knowledge provides a reliable source of information,
enabling potential solutions to address issues in text generation, e.g.,
hallucination, and lack of domain knowledge. To evaluate the integration of
graph knowledge into language models, a dedicated dataset is needed. However,
there is currently no benchmark dataset specifically designed for multimodal
graph-language models. To address this gap, we propose GraphextQA, a question
answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate
the evaluation and future development of graph-language models. Additionally,
we introduce a baseline model called CrossGNN, which conditions answer
generation on the paired graphs by cross-attending question-aware graph
features at decoding. The proposed dataset is designed to evaluate
graph-language models' ability to understand graphs and make use of it for
answer generation. We perform experiments with language-only models and the
proposed graph-language model to validate the usefulness of the paired graphs
and to demonstrate the difficulty of the task.
","2023-10-13","2310.08487v1.pdf"
"2310.08491","Seungone Kim","Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran
  Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo","Prometheus: Inducing Fine-grained Evaluation Capability in Language
  Models","Work in Progress","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,
GPT-4) as an evaluator for long-form responses has become the de facto
standard. However, for practitioners with large-scale evaluation tasks and
custom criteria in consideration (e.g., child-readability), using proprietary
LLMs as an evaluator is unreliable due to the closed-source nature,
uncontrolled versioning, and prohibitive costs. In this work, we propose
Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation
capabilities when the appropriate reference materials (reference answer, score
rubric) are accompanied. We first construct the Feedback Collection, a new
dataset that consists of 1K fine-grained score rubrics, 20K instructions, and
100K responses and language feedback generated by GPT-4. Using the Feedback
Collection, we train Prometheus, a 13B evaluator LLM that can assess any given
long-form text based on customized score rubric provided by the user.
Experimental results show that Prometheus scores a Pearson correlation of 0.897
with human evaluators when evaluating with 45 customized score rubrics, which
is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).
Furthermore, measuring correlation with GPT-4 with 1222 customized score
rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask
Eval) shows similar trends, bolstering Prometheus's capability as an evaluator
LLM. Lastly, Prometheus achieves the highest accuracy on two human preference
benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced
reward models explicitly trained on human preference datasets, highlighting its
potential as an universal reward model. We open-source our code, dataset, and
model at https://github.com/kaistAI/Prometheus.
","2023-10-13","2310.08491v1.pdf"
"2310.08511","Santiago Miret","Yu Song, Santiago Miret, Huan Zhang, Bang Liu","HoneyBee: Progressive Instruction Finetuning of Large Language Models
  for Materials Science","","","","","cs.CL cond-mat.mtrl-sci cs.AI","http://creativecommons.org/licenses/by/4.0/","  We propose an instruction-based process for trustworthy data curation in
materials science (MatSci-Instruct), which we then apply to finetune a
LLaMa-based language model targeted for materials science (HoneyBee).
MatSci-Instruct helps alleviate the scarcity of relevant, high-quality
materials science textual data available in the open literature, and HoneyBee
is the first billion-parameter language model specialized to materials science.
In MatSci-Instruct we improve the trustworthiness of generated data by
prompting multiple commercially available large language models for generation
with an Instructor module (e.g. Chat-GPT) and verification from an independent
Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of
multiple tasks and measure the quality of our dataset along multiple
dimensions, including accuracy against known facts, relevance to materials
science, as well as completeness and reasonableness of the data. Moreover, we
iteratively generate more targeted instructions and instruction-data in a
finetuning-evaluation-feedback loop leading to progressively better performance
for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark
shows HoneyBee's outperformance of existing language models on materials
science tasks and iterative improvement in successive stages of
instruction-data refinement. We study the quality of HoneyBee's language
modeling through automatic evaluation and analyze case studies to further
understand the model's capabilities and limitations. Our code and relevant
datasets are publicly available at
\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.
","2023-10-13","2310.08511v1.pdf"
"2310.08535","Maxwell Crouse","Maxwell Crouse, Ibrahim Abdelaziz, Kinjal Basu, Soham Dan, Sadhana
  Kumaravel, Achille Fokoue, Pavan Kapanipathi, Luis Lastras","Formally Specifying the High-Level Behavior of LLM-Based Agents","Preprint under review","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  LLM-based agents have recently emerged as promising tools for solving
challenging problems without the need for task-specific finetuned models that
can be expensive to procure. Currently, the design and implementation of such
agents is ad hoc, as the wide variety of tasks that LLM-based agents may be
applied to naturally means there can be no one-size-fits-all approach to agent
design. In this work we aim to alleviate the difficulty of designing and
implementing new agents by proposing a minimalistic, high-level generation
framework that simplifies the process of building agents. The framework we
introduce allows the user to specify desired agent behaviors in Linear Temporal
Logic (LTL). The declarative LTL specification is then used to construct a
constrained decoder that guarantees the LLM will produce an output exhibiting
the desired behavior. By designing our framework in this way, we obtain several
benefits, including the ability to enforce complex agent behavior, the ability
to formally validate prompt examples, and the ability to seamlessly incorporate
content-focused logical constraints into generation. In particular, our
declarative approach, in which the desired behavior is simply described without
concern for how it should be implemented or enforced, enables rapid design,
implementation and experimentation with different LLM-based agents. We
demonstrate how the proposed framework can be used to implement recent
LLM-based agents, and show how the guardrails our approach provides can lead to
improvements in agent performance. In addition, we release our code for general
use.
","2023-10-13","2310.08535v1.pdf"
"2310.08540","Lingfeng Shen","Lingfeng Shen, Aayush Mishra, Daniel Khashabi","Do pretrained Transformers Really Learn In-context by Gradient Descent?","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?
Several recent works draw analogies between the dynamics of GD and the emergent
behavior of ICL in large language models. However, these works make assumptions
far from the realistic natural language setting in which language models are
trained. Such discrepancies between theory and practice, therefore, necessitate
further investigation to validate their applicability.
  We start by highlighting the weaknesses in prior works that construct
Transformer weights to simulate gradient descent. Their experiments with
training Transformers on ICL objective, inconsistencies in the order
sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity
to parameter changes are some examples of a mismatch from the real-world
setting.
  Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural
setting. We conduct comprehensive empirical analyses on language models
pretrained on natural data (LLaMa-7B). Our comparisons on various performance
metrics highlight the inconsistent behavior of ICL and GD as a function of
various factors such as datasets, models, and number of demonstrations. We
observe that ICL and GD adapt the output distribution of language models
differently. These results indicate that the equivalence between ICL and GD is
an open hypothesis, requires nuanced considerations and calls for further
studies.
","2023-10-13","2310.08540v1.pdf"
"2310.08541","Zhengyuan Yang","Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,
  Zicheng Liu, Lijuan Wang","Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic
  Image Design and Generation","Project page at https://idea2img.github.io/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce ``Idea to Image,'' a system that enables multimodal iterative
self-refinement with GPT-4V(ision) for automatic image design and generation.
Humans can quickly identify the characteristics of different text-to-image
(T2I) models via iterative explorations. This enables them to efficiently
convert their high-level generation ideas into effective T2I prompts that can
produce good images. We investigate if systems based on large multimodal models
(LMMs) can develop analogous multimodal self-refinement abilities that enable
exploring unknown models or environments via self-refining tries. Idea2Img
cyclically generates revised T2I prompts to synthesize draft images, and
provides directional feedback for prompt revision, both conditioned on its
memory of the probed T2I model's characteristics. The iterative self-refinement
brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img
can process input ideas with interleaved image-text sequences, follow ideas
with design instructions, and generate images of better semantic and visual
qualities. The user preference study validates the efficacy of multimodal
iterative self-refinement on automatic image design and generation.
","2023-10-13","2310.08541v1.pdf"
"2310.08549","Lucy Shi","Lucy Xiaoyang Shi and Yunfan Jiang and Jake Grigsby and Linxi ""Jim""
  Fan and Yuke Zhu","Cross-Episodic Curriculum for Transformer Agents","To appear in NeurIPS 2023; The first two authors contributed equally","","","","cs.LG cs.AI cs.RO","http://creativecommons.org/licenses/by/4.0/","  We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the
learning efficiency and generalization of Transformer agents. Central to CEC is
the placement of cross-episodic experiences into a Transformer's context, which
forms the basis of a curriculum. By sequentially structuring online learning
trials and mixed-quality demonstrations, CEC constructs curricula that
encapsulate learning progression and proficiency increase across episodes. Such
synergy combined with the potent pattern recognition capabilities of
Transformer models delivers a powerful cross-episodic attention mechanism. The
effectiveness of CEC is demonstrated under two representative scenarios: one
involving multi-task reinforcement learning with discrete control, such as in
DeepMind Lab, where the curriculum captures the learning progression in both
individual and progressively complex settings; and the other involving
imitation learning with mixed-quality data for continuous control, as seen in
RoboMimic, where the curriculum captures the improvement in demonstrators'
expertise. In all instances, policies resulting from CEC exhibit superior
performance and strong generalization. Code is open-sourced at
https://cec-agent.github.io/ to facilitate research on Transformer agent
learning.
","2023-10-13","2310.08549v1.pdf"
"2310.08559","Linlu Qiu","Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin,
  Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang
  Ren","Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of
  Language Models with Hypothesis Refinement","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The ability to derive underlying principles from a handful of observations
and then generalize to novel situations -- known as inductive reasoning -- is
central to human intelligence. Prior work suggests that language models (LMs)
often fall short on inductive reasoning, despite achieving impressive success
on research benchmarks. In this work, we conduct a systematic study of the
inductive reasoning capabilities of LMs through iterative hypothesis
refinement, a technique that more closely mirrors the human inductive process
than standard input-output prompting. Iterative hypothesis refinement employs a
three-step process: proposing, selecting, and refining hypotheses in the form
of textual rules. By examining the intermediate rules, we observe that LMs are
phenomenal hypothesis proposers (i.e., generating candidate rules), and when
coupled with a (task-specific) symbolic interpreter that is able to
systematically filter the proposed set of rules, this hybrid approach achieves
strong results across inductive reasoning benchmarks that require inducing
causal relations, language-like instructions, and symbolic concepts. However,
they also behave as puzzling inductive reasoners, showing notable performance
gaps in rule induction (i.e., identifying plausible rules) and rule application
(i.e., applying proposed rules to instances), suggesting that LMs are proposing
hypotheses without being able to actually apply the rules. Through empirical
and human analyses, we further reveal several discrepancies between the
inductive reasoning processes of LMs and humans, shedding light on both the
potentials and limitations of using LMs in inductive reasoning tasks.
","2023-10-13","2310.08559v1.pdf"
"2310.08560","Charles Packer","Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah
  Wooders, Joseph E. Gonzalez","MemGPT: Towards LLMs as Operating Systems","Code and data available at https://memgpt.ai","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have revolutionized AI, but are constrained by
limited context windows, hindering their utility in tasks like extended
conversations and document analysis. To enable using context beyond limited
context windows, we propose virtual context management, a technique drawing
inspiration from hierarchical memory systems in traditional operating systems
that provide the appearance of large memory resources through data movement
between fast and slow memory. Using this technique, we introduce MemGPT
(Memory-GPT), a system that intelligently manages different memory tiers in
order to effectively provide extended context within the LLM's limited context
window, and utilizes interrupts to manage control flow between itself and the
user. We evaluate our OS-inspired design in two domains where the limited
context windows of modern LLMs severely handicaps their performance: document
analysis, where MemGPT is able to analyze large documents that far exceed the
underlying LLM's context window, and multi-session chat, where MemGPT can
create conversational agents that remember, reflect, and evolve dynamically
through long-term interactions with their users. We release MemGPT code and
data for our experiments at https://memgpt.ai.
","2023-10-13","2310.08560v1.pdf"
"2310.08566","Licong Lin","Licong Lin, Yu Bai, Song Mei","Transformers as Decision Makers: Provable In-Context Reinforcement
  Learning via Supervised Pretraining","","","","","cs.LG cs.AI cs.CL math.ST stat.ML stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large transformer models pretrained on offline reinforcement learning
datasets have demonstrated remarkable in-context reinforcement learning (ICRL)
capabilities, where they can make good decisions when prompted with interaction
trajectories from unseen environments. However, when and how transformers can
be trained to perform ICRL have not been theoretically well-understood. In
particular, it is unclear which reinforcement-learning algorithms transformers
can perform in context, and how distribution mismatch in offline training data
affects the learned algorithms. This paper provides a theoretical framework
that analyzes supervised pretraining for ICRL. This includes two recently
proposed training methods -- algorithm distillation and decision-pretrained
transformers. First, assuming model realizability, we prove the
supervised-pretrained transformer will imitate the conditional expectation of
the expert algorithm given the observed trajectory. The generalization error
will scale with model capacity and a distribution divergence factor between the
expert and offline algorithms. Second, we show transformers with ReLU attention
can efficiently approximate near-optimal online reinforcement learning
algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and
UCB-VI for tabular Markov decision processes. This provides the first
quantitative analysis of the ICRL capabilities of transformers pretrained from
offline trajectories.
","2023-10-13","2310.08566v1.pdf"
"2310.08571","Jan Dubi\'nski","Jan Dubi\'nski, Stanis{\l}aw Pawlak, Franziska Boenisch, Tomasz
  Trzci\'nski, Adam Dziedzic","Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Machine Learning as a Service (MLaaS) APIs provide ready-to-use and
high-utility encoders that generate vector representations for given inputs.
Since these encoders are very costly to train, they become lucrative targets
for model stealing attacks during which an adversary leverages query access to
the API to replicate the encoder locally at a fraction of the original training
costs. We propose Bucks for Buckets (B4B), the first active defense that
prevents stealing while the attack is happening without degrading
representation quality for legitimate API users. Our defense relies on the
observation that the representations returned to adversaries who try to steal
the encoder's functionality cover a significantly larger fraction of the
embedding space than representations of legitimate users who utilize the
encoder to solve a particular downstream task.vB4B leverages this to adaptively
adjust the utility of the returned representations according to a user's
coverage of the embedding space. To prevent adaptive adversaries from eluding
our defense by simply creating multiple user accounts (sybils), B4B also
individually transforms each user's representations. This prevents the
adversary from directly aggregating representations over multiple accounts to
create their stolen encoder copy. Our active defense opens a new path towards
securely sharing and democratizing encoders over public APIs.
","2023-10-13","2310.08571v1.pdf"
"2310.08577","Vishaal Udandarao","Vishaal Udandarao, Max F. Burg, Samuel Albanie, Matthias Bethge","Visual Data-Type Understanding does not emerge from Scaling
  Vision-Language Models","","","","","cs.CV cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of Visual Data-Type Identification, a basic perceptual skill
with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual data-types, spanning four broad categories. An extensive zero-shot
evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced
performance landscape. While VLMs are reasonably good at identifying certain
stylistic \textit{data-types}, such as cartoons and sketches, they struggle
with simpler data-types arising from basic manipulations like image rotations
or additive noise. Our findings reveal that (i) model scaling alone yields
marginal gains for contrastively-trained models like CLIP, and (ii) there is a
pronounced drop in performance for the largest auto-regressively trained VLMs
like OpenFlamingo. This finding points to a blind spot in current frontier
VLMs: they excel in recognizing semantic content but fail to acquire an
understanding of visual data-types through scaling. By analyzing the
pre-training distributions of these models and incorporating data-type
information into the captions during fine-tuning, we achieve a significant
enhancement in performance. By exploring this previously uncharted task, we aim
to set the stage for further advancing VLMs to equip them with visual data-type
understanding. Code and datasets are released at
https://github.com/bethgelab/DataTypeIdentification.
","2023-10-17","2310.08577v1.pdf"
"2310.08579","Xian Liu","Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li,
  Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov","HyperHuman: Hyper-Realistic Human Generation with Latent Structural
  Diffusion","Project Page: https://snap-research.github.io/HyperHuman/","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Despite significant advances in large-scale text-to-image models, achieving
hyper-realistic human image generation remains a desirable yet unsolved task.
Existing models like Stable Diffusion and DALL-E 2 tend to generate human
images with incoherent parts or unnatural poses. To tackle these challenges,
our key insight is that human image is inherently structural over multiple
granularities, from the coarse-level body skeleton to fine-grained spatial
geometry. Therefore, capturing such correlations between the explicit
appearance and latent structure in one model is essential to generate coherent
and natural human images. To this end, we propose a unified framework,
HyperHuman, that generates in-the-wild human images of high realism and diverse
layouts. Specifically, 1) we first build a large-scale human-centric dataset,
named HumanVerse, which consists of 340M images with comprehensive annotations
like human pose, depth, and surface normal. 2) Next, we propose a Latent
Structural Diffusion Model that simultaneously denoises the depth and surface
normal along with the synthesized RGB image. Our model enforces the joint
learning of image appearance, spatial relationship, and geometry in a unified
network, where each branch in the model complements to each other with both
structural awareness and textural richness. 3) Finally, to further boost the
visual quality, we propose a Structure-Guided Refiner to compose the predicted
conditions for more detailed generation of higher resolution. Extensive
experiments demonstrate that our framework yields the state-of-the-art
performance, generating hyper-realistic human images under diverse scenarios.
Project Page: https://snap-research.github.io/HyperHuman/
","2023-10-13","2310.08579v1.pdf"
"2310.08582","Mingyu Ding","Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao,
  Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo","Tree-Planner: Efficient Close-loop Task Planning with Large Language
  Models","","","","","cs.CL cs.AI cs.LG cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper studies close-loop task planning, which refers to the process of
generating a sequence of skills (a plan) to accomplish a specific goal while
adapting the plan based on real-time observations. Recently, prompting Large
Language Models (LLMs) to generate actions iteratively has become a prevalent
paradigm due to its superior performance and user-friendliness. However, this
paradigm is plagued by two inefficiencies: high token consumption and redundant
error correction, both of which hinder its scalability for large-scale testing
and applications. To address these issues, we propose Tree-Planner, which
reframes task planning with LLMs into three distinct phases: plan sampling,
action tree construction, and grounded deciding. Tree-Planner starts by using
an LLM to sample a set of potential plans before execution, followed by the
aggregation of them to form an action tree. Finally, the LLM performs a
top-down decision-making process on the tree, taking into account real-time
environmental information. Experiments show that Tree-Planner achieves
state-of-the-art performance while maintaining high efficiency. By decomposing
LLM queries into a single plan-sampling call and multiple grounded-deciding
calls, a considerable part of the prompt are less likely to be repeatedly
consumed. As a result, token consumption is reduced by 92.2% compared to the
previously best-performing model. Additionally, by enabling backtracking on the
action tree as needed, the correction process becomes more flexible, leading to
a 40.5% decrease in error corrections. Project page:
https://tree-planner.github.io/
","2023-10-13","2310.08582v1.pdf"
"2310.08588","Jingkang Yang","Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng
  Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu","Octopus: Embodied Vision-Language Programmer from Environmental Feedback","Project Page: https://choiszt.github.io/Octopus/, Codebase:
  https://github.com/dongyh20/Octopus","","","","cs.CV cs.AI cs.LG cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. Furthermore, when seamlessly integrated
into an embodied agent, it signifies a crucial stride towards the creation of
autonomous and context-aware systems capable of formulating plans and executing
commands with precision. In this paper, we introduce Octopus, a novel VLM
designed to proficiently decipher an agent's vision and textual task objectives
and to formulate intricate action sequences and generate executable code. Our
design allows the agent to adeptly handle a wide spectrum of tasks, ranging
from mundane daily chores in simulators to sophisticated interactions in
complex video games. Octopus is trained by leveraging GPT-4 to control an
explorative agent to generate training data, i.e., action blueprints and the
corresponding executable code, within our experimental environment called
OctoVerse. We also collect the feedback that allows the enhanced training
scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a
series of experiments, we illuminate Octopus's functionality and present
compelling results, and the proposed RLEF turns out to refine the agent's
decision-making. By open-sourcing our model architecture, simulator, and
dataset, we aspire to ignite further innovation and foster collaborative
applications within the broader embodied AI community.
","2023-10-13","2310.08588v1.pdf"
"2310.08618","Peter R. Wiecha","Peter R. Wiecha","Deep learning for nano-photonic materials -- The solution to
  everything!?","11 pages, 7 figures","","","","physics.comp-ph physics.data-an physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep learning is currently being hyped as an almost magical tool for solving
all kinds of difficult problems that computers have not been able to solve in
the past. Particularly in the fields of computer vision and natural language
processing, spectacular results have been achieved. The hype has now
infiltrated several scientific communities. In (nano-)photonics, researchers
are trying to apply deep learning to all kinds of forward and inverse problems.
A particularly challenging problem is for instance the rational design of
nanophotonic materials and devices. In this opinion article, I will first
discuss the public expectations of deep learning and give an overview of the
quite different scales at which actors from industry and research are operating
their deep learning models. I then examine the weaknesses and dangers
associated with deep learning. Finally, I'll discuss the key strengths that
make this new set of statistical methods so attractive, and review a personal
selection of opportunities that shouldn't be missed in the current
developments.
","2023-10-16","2310.08618v1.pdf"
"2310.08659","Yixiao Li","Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis,
  Weizhu Chen, Tuo Zhao","LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Quantization is an indispensable technique for serving Large Language Models
(LLMs) and has recently found its way into LoRA fine-tuning. In this work we
focus on the scenario where quantization and LoRA fine-tuning are applied
together on a pre-trained model. In such cases it is common to observe a
consistent gap in the performance on downstream tasks between full fine-tuning
and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ
(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that
simultaneously quantizes an LLM and finds a proper low-rank initialization for
LoRA fine-tuning. Such an initialization alleviates the discrepancy between the
quantized and full-precision model and significantly improves the
generalization in downstream tasks. We evaluate our method on natural language
understanding, question answering, summarization, and natural language
generation tasks. Experiments show that our method is highly effective and
outperforms existing quantization methods, especially in the challenging 2-bit
and 2/4-bit mixed precision regimes. We will release our code.
","2023-10-24","2310.08659v1.pdf"
"2310.08669","Yao-Hung Tsai","Yao-Hung Hubert Tsai, Vansh Dhar, Jialu Li, Bowen Zhang, Jian Zhang","Multimodal Large Language Model for Visual Navigation","","","","","cs.CV cs.RO","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Recent efforts to enable visual navigation using large language models have
mainly focused on developing complex prompt systems. These systems incorporate
instructions, observations, and history into massive text prompts, which are
then combined with pre-trained large language models to facilitate visual
navigation. In contrast, our approach aims to fine-tune large language models
for visual navigation without extensive prompt engineering. Our design involves
a simple text prompt, current observations, and a history collector model that
gathers information from previous observations as input. For output, our design
provides a probability distribution of possible actions that the agent can take
during navigation. We train our model using human demonstrations and collision
signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results
demonstrate that our method outperforms state-of-the-art behavior cloning
methods and effectively reduces collision rates.
","2023-10-16","2310.08669v1.pdf"
"2310.08678","Yulong Pei","Ethan Callanan, Amarachi Mbakwe, Antony Papadimitriou, Yulong Pei,
  Mathieu Sibue, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, Sameena Shah","Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4
  on mock CFA Exams","","","","","cs.CL cs.AI q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated remarkable performance on a
wide range of Natural Language Processing (NLP) tasks, often matching or even
beating state-of-the-art task-specific models. This study aims at assessing the
financial reasoning capabilities of LLMs. We leverage mock exam questions of
the Chartered Financial Analyst (CFA) Program to conduct a comprehensive
evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot
(ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an
in-depth analysis of the models' performance and limitations, and estimate
whether they would have a chance at passing the CFA exams. Finally, we outline
insights into potential strategies and improvements to enhance the
applicability of LLMs in finance. In this perspective, we hope this work paves
the way for future studies to continue enhancing LLMs for financial reasoning
through rigorous evaluation.
","2023-10-16","2310.08678v1.pdf"
"2310.08704","Hyo Kang","Sukwoong Choi, Hyo Kang, Namil Kim, Junsik Kim","How Does Artificial Intelligence Improve Human Decision-Making? Evidence
  from the AI-Powered Go Program","","","","","econ.GN q-fin.EC","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We study how humans learn from AI, exploiting an introduction of an
AI-powered Go program (APG) that unexpectedly outperformed the best
professional player. We compare the move quality of professional players to
that of APG's superior solutions around its public release. Our analysis of
749,190 moves demonstrates significant improvements in players' move quality,
accompanied by decreased number and magnitude of errors. The effect is
pronounced in the early stages of the game where uncertainty is highest. In
addition, younger players and those in AI-exposed countries experience greater
improvement, suggesting potential inequality in learning from AI. Further,
while players of all levels learn, less skilled players derive higher marginal
benefits. These findings have implications for managers seeking to adopt and
utilize AI effectively within their organizations.
","2023-10-16","2310.08704v1.pdf"
"2310.08740","Tao Li","Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, Yang Li","A Zero-Shot Language Agent for Computer Control with Structured
  Reflection","Accepted at Findings of EMNLP 2023","","","","cs.CL cs.SY eess.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have shown increasing capacity at planning and
executing a high-level goal in a live computer environment (e.g. MiniWoB++). To
perform a task, recent works often require a model to learn from trace examples
of the task via either supervised learning or few/many-shot prompting. Without
these trace examples, it remains a challenge how an agent can autonomously
learn and improve its control on a computer, which limits the ability of an
agent to perform a new task. We approach this problem with a zero-shot agent
that requires no given expert traces. Our agent plans for executable actions on
a partially observed environment, and iteratively progresses a task by
identifying and learning from its mistakes via self-reflection and structured
thought management. On the easy tasks of MiniWoB++, we show that our zero-shot
agent often outperforms recent SoTAs, with more efficient reasoning. For tasks
with more complexity, our reflective agent performs on par with prior best
models, even though previous works had the advantages of accessing expert
traces or additional screen information.
","2023-10-24","2310.08740v1.pdf"
"2310.08744","Jack Merullo","Jack Merullo, Carsten Eickhoff, Ellie Pavlick","Circuit Component Reuse Across Tasks in Transformer Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recent work in mechanistic interpretability has shown that behaviors in
language models can be successfully reverse-engineered through circuit
analysis. A common criticism, however, is that each circuit is task-specific,
and thus such analysis cannot contribute to understanding the models at a
higher level. In this work, we present evidence that insights (both low-level
findings about specific heads and higher-level findings about general
algorithms) can indeed generalize across tasks. Specifically, we study the
circuit discovered in Wang et al. (2022) for the Indirect Object Identification
(IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that
it is mostly reused to solve a seemingly different task: Colored Objects
(Ippolito & Callison-Burch, 2023). We provide evidence that the process
underlying both tasks is functionally very similar, and contains about a 78%
overlap in in-circuit attention heads. We further present a proof-of-concept
intervention experiment, in which we adjust four attention heads in middle
layers in order to 'repair' the Colored Objects circuit and make it behave like
the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the
Colored Objects task and explain most sources of error. The intervention
affects downstream attention heads in specific ways predicted by their
interactions in the IOI circuit, indicating that this subcircuit behavior is
invariant to the different task inputs. Overall, our results provide evidence
that it may yet be possible to explain large language models' behavior in terms
of a relatively small number of interpretable task-general algorithmic building
blocks and computational components.
","2023-10-16","2310.08744v1.pdf"
"2310.08750","Jinsung Yoon","Jinsung Yoon, Sercan O Arik, Yanfei Chen, Tomas Pfister","Search-Adaptor: Text Embedding Customization for Information Retrieval","9 pages, 2 figures","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Text embeddings extracted by pre-trained Large Language Models (LLMs) have
significant potential to improve information retrieval and search. Beyond the
zero-shot setup in which they are being conventionally used, being able to take
advantage of the information from the relevant query-corpus paired data has the
power to further boost the LLM capabilities. In this paper, we propose a novel
method, Search-Adaptor, for customizing LLMs for information retrieval in an
efficient and robust way. Search-Adaptor modifies the original text embedding
generated by pre-trained LLMs, and can be integrated with any LLM, including
those only available via APIs. On multiple real-world English and multilingual
retrieval datasets, we show consistent and significant performance benefits for
Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding
APIs in nDCG@10 averaged over 13 BEIR datasets.
","2023-10-16","2310.08750v1.pdf"
"2310.08754","Mehdi Ali","Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max
  L\""ubbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper
  Schulze Buschhoff, Charvi Jain, Alexander Arno Weber, Lena Jurkschat, Hammam
  Abdelwahab, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Samuel
  Weinbach, Rafet Sifa, Stefan Kesselheim, Nicolas Flores-Herr","Tokenizer Choice For LLM Training: Negligible or Crucial?","","","","","cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  The recent success of LLMs has been predominantly driven by curating the
training dataset composition, scaling of model architectures and dataset sizes
and advancements in pretraining objectives, leaving tokenizer influence as a
blind spot. Shedding light on this underexplored area, we conduct a
comprehensive study on the influence of tokenizer choice on LLM downstream
performance by training 24 mono- and multilingual LLMs at a 2.6B parameter
scale, ablating different tokenizer algorithms and parameterizations. Our
studies highlight that the tokenizer choice can significantly impact the
model's downstream performance, training and inference costs. In particular, we
find that the common tokenizer evaluation metrics fertility and parity are not
always predictive of model downstream performance, rendering these metrics a
questionable proxy for the model's downstream performance. Furthermore, we show
that multilingual tokenizers trained on the five most frequent European
languages require vocabulary size increases of factor three in comparison to
English. While English-only tokenizers have been applied to the training of
multi-lingual LLMs, we find that this approach results in a severe downstream
performance degradation and additional training costs of up to 68%, due to an
inefficient tokenization vocabulary.
","2023-10-19","2310.08754v1.pdf"
"2310.08759","Jayetri Bardhan","Jayetri Bardhan, Kirk Roberts, Daisy Zhe Wang","Question Answering for Electronic Health Records: A Scoping Review of
  datasets and models","5 tables, 6 figures","","","","cs.LG cs.IR","http://creativecommons.org/licenses/by/4.0/","  Question Answering (QA) systems on patient-related data can assist both
clinicians and patients. They can, for example, assist clinicians in
decision-making and enable patients to have a better understanding of their
medical history. Significant amounts of patient data are stored in Electronic
Health Records (EHRs), making EHR QA an important research area. In EHR QA, the
answer is obtained from the medical record of the patient. Because of the
differences in data format and modality, this differs greatly from other
medical QA tasks that employ medical websites or scientific papers to retrieve
answers, making it critical to research EHR question answering. This study
aimed to provide a methodological review of existing works on QA over EHRs. We
searched for articles from January 1st, 2005 to September 30th, 2023 in four
digital sources including Google Scholar, ACL Anthology, ACM Digital Library,
and PubMed to collect relevant publications on EHR QA. 4111 papers were
identified for our study, and after screening based on our inclusion criteria,
we obtained a total of 47 papers for further study. Out of the 47 papers, 25
papers were about EHR QA datasets, and 37 papers were about EHR QA models. It
was observed that QA on EHRs is relatively new and unexplored. Most of the
works are fairly recent. Also, it was observed that emrQA is by far the most
popular EHR QA dataset, both in terms of citations and usage in other papers.
Furthermore, we identified the different models used in EHR QA along with the
evaluation metrics used for these models.
","2023-10-16","2310.08759v1.pdf"
"2310.08773","Karen D. Wang","Karen D. Wang, Eric Burkholder, Carl Wieman, Shima Salehi, Nick Haber","Examining the Potential and Pitfalls of ChatGPT in Science and
  Engineering Problem-Solving","12 pages, 2 figures","","","","cs.AI cs.CE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The study explores the capabilities of OpenAI's ChatGPT in solving different
types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of
40 problems from a college-level engineering physics course. These problems
ranged from well-specified problems, where all data required for solving the
problem was provided, to under-specified, real-world problems where not all
necessary data were given. Our findings show that ChatGPT could successfully
solve 62.5\% of the well-specified problems, but its accuracy drops to 8.3\%
for under-specified problems. Analysis of the model's incorrect solutions
revealed three distinct failure modes: 1) failure to construct accurate models
of the physical world, 2) failure to make reasonable assumptions about missing
data, and 3) calculation errors. The study offers implications for how to
leverage LLM-augmented instructional materials to enhance STEM education. The
insights also contribute to the broader discourse on AI's strengths and
limitations, serving both educators aiming to leverage the technology and
researchers investigating human-AI collaboration frameworks for problem-solving
and decision-making.
","2023-10-16","2310.08773v1.pdf"
"2310.08780","Abel Salinas","Abel Salinas, Louis Penafiel, Robert McCormack, Fred Morstatter","""Im not Racist but..."": Discovering Bias in the Internal Knowledge of
  Large Language Models","Warning: This paper discusses and contains content that is offensive
  or upsetting","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have garnered significant attention for their
remarkable performance in a continuously expanding set of natural language
processing tasks. However, these models have been shown to harbor inherent
societal biases, or stereotypes, which can adversely affect their performance
in their many downstream applications. In this paper, we introduce a novel,
purely prompt-based approach to uncover hidden stereotypes within any arbitrary
LLM. Our approach dynamically generates a knowledge representation of internal
stereotypes, enabling the identification of biases encoded within the LLM's
internal knowledge. By illuminating the biases present in LLMs and offering a
systematic methodology for their analysis, our work contributes to advancing
transparency and promoting fairness in natural language processing systems.
","2023-10-16","2310.08780v1.pdf"
"2310.08795","Mingyu Derek Ma","Mingyu Derek Ma, Jiun-Yu Kao, Arpit Gupta, Yu-Hsiang Lin, Wenbo Zhao,
  Tagyoung Chung, Wei Wang, Kai-Wei Chang, Nanyun Peng","Mitigating Bias for Question Answering Models by Tracking Bias Influence","","","","","cs.CL cs.AI cs.CY cs.LG","http://creativecommons.org/licenses/by/4.0/","  Models of various NLP tasks have been shown to exhibit stereotypes, and the
bias in the question answering (QA) models is especially harmful as the output
answers might be directly consumed by the end users. There have been datasets
to evaluate bias in QA models, while bias mitigation technique for the QA
models is still under-explored. In this work, we propose BMBI, an approach to
mitigate the bias of multiple-choice QA models. Based on the intuition that a
model would lean to be more biased if it learns from a biased example, we
measure the bias level of a query instance by observing its influence on
another instance. If the influenced instance is more biased, we derive that the
query instance is biased. We then use the bias level detected as an
optimization objective to form a multi-task learning setting in addition to the
original QA task. We further introduce a new bias evaluation metric to quantify
bias in a comprehensive and sensitive way. We show that our method could be
applied to multiple QA formulations across multiple bias categories. It can
significantly reduce the bias level in all 9 bias categories in the BBQ dataset
while maintaining comparable QA accuracy.
","2023-10-16","2310.08795v1.pdf"
"2310.08796","Hanlin Zhu","Hanlin Zhu, Andrew Cohen, Danqing Wang, Kevin Yang, Xiaomeng Yang,
  Jiantao Jiao, Yuandong Tian","End-to-end Story Plot Generator","17 pages","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Story plots, while short, carry most of the essential information of a full
story that may contain tens of thousands of words. We study the problem of
automatic generation of story plots, which includes story premise, character
descriptions, plot outlines, etc. To generate a single engaging plot, existing
plot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands
of calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot,
which is costly and takes at least several minutes. Moreover, the hard-wired
nature of the method makes the pipeline non-differentiable, blocking fast
specialization and personalization of the plot generator. In this paper, we
propose three models, $\texttt{OpenPlot}$, $\texttt{E2EPlot}$ and
$\texttt{RLPlot}$, to address these challenges. $\texttt{OpenPlot}$ replaces
expensive OpenAI API calls with LLaMA2 (Touvron et al., 2023) calls via careful
prompt designs, which leads to inexpensive generation of high-quality training
datasets of story plots. We then train an end-to-end story plot generator,
$\texttt{E2EPlot}$, by supervised fine-tuning (SFT) using approximately 13000
story plots generated by $\texttt{OpenPlot}$. $\texttt{E2EPlot}$ generates
story plots of comparable quality to $\texttt{OpenPlot}$, and is > 10$\times$
faster (1k tokens in only 30 seconds on average). Finally, we obtain
$\texttt{RLPlot}$ that is further fine-tuned with RLHF on several different
reward models for different aspects of story quality, which yields 60.0$\%$
winning rate against $\texttt{E2EPlot}$ along the aspect of suspense and
surprise.
","2023-10-16","2310.08796v1.pdf"
"2310.08797","Takuma Udagawa","Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan
  Bhattacharjee","A Comparative Analysis of Task-Agnostic Distillation Methods for
  Compressing Transformer Language Models","Accepted to EMNLP 2023 Industry Track","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models have become a vital component in modern NLP, achieving
state of the art performance in a variety of tasks. However, they are often
inefficient for real-world deployment due to their expensive inference costs.
Knowledge distillation is a promising technique to improve their efficiency
while retaining most of their effectiveness. In this paper, we reproduce,
compare and analyze several representative methods for task-agnostic
(general-purpose) distillation of Transformer language models. Our target of
study includes Output Distribution (OD) transfer, Hidden State (HS) transfer
with various layer mapping strategies, and Multi-Head Attention (MHA) transfer
based on MiniLMv2. Through our extensive experiments, we study the
effectiveness of each method for various student architectures in both
monolingual (English) and multilingual settings. Overall, we show that MHA
transfer based on MiniLMv2 is generally the best option for distillation and
explain the potential reasons behind its success. Moreover, we show that HS
transfer remains as a competitive baseline, especially under a sophisticated
layer mapping strategy, while OD transfer consistently lags behind other
approaches. Findings from this study helped us deploy efficient yet effective
student models for latency-critical applications.
","2023-10-16","2310.08797v1.pdf"
"2310.08803","Palaash Agrawal","Palaash Agrawal, Cheston Tan and Heena Rathore","Advancing Perception in Artificial Intelligence through Principles of
  Cognitive Science","Summary: a detailed review of the current state of perception models
  through the lens of cognitive AI","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Although artificial intelligence (AI) has achieved many feats at a rapid
pace, there still exist open problems and fundamental shortcomings related to
performance and resource efficiency. Since AI researchers benchmark a
significant proportion of performance standards through human intelligence,
cognitive sciences-inspired AI is a promising domain of research. Studying
cognitive science can provide a fresh perspective to building fundamental
blocks in AI research, which can lead to improved performance and efficiency.
In this review paper, we focus on the cognitive functions of perception, which
is the process of taking signals from one's surroundings as input, and
processing them to understand the environment. Particularly, we study and
compare its various processes through the lens of both cognitive sciences and
AI. Through this study, we review all current major theories from various
sub-disciplines of cognitive science (specifically neuroscience, psychology and
linguistics), and draw parallels with theories and techniques from current
practices in AI. We, hence, present a detailed collection of methods in AI for
researchers to build AI systems inspired by cognitive science. Further, through
the process of reviewing the state of cognitive-inspired AI, we point out many
gaps in the current state of AI (with respect to the performance of the human
brain), and hence present potential directions for researchers to develop
better perception systems in AI.
","2023-10-16","2310.08803v1.pdf"
"2310.08825","Yuchen Liu","Dongsheng Jiang, Yuchen Liu, Songlin Liu, Xiaopeng Zhang, Jin Li,
  Hongkai Xiong, Qi Tian","From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language
  Models","The paper is under the corporation's legal review","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multi-modal Large Language Models (MLLMs) have made significant strides in
expanding the capabilities of Large Language Models (LLMs) through the
incorporation of visual perception interfaces. Despite the emergence of
exciting applications and the availability of diverse instruction tuning data,
existing approaches often rely on CLIP or its variants as the visual branch,
and merely extract features from the deep layers. However, these methods lack a
comprehensive analysis of the visual encoders in MLLMs. In this paper, we
conduct an extensive investigation into the effectiveness of different vision
encoders within MLLMs. Our findings reveal that the shallow layer features of
CLIP offer particular advantages for fine-grained tasks such as grounding and
region understanding. Surprisingly, the vision-only model DINO, which is not
pretrained with text-image alignment, demonstrates promising performance as a
visual branch within MLLMs. By simply equipping it with an MLP layer for
alignment, DINO surpasses CLIP in fine-grained related perception tasks.
Building upon these observations, we propose a simple yet effective feature
merging strategy, named COMM, that integrates CLIP and DINO with Multi-level
features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM
through comprehensive experiments on a wide range of benchmarks, including
image captioning, visual question answering, visual grounding, and object
hallucination. Experimental results demonstrate the superior performance of
COMM compared to existing methods, showcasing its enhanced visual capabilities
within MLLMs. Code will be made available at
https://github.com/YuchenLiu98/COMM.
","2023-10-19","2310.08825v1.pdf"
"2310.08837","Gang Fan","Gang Fan, Xiaoheng Xie, Xunjin Zheng, Yinan Liang, Peng Di","Static Code Analysis in the AI Era: An In-depth Exploration of the
  Concept, Function, and Potential of Intelligent Code Analysis Agents","","","","","cs.SE","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The escalating complexity of software systems and accelerating development
cycles pose a significant challenge in managing code errors and implementing
business logic. Traditional techniques, while cornerstone for software quality
assurance, exhibit limitations in handling intricate business logic and
extensive codebases. To address these challenges, we introduce the Intelligent
Code Analysis Agent (ICAA), a novel concept combining AI models, engineering
process designs, and traditional non-AI components. The ICAA employs the
capabilities of large language models (LLMs) such as GPT-3 or GPT-4 to
automatically detect and diagnose code errors and business logic
inconsistencies. In our exploration of this concept, we observed a substantial
improvement in bug detection accuracy, reducing the false-positive rate to 66\%
from the baseline's 85\%, and a promising recall rate of 60.8\%. However, the
token consumption cost associated with LLMs, particularly the average cost for
analyzing each line of code, remains a significant consideration for widespread
adoption. Despite this challenge, our findings suggest that the ICAA holds
considerable potential to revolutionize software quality assurance,
significantly enhancing the efficiency and accuracy of bug detection in the
software development process. We hope this pioneering work will inspire further
research and innovation in this field, focusing on refining the ICAA concept
and exploring ways to mitigate the associated costs.
","2023-10-16","2310.08837v1.pdf"
"2310.08840","Hongru Wang","Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang,
  Yasheng Wang, Wai-Chung Kwan, Irwin King, Kam-Fai Wong","Large Language Models as Source Planner for Personalized
  Knowledge-grounded Dialogue","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Open-domain dialogue system usually requires different sources of knowledge
to generate more informative and evidential responses. However, existing
knowledge-grounded dialogue systems either focus on a single knowledge source
or overlook the dependency between multiple sources of knowledge, which may
result in generating inconsistent or even paradoxical responses. To incorporate
multiple knowledge sources and dependencies between them, we propose SAFARI, a
novel framework that leverages the exceptional capabilities of large language
models (LLMs) in planning, understanding, and incorporating under both
supervised and unsupervised settings. Specifically, SAFARI decouples the
knowledge grounding into multiple sources and response generation, which allows
easy extension to various knowledge sources including the possibility of not
using any sources. To study the problem, we construct a personalized
knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind
\textbf{P}ersona}~(\textbf{KBP}), which is the first to consider the dependency
between persona and implicit knowledge. Experimental results on the KBP dataset
demonstrate that the SAFARI framework can effectively produce
persona-consistent and knowledge-enhanced responses.
","2023-10-16","2310.08840v1.pdf"
"2310.08842","Ian Watson Prof","Ian Watson","A Case-Based Persistent Memory for a Large Language Model","8 pages, 1 figure","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Case-based reasoning (CBR) as a methodology for problem-solving can use any
appropriate computational technique. This position paper argues that CBR
researchers have somewhat overlooked recent developments in deep learning and
large language models (LLMs). The underlying technical developments that have
enabled the recent breakthroughs in AI have strong synergies with CBR and could
be used to provide a persistent memory for LLMs to make progress towards
Artificial General Intelligence.
","2023-10-16","2310.08842v1.pdf"
"2310.08863","Christopher Fifty","Christopher Fifty, Jure Leskovec, Sebastian Thrun","In-Context Learning for Few-Shot Molecular Property Prediction","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In-context learning has become an important approach for few-shot learning in
Large Language Models because of its ability to rapidly adapt to new tasks
without fine-tuning model parameters. However, it is restricted to applications
in natural language and inapplicable to other domains. In this paper, we adapt
the concepts underpinning in-context learning to develop a new algorithm for
few-shot molecular property prediction. Our approach learns to predict
molecular properties from a context of (molecule, property measurement) pairs
and rapidly adapts to new properties without fine-tuning. On the FS-Mol and
BACE molecular property prediction benchmarks, we find this method surpasses
the performance of recent meta-learning algorithms at small support sizes and
is competitive with the best methods at large support sizes.
","2023-10-16","2310.08863v1.pdf"
"2310.08864","Quan Vuong","Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley,
  Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky,
  Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben
  Burgess-Limerick, Beomjoon Kim, Bernhard Sch\""olkopf, Brian Ichter, Cewu Lu,
  Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine
  Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv
  Shah, Dieter B\""uchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns,
  Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam
  Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Hao-Shu Fang, Haochen
  Shi, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie
  Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad
  Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette
  Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie
  Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J.
  Lim, Jo\~ao Silv\'erio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol
  Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne,
  Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, Krishnan
  Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa
  Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu
  Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa,
  Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Norman Di
  Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R
  Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan
  Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart\'in-Mart\'in,
  Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante,
  Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham
  Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon
  Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep
  Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima,
  Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis
  Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan
  Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao
  Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan
  Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-Hua Wu, Yujin Tang, Yuke
  Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui","Open X-Embodiment: Robotic Learning Datasets and RT-X Models","","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  Large, high-capacity models trained on diverse datasets have shown remarkable
successes on efficiently tackling downstream applications. In domains from NLP
to Computer Vision, this has led to a consolidation of pretrained models, with
general pretrained backbones serving as a starting point for many applications.
Can such a consolidation happen in robotics? Conventionally, robotic learning
methods train a separate model for every application, every robot, and even
every environment. Can we instead train generalist X-robot policy that can be
adapted efficiently to new robots, tasks, and environments? In this paper, we
provide datasets in standardized data formats and models to make it possible to
explore this possibility in the context of robotic manipulation, alongside
experimental results that provide an example of effective X-robot policies. We
assemble a dataset from 22 different robots collected through a collaboration
between 21 institutions, demonstrating 527 skills (160266 tasks). We show that
a high-capacity model trained on this data, which we call RT-X, exhibits
positive transfer and improves the capabilities of multiple robots by
leveraging experience from other platforms. More details can be found on the
project website
$\href{https://robotics-transformer-x.github.io}{\text{robotics-transformer-x.github.io}}$.
","2023-10-18","2310.08864v1.pdf"
"2310.08866","Samira Abnar","Samira Abnar, Omid Saremi, Laurent Dinh, Shantel Wilson, Miguel Angel
  Bautista, Chen Huang, Vimal Thilak, Etai Littwin, Jiatao Gu, Josh Susskind,
  Samy Bengio","Adaptivity and Modularity for Efficient Generalization Over Task
  Complexity","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Can transformers generalize efficiently on problems that require dealing with
examples with different levels of difficulty? We introduce a new task tailored
to assess generalization over different complexities and present results that
indicate that standard transformers face challenges in solving these tasks.
These tasks are variations of pointer value retrieval previously introduced by
Zhang et al. (2021). We investigate how the use of a mechanism for adaptive and
modular computation in transformers facilitates the learning of tasks that
demand generalization over the number of sequential computation steps (i.e.,
the depth of the computation graph). Based on our observations, we propose a
transformer-based architecture called Hyper-UT, which combines dynamic function
generation from hyper networks with adaptive depth from Universal Transformers.
This model demonstrates higher accuracy and a fairer allocation of
computational resources when generalizing to higher numbers of computation
steps. We conclude that mechanisms for adaptive depth and modularity complement
each other in improving efficient generalization concerning example complexity.
Additionally, to emphasize the broad applicability of our findings, we
illustrate that in a standard image recognition task, Hyper- UT's performance
matches that of a ViT model but with considerably reduced computational demands
(achieving over 70\% average savings by effectively using fewer layers).
","2023-10-16","2310.08866v1.pdf"
"2310.08872","Jiayu Xiao","Jiayu Xiao, Liang Li, Henglei Lv, Shuhui Wang, Qingming Huang","R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image
  Generation","Preprint. Under review. Project page:
  https://sagileo.github.io/Region-and-Boundary","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent text-to-image (T2I) diffusion models have achieved remarkable progress
in generating high-quality images given text-prompts as input. However, these
models fail to convey appropriate spatial composition specified by a layout
instruction. In this work, we probe into zero-shot grounded T2I generation with
diffusion models, that is, generating images corresponding to the input layout
information without training auxiliary modules or finetuning diffusion models.
We propose a Region and Boundary (R&B) aware cross-attention guidance approach
that gradually modulates the attention maps of diffusion model during
generative process, and assists the model to synthesize images (1) with high
fidelity, (2) highly compatible with textual input, and (3) interpreting layout
instructions accurately. Specifically, we leverage the discrete sampling to
bridge the gap between consecutive attention maps and discrete layout
constraints, and design a region-aware loss to refine the generative layout
during diffusion process. We further propose a boundary-aware loss to
strengthen object discriminability within the corresponding regions.
Experimental results show that our method outperforms existing state-of-the-art
zero-shot grounded T2I generation methods by a large margin both qualitatively
and quantitatively on several benchmarks.
","2023-10-27","2310.08872v1.pdf"
"2310.08873","Zhen Zhang","Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, and K. W.
  Samuel Au","Interactive Navigation in Environments with Traversable Obstacles Using
  Large Language and Vision-Language Models","7 pages, 8 figures","","","","cs.RO cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper proposes an interactive navigation framework by using large
language and vision-language models, allowing robots to navigate in
environments with traversable obstacles. We utilize the large language model
(GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an
action-aware costmap to perform effective path planning without fine-tuning.
With the large models, we can achieve an end-to-end system from textual
instructions like ""Can you pass through the curtains to deliver medicines to
me?"", to bounding boxes (e.g., curtains) with action-aware attributes. They can
be used to segment LiDAR point clouds into two parts: traversable and
untraversable parts, and then an action-aware costmap is constructed for
generating a feasible path. The pre-trained large models have great
generalization ability and do not require additional annotated data for
training, allowing fast deployment in the interactive navigation tasks. We
choose to use multiple traversable objects such as curtains and grasses for
verification by instructing the robot to traverse them. Besides, traversing
curtains in a medical scenario was tested. All experimental results
demonstrated the proposed framework's effectiveness and adaptability to diverse
environments.
","2023-10-17","2310.08873v1.pdf"
"2310.08877","Weizhou Shen","Weizhou Shen, Yingqi Gao, Canbin Huang, Fanqi Wan, Xiaojun Quan, Wei
  Bi","Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue
  System","Accepted to EMNLP 2023 Main Conference","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Developing an efficient retriever to retrieve knowledge from a large-scale
knowledge base (KB) is critical for task-oriented dialogue systems to
effectively handle localized and specialized tasks. However, widely used
generative models such as T5 and ChatGPT often struggle to differentiate subtle
differences among the retrieved KB records when generating responses, resulting
in suboptimal quality of generated responses. In this paper, we propose the
application of maximal marginal likelihood to train a perceptive retriever by
utilizing signals from response generation for supervision. In addition, our
approach goes beyond considering solely retrieved entities and incorporates
various meta knowledge to guide the generator, thus improving the utilization
of knowledge. We evaluate our approach on three task-oriented dialogue datasets
using T5 and ChatGPT as the backbone models. The results demonstrate that when
combined with meta knowledge, the response generator can effectively leverage
high-quality knowledge records from the retriever and enhance the quality of
generated responses. The codes and models of this paper are available at
https://github.com/shenwzh3/MK-TOD.
","2023-10-23","2310.08877v1.pdf"
"2310.08884","Zehan Wang","Zehan Wang, Ziang Zhang, Luping Liu, Yang Zhao, Haifeng Huang, Tao
  Jin, Zhou Zhao","Extending Multi-modal Contrastive Representations","Our code is available at https://github.com/MCR-PEFT/Ex-MCR","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Multi-modal contrastive representation (MCR) of more than three modalities is
critical in multi-modal learning. Although recent methods showcase impressive
achievements, the high dependence on large-scale, high-quality paired data and
the expensive training costs limit their further development. Inspired by
recent C-MCR, this paper proposes Extending Multimodal Contrastive
Representation (Ex-MCR), a training-efficient and paired-data-free method to
flexibly learn unified contrastive representation space for more than three
modalities by integrating the knowledge of existing MCR spaces. Specifically,
Ex-MCR aligns multiple existing MCRs into the same based MCR, which can
effectively preserve the original semantic alignment of the based MCR. Besides,
we comprehensively enhance the entire learning pipeline for aligning MCR spaces
from the perspectives of training data, architecture, and learning objectives.
With the preserved original modality alignment and the enhanced space
alignment, Ex-MCR shows superior representation learning performance and
excellent modality extensibility. To demonstrate the effectiveness of Ex-MCR,
we align the MCR spaces of CLAP (audio-text) and ULIP (3D-vision) into the CLIP
(vision-text), leveraging the overlapping text and image modality,
respectively. Remarkably, without using any paired data, Ex-MCR learns a
3D-image-text-audio unified contrastive representation, and it achieves
state-of-the-art performance on audio-visual, 3D-image, audio-text, visual-text
retrieval, and 3D object classification tasks. More importantly, extensive
qualitative results further demonstrate the emergent semantic alignment between
the extended modalities (e.g., audio and 3D), which highlights the great
potential of modality extensibility.
","2023-10-16","2310.08884v1.pdf"
"2310.08885","Bryan Wilie","Willy Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia, Pascale
  Fung","InstructTODS: Large Language Models for End-to-End Task-Oriented
  Dialogue Systems","","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Large language models (LLMs) have been used for diverse tasks in natural
language processing (NLP), yet remain under-explored for task-oriented dialogue
systems (TODS), especially for end-to-end TODS. We present InstructTODS, a
novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue
systems that can adapt to diverse domains without fine-tuning. By leveraging
LLMs, InstructTODS generates a proxy belief state that seamlessly translates
user intentions into dynamic queries for efficient interaction with any KB. Our
extensive experiments demonstrate that InstructTODS achieves comparable
performance to fully fine-tuned TODS in guiding dialogues to successful
completion without prior knowledge or task-specific data. Furthermore, a
rigorous human evaluation of end-to-end TODS shows that InstructTODS produces
dialogue responses that notably outperform both the gold responses and the
state-of-the-art TODS in terms of helpfulness, informativeness, and humanness.
Moreover, the effectiveness of LLMs in TODS is further supported by our
comprehensive evaluations on TODS subtasks: dialogue state tracking, intent
classification, and response generation. Code and implementations could be
found here https://github.com/WillyHC22/InstructTODS/
","2023-10-16","2310.08885v1.pdf"
"2310.08889","Pengyu Wang","Linyang Li, Ke Ren, Yunfan Shao, Pengyu Wang, Xipeng Qiu","PerturbScore: Connecting Discrete and Continuous Perturbations in NLP","Accepted by Findings of EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the rapid development of neural network applications in NLP, model
robustness problem is gaining more attention. Different from computer vision,
the discrete nature of texts makes it more challenging to explore robustness in
NLP. Therefore, in this paper, we aim to connect discrete perturbations with
continuous perturbations, therefore we can use such connections as a bridge to
help understand discrete perturbations in NLP models. Specifically, we first
explore how to connect and measure the correlation between discrete
perturbations and continuous perturbations. Then we design a regression task as
a PerturbScore to learn the correlation automatically. Through experimental
results, we find that we can build a connection between discrete and continuous
perturbations and use the proposed PerturbScore to learn such correlation,
surpassing previous methods used in discrete perturbation measuring. Further,
the proposed PerturbScore can be well generalized to different datasets,
perturbation methods, indicating that we can use it as a powerful tool to study
model robustness in NLP.
","2023-10-16","2310.08889v1.pdf"
"2310.08899","Hao Liu","Hao Liu, Matei Zaharia, Pieter Abbeel","Exploration with Principles for Diverse AI Supervision","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Training large transformers using next-token prediction has given rise to
groundbreaking advancements in AI. While this generative AI approach has
produced impressive results, it heavily leans on human supervision. Even
state-of-the-art AI models like ChatGPT depend on fine-tuning through human
demonstrations, demanding extensive human input and domain expertise. This
strong reliance on human oversight poses a significant hurdle to the
advancement of AI innovation. To address this limitation, we propose a novel
paradigm termed Exploratory AI (EAI) aimed at autonomously generating
high-quality training data. Drawing inspiration from unsupervised reinforcement
learning (RL) pretraining, EAI achieves exploration within the natural language
space. We accomplish this by harnessing large language models to assess the
novelty of generated content. Our approach employs two key components: an actor
that generates novel content following exploration principles and a critic that
evaluates the generated content, offering critiques to guide the actor.
Empirical evaluations demonstrate that EAI significantly boosts model
performance on complex reasoning tasks, addressing the limitations of
human-intensive supervision.
","2023-10-16","2310.08899v1.pdf"
"2310.08901","Gabriel Mukobi","Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan
  Chan, Jesse Clifton","Welfare Diplomacy: Benchmarking Language Model Cooperation","","","","","cs.MA cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  The growing capabilities and increasingly widespread deployment of AI systems
necessitate robust benchmarks for measuring their cooperative capabilities.
Unfortunately, most multi-agent benchmarks are either zero-sum or purely
cooperative, providing limited opportunities for such measurements. We
introduce a general-sum variant of the zero-sum board game Diplomacy -- called
Welfare Diplomacy -- in which players must balance investing in military
conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both
a clearer assessment of and stronger training incentives for cooperative
capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules
and implementing them via an open-source Diplomacy engine; (2) constructing
baseline agents using zero-shot prompted language models; and (3) conducting
experiments where we find that baselines using state-of-the-art models attain
high social welfare but are exploitable. Our work aims to promote societal
safety by aiding researchers in developing and assessing multi-agent AI
systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is
available at https://github.com/mukobi/welfare-diplomacy.
","2023-10-16","2310.08901v1.pdf"
"2310.08903","Pengyu Wang","Pengyu Wang, Linyang Li, Ke Ren, Botian Jiang, Dong Zhang, Xipeng Qiu","SeqXGPT: Sentence-Level AI-Generated Text Detection","Accepted by EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Widely applied large language models (LLMs) can generate human-like content,
raising concerns about the abuse of LLMs. Therefore, it is important to build
strong AI-generated text (AIGT) detectors. Current works only consider
document-level AIGT detection, therefore, in this paper, we first introduce a
sentence-level detection challenge by synthesizing a dataset that contains
documents that are polished with LLMs, that is, the documents contain sentences
written by humans and sentences modified by LLMs. Then we propose
\textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes
log probability lists from white-box LLMs as features for sentence-level AIGT
detection. These features are composed like \textit{waves} in speech processing
and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution
and self-attention networks. We test it in both sentence and document-level
detection challenges. Experimental results show that previous methods struggle
in solving sentence-level AIGT detection, while our method not only
significantly surpasses baseline methods in both sentence and document-level
detection challenges but also exhibits strong generalization capabilities.
","2023-10-16","2310.08903v1.pdf"
"2310.08908","Runzhe Zhan","Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, Lidia S. Chao","Human-in-the-loop Machine Translation with Large Language Model","Accepted to MT Summit 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The large language model (LLM) has garnered significant attention due to its
in-context learning mechanisms and emergent capabilities. The research
community has conducted several pilot studies to apply LLMs to machine
translation tasks and evaluate their performance from diverse perspectives.
However, previous research has primarily focused on the LLM itself and has not
explored human intervention in the inference process of LLM. The
characteristics of LLM, such as in-context learning and prompt engineering,
closely mirror human cognitive abilities in language tasks, offering an
intuitive solution for human-in-the-loop generation. In this study, we propose
a human-in-the-loop pipeline that guides LLMs to produce customized outputs
with revision instructions. The pipeline initiates by prompting the LLM to
produce a draft translation, followed by the utilization of automatic retrieval
or human feedback as supervision signals to enhance the LLM's translation
through in-context learning. The human-machine interactions generated in this
pipeline are also stored in an external database to expand the in-context
retrieval database, enabling us to leverage human supervision in an offline
setting. We evaluate the proposed pipeline using GPT-3.5-turbo API on five
domain-specific benchmarks for German-English translation. The results
demonstrate the effectiveness of the pipeline in tailoring in-domain
translations and improving translation performance compared to direct
translation. Additionally, we discuss the results from the following
perspectives: 1) the effectiveness of different in-context retrieval methods;
2) the construction of a retrieval database under low-resource scenarios; 3)
the observed domains differences; 4) the quantitative analysis of linguistic
statistics; and 5) the qualitative analysis of translation cases. The code and
data are available at https://github.com/NLP2CT/HIL-MT/.
","2023-10-16","2310.08908v1.pdf"
"2310.08915","Yuxin Zhang","Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia
  Han, Jared Tanner, Shiwei Liu, Rongrong Ji","Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The ever-increasing large language models (LLMs), though opening a potential
path for the upcoming artificial general intelligence, sadly drops a daunting
obstacle on the way towards their on-device deployment. As one of the most
well-established pre-LLMs approaches in reducing model complexity, network
pruning appears to lag behind in the era of LLMs, due mostly to its costly
fine-tuning (or re-training) necessity under the massive volumes of model
parameter and training data. To close this industry-academia gap, we introduce
Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that
slightly updates sparse LLMs without the expensive backpropagation and any
weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the
reconstruction error between the dense and sparse LLMs, in the fashion of
performing iterative weight pruning-and-growing on top of sparse LLMs. To
accomplish this purpose, DSnoT particularly takes into account the anticipated
reduction in reconstruction error for pruning and growing, as well as the
variance w.r.t. different input data for growing each weight. This practice can
be executed efficiently in linear time since its obviates the need of
backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,
Vicuna, and OPT across various benchmarks demonstrate the effectiveness of
DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity
levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by
26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights
into how to fine-tune sparse LLMs in an efficient training-free manner and open
new venues to scale the great potential of sparsity to LLMs. Codes are
available at https://github.com/zyxxmu/DSnoT.
","2023-10-18","2310.08915v1.pdf"
"2310.08920","Ryoma Sato","Ryoma Sato, Yuki Takezawa, Han Bao, Kenta Niwa, Makoto Yamada","Embarrassingly Simple Text Watermarks","","","","","cs.LG cs.AI cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose Easymark, a family of embarrassingly simple yet effective
watermarks. Text watermarking is becoming increasingly important with the
advent of Large Language Models (LLM). LLMs can generate texts that cannot be
distinguished from human-written texts. This is a serious problem for the
credibility of the text. Easymark is a simple yet effective solution to this
problem. Easymark can inject a watermark without changing the meaning of the
text at all while a validator can detect if a text was generated from a system
that adopted Easymark or not with high credibility. Easymark is extremely easy
to implement so that it only requires a few lines of code. Easymark does not
require access to LLMs, so it can be implemented on the user-side when the LLM
providers do not offer watermarked LLMs. In spite of its simplicity, it
achieves higher detection accuracy and BLEU scores than the state-of-the-art
text watermarking methods. We also prove the impossibility theorem of perfect
watermarking, which is valuable in its own right. This theorem shows that no
matter how sophisticated a watermark is, a malicious user could remove it from
the text, which motivate us to use a simple watermark such as Easymark. We
carry out experiments with LLM-generated texts and confirm that Easymark can be
detected reliably without any degradation of BLEU and perplexity, and
outperform state-of-the-art watermarks in terms of both quality and
reliability.
","2023-10-16","2310.08920v1.pdf"
"2310.08922","Zongqing Lu","Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu","LLaMA Rider: Spurring Large Language Models to Explore the Open World","18 pages","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round feedback-revision mechanism is
utilized to encourage LLMs to actively select appropriate revision actions
guided by feedback information from the environment. This facilitates
exploration and enhances the model's performance. Besides, we integrate
sub-task relabeling to assist LLMs in maintaining consistency in sub-task
planning and help the model learn the combinatorial nature between tasks,
enabling it to complete a wider range of tasks through training based on the
acquired exploration experiences. By evaluation in Minecraft, an open-ended
sandbox world, we demonstrate that our approach LLaMA-Rider enhances the
efficiency of the LLM in exploring the environment, and effectively improves
the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k
instances of collected data, showing minimal training costs compared to the
baseline using reinforcement learning.
","2023-10-16","2310.08922v1.pdf"
"2310.08923","Hongfu Liu","Hongfu Liu, Ye Wang","Towards Informative Few-Shot Prompt with Maximum Information Gain for
  In-Context Learning","Accepted to the Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language models (LLMs) possess the capability to engage In-context
Learning (ICL) by leveraging a few demonstrations pertaining to a new
downstream task as conditions. However, this particular learning paradigm
suffers from high instability stemming from substantial variances induced by
factors such as the input distribution of selected examples, their ordering,
and prompt formats. In this work, we demonstrate that even when all these
factors are held constant, the random selection of examples still results in
high variance. Consequently, we aim to explore the informative ability of data
examples by quantifying the Information Gain (IG) obtained in prediction after
observing a given example candidate. Then we propose to sample those with
maximum IG. Additionally, we identify the presence of template bias, which can
lead to unfair evaluations of IG during the sampling process. To mitigate this
bias, we introduce Calibration Before Sampling strategy. The experimental
results illustrate that our proposed method can yield an average relative
improvement of 14.3% across six classification tasks using three LLMs.
","2023-10-16","2310.08923v1.pdf"
"2310.08949","Xiangyu Zhao","Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi, Xiao-Ming Wu","Making Multimodal Generation Easier: When Diffusion Models Meet LLMs","","","","","cs.AI cs.CL cs.CV","http://creativecommons.org/licenses/by/4.0/","  We present EasyGen, an efficient model designed to enhance multimodal
understanding and generation by harnessing the capabilities of diffusion models
and large language models (LLMs). Unlike existing multimodal models that
predominately depend on encoders like CLIP or ImageBind and need ample amounts
of training data to bridge the gap between modalities, EasyGen is built upon a
bidirectional conditional diffusion model named BiDiffuser, which promotes more
efficient interactions between modalities. EasyGen handles image-to-text
generation by integrating BiDiffuser and an LLM via a simple projection layer.
Unlike most existing multimodal models that are limited to generating text
responses, EasyGen can also facilitate text-to-image generation by leveraging
the LLM to create textual descriptions, which can be interpreted by BiDiffuser
to generate appropriate visual responses. Extensive quantitative and
qualitative experiments demonstrate the effectiveness of EasyGen, whose
training can be easily achieved in a lab setting. The source code is available
at https://github.com/zxy556677/EasyGen.
","2023-10-16","2310.08949v1.pdf"
"2310.08958","Chen Zhang","Chen Zhang, Luis Fernando D'Haro, Chengguang Tang, Ke Shi, Guohua
  Tang, Haizhou Li","xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark","Accepted to EMNLP-2023 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in reference-free learned metrics for open-domain
dialogue evaluation have been driven by the progress in pre-trained language
models and the availability of dialogue data with high-quality human
annotations. However, current studies predominantly concentrate on English
dialogues, and the generalization of these metrics to other languages has not
been fully examined. This is largely due to the absence of a multilingual
dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval,
built on top of open-source English dialogue evaluation datasets. xDial-Eval
includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930
annotated turns and 8691 annotated dialogues respectively. The English dialogue
data are extended to nine other languages with commercial machine translation
systems. On xDial-Eval, we conduct comprehensive analyses of previous
BERT-based metrics and the recently-emerged large language models. Lastly, we
establish strong self-supervised and multilingual baselines. In terms of
average Pearson correlations over all datasets and languages, the best baseline
outperforms OpenAI's ChatGPT by absolute improvements of 6.5% and 4.6% at the
turn and dialogue levels respectively, albeit with much fewer parameters. The
data and code are publicly available at https://github.com/e0397123/xDial-Eval.
","2023-10-16","2310.08958v1.pdf"
"2310.08975","Haoran Luo","Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai
  Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin","ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question
  Answering with Fine-tuned Large Language Models","Preprint","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Knowledge Base Question Answering (KBQA) aims to derive answers to natural
language questions over large-scale knowledge bases (KBs), which are generally
divided into two research components: knowledge retrieval and semantic parsing.
However, three core challenges remain, including inefficient knowledge
retrieval, retrieval errors adversely affecting semantic parsing, and the
complexity of previous KBQA methods. In the era of large language models
(LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework
built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2.
ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then
retrieving and replacing entities and relations through an unsupervised
retrieval method, which improves both generation and retrieval more
straightforwardly. Experimental results reveal that ChatKBQA achieves new
state-of-the-art performance on standard KBQA datasets, WebQSP, and
ComplexWebQuestions (CWQ). This work also provides a new paradigm for combining
LLMs with knowledge graphs (KGs) for interpretable and knowledge-required
question answering. Our code is publicly available.
","2023-10-16","2310.08975v1.pdf"
"2310.08992","Hung Le","Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, Shafiq
  Joty","CodeChain: Towards Modular Code Generation Through Chain of
  Self-revisions with Representative Sub-modules","","","","","cs.AI cs.CL cs.PL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have already become quite proficient at solving
simpler programming tasks like those in HumanEval or MBPP benchmarks. However,
solving more complex and competitive programming tasks is still quite
challenging for these models - possibly due to their tendency to generate
solutions as monolithic code blocks instead of decomposing them into logical
sub-tasks and sub-modules. On the other hand, experienced programmers
instinctively write modularized code with abstraction for solving complex
tasks, often reusing previously developed modules. To address this gap, we
propose CodeChain, a novel framework for inference that elicits modularized
code generation through a chain of self-revisions, each being guided by some
representative sub-modules generated in previous iterations. Concretely,
CodeChain first instructs the LLM to generate modularized codes through
chain-of-thought prompting. Then it applies a chain of self-revisions by
iterating the two steps: 1) extracting and clustering the generated sub-modules
and selecting the cluster representatives as the more generic and re-usable
implementations, and 2) augmenting the original chain-of-thought prompt with
these selected module-implementations and instructing the LLM to re-generate
new modularized solutions. We find that by naturally encouraging the LLM to
reuse the previously developed and verified sub-modules, CodeChain can
significantly boost both modularity as well as correctness of the generated
solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on
CodeContests. It is shown to be effective on both OpenAI LLMs as well as
open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation
studies with different methods of prompting, number of clusters, model sizes,
program qualities, etc., to provide useful insights that underpin CodeChain's
success.
","2023-10-16","2310.08992v1.pdf"
"2310.09017","Aviv Slobodkin","Aviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan","Dont Add, dont Miss: Effective Content Preserving Generation from
  Pre-Selected Text Spans","EMNLP 2023, findings","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The recently introduced Controlled Text Reduction (CTR) task isolates the
text generation step within typical summarization-style tasks. It does so by
challenging models to generate coherent text conforming to pre-selected content
within the input text (""highlights"").
  This framing enables increased modularity in summarization-like tasks,
allowing to couple a single CTR model with various content-selection setups and
modules.
  However, there are currently no reliable CTR models, while the performance of
the existing baseline for the task is mediocre, falling short of practical
utility.
  Here, we address this gap by introducing a high-quality, open-source CTR
model that tackles two prior key limitations: inadequate enforcement of the
content-preservation constraint, and suboptimal silver training data.
  Addressing these, we amplify the content-preservation constraint in both
training, via RL, and inference, via a controlled decoding strategy.
  Further, we substantially improve the silver training data quality via GPT-4
distillation.
  Overall, pairing the distilled dataset with the highlight-adherence
strategies yields marked gains over the current baseline, of up to 30 ROUGE-L
points, providing a reliable CTR model for downstream use.
","2023-10-16","2310.09017v1.pdf"
"2310.09044","Sehyun Choi","Sehyun Choi, Tianqing Fang, Zhaowei Wang, Yangqiu Song","KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level
  Hallucination Detection","Accepted at EMNLP 2023 Main Conference","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated remarkable human-level natural
language generation capabilities. However, their potential to generate
misinformation, often called the hallucination problem, poses a significant
risk to their deployment. A common approach to address this issue is to
retrieve relevant knowledge and fine-tune the LLM with the knowledge in its
input. Unfortunately, this method incurs high training costs and may cause
catastrophic forgetting for multi-tasking models. To overcome these
limitations, we propose a knowledge-constrained decoding method called KCTS
(Knowledge-Constrained Tree Search), which guides a frozen LM to generate text
aligned with the reference knowledge at each decoding step using a knowledge
classifier score and MCTS (Monte-Carlo Tree Search). To adapt the
sequence-level knowledge classifier to token-level guidance, we also propose a
novel token-level hallucination detection method called RIPA (Reward Inflection
Point Approximation). Our empirical results on knowledge-grounded dialogue and
abstractive summarization demonstrate the strength of KCTS as a plug-and-play,
model-agnostic decoding method that can effectively reduce hallucinations in
natural language generation.
","2023-10-16","2310.09044v1.pdf"
"2310.09049","Lei Yao","Lei Yao, Yong Zhang, Zilong Yan and Jialu Tian","SAI: Solving AI Tasks with Systematic Artificial Intelligence in
  Communication Network","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the rapid development of artificial intelligence, solving complex AI tasks
is a crucial technology in intelligent mobile networks. Despite the good
performance of specialized AI models in intelligent mobile networks, they are
unable to handle complicated AI tasks. To address this challenge, we propose
Systematic Artificial Intelligence (SAI), which is a framework designed to
solve AI tasks by leveraging Large Language Models (LLMs) and JSON-format
intent-based input to connect self-designed model library and database.
Specifically, we first design a multi-input component, which simultaneously
integrates Large Language Models (LLMs) and JSON-format intent-based inputs to
fulfill the diverse intent requirements of different users. In addition, we
introduce a model library module based on model cards which employ model cards
to pairwise match between different modules for model composition. Model cards
contain the corresponding model's name and the required performance metrics.
Then when receiving user network requirements, we execute each subtask for
multiple selected model combinations and provide output based on the execution
results and LLM feedback. By leveraging the language capabilities of LLMs and
the abundant AI models in the model library, SAI can complete numerous complex
AI tasks in the communication network, achieving impressive results in network
optimization, resource allocation, and other challenging tasks.
","2023-10-16","2310.09049v1.pdf"
"2310.09089","Junling Liu","Qichen Ye, Junling Liu, Dading Chong, Peilin Zhou, Yining Hua, Andrew
  Liu","Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large
  Language Model","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Integrating large language models (LLMs) into healthcare presents potential
but faces challenges. Directly pre-training LLMs for domains like medicine is
resource-heavy and sometimes unfeasible. Sole reliance on Supervised
Fine-tuning (SFT) can result in overconfident predictions and may not tap into
domain specific insights. Addressing these challenges, we present a multi-stage
training method combining Domain-specific Continued Pre-training (DCPT), SFT,
and Direct Preference Optimization (DPO). A notable contribution of our study
is the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing
medical question answering, plain texts, knowledge graphs, and dialogues,
segmented into three training stages. The medical LLM trained with our
pipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and
SFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing
Baichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores
16.66 in BLEU-1 and 27.44 in ROUGE1, outperforming the SFT's 12.69 and 24.21.
This highlights the strength of our training approach in refining LLMs for
medical applications.
","2023-10-16","2310.09089v1.pdf"
"2310.09130","Peihua Mai","Peihua Mai, Ran Yan, Zhe Huang, Youjia Yang, Yan Pang","Split-and-Denoise: Protect large language model inference with local
  differential privacy","15 pages","","","","cs.AI cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) shows powerful capability in natural language
understanding by capturing hidden semantics in vector space. This process
enriches the value of the text embeddings for various downstream tasks, thereby
fostering the Embedding-as-a-Service (EaaS) business model. However, the direct
transmission of text to servers poses a largely unaddressed risk of privacy
leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an
innovative framework that split the model to execute the token embedding layer
on the client side at minimal computational cost. This allows the client to
introduce noise prior to transmitting the embeddings to the server, and
subsequently receive and denoise the perturbed output embeddings for downstream
tasks. Our approach is designed for the inference stage of LLMs and requires no
modifications to the model parameters. Extensive experiments demonstrate SnD's
effectiveness in optimizing the privacy-utility tradeoff across various LLM
architectures and diverse downstream tasks. The results reveal a significant
performance improvement under the same privacy budget compared to the baseline,
offering clients a privacy-preserving solution for local privacy protection.
","2023-10-16","2310.09130v1.pdf"
"2310.09135","Fendi Zhang","Junwen Zhang and Yin Zhang","HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework
  for Cross-Domain Zero-Shot Slot Filling","Accepted by Findings of EMNLP2023 as a long paper","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In task-oriented dialogue scenarios, cross-domain zero-shot slot filling
plays a vital role in leveraging source domain knowledge to learn a model with
high generalization ability in unknown target domain where annotated data is
unavailable. However, the existing state-of-the-art zero-shot slot filling
methods have limited generalization ability in target domain, they only show
effective knowledge transfer on seen slots and perform poorly on unseen slots.
To alleviate this issue, we present a novel Hierarchical Contrastive Learning
Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarse-
to fine-grained contrastive learning based on Gaussian-distributed embedding to
learn the generalized deep semantic relations between utterance-tokens, by
optimizing inter- and intra-token distribution distance. This encourages HiCL
to generalize to the slot types unseen at training phase. Furthermore, we
present a new iterative label set semantics inference method to unbiasedly and
separately evaluate the performance of unseen slot types which entangled with
their counterparts (i.e., seen slot types) in the previous zero-shot slot
filling evaluation methods. The extensive empirical experiments on four
datasets demonstrate that the proposed method achieves comparable or even
better performance than the current state-of-the-art zero-shot slot filling
approaches.
","2023-10-23","2310.09135v1.pdf"
"2310.09139","Athul Paul Jacob","Athul Paul Jacob, Yikang Shen, Gabriele Farina and Jacob Andreas","The Consensus Game: Language Model Generation via Equilibrium Search","","","","","cs.GT cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  When applied to question answering and other text generation tasks, language
models (LMs) may be queried generatively (by sampling answers from their output
distribution) or discriminatively (by using them to score or rank a set of
candidate outputs). These procedures sometimes yield very different
predictions. How do we reconcile mutually incompatible scoring procedures to
obtain coherent LM predictions? We introduce a new, a training-free,
game-theoretic procedure for language model decoding. Our approach casts
language model decoding as a regularized imperfect-information sequential
signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks
to communicate an abstract correctness parameter using natural language
sentences to a DISCRIMINATOR. We develop computational procedures for finding
approximate equilibria of this game, resulting in a decoding algorithm we call
EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading
comprehension, commonsense reasoning, mathematical problem-solving, and
dialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially,
improves performance over existing LM decoding procedures - on multiple
benchmarks, we observe that applying EQUILIBRIUM-RANKING to LLaMA-7B
outperforms the much larger LLaMA-65B and PaLM-540B models. These results
highlight the promise of game-theoretic tools for addressing fundamental
challenges of truthfulness and consistency in LMs.
","2023-10-16","2310.09139v1.pdf"
"2310.09141","Vukosi Marivate","Vukosi Marivate, Moseli Mots'Oehli, Valencia Wagner, Richard Lastrucci
  and Isheanesu Dzingirai","PuoBERTa: Training and evaluation of a curated language model for
  Setswana","Accepted for SACAIR 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Natural language processing (NLP) has made significant progress for
well-resourced languages such as English but lagged behind for low-resource
languages like Setswana. This paper addresses this gap by presenting PuoBERTa,
a customised masked language model trained specifically for Setswana. We cover
how we collected, curated, and prepared diverse monolingual texts to generate a
high-quality corpus for PuoBERTa's training. Building upon previous efforts in
creating monolingual resources for Setswana, we evaluated PuoBERTa across
several NLP tasks, including part-of-speech (POS) tagging, named entity
recognition (NER), and news categorisation. Additionally, we introduced a new
Setswana news categorisation dataset and provided the initial benchmarks using
PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP
capabilities for understudied languages like Setswana and paves the way for
future research directions.
","2023-10-25","2310.09141v1.pdf"
"2310.09144","Jacek Karwowski","Jacek Karwowski, Oliver Hayman, Xingjian Bai, Klaus Kiendlhofer,
  Charlie Griffin, Joar Skalse","Goodhart's Law in Reinforcement Learning","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Implementing a reward function that perfectly captures a complex task in the
real world is impractical. As a result, it is often appropriate to think of the
reward function as a proxy for the true objective rather than as its
definition. We study this phenomenon through the lens of Goodhart's law, which
predicts that increasing optimisation of an imperfect proxy beyond some
critical point decreases performance on the true objective. First, we propose a
way to quantify the magnitude of this effect and show empirically that
optimising an imperfect proxy reward often leads to the behaviour predicted by
Goodhart's law for a wide range of environments and reward functions. We then
provide a geometric explanation for why Goodhart's law occurs in Markov
decision processes. We use these theoretical insights to propose an optimal
early stopping method that provably avoids the aforementioned pitfall and
derive theoretical regret bounds for this method. Moreover, we derive a
training method that maximises worst-case reward, for the setting where there
is uncertainty about the true reward function. Finally, we evaluate our early
stopping method experimentally. Our results support a foundation for a
theoretically-principled study of reinforcement learning under reward
misspecification.
","2023-10-16","2310.09144v1.pdf"
"2310.09168","Fanqi Wan","Fanqi Wan, Xinting Huang, Tao Yang, Xiaojun Quan, Wei Bi, Shuming Shi","Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through
  Active Exploration","Accepted to EMNLP 2023 (Main Conference)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instruction-tuning can be substantially optimized through enhanced diversity,
resulting in models capable of handling a broader spectrum of tasks. However,
existing data employed for such tuning often exhibit an inadequate coverage of
individual domains, limiting the scope for nuanced comprehension and
interactions within these areas. To address this deficiency, we propose
Explore-Instruct, a novel approach to enhance the data coverage to be used in
domain-specific instruction-tuning through active exploration via Large
Language Models (LLMs). Built upon representative domain use cases,
Explore-Instruct explores a multitude of variations or possibilities by
implementing a search algorithm to obtain diversified and domain-focused
instruction-tuning data. Our data-centric analysis validates the effectiveness
of this proposed approach in improving domain-specific instruction coverage.
Moreover, our model's performance demonstrates considerable advancements over
multiple baselines, including those utilizing domain-specific data enhancement.
Our findings offer a promising opportunity to improve instruction coverage,
especially in domain-specific contexts, thereby advancing the development of
adaptable language models. Our code, model weights, and data are public at
\url{https://github.com/fanqiwan/Explore-Instruct}.
","2023-10-25","2310.09168v1.pdf"
"2310.09199","Xiaohua Zhai","Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul
  Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr
  Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong,
  Tianli Yu, Daniel Keysers, Xiaohua Zhai, Radu Soricut","PaLI-3 Vision Language Models: Smaller, Faster, Stronger","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  This paper presents PaLI-3, a smaller, faster, and stronger vision language
model (VLM) that compares favorably to similar models that are 10x larger. As
part of arriving at this strong performance, we compare Vision Transformer
(ViT) models pretrained using classification objectives to contrastively
(SigLIP) pretrained ones. We find that, while slightly underperforming on
standard image classification benchmarks, SigLIP-based PaLI shows superior
performance across various multimodal benchmarks, especially on localization
and visually-situated text understanding. We scale the SigLIP image encoder up
to 2 billion parameters, and achieves a new state-of-the-art on multilingual
cross-modal retrieval. We hope that PaLI-3, at only 5B parameters, rekindles
research on fundamental pieces of complex VLMs, and could fuel a new generation
of scaled-up models.
","2023-10-19","2310.09199v1.pdf"
"2310.09219","Yixin Wan","Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang,
  Nanyun Peng","""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in
  LLM-Generated Reference Letters","","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  As generative language models advance, users have started to utilize Large
Language Models (LLMs) to assist in writing various types of content, including
professional documents such as recommendation letters. Despite their
convenience, these applications introduce unprecedented fairness concerns. As
generated reference letters might be directly utilized by users in professional
or academic scenarios, they have the potential to cause direct social harms,
such as lowering success rates for female applicants. Therefore, it is imminent
and necessary to comprehensively study fairness issues and associated harms in
such real-world use cases for future mitigation and monitoring. In this paper,
we critically examine gender bias in LLM-generated reference letters. Inspired
by findings in social science, we design evaluation methods to manifest gender
biases in LLM-generated letters through 2 dimensions: biases in language style
and biases in lexical content. Furthermore, we investigate the extent of bias
propagation by separately analyze bias amplification in model-hallucinated
contents, which we define to be the hallucination bias of model-generated
documents. Through benchmarking evaluation on 4 popular LLMs, including
ChatGPT, Alpaca, Vicuna and StableLM, our study reveals significant gender
biases in LLM-generated recommendation letters. Our findings further point
towards the importance and imminence to recognize biases in LLM-generated
professional documents.
","2023-10-16","2310.09219v1.pdf"
"2310.09223","Emilio Ferrara","Eun Cheol Choi and Emilio Ferrara","Automated Claim Matching with Large Language Models: Empowering
  Fact-Checkers in the Fight Against Misinformation","","","","","cs.CL cs.CY cs.HC","http://creativecommons.org/licenses/by/4.0/","  In today's digital era, the rapid spread of misinformation poses threats to
public well-being and societal trust. As online misinformation proliferates,
manual verification by fact checkers becomes increasingly challenging. We
introduce FACT-GPT (Fact-checking Augmentation with Claim matching
Task-oriented Generative Pre-trained Transformer), a framework designed to
automate the claim matching phase of fact-checking using Large Language Models
(LLMs). This framework identifies new social media content that either supports
or contradicts claims previously debunked by fact-checkers. Our approach
employs GPT-4 to generate a labeled dataset consisting of simulated social
media posts. This data set serves as a training ground for fine-tuning more
specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media
content related to public health. The results indicate that our fine-tuned LLMs
rival the performance of larger pre-trained LLMs in claim matching tasks,
aligning closely with human annotations. This study achieves three key
milestones: it provides an automated framework for enhanced fact-checking;
demonstrates the potential of LLMs to complement human expertise; offers public
resources, including datasets and models, to further research and applications
in the fact-checking domain.
","2023-10-16","2310.09223v1.pdf"
"2310.09237","Cecilia Delgado Solorzano","Cecilia Delgado Solorzano, Carlos Toxtli Hernandez","Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT's
  Perceptions of Indigenous Roles in Diverse Scenarios","5 pages, 3 figures","","10.13140/RG.2.2.30617.39520","","cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs), like ChatGPT, are fundamentally tools trained
on vast data, reflecting diverse societal impressions. This paper aims to
investigate LLMs' self-perceived bias concerning indigeneity when simulating
scenarios of indigenous people performing various roles. Through generating and
analyzing multiple scenarios, this work offers a unique perspective on how
technology perceives and potentially amplifies societal biases related to
indigeneity in social computing. The findings offer insights into the broader
implications of indigeneity in critical computing.
","2023-10-16","2310.09237v1.pdf"
"2310.09238","Saumajit Saha","Saumajit Saha and Albert Nanda","BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models
  for Sentiment Analysis of Bangla Social Media Posts","7 pages, 2 figures, workshop","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Bangla is the 7th most widely spoken language globally, with a staggering 234
million native speakers primarily hailing from India and Bangladesh. This
morphologically rich language boasts a rich literary tradition, encompassing
diverse dialects and language-specific challenges. Despite its linguistic
richness and history, Bangla remains categorized as a low-resource language
within the natural language processing (NLP) and speech community. This paper
presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media
Posts) of the BLP Workshop. We experiment with various Transformer-based
architectures to solve this task. Our quantitative results show that transfer
learning really helps in better learning of the models in this low-resource
language scenario. This becomes evident when we further finetune a model which
has already been finetuned on twitter data for sentiment analysis task and that
finetuned model performs the best among all other models. We also perform a
detailed error analysis where we find some instances where ground truth labels
need to be relooked at. We obtain a micro-F1 of 67.02\% on the test set and our
performance in this shared task is ranked at 21 in the leaderboard.
","2023-10-19","2310.09238v1.pdf"
"2310.09241","Yiquan Wu","Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating
  Zhang, Changlong Sun, Fei Wu, Kun Kuang","Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model
  Collaboration","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Legal Judgment Prediction (LJP) has become an increasingly crucial task in
Legal AI, i.e., predicting the judgment of the case in terms of case fact
description. Precedents are the previous legal cases with similar facts, which
are the basis for the judgment of the subsequent case in national legal
systems. Thus, it is worthwhile to explore the utilization of precedents in the
LJP. Recent advances in deep learning have enabled a variety of techniques to
be used to solve the LJP task. These can be broken down into two categories:
large language models (LLMs) and domain-specific models. LLMs are capable of
interpreting and generating complex natural language, while domain models are
efficient in learning task-specific information. In this paper, we propose the
precedent-enhanced LJP framework (PLJP), a system that leverages the strength
of both LLM and domain models in the context of precedents. Specifically, the
domain models are designed to provide candidate labels and find the proper
precedents efficiently, and the large models will make the final prediction
with an in-context precedents comprehension. Experiments on the real-world
dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a
promising direction for LLM and domain-model collaboration that can be
generalized to other vertical domains.
","2023-10-16","2310.09241v1.pdf"
"2310.09259","Saleh Ashkboos","Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng
  Wang, Jie Ren, Torsten Hoefler, Dan Alistarh","Towards End-to-end 4-Bit Inference on Generative Large Language Models","9 pages","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that the majority of the inference computations for large generative
models such as LLaMA and OPT can be performed with both weights and activations
being cast to 4 bits, in a way that leads to practical speedups while at the
same time maintaining good accuracy. We achieve this via a hybrid quantization
strategy called QUIK, which compresses most of the weights and activations to
4-bit, while keeping some outlier weights and activations in higher-precision.
Crucially, our scheme is designed with computational efficiency in mind: we
provide GPU kernels with highly-efficient layer-wise runtimes, which lead to
practical end-to-end throughput improvements of up to 3.1x relative to FP16
execution. Code and models are provided at https://github.com/IST-DASLab/QUIK.
","2023-10-16","2310.09259v1.pdf"
"2310.09265","Chufan Gao","Chufan Gao, Xulin Fan, Jimeng Sun, Xuan Wang","PromptRE: Weakly-Supervised Document-Level Relation Extraction via
  Prompting-Based Data Programming","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Relation extraction aims to classify the relationships between two entities
into pre-defined categories. While previous research has mainly focused on
sentence-level relation extraction, recent studies have expanded the scope to
document-level relation extraction. Traditional relation extraction methods
heavily rely on human-annotated training data, which is time-consuming and
labor-intensive. To mitigate the need for manual annotation, recent
weakly-supervised approaches have been developed for sentence-level relation
extraction while limited work has been done on document-level relation
extraction. Weakly-supervised document-level relation extraction faces
significant challenges due to an imbalanced number ""no relation"" instances and
the failure of directly probing pretrained large language models for document
relation extraction. To address these challenges, we propose PromptRE, a novel
weakly-supervised document-level relation extraction method that combines
prompting-based techniques with data programming. Furthermore, PromptRE
incorporates the label distribution and entity types as prior knowledge to
improve the performance. By leveraging the strengths of both prompting and data
programming, PromptRE achieves improved performance in relation classification
and effectively handles the ""no relation"" problem. Experimental results on
ReDocRED, a benchmark dataset for document-level relation extraction,
demonstrate the superiority of PromptRE over baseline approaches.
","2023-10-16","2310.09265v1.pdf"
"2310.09266","Nikhil Kandpal","Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz,
  Christopher A. Choquette-Choo, Zheng Xu","User Inference Attacks on Large Language Models","","","","","cs.CR cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Fine-tuning is a common and effective method for tailoring large language
models (LLMs) to specialized tasks and applications. In this paper, we study
the privacy implications of fine-tuning LLMs on user data. To this end, we
define a realistic threat model, called user inference, wherein an attacker
infers whether or not a user's data was used for fine-tuning. We implement
attacks for this threat model that require only a small set of samples from a
user (possibly different from the samples used for training) and black-box
access to the fine-tuned LLM. We find that LLMs are susceptible to user
inference attacks across a variety of fine-tuning datasets, at times with near
perfect attack success rates. Further, we investigate which properties make
users vulnerable to user inference, finding that outlier users (i.e. those with
data distributions sufficiently different from other users) and users who
contribute large quantities of data are most susceptible to attack. Finally, we
explore several heuristics for mitigating privacy attacks. We find that
interventions in the training algorithm, such as batch or per-example gradient
clipping and early stopping fail to prevent user inference. However, limiting
the number of fine-tuning samples from a single user can reduce attack
effectiveness, albeit at the cost of reducing the total amount of fine-tuning
data.
","2023-10-16","2310.09266v1.pdf"
"2310.09291","Karsten Roth","Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata","Vision-by-Language for Training-Free Compositional Image Retrieval","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Given an image and a target modification (e.g an image of the Eiffel tower
and the text ""without people and at night-time""), Compositional Image Retrieval
(CIR) aims to retrieve the relevant target image in a database. While
supervised approaches rely on annotating triplets that is costly (i.e. query
image, textual modification, and target image), recent research sidesteps this
need by using large-scale vision-language models (VLMs), performing Zero-Shot
CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require
training task-specific, customized models over large amounts of image-text
pairs. In this work, we propose to tackle CIR in a training-free manner via our
Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple,
yet human-understandable and scalable pipeline that effectively recombines
large-scale VLMs with large language models (LLMs). By captioning the reference
image using a pre-trained generative VLM and asking a LLM to recompose the
caption based on the textual target modification for subsequent retrieval via
e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we
find competitive, in-part state-of-the-art performance - improving over
supervised methods. Moreover, the modularity of CIReVL offers simple
scalability without re-training, allowing us to both investigate scaling laws
and bottlenecks for ZS-CIR while easily scaling up to in parts more than double
of previously reported results. Finally, we show that CIReVL makes CIR
human-understandable by composing image and text in a modular fashion in the
language domain, thereby making it intervenable, allowing to post-hoc re-align
failure cases. Code will be released upon acceptance.
","2023-10-16","2310.09291v1.pdf"
"2310.09336","Maya Okawa","Maya Okawa, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka","Compositional Abilities Emerge Multiplicatively: Exploring Diffusion
  Models on a Synthetic Task","37th Conference on Neural Information Processing Systems (NeurIPS)","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modern generative models exhibit unprecedented capabilities to generate
extremely realistic data. However, given the inherent compositionality of the
real world, reliable use of these models in practical applications requires
that they exhibit the capability to compose a novel set of concepts to generate
outputs not seen in the training data set. Prior work demonstrates that recent
diffusion models do exhibit intriguing compositional generalization abilities,
but also fail unpredictably. Motivated by this, we perform a controlled study
for understanding compositional generalization in conditional diffusion models
in a synthetic setting, varying different attributes of the training data and
measuring the model's ability to generate samples out-of-distribution. Our
results show: (i) the order in which the ability to generate samples from a
concept and compose them emerges is governed by the structure of the underlying
data-generating process; (ii) performance on compositional tasks exhibits a
sudden ``emergence'' due to multiplicative reliance on the performance of
constituent tasks, partially explaining emergent phenomena seen in generative
models; and (iii) composing concepts with lower frequency in the training data
to generate out-of-distribution samples requires considerably more optimization
steps compared to generating in-distribution samples. Overall, our study lays a
foundation for understanding capabilities and compositionality in generative
models from a data-centric perspective.
","2023-10-17","2310.09336v1.pdf"
"2310.09342","Saikat Chakraborty","Saikat Chakraborty, Shuvendu K. Lahiri, Sarah Fakhoury, Madanlal
  Musuvathi, Akash Lal, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma,
  Nikhil Swamy","Ranking LLM-Generated Loop Invariants for Program Verification","Findings of The 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP-findings 2023)","","","","cs.PL cs.AI cs.CL cs.SE","http://creativecommons.org/licenses/by/4.0/","  Synthesizing inductive loop invariants is fundamental to automating program
verification. In this work, we observe that Large Language Models (such as
gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of
programs in a 0-shot setting, yet require several samples to generate the
correct invariants. This can lead to a large number of calls to a program
verifier to establish an invariant. To address this issue, we propose a {\it
re-ranking} approach for the generated results of LLMs. We have designed a
ranker that can distinguish between correct inductive invariants and incorrect
attempts based on the problem definition. The ranker is optimized as a
contrastive ranker. Experimental results demonstrate that this re-ranking
mechanism significantly improves the ranking of correct invariants among the
generated candidates, leading to a notable reduction in the number of calls to
a verifier.
","2023-10-20","2310.09342v1.pdf"
"2310.09343","Hyungjoo Chae","Hyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin
  Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung Yeo","Dialogue Chain-of-Thought Distillation for Commonsense-aware
  Conversational Agents","25 pages, 8 figures, Accepted to EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Human-like chatbots necessitate the use of commonsense reasoning in order to
effectively comprehend and respond to implicit information present within
conversations. Achieving such coherence and informativeness in responses,
however, is a non-trivial task. Even for large language models (LLMs), the task
of identifying and aggregating key evidence within a single hop presents a
substantial challenge. This complexity arises because such evidence is
scattered across multiple turns in a conversation, thus necessitating
integration over multiple hops. Hence, our focus is to facilitate such
multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought
(CoT) reasoning. To this end, we propose a knowledge distillation framework
that leverages LLMs as unreliable teachers and selectively distills consistent
and helpful rationales via alignment filters. We further present DOCTOR, a
DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for
response generation. We conduct extensive experiments to show that enhancing
dialogue agents with high-quality rationales from DOCTOR significantly improves
the quality of their responses.
","2023-10-24","2310.09343v1.pdf"
"2310.09350","Carlos Dominguez Becerril","Carlos Dominguez, Jon Ander Campos, Eneko Agirre, Gorka Azkune","Unsupervised Domain Adaption for Neural Information Retrieval","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Neural information retrieval requires costly annotated data for each target
domain to be competitive. Synthetic annotation by query generation using Large
Language Models or rule-based string manipulation has been proposed as an
alternative, but their relative merits have not been analysed. In this paper,
we compare both methods head-to-head using the same neural IR architecture. We
focus on the BEIR benchmark, which includes test datasets from several domains
with no training data, and explore two scenarios: zero-shot, where the
supervised system is trained in a large out-of-domain dataset (MS-MARCO); and
unsupervised domain adaptation, where, in addition to MS-MARCO, the system is
fine-tuned in synthetic data from the target domain. Our results indicate that
Large Language Models outperform rule-based methods in all scenarios by a large
margin, and, more importantly, that unsupervised domain adaptation is effective
compared to applying a supervised IR system in a zero-shot fashion. In addition
we explore several sizes of open Large Language Models to generate synthetic
data and find that a medium-sized model suffices. Code and models are publicly
available for reproducibility.
","2023-10-17","2310.09350v1.pdf"
"2310.09361","Zifan Wang","Ravi Mangal, Klas Leino, Zifan Wang, Kai Hu, Weicheng Yu, Corina
  Pasareanu, Anupam Datta, Matt Fredrikson","Is Certifying $\ell_p$ Robustness Still Worthwhile?","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Over the years, researchers have developed myriad attacks that exploit the
ubiquity of adversarial examples, as well as defenses that aim to guard against
the security vulnerabilities posed by such attacks. Of particular interest to
this paper are defenses that provide provable guarantees against the class of
$\ell_p$-bounded attacks. Certified defenses have made significant progress,
taking robustness certification from toy models and datasets to large-scale
problems like ImageNet classification. While this is undoubtedly an interesting
academic problem, as the field has matured, its impact in practice remains
unclear, thus we find it useful to revisit the motivation for continuing this
line of research. There are three layers to this inquiry, which we address in
this paper: (1) why do we care about robustness research? (2) why do we care
about the $\ell_p$-bounded threat model? And (3) why do we care about
certification as opposed to empirical defenses? In brief, we take the position
that local robustness certification indeed confers practical value to the field
of machine learning. We focus especially on the latter two questions from
above. With respect to the first of the two, we argue that the $\ell_p$-bounded
threat model acts as a minimal requirement for safe application of models in
security-critical domains, while at the same time, evidence has mounted
suggesting that local robustness may lead to downstream external benefits not
immediately related to robustness. As for the second, we argue that (i)
certification provides a resolution to the cat-and-mouse game of adversarial
attacks; and furthermore, that (ii) perhaps contrary to popular belief, there
may not exist a fundamental trade-off between accuracy, robustness, and
certifiability, while moreover, certified training techniques constitute a
particularly promising way for learning robust models.
","2023-10-17","2310.09361v1.pdf"
"2310.09411","Guanghua Wang","Guanghua Wang, Weili Wu","Surveying the Landscape of Text Summarization with Deep Learning: A
  Comprehensive Review","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, deep learning has revolutionized natural language processing
(NLP) by enabling the development of models that can learn complex
representations of language data, leading to significant improvements in
performance across a wide range of NLP tasks. Deep learning models for NLP
typically use large amounts of data to train deep neural networks, allowing
them to learn the patterns and relationships in language data. This is in
contrast to traditional NLP approaches, which rely on hand-engineered features
and rules to perform NLP tasks. The ability of deep neural networks to learn
hierarchical representations of language data, handle variable-length input
sequences, and perform well on large datasets makes them well-suited for NLP
applications. Driven by the exponential growth of textual data and the
increasing demand for condensed, coherent, and informative summaries, text
summarization has been a critical research area in the field of NLP. Applying
deep learning to text summarization refers to the use of deep neural networks
to perform text summarization tasks. In this survey, we begin with a review of
fashionable text summarization tasks in recent years, including extractive,
abstractive, multi-document, and so on. Next, we discuss most deep
learning-based models and their experimental results on these tasks. The paper
also covers datasets and data representation for summarization tasks. Finally,
we delve into the opportunities and challenges associated with summarization
tasks and their corresponding methodologies, aiming to inspire future research
efforts to advance the field further. A goal of our survey is to explain how
these methods differ in their requirements as understanding them is essential
for choosing a technique suited for a specific setting.
","2023-10-17","2310.09411v1.pdf"
"2310.09424","Zhehuai Chen","Zhehuai Chen, He Huang, Andrei Andrusenko, Oleksii Hrinchuk, Krishna
  C. Puvvada, Jason Li, Subhankar Ghosh, Jagadeesh Balam, Boris Ginsburg","SALM: Speech-augmented Language Model with In-context Learning for
  Speech Recognition and Translation","submit to ICASSP 2024","","","","cs.CL cs.HC cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
","2023-10-17","2310.09424v1.pdf"
"2310.09430","Qiming Bao","Qiming Bao, Gael Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan,
  Yang Chen, Michael Witbrock, Jiamou Liu","A Systematic Evaluation of Large Language Models on Out-of-Distribution
  Logical Reasoning Tasks","Accepted for oral presentation at the LLM@IJCAI 2023 non-archival
  symposium","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly
advanced the performance of artificial systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness to perform logical reasoning remain under-evaluated. To probe this
ability, we propose three new logical reasoning datasets named ""ReClor-plus"",
""LogiQA-plus"" and ""LogiQAv2-plus"", each featuring three subsets: the first with
randomly shuffled options, the second with the correct choices replaced by
""none of the other options are correct"", and a combination of the previous two
subsets. We carry out experiments on these datasets with both discriminative
and generative LLMs and show that these simple tricks greatly hinder the
performance of the language models. Despite their superior performance on the
original publicly available datasets, we find that all models struggle to
answer our newly constructed datasets. We show that introducing task variations
by perturbing a sizable training set can markedly improve the model's
generalisation and robustness in logical reasoning tasks. Moreover, applying
logic-driven data augmentation for fine-tuning, combined with prompting can
enhance the generalisation performance of both discriminative large language
models and generative large language models. These results offer insights into
assessing and improving the generalisation and robustness of large language
models for logical reasoning tasks. We make our source code and data publicly
available
\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.
","2023-10-20","2310.09430v1.pdf"
"2310.09435","Liming Xu Dr","Liming Xu and Stephen Mak and Maria Minaricova and Alexandra Brintrup","On Implementing Autonomous Supply Chains: a Multi-Agent System Approach","This paper includes 30 pages and 14 figures","","","","cs.MA","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Trade restrictions, the COVID-19 pandemic, and geopolitical conflicts has
significantly exposed vulnerabilities within traditional global supply chains.
These events underscore the need for organisations to establish more resilient
and flexible supply chains. To address these challenges, the concept of the
autonomous supply chain (ASC), characterised by predictive and
self-decision-making capabilities, has recently emerged as promising solution.
However, research on ASCs is relatively limited, with no existing studies on
their implementations. This paper aims to address this gap by presenting an
implementation of ASC using a multi-agent approach. It proposes a methodology
for the analysis and design of such an agent-based ASC system (A2SC). This
paper provides a concrete case study, the autonomous meat supply chain, which
showcases the practical implementation of the A2SC system using the proposed
methodology. Additionally, a system architecture and a toolkit for developing
A2SC systems are presented. Despite with limitations, this paper demonstrates a
promising approach for implementing an effective ASC system.
","2023-10-17","2310.09435v1.pdf"
"2310.09440","Jessica Clark","Jessica Clark","Target Variable Engineering","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  How does the formulation of a target variable affect performance within the
ML pipeline? The experiments in this study examine numeric targets that have
been binarized by comparing against a threshold. We compare the predictive
performance of regression models trained to predict the numeric targets vs.
classifiers trained to predict their binarized counterparts. Specifically, we
make this comparison at every point of a randomized hyperparameter optimization
search to understand the effect of computational resource budget on the
tradeoff between the two. We find that regression requires significantly more
computational effort to converge upon the optimal performance, and is more
sensitive to both randomness and heuristic choices in the training process.
Although classification can and does benefit from systematic hyperparameter
tuning and model selection, the improvements are much less than for regression.
This work comprises the first systematic comparison of regression and
classification within the framework of computational resource requirements. Our
findings contribute to calls for greater replicability and efficiency within
the ML pipeline for the sake of building more sustainable and robust AI
systems.
","2023-10-17","2310.09440v1.pdf"
"2310.09454","Yash Shukla","Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert
  Wright, Jivko Sinapov","LgTS: Dynamic Task Sampling using LLM-generated sub-goals for
  Reinforcement Learning Agents","","","","","cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in reasoning abilities of Large Language Models (LLM) has
promoted their usage in problems that require high-level planning for robots
and artificial agents. However, current techniques that utilize LLMs for such
planning tasks make certain key assumptions such as, access to datasets that
permit finetuning, meticulously engineered prompts that only provide relevant
and essential information to the LLM, and most importantly, a deterministic
approach to allow execution of the LLM responses either in the form of existing
policies or plan operators. In this work, we propose LgTS (LLM-guided
Teacher-Student learning), a novel approach that explores the planning
abilities of LLMs to provide a graphical representation of the sub-goals to a
reinforcement learning (RL) agent that does not have access to the transition
dynamics of the environment. The RL agent uses Teacher-Student learning
algorithm to learn a set of successful policies for reaching the goal state
from the start state while simultaneously minimizing the number of
environmental interactions. Unlike previous methods that utilize LLMs, our
approach does not assume access to a propreitary or a fine-tuned LLM, nor does
it require pre-trained policies that achieve the sub-goals proposed by the LLM.
Through experiments on a gridworld based DoorKey domain and a search-and-rescue
inspired domain, we show that generating a graphical structure of sub-goals
helps in learning policies for the LLM proposed sub-goals and the
Teacher-Student learning algorithm minimizes the number of environment
interactions when the transition dynamics are unknown.
","2023-10-17","2310.09454v1.pdf"
"2310.09478","Jun Chen","Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan
  Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed
  Elhoseiny","MiniGPT-v2: large language model as a unified interface for
  vision-language multi-task learning","fix small typos","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Large language models have shown their remarkable capabilities as a general
interface for various language-related applications. Motivated by this, we
target to build a unified interface for completing many vision-language tasks
including image description, visual question answering, and visual grounding,
among others. The challenge is to use a single model for performing diverse
vision-language tasks effectively with simple multi-modal instructions. Towards
this objective, we introduce MiniGPT-v2, a model that can be treated as a
unified interface for better handling various vision-language tasks. We propose
using unique identifiers for different tasks when training the model. These
identifiers enable our model to better distinguish each task instruction
effortlessly and also improve the model learning efficiency for each task.
After the three-stage training, the experimental results show that MiniGPT-v2
achieves strong performance on many visual question-answering and visual
grounding benchmarks compared to other vision-language generalist models. Our
model and codes are available at https://minigpt-v2.github.io/
","2023-10-27","2310.09478v1.pdf"
"2310.09499","Shao Hang","Hang Shao, Bei Liu, Yanmin Qian","One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language
  Models","Submitted to ICASSP2024","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Various Large Language Models(LLMs) from the Generative Pretrained
Transformer~(GPT) family have achieved outstanding performances in a wide range
of text generation tasks. However, the enormous model sizes have hindered their
practical use in real-world applications due to high inference latency.
Therefore, improving the efficiencies of LLMs through quantization, pruning,
and other means has been a key issue in LLM studies. In this work, we propose a
method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs
to at least 50\% sparsity without the need of any retraining. It allocates
sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced
error while maintaining the overall sparsity level. The advantages of the
proposed method exhibit even more when the sparsity is extremely high.
Furthermore, our method is compatible with quantization, enabling further
compression of LLMs.
","2023-10-17","2310.09499v1.pdf"
"2310.09506","Jihong Park","Jihong Park, Seung-Woo Ko, Jinho Choi, Seong-Lyun Kim, Mehdi Bennis","Towards Semantic Communication Protocols for 6G: From Protocol Learning
  to Language-Oriented Approaches","11 pages, 13 figures, submitted to IEEE BITS the Information Theory
  Magazine","","","","cs.IT cs.AI cs.LG cs.NI math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The forthcoming 6G systems are expected to address a wide range of
non-stationary tasks. This poses challenges to traditional medium access
control (MAC) protocols that are static and predefined. In response,
data-driven MAC protocols have recently emerged, offering ability to tailor
their signaling messages for specific tasks. This article presents a novel
categorization of these data-driven MAC protocols into three levels: Level 1
MAC. task-oriented neural protocols constructed using multi-agent deep
reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic
protocols developed by converting Level 1 MAC outputs into explicit symbols;
and Level 3 MAC. language-oriented semantic protocols harnessing large language
models (LLMs) and generative models. With this categorization, we aim to
explore the opportunities and challenges of each level by delving into their
foundational techniques. Drawing from information theory and associated
principles as well as selected case studies, this study provides insights into
the trajectory of data-driven MAC protocols and sheds light on future research
directions.
","2023-10-17","2310.09506v1.pdf"
"2310.09518","Hyunsoo Cho","Bruce W. Lee, Hyunsoo Cho, Kang Min Yoo","Instruction Tuning with Human Curriculum","Under review","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  The dominant paradigm for instruction tuning is the random-shuffled training
of maximally diverse instruction-response pairs. This paper explores the
potential benefits of applying a structured cognitive learning approach to
instruction tuning in contemporary large language models like ChatGPT and
GPT-4. Unlike the previous conventional randomized instruction dataset, we
propose a highly structured synthetic dataset that mimics the progressive and
organized nature of human education. We curate our dataset by aligning it with
educational frameworks, incorporating meta information including its topic and
cognitive rigor level for each sample. Our dataset covers comprehensive
fine-grained topics spanning diverse educational stages (from middle school to
graduate school) with various questions for each topic to enhance conceptual
depth using Bloom's taxonomy-a classification framework distinguishing various
levels of human cognition for each concept. The results demonstrate that this
cognitive rigorous training approach yields significant performance
enhancements - +3.06 on the MMLU benchmark and an additional +1.28 on AI2
Reasoning Challenge (hard set) - compared to conventional randomized training,
all while avoiding additional computational costs. This research highlights the
potential of leveraging human learning principles to enhance the capabilities
of language models in comprehending and responding to complex instructions and
tasks.
","2023-10-17","2310.09518v1.pdf"
"2310.09520","Haikang Deng","Haikang Deng, Colin Raffel","Reward-Augmented Decoding: Efficient Controlled Text Generation With a
  Unidirectional Reward Model","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.
","2023-10-18","2310.09520v1.pdf"
"2310.09528","Woojin Cho","Woojin Cho, Kookjin Lee, Donsub Rim, Noseong Park","Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural
  Networks","","","","","cs.LG cs.NA math.NA physics.comp-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In various engineering and applied science applications, repetitive numerical
simulations of partial differential equations (PDEs) for varying input
parameters are often required (e.g., aircraft shape optimization over many
design parameters) and solvers are required to perform rapid execution. In this
study, we suggest a path that potentially opens up a possibility for
physics-informed neural networks (PINNs), emerging deep-learning-based solvers,
to be considered as one such solver. Although PINNs have pioneered a proper
integration of deep-learning and scientific computing, they require repetitive
time-consuming training of neural networks, which is not suitable for
many-query scenarios. To address this issue, we propose a lightweight low-rank
PINNs containing only hundreds of model parameters and an associated
hypernetwork-based meta-learning algorithm, which allows efficient
approximation of solutions of PDEs for varying ranges of PDE input parameters.
Moreover, we show that the proposed method is effective in overcoming a
challenging issue, known as ""failure modes"" of PINNs.
","2023-10-17","2310.09528v1.pdf"
"2310.09536","Md Rashad Al Hasan Rony","Md Rashad Al Hasan Rony, Christian Suess, Sinchana Ramakanth Bhat,
  Viju Sudhi, Julia Schneider, Maximilian Vogel, Roman Teucher, Ken E. Friedl,
  Soumya Sahoo","CarExpert: Leveraging Large Language Models for In-Car Conversational
  Question Answering","Accepted into EMNLP 2023 (industry track), corresponding Author: Md
  Rashad Al Hasan Rony","","","","cs.CL cs.IR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have demonstrated remarkable performance by
following natural language instructions without fine-tuning them on
domain-specific tasks and data. However, leveraging LLMs for domain-specific
question answering suffers from severe limitations. The generated answer tends
to hallucinate due to the training data collection time (when using
off-the-shelf), complex user utterance and wrong retrieval (in
retrieval-augmented generation). Furthermore, due to the lack of awareness
about the domain and expected output, such LLMs may generate unexpected and
unsafe answers that are not tailored to the target domain. In this paper, we
propose CarExpert, an in-car retrieval-augmented conversational
question-answering system leveraging LLMs for different tasks. Specifically,
CarExpert employs LLMs to control the input, provide domain-specific documents
to the extractive and generative answering components, and controls the output
to ensure safe and domain-specific answers. A comprehensive empirical
evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in
generating natural, safe and car-specific answers.
","2023-10-17","2310.09536v1.pdf"
"2310.09550","Haonan Li","Yixuan Zhang and Haonan Li","Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test
  on ACLUE","Accepted at RANLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have showcased remarkable capabilities in
understanding and generating language. However, their ability in comprehending
ancient languages, particularly ancient Chinese, remains largely unexplored. To
bridge this gap, we present ACLUE, an evaluation benchmark designed to assess
the capability of language models in comprehending ancient Chinese. ACLUE
consists of 15 tasks cover a range of skills, spanning phonetic, lexical,
syntactic, semantic, inference and knowledge. Through the evaluation of eight
state-of-the-art LLMs, we observed a noticeable disparity in their performance
between modern Chinese and ancient Chinese. Among the assessed models, ChatGLM2
demonstrates the most remarkable performance, achieving an average score of
37.4%. We have made our code and data public available.
","2023-10-17","2310.09550v1.pdf"
"2310.09573","Yi Cheng","Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, Wenjie Li","Self-Detoxifying Language Models via Toxification Reversal","Accepted by EMNLP 2023 main conference","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Language model detoxification aims to minimize the risk of generating
offensive or harmful content in pretrained language models (PLMs) for safer
deployment. Existing methods can be roughly categorized as finetuning-based and
decoding-based. However, the former is often resource-intensive, while the
latter relies on additional components and potentially compromises the
generation fluency. In this paper, we propose a more lightweight approach that
enables the PLM itself to achieve ""self-detoxification"". Our method is built
upon the observation that prepending a negative steering prompt can effectively
induce PLMs to generate toxic content. At the same time, we are inspired by the
recent research in the interpretability field, which formulates the evolving
contextualized representations within the PLM as an information stream
facilitated by the attention layers. Drawing on this idea, we devise a method
to identify the toxification direction from the normal generation process to
the one prompted with the negative prefix, and then steer the generation to the
reversed direction by manipulating the information movement within the
attention layers. Experimental results show that our approach, without any
fine-tuning or extra components, can achieve comparable performance with
state-of-the-art methods.
","2023-10-17","2310.09573v1.pdf"
"2310.09590","Yi Bin","Yi Bin, Wenhao Shi, Yujuan Ding, Yang Yang, See-Kiong Ng","Solving Math Word Problems with Reexamination","7 pages, 1 figure","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Math word problem (MWP) solving aims to understand the descriptive math
problem and calculate the result, for which previous efforts are mostly devoted
to upgrade different technical modules. This paper brings a different
perspective of \textit{reexamination process} during training by introducing a
pseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual)
learning scheme to model such process, which is model-agnostic thus can be
adapted to any existing MWP solvers. The pseudo-dual task is specifically
defined as filling the numbers in the expression back into the original word
problem with numbers masked. To facilitate the effective joint learning of the
two tasks, we further design a scheduled fusion strategy for the number
infilling task, which smoothly switches the input from the ground-truth math
expressions to the predicted ones. Our pseudo-dual learning scheme has been
tested and proven effective when being equipped in several representative MWP
solvers through empirical studies. \textit{The codes and trained models are
available at:} \url{https://github.com/steven640pixel/PsedualMWP}.
\end{abstract}
","2023-10-17","2310.09590v1.pdf"
"2310.09605","Huatao Xu","Huatao Xu, Liying Han, Mo Li, Mani Srivastava","Penetrative AI: Making LLMs Comprehend the Physical World","2pages","","","","cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent developments in Large Language Models (LLMs) have demonstrated their
remarkable capabilities across a range of tasks. Questions, however, persist
about the nature of LLMs and their potential to integrate common-sense human
knowledge when performing tasks involving information about the real physical
world. This paper delves into these questions by exploring how LLMs can be
extended to interact with and reason about the physical world through IoT
sensors and actuators, a concept that we term ""\textit{Penetrative AI}"". The
paper explores such an extension at two levels of LLMs' ability to penetrate
into the physical world via the processing of sensory signals. Our preliminary
findings indicate that LLMs, with ChatGPT being the representative example in
our exploration, have considerable and unique proficiency in employing the
knowledge they learned during training for interpreting IoT sensor data and
reasoning over them about tasks in the physical realm. Not only this opens up
new applications for LLMs beyond traditional text-based tasks, but also enables
new ways of incorporating human knowledge in cyber-physical systems.
","2023-10-17","2310.09605v1.pdf"
"2310.09611","Nam Wook Kim","Joshua Gorniak, Yoon Kim, Stephen Gwon, Donglai Wei, Nam Wook Kim","VizAbility: Multimodal Accessible Data Visualization with Keyboard
  Navigation and Conversational Interaction","13 pages, 7 figures","","","","cs.HC","http://creativecommons.org/licenses/by/4.0/","  Data visualization serves as a crucial tool for communicating important
information in our society. Yet, as visualizations grow more complex, they
become less accessible to individuals with visual impairments. Traditional
accessibility approaches like alternative text and data tables often fall short
of capturing the full potential of data visualization. To bridge this gap, we
introduce VizAbility, a novel multimodal accessible system that combines
keyboard navigation with conventional interaction, enabling individuals with
visual impairments to actively engage with and explore data visualizations.
VizAbility utilizes an LLM-based pipeline, seamlessly integrating data, chart
structures, user locality, and web-based information to provide comprehensive
answers. Our quantitative evaluation validates the LLM-based
question-and-answer pipeline, and a user study involving six participants
underscores the promising potential of VizAbility's multimodal approach. We
discuss how current visualization tools can integrate VizAbility to enhance the
accessibility of data visualizations online.
","2023-10-17","2310.09611v1.pdf"
"2310.09624","Alex Mei","Alex Mei, Sharon Levy, William Yang Wang","ASSERT: Automated Safety Scenario Red Teaming for Evaluating the
  Robustness of Large Language Models","In Findings of the 2023 Conference on Empirical Methods in Natural
  Language Processing","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  As large language models are integrated into society, robustness toward a
suite of prompts is increasingly important to maintain reliability in a
high-variance environment.Robustness evaluations must comprehensively
encapsulate the various settings in which a user may invoke an intelligent
system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,
consisting of three methods -- semantically aligned augmentation, target
bootstrapping, and adversarial knowledge injection. For robust safety
evaluation, we apply these methods in the critical domain of AI safety to
algorithmically generate a test suite of prompts covering diverse robustness
settings -- semantic equivalence, related scenarios, and adversarial. We
partition our prompts into four safety domains for a fine-grained analysis of
how the domain affects model performance. Despite dedicated safeguards in
existing state-of-the-art models, we find statistically significant performance
differences of up to 11% in absolute classification accuracy among semantically
related scenarios and error rates of up to 19% absolute error in zero-shot
adversarial settings, raising concerns for users' physical safety.
","2023-10-17","2310.09624v1.pdf"
"2310.09639","Liang Zhang","Liang Zhang, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He","DPZero: Dimension-Independent and Differentially Private Zeroth-Order
  Optimization","","","","","cs.LG cs.CR math.OC stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The widespread practice of fine-tuning pretrained large language models
(LLMs) on domain-specific data faces two major challenges in memory and
privacy. First, as the size of LLMs continue to grow, encompassing billions of
parameters, the memory demands of gradient-based training methods via
backpropagation become prohibitively high. Second, given the tendency of LLMs
to memorize and disclose sensitive training data, the privacy of fine-tuning
data must be respected. To this end, we explore the potential of zeroth-order
methods in differentially private optimization for fine-tuning LLMs.
Zeroth-order methods, which rely solely on forward passes, substantially reduce
memory consumption during training. However, directly combining them with
standard differential privacy mechanism poses dimension-dependent complexity.
To bridge the gap, we introduce DPZero, a novel differentially private
zeroth-order algorithm with nearly dimension-independent rates. Our theoretical
analysis reveals that its complexity hinges primarily on the problem's
intrinsic dimension and exhibits only a logarithmic dependence on the ambient
dimension. This renders DPZero a highly practical option for real-world LLMs
deployments.
","2023-10-17","2310.09639v1.pdf"
"2310.09656","Hengrui Zhang","Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan
  Shen, Xiao Qin, Christos Faloutsos, Huzefa Rangwala and George Karypis","Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent
  Space","","","","","cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Recent advances in tabular data generation have greatly enhanced synthetic
data quality. However, extending diffusion models to tabular data is
challenging due to the intricately varied distributions and a blend of data
types of tabular data. This paper introduces TABSYN, a methodology that
synthesizes tabular data by leveraging a diffusion model within a variational
autoencoder (VAE) crafted latent space. The key advantages of the proposed
TABSYN include (1) Generality: the ability to handle a broad spectrum of data
types by converting them into a single unified space and explicitly capture
inter-column relations; (2) Quality: optimizing the distribution of latent
embeddings to enhance the subsequent training of diffusion models, which helps
generate high-quality synthetic data, (3) Speed: much fewer number of reverse
steps and faster synthesis speed than existing diffusion-based methods.
Extensive experiments on six datasets with five metrics demonstrate that TABSYN
outperforms existing methods. Specifically, it reduces the error rates by 86%
and 67% for column-wise distribution and pair-wise column correlation
estimations compared with the most competitive baselines.
","2023-10-17","2310.09656v1.pdf"
"2310.09668","Chenyang Yang","Chenyang Yang, Rishabh Rustogi, Rachel Brower-Sinning, Grace A. Lewis,
  Christian K\""astner, Tongshuang Wu","Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using
  LLMs","","","","","cs.CL cs.SE","http://creativecommons.org/licenses/by/4.0/","  Current model testing work has mostly focused on creating test cases.
Identifying what to test is a step that is largely ignored and poorly
supported. We propose Weaver, an interactive tool that supports requirements
elicitation for guiding model testing. Weaver uses large language models to
generate knowledge bases and recommends concepts from them interactively,
allowing testers to elicit requirements for further testing. Weaver provides
rich external knowledge to testers and encourages testers to systematically
explore diverse concepts beyond their own biases. In a user study, we show that
both NLP experts and non-experts identified more, as well as more diverse
concepts worth testing when using Weaver. Collectively, they found more than
200 failing test cases for stance detection with zero-shot ChatGPT. Our case
studies further show that Weaver can help practitioners test models in
real-world settings, where developers define more nuanced application scenarios
(e.g., code understanding and transcript summarization) using LLMs.
","2023-10-17","2310.09668v1.pdf"
"2310.09676","Jiachen Li","Jiachen Li, Qiaozi Gao, Michael Johnston, Xiaofeng Gao, Xuehai He,
  Suhaila Shakiah, Hangjie Shi, Reza Ghanadan, William Yang Wang","Mastering Robot Manipulation with Multimodal Prompts through Pretraining
  and Multi-task Fine-tuning","","","","","cs.RO cs.AI","http://creativecommons.org/licenses/by/4.0/","  Prompt-based learning has been demonstrated as a compelling paradigm
contributing to large language models' tremendous success (LLMs). Inspired by
their success in language tasks, existing research has leveraged LLMs in
embodied instruction following and task planning. However, not much attention
has been paid to embodied tasks with multimodal prompts, combining vision
signals with text descriptions. This type of task poses a major challenge to
robots' capability to understand the interconnection and complementarity
between vision and language signals. In this work, we introduce an effective
framework that learns a policy to perform robot manipulation with multimodal
prompts from multi-task expert trajectories. Our methods consist of a two-stage
training pipeline that performs inverse dynamics pretraining and multi-task
finetuning. To facilitate multimodal understanding, we design our multimodal
prompt encoder by augmenting a pretrained LM with a residual connection to the
visual input and model the dependencies among action dimensions. Empirically,
we evaluate the efficacy of our method on the VIMA-BENCH and establish a new
state-of-the-art (10% improvement in success rate). Moreover, we demonstrate
that our model exhibits remarkable in-context learning ability.
","2023-10-17","2310.09676v1.pdf"
"2310.09685","Carlos Outeiral Rubiera","Adam Winnifrith, Carlos Outeiral and Brian Hie","Generative artificial intelligence for de novo protein design","32 pages, 5 figures, 1 table","","","","cs.LG cs.AI q-bio.BM","http://creativecommons.org/licenses/by/4.0/","  Engineering new molecules with desirable functions and properties has the
potential to extend our ability to engineer proteins beyond what nature has so
far evolved. Advances in the so-called ""de novo"" design problem have recently
been brought forward by developments in artificial intelligence. Generative
architectures, such as language models and diffusion processes, seem adept at
generating novel, yet realistic proteins that display desirable properties and
perform specified functions. State-of-the-art design protocols now achieve
experimental success rates nearing 20%, thus widening the access to de novo
designed proteins. Despite extensive progress, there are clear field-wide
challenges, for example in determining the best in silico metrics to prioritise
designs for experimental testing, and in designing proteins that can undergo
large conformational changes or be regulated by post-translational
modifications and other cellular processes. With an increase in the number of
models being developed, this review provides a framework to understand how
these tools fit into the overall process of de novo protein design. Throughout,
we highlight the power of incorporating biochemical knowledge to improve
performance and interpretability.
","2023-10-17","2310.09685v1.pdf"
"2310.09696","XingJiao Wu","Shuwen Yang, Anran Wu, Xingjiao Wu, Luwei Xiao, Tianlong Ma, Cheng
  Jin, Liang He","Progressive Evidence Refinement for Open-domain Multimodal Retrieval
  Question Answering","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pre-trained multimodal models have achieved significant success in
retrieval-based question answering. However, current multimodal retrieval
question-answering models face two main challenges. Firstly, utilizing
compressed evidence features as input to the model results in the loss of
fine-grained information within the evidence. Secondly, a gap exists between
the feature extraction of evidence and the question, which hinders the model
from effectively extracting critical features from the evidence based on the
given question. We propose a two-stage framework for evidence retrieval and
question-answering to alleviate these issues. First and foremost, we propose a
progressive evidence refinement strategy for selecting crucial evidence. This
strategy employs an iterative evidence retrieval approach to uncover the
logical sequence among the evidence pieces. It incorporates two rounds of
filtering to optimize the solution space, thus further ensuring temporal
efficiency. Subsequently, we introduce a semi-supervised contrastive learning
training strategy based on negative samples to expand the scope of the question
domain, allowing for a more thorough exploration of latent knowledge within
known samples. Finally, in order to mitigate the loss of fine-grained
information, we devise a multi-turn retrieval and question-answering strategy
to handle multimodal inputs. This strategy involves incorporating multimodal
evidence directly into the model as part of the historical dialogue and
question. Meanwhile, we leverage a cross-modal attention mechanism to capture
the underlying connections between the evidence and the question, and the
answer is generated through a decoding generation approach. We validate the
model's effectiveness through extensive experiments, achieving outstanding
performance on WebQA and MultimodelQA benchmark tests.
","2023-10-17","2310.09696v1.pdf"
"2310.09706","Yang Yu","Yang Yu, Qi Liu, Kai Zhang, Yuren Zhang, Chao Song, Min Hou, Yuqing
  Yuan, Zhihao Ye, Zaixi Zhang, Sanshi Lei Yu","AdaptSSR: Pre-training User Model with Augmentation-Adaptive
  Self-Supervised Ranking","Accepted by NeurIPS 2023","","","","cs.IR cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  User modeling, which aims to capture users' characteristics or interests,
heavily relies on task-specific labeled data and suffers from the data sparsity
issue. Several recent studies tackled this problem by pre-training the user
model on massive user behavior sequences with a contrastive learning task.
Generally, these methods assume different views of the same behavior sequence
constructed via data augmentation are semantically consistent, i.e., reflecting
similar characteristics or interests of the user, and thus maximizing their
agreement in the feature space. However, due to the diverse interests and heavy
noise in user behaviors, existing augmentation methods tend to lose certain
characteristics of the user or introduce noisy behaviors. Thus, forcing the
user model to directly maximize the similarity between the augmented views may
result in a negative transfer. To this end, we propose to replace the
contrastive learning task with a new pretext task: Augmentation-Adaptive
SelfSupervised Ranking (AdaptSSR), which alleviates the requirement of semantic
consistency between the augmented views while pre-training a discriminative
user model. Specifically, we adopt a multiple pairwise ranking loss which
trains the user model to capture the similarity orders between the implicitly
augmented view, the explicitly augmented view, and views from other users. We
further employ an in-batch hard negative sampling strategy to facilitate model
training. Moreover, considering the distinct impacts of data augmentation on
different behavior sequences, we design an augmentation-adaptive fusion
mechanism to automatically adjust the similarity order constraint applied to
each sample based on the estimated similarity between the augmented views.
Extensive experiments on both public and industrial datasets with six
downstream tasks verify the effectiveness of AdaptSSR.
","2023-10-25","2310.09706v1.pdf"
"2310.09716","Fanghua Ye","Fanghua Ye, Meng Fang, Shenghui Li, Emine Yilmaz","Enhancing Conversational Search: Large Language Model-Aided Informative
  Query Rewriting","22 pages, accepted to EMNLP Findings 2023","","","","cs.HC cs.AI cs.CL cs.IR","http://creativecommons.org/licenses/by/4.0/","  Query rewriting plays a vital role in enhancing conversational search by
transforming context-dependent user queries into standalone forms. Existing
approaches primarily leverage human-rewritten queries as labels to train query
rewriting models. However, human rewrites may lack sufficient information for
optimal retrieval performance. To overcome this limitation, we propose
utilizing large language models (LLMs) as query rewriters, enabling the
generation of informative query rewrites through well-designed instructions. We
define four essential properties for well-formed rewrites and incorporate all
of them into the instruction. In addition, we introduce the role of rewrite
editors for LLMs when initial query rewrites are available, forming a
""rewrite-then-edit"" process. Furthermore, we propose distilling the rewriting
capabilities of LLMs into smaller models to reduce rewriting latency. Our
experimental evaluation on the QReCC dataset demonstrates that informative
query rewrites can yield substantially improved retrieval performance compared
to human rewrites, especially with sparse retrievers.
","2023-10-19","2310.09716v1.pdf"
"2310.09753","Enric Boix-Adser\`a","Enric Boix-Adsera and Omid Saremi and Emmanuel Abbe and Samy Bengio
  and Etai Littwin and Joshua Susskind","When can transformers reason with abstract symbols?","24 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate the capabilities of transformer large language models (LLMs)
on relational reasoning tasks involving abstract symbols. Such tasks have long
been studied in the neuroscience literature as fundamental building blocks for
more complex abilities in programming, mathematics, and verbal reasoning. For
(i) regression tasks, we prove that transformers generalize when trained, but
require astonishingly large quantities of training data. For (ii)
next-token-prediction tasks with symbolic labels, we show an ""inverse scaling
law"": transformers fail to generalize as their embedding dimension increases.
For both settings (i) and (ii), we propose subtle transformer modifications
which can reduce the amount of data needed by adding two trainable parameters
per head.
","2023-10-17","2310.09753v1.pdf"
"2310.09754","Huanhuan Ma","Huanhuan Ma and Weizhi Xu and Yifan Wei and Liuji Chen and Liang Wang
  and Qiang Liu and Shu Wu and Liang Wang","EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Fact verification aims to automatically probe the veracity of a claim based
on several pieces of evidence. Existing works are always engaging in the
accuracy improvement, let alone the explainability, a critical capability of
fact verification system. Constructing an explainable fact verification system
in a complex multi-hop scenario is consistently impeded by the absence of a
relevant high-quality dataset. Previous dataset either suffer from excessive
simplification or fail to incorporate essential considerations for
explainability. To address this, we present EX-FEVER, a pioneering dataset for
multi-hop explainable fact verification. With over 60,000 claims involving
2-hop and 3-hop reasoning, each is created by summarizing and modifying
information from hyperlinked Wikipedia documents. Each instance is accompanied
by a veracity label and an explanation that outlines the reasoning path
supporting the veracity classification. Additionally, we demonstrate a novel
baseline system on our EX-FEVER dataset, showcasing document retrieval,
explanation generation, and claim verification and observe that existing fact
verification models trained on previous datasets struggle to perform well on
our dataset. Furthermore, we highlight the potential of utilizing Large
Language Models in the fact verification task. We hope our dataset could make a
significant contribution by providing ample opportunities to explore the
integration of natural language explanations in the domain of fact
verification.
","2023-10-17","2310.09754v1.pdf"
"2310.09755","Sumedh Rasal","Sumedh Rasal and Sanjay Kumar Boddhu","Beyond Segmentation: Road Network Generation with Multi-Modal LLMs","","","10.48550/arXiv.2310.09755","arXiv:2310.09755","cs.CV cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This paper introduces an innovative approach to road network generation
through the utilization of a multi-modal Large Language Model (LLM). Our model
is specifically designed to process aerial images of road layouts and produce
detailed, navigable road networks within the input images. The core innovation
of our system lies in the unique training methodology employed for the large
language model to generate road networks as its output. This approach draws
inspiration from the BLIP-2 architecture arXiv:2301.12597, leveraging
pre-trained frozen image encoders and large language models to create a
versatile multi-modal LLM.
  Our work also offers an alternative to the reasoning segmentation method
proposed in the LISA paper arXiv:2308.00692. By training the large language
model with our approach, the necessity for generating binary segmentation
masks, as suggested in the LISA paper arXiv:2308.00692, is effectively
eliminated. Experimental results underscore the efficacy of our multi-modal LLM
in providing precise and valuable navigational guidance. This research
represents a significant stride in bolstering autonomous navigation systems,
especially in road network scenarios, where accurate guidance is of paramount
importance.
","2023-10-19","2310.09755v1.pdf"
"2310.09761","Yulei Qin","Yulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou Fu, Yun Gu, Ke Li, Xing
  Sun, Rongrong Ji","CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes","Accepted at NeurIPS2023","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Webly supervised learning has attracted increasing attention for its
effectiveness in exploring publicly accessible data at scale without manual
annotation. However, most existing methods of learning with web datasets are
faced with challenges from label noise, and they have limited assumptions on
clean samples under various noise. For instance, web images retrieved with
queries of tiger cat (a cat species) and drumstick (a musical instrument) are
almost dominated by images of tigers and chickens, which exacerbates the
challenge of fine-grained visual concept learning. In this case, exploiting
both web images and their associated texts is a requisite solution to combat
real-world noise. In this paper, we propose Cross-modality Aligned Prototypes
(CAPro), a unified prototypical contrastive learning framework to learn visual
representations with correct semantics. For one thing, we leverage textual
prototypes, which stem from the distinct concept definition of classes, to
select clean images by text matching and thus disambiguate the formation of
visual prototypes. For another, to handle missing and mismatched noisy texts,
we resort to the visual feature space to complete and enhance individual texts
and thereafter improve text matching. Such semantically aligned visual
prototypes are further polished up with high-quality samples, and engaged in
both cluster regularization and noise removal. Besides, we propose collective
bootstrapping to encourage smoother and wiser label reference from
appearance-similar instances in a manner of dictionary look-up. Extensive
experiments on WebVision1k and NUS-WIDE (Web) demonstrate that CAPro well
handles realistic noise under both single-label and multi-label scenarios.
CAPro achieves new state-of-the-art performance and exhibits robustness to
open-set recognition. Codes are available at https://github.com/yuleiqin/capro.
","2023-10-17","2310.09761v1.pdf"
"2310.09765","Debtanu Datta","Sayan Mahapatra, Debtanu Datta, Shubham Soni, Adrijit Goswami,
  Saptarshi Ghosh","Improving Access to Justice for the Indian Population: A Benchmark for
  Evaluating Translation of Legal Text to Indian Languages","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Most legal text in the Indian judiciary is written in complex English due to
historical reasons. However, only about 10% of the Indian population is
comfortable in reading English. Hence legal text needs to be made available in
various Indian languages, possibly by translating the available legal text from
English. Though there has been a lot of research on translation to and between
Indian languages, to our knowledge, there has not been much prior work on such
translation in the legal domain. In this work, we construct the first
high-quality legal parallel corpus containing aligned text units in English and
nine Indian languages, that includes several low-resource languages. We also
benchmark the performance of a wide variety of Machine Translation (MT) systems
over this corpus, including commercial MT systems, open-source MT systems and
Large Language Models. Through a comprehensive survey by Law practitioners, we
check how satisfied they are with the translations by some of these MT systems,
and how well automatic MT evaluation metrics agree with the opinions of Law
practitioners.
","2023-10-17","2310.09765v1.pdf"
"2310.09810","Chakkrit Tantithamthavorn","Michael Fu, Chakkrit Tantithamthavorn, Van Nguyen, Trung Le","ChatGPT for Vulnerability Detection, Classification, and Repair: How Far
  Are We?","Accepted at the 30th Asia-Pacific Software Engineering Conference
  (APSEC 2023)","","","","cs.SE cs.CR","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4)
exhibited remarkable advancement in a range of software engineering tasks
associated with source code such as code review and code generation. In this
paper, we undertake a comprehensive study by instructing ChatGPT for four
prevalent vulnerability tasks: function and line-level vulnerability
prediction, vulnerability classification, severity estimation, and
vulnerability repair. We compare ChatGPT with state-of-the-art language models
designed for software vulnerability purposes. Through an empirical assessment
employing extensive real-world datasets featuring over 190,000 C/C++ functions,
we found that ChatGPT achieves limited performance, trailing behind other
language models in vulnerability contexts by a significant margin. The
experimental outcomes highlight the challenging nature of vulnerability
prediction tasks, requiring domain-specific expertise. Despite ChatGPT's
substantial model scale, exceeding that of source code-pre-trained language
models (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuning
remains imperative for ChatGPT to generalize for vulnerability prediction
tasks. We publish the studied dataset, experimental prompts for ChatGPT, and
experimental results at https://github.com/awsm-research/ChatGPT4Vul.
","2023-10-17","2310.09810v1.pdf"
"2310.09820","Weixuan Wang","Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng","Assessing the Reliability of Large Language Model Knowledge","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have been treated as knowledge bases due to
their strong performance in knowledge probing tasks. LLMs are typically
evaluated using accuracy, yet this metric does not capture the vulnerability of
LLMs to hallucination-inducing factors like prompt and context variability. How
do we evaluate the capabilities of LLMs to consistently produce factually
correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe
(MONITOR), a novel metric designed to directly measure LLMs' factual
reliability. MONITOR computes the distance between the probability
distributions of a valid output and its counterparts produced by the same LLM
probing the same fact using different styles of prompts and
contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the
effectiveness of MONITOR in evaluating the factual reliability of LLMs while
maintaining a low computational overhead. In addition, we release the FKTC
(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total
to foster research along this line (https://github.com/Vicky-Wil/MONITOR).
","2023-10-17","2310.09820v1.pdf"
"2310.09846","Zihan Wang","Zihan Wang, Ziqi Zhao, Zhumin Chen, Pengjie Ren, Maarten de Rijke and
  Zhaochun Ren","Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with
  Type-Related Features","Accepted at EMNLP findings","","","","cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Few-shot named entity recognition (NER) has shown remarkable progress in
identifying entities in low-resource domains. However, few-shot NER methods
still struggle with out-of-domain (OOD) examples due to their reliance on
manual labeling for the target domain. To address this limitation, recent
studies enable generalization to an unseen target domain with only a few
labeled examples using data augmentation techniques. Two important challenges
remain: First, augmentation is limited to the training data, resulting in
minimal overlap between the generated data and OOD examples. Second, knowledge
transfer is implicit and insufficient, severely hindering model
generalizability and the integration of knowledge from the source domain. In
this paper, we propose a framework, prompt learning with type-related features
(PLTR), to address these challenges. To identify useful knowledge in the source
domain and enhance knowledge transfer, PLTR automatically extracts entity
type-related features (TRFs) based on mutual information criteria. To bridge
the gap between training and OOD data, PLTR generates a unique prompt for each
unseen example by selecting relevant TRFs. We show that PLTR achieves
significant performance improvements on in-domain and cross-domain datasets.
The use of PLTR facilitates model adaptation and increases representation
similarities between the source and unseen domains.
","2023-10-17","2310.09846v1.pdf"
"2310.09848","Hong Zhang","Hong Zhang, Prasanta Bhattacharya, Wei Gao, Liang Ze Wong, Brandon
  Siyuan Loh, Joseph J. P. Simons, Jisun An","Enhancing Stance Classification with Quantified Moral Foundations","11 pages, 5 figures","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This study enhances stance detection on social media by incorporating deeper
psychological attributes, specifically individuals' moral foundations. These
theoretically-derived dimensions aim to provide a comprehensive profile of an
individual's moral concerns which, in recent work, has been linked to behaviour
in a range of domains, including society, politics, health, and the
environment. In this paper, we investigate how moral foundation dimensions can
contribute to predicting an individual's stance on a given target. Specifically
we incorporate moral foundation features extracted from text, along with
message semantic features, to classify stances at both message- and user-levels
across a range of targets and models. Our preliminary results suggest that
encoding moral foundations can enhance the performance of stance detection
tasks and help illuminate the associations between specific moral foundations
and online stances on target topics. The results highlight the importance of
considering deeper psychological attributes in stance analysis and underscores
the role of moral foundations in guiding online social behavior.
","2023-10-17","2310.09848v1.pdf"
"2310.09874","Jiahao Wu","Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing
  Li, Xiao-Ming Wu, Ke Tang","Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset
  Condensation for Content-Based Recommendation","","","","","cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modern techniques in Content-based Recommendation (CBR) leverage item content
information to provide personalized services to users, but suffer from
resource-intensive training on large datasets. To address this issue, we
explore the dataset condensation for textual CBR in this paper. The goal of
dataset condensation is to synthesize a small yet informative dataset, upon
which models can achieve performance comparable to those trained on large
datasets. While existing condensation approaches are tailored to classification
tasks for continuous data like images or embeddings, direct application of them
to CBR has limitations. To bridge this gap, we investigate efficient dataset
condensation for content-based recommendation. Inspired by the remarkable
abilities of large language models (LLMs) in text comprehension and generation,
we leverage LLMs to empower the generation of textual content during
condensation. To handle the interaction data involving both users and items, we
devise a dual-level condensation method: content-level and user-level. At
content-level, we utilize LLMs to condense all contents of an item into a new
informative title. At user-level, we design a clustering-based synthesis
module, where we first utilize LLMs to extract user interests. Then, the user
interests and user embeddings are incorporated to condense users and generate
interactions for condensed users. Notably, the condensation paradigm of this
method is forward and free from iterative optimization on the synthesized
dataset. Extensive empirical findings from our study, conducted on three
authentic datasets, substantiate the efficacy of the proposed method.
Particularly, we are able to approximate up to 97% of the original performance
while reducing the dataset size by 95% (i.e., on dataset MIND).
","2023-10-17","2310.09874v1.pdf"
"2310.09881","Chengwei Qin","Chengwei Qin, Aston Zhang, Anirudh Dagar, Wenming Ye","In-Context Learning with Iterative Demonstration Selection","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Spurred by advancements in scale, large language models (LLMs) have
demonstrated strong few-shot learning ability via in-context learning (ICL).
However, the performance of ICL has been shown to be highly sensitive to the
selection of few-shot demonstrations. Selecting the most suitable examples as
context remains an ongoing challenge and an open problem. Existing literature
has highlighted the importance of selecting examples that are diverse or
semantically similar to the test sample while ignoring the fact that the
optimal selection dimension, i.e., diversity or similarity, is task-specific.
Leveraging the merits of both dimensions, we propose Iterative Demonstration
Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),
IDS iteratively selects examples that are diverse but still strongly correlated
with the test sample as ICL demonstrations. Specifically, IDS applies
Zero-shot-CoT to the test sample before demonstration selection. The output
reasoning path is then used to choose demonstrations that are prepended to the
test sample for inference. The generated answer is accompanied by its
corresponding reasoning path for extracting a new set of demonstrations in the
next iteration. After several iterations, IDS adopts majority voting to obtain
the final result. Through extensive experiments on tasks including commonsense
reasoning, question answering, topic classification, and sentiment analysis, we
demonstrate that IDS can consistently outperform existing ICL demonstration
selection methods.
","2023-10-24","2310.09881v1.pdf"
"2310.09909","Chaoyi Wu","Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman
  Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang and Weidi Xie","Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for
  Multimodal Medical Diagnosis","","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by/4.0/","  Driven by the large foundation models, the development of artificial
intelligence has witnessed tremendous progress lately, leading to a surge of
general interest from the public. In this study, we aim to assess the
performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm
of multimodal medical diagnosis. Our evaluation encompasses 17 human body
systems, including Central Nervous System, Head and Neck, Cardiac, Chest,
Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,
Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,
Pediatrics, with images taken from 8 modalities used in daily clinic routine,
e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),
Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA),
Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on
multiple clinical tasks with or without patent history provided, including
imaging modality and anatomy recognition, disease diagnosis, report generation,
disease localisation.
  Our observation shows that, while GPT-4V demonstrates proficiency in
distinguishing between medical image modalities and anatomy, it faces
significant challenges in disease diagnosis and generating comprehensive
reports. These findings underscore that while large multimodal models have made
significant advancements in computer vision and natural language processing, it
remains far from being used to effectively support real-world medical
applications and clinical decision-making.
  All images used in this report can be found in
https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
","2023-10-18","2310.09909v1.pdf"
"2310.09926","Shiladitya Dutta","Shiladitya Dutta, Hongbo Wei, Lars van der Laan, Ahmed M. Alaa","Estimating Uncertainty in Multimodal Foundation Models using Public
  Internet Data","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., ""an image of a <category>"", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.
","2023-10-17","2310.09926v1.pdf"
"2310.09929","Zhiqiu Lin","Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong","Prompting Scientific Names for Zero-Shot Species Recognition","EMNLP 2023","","","","cs.CV cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as
CLIP can recognize images of common objects in a zero-shot fashion. However, it
is underexplored how to use CLIP for zero-shot recognition of highly
specialized concepts, e.g., species of birds, plants, and animals, for which
their scientific names are written in Latin or Greek. Indeed, CLIP performs
poorly for zero-shot species recognition with prompts that use scientific
names, e.g., ""a photo of Lepus Timidus"" (which is a scientific name in Latin).
Because these names are usually not included in CLIP's training set. To improve
performance, prior works propose to use large-language models (LLMs) to
generate descriptions (e.g., of species color and shape) and additionally use
them in prompts. We find that they bring only marginal gains. Differently, we
are motivated to translate scientific names (e.g., Lepus Timidus) to common
English names (e.g., mountain hare) and use such in the prompts. We find that
common names are more likely to be included in CLIP's training set, and
prompting them achieves 2$\sim$5 times higher accuracy on benchmarking datasets
of fine-grained species recognition.
","2023-10-17","2310.09929v1.pdf"
"2310.09930","Tianxiao Shen","Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, Yejin
  Choi","FiLM: Fill-in Language Models for Any-Order Generation","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models have become the backbone of today's AI systems. However,
their predominant left-to-right generation limits the use of bidirectional
context, which is essential for tasks that involve filling text in the middle.
We propose the Fill-in Language Model (FiLM), a new language modeling approach
that allows for flexible generation at any position without adhering to a
specific generation order. Its training extends the masked language modeling
objective by adopting varying mask probabilities sampled from the Beta
distribution to enhance the generative capabilities of FiLM. During inference,
FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring
that the outputs are fluent and are coherent with the surrounding context. In
both automatic and human evaluations, FiLM outperforms existing infilling
methods that rely on left-to-right language models trained on rearranged text
segments. FiLM is easy to implement and can be either trained from scratch or
fine-tuned from a left-to-right language model. Notably, as the model size
grows, FiLM's perplexity approaches that of strong left-to-right language
models of similar sizes, indicating FiLM's scalability and potential as a large
language model.
","2023-10-17","2310.09930v1.pdf"
"2310.09949","Wenqi Jiang","Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo
  Alonso","Chameleon: a Heterogeneous and Disaggregated Accelerator System for
  Retrieval-Augmented Language Models","","","","","cs.LG cs.AI cs.AR cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
","2023-10-17","2310.09949v1.pdf"
"2310.09983","Noveen Sachdeva","Noveen Sachdeva, Zexue He, Wang-Cheng Kang, Jianmo Ni, Derek Zhiyuan
  Cheng, Julian McAuley","Farzi Data: Autoregressive Data Distillation","Under review. 23 pages, 9 figures","","","","cs.LG cs.AI cs.CL cs.IR","http://creativecommons.org/licenses/by/4.0/","  We study data distillation for auto-regressive machine learning tasks, where
the input and output have a strict left-to-right causal structure. More
specifically, we propose Farzi, which summarizes an event sequence dataset into
a small number of synthetic sequences -- Farzi Data -- which are optimized to
maintain (if not improve) model performance compared to training on the full
dataset. Under the hood, Farzi conducts memory-efficient data distillation by
(i) deriving efficient reverse-mode differentiation of the Adam optimizer by
leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional
discrete event-space into a latent-space which provably promotes implicit
regularization. Empirically, for sequential recommendation and language
modeling tasks, we are able to achieve 98-120% of downstream full-data
performance when training state-of-the-art models on Farzi Data of size as
little as 0.1% of the original dataset. Notably, being able to train better
models with significantly less data sheds light on the design of future large
auto-regressive models, and opens up new opportunities to further scale up
model and data sizes.
","2023-10-17","2310.09983v1.pdf"
"2310.09985","Shm Almeda","Shm Garanganao Almeda, J.D. Zamfirescu-Pereira, Kyu Won Kim, Pradeep
  Mani Rathnam, Bjoern Hartmann","Prompting for Discovery: Flexible Sense-Making for AI Art-Making with
  Dreamsheets","13 pages, 14 figures, currently under review","","","","cs.HC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Design space exploration (DSE) for Text-to-Image (TTI) models entails
navigating a vast, opaque space of possible image outputs, through a
commensurately vast input space of hyperparameters and prompt text. Minor
adjustments to prompt input can surface unexpectedly disparate images. How can
interfaces support end-users in reliably steering prompt-space explorations
towards interesting results? Our design probe, DreamSheets, supports
exploration strategies with LLM-based functions for assisted prompt
construction and simultaneous display of generated results, hosted in a
spreadsheet interface. The flexible layout and novel generative functions
enable experimentation with user-defined workflows. Two studies, a preliminary
lab study and a longitudinal study with five expert artists, revealed a set of
strategies participants use to tackle the challenges of TTI design space
exploration, and the interface features required to support them - like using
text-generation to define local ""axes"" of exploration. We distill these
insights into a UI mockup to guide future interfaces.
","2023-10-17","2310.09985v1.pdf"
"2310.10010","Sheng Zheng","Sheng Zheng, Chaoning Zhang","Black-box Targeted Adversarial Attack on Segment Anything (SAM)","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep recognition models are widely vulnerable to adversarial examples, which
change the model output by adding quasi-imperceptible perturbation to the image
input. Recently, Segment Anything Model (SAM) has emerged to become a popular
foundation model in computer vision due to its impressive generalization to
unseen data and tasks. Realizing flexible attacks on SAM is beneficial for
understanding the robustness of SAM in the adversarial context. To this end,
this work aims to achieve a targeted adversarial attack (TAA) on SAM.
Specifically, under a certain prompt, the goal is to make the predicted mask of
an adversarial example resemble that of a given target image. The task of TAA
on SAM has been realized in a recent arXiv work in the white-box setup by
assuming access to prompt and model, which is thus less practical. To address
the issue of prompt dependence, we propose a simple yet effective approach by
only attacking the image encoder. Moreover, we propose a novel regularization
loss to enhance the cross-model transferability by increasing the feature
dominance of adversarial images over random natural images. Extensive
experiments verify the effectiveness of our proposed simple techniques to
conduct a successful black-box TAA on SAM.
","2023-10-17","2310.10010v1.pdf"
"2310.10021","Jesse Zhang","Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk
  Chang, Shao-Hua Sun, Joseph J. Lim","Bootstrap Your Own Skills: Learning to Solve New Tasks with Large
  Language Model Guidance","CoRL 2023 (Oral); 24 pages, 11 figures","","","","cs.RO cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  We propose BOSS, an approach that automatically learns to solve new
long-horizon, complex, and meaningful tasks by growing a learned skill library
with minimal supervision. Prior work in reinforcement learning require expert
supervision, in the form of demonstrations or rich reward functions, to learn
long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills)
learns to accomplish new tasks by performing ""skill bootstrapping,"" where an
agent with a set of primitive skills interacts with the environment to practice
new skills without receiving reward feedback for tasks outside of the initial
skill set. This bootstrapping phase is guided by large language models (LLMs)
that inform the agent of meaningful skills to chain together. Through this
process, BOSS builds a wide range of complex and useful behaviors from a basic
set of primitive skills. We demonstrate through experiments in realistic
household environments that agents trained with our LLM-guided bootstrapping
procedure outperform those trained with naive bootstrapping as well as prior
unsupervised skill acquisition methods on zero-shot execution of unseen,
long-horizon tasks in new environments. Website at clvrai.com/boss.
","2023-10-18","2310.10021v1.pdf"
"2310.10035","Tingyu Xie","Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, Hongwei Wang","Empirical Study of Zero-Shot NER with ChatGPT","Accepted to EMNLP 2023 (Main Conference)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) exhibited powerful capability in various natural
language processing tasks. This work focuses on exploring LLM performance on
zero-shot information extraction, with a focus on the ChatGPT and named entity
recognition (NER) task. Inspired by the remarkable reasoning capability of LLM
on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods
to NER and propose reasoning strategies tailored for NER. First, we explore a
decomposed question-answering paradigm by breaking down the NER task into
simpler subproblems by labels. Second, we propose syntactic augmentation to
stimulate the model's intermediate thinking in two ways: syntactic prompting,
which encourages the model to analyze the syntactic structure itself, and tool
augmentation, which provides the model with the syntactic information generated
by a parsing tool. Besides, we adapt self-consistency to NER by proposing a
two-stage majority voting strategy, which first votes for the most consistent
mentions, then the most consistent types. The proposed methods achieve
remarkable improvements for zero-shot NER across seven benchmarks, including
Chinese and English datasets, and on both domain-specific and general-domain
scenarios. In addition, we present a comprehensive analysis of the error types
with suggestions for optimization directions. We also verify the effectiveness
of the proposed methods on the few-shot setting and other LLMs.
","2023-10-17","2310.10035v1.pdf"
"2310.10046","Baodong Wu","Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo,
  Tieyao Xiang, Yuheng Chen, Shigang Li","TRANSOM: An Efficient Fault-Tolerant System for Training LLMs","14 pages, 9 figures","","","","cs.DC cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large language models (LLMs) with hundreds of billions or trillions of
parameters, represented by chatGPT, have achieved profound impact on various
fields. However, training LLMs with super-large-scale parameters requires large
high-performance GPU clusters and long training periods lasting for months. Due
to the inevitable hardware and software failures in large-scale clusters,
maintaining uninterrupted and long-duration training is extremely challenging.
As a result, A substantial amount of training time is devoted to task
checkpoint saving and loading, task rescheduling and restart, and task manual
anomaly checks, which greatly harms the overall training efficiency. To address
these issues, we propose TRANSOM, a novel fault-tolerant LLM training system.
In this work, we design three key subsystems: the training pipeline automatic
fault tolerance and recovery mechanism named Transom Operator and Launcher
(TOL), the training task multi-dimensional metric automatic anomaly detection
system named Transom Eagle Eye (TEE), and the training checkpoint asynchronous
access automatic fault tolerance and recovery technology named Transom
Checkpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks,
while TEE is responsible for task monitoring and anomaly reporting. TEE detects
training anomalies and reports them to TOL, who automatically enters the fault
tolerance strategy to eliminate abnormal nodes and restart the training task.
And the asynchronous checkpoint saving and loading functionality provided by
TCE greatly shorten the fault tolerance overhead. The experimental results
indicate that TRANSOM significantly enhances the efficiency of large-scale LLM
training on clusters. Specifically, the pre-training time for GPT3-175B has
been reduced by 28%, while checkpoint saving and loading performance have
improved by a factor of 20.
","2023-10-19","2310.10046v1.pdf"
"2310.10047","Yixin Liu","Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, Peter J.
  Liu","Improving Large Language Model Fine-tuning for Solving Math Problems","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite their success in many natural language tasks, solving math problems
remains a significant challenge for large language models (LLMs). A large gap
exists between LLMs' pass-at-one and pass-at-N performance in solving math
problems, suggesting LLMs might be close to finding correct solutions,
motivating our exploration of fine-tuning methods to unlock LLMs' performance.
Using the challenging MATH dataset, we investigate three fine-tuning
strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed
solution for a given math problem; (2) solution-cluster re-ranking, where the
LLM is fine-tuned as a solution verifier/evaluator to choose among generated
candidate solution clusters; (3) multi-task sequential fine-tuning, which
integrates both solution generation and evaluation tasks together efficiently
to enhance the LLM performance. With these methods, we present a thorough
empirical study on a series of PaLM 2 models and find: (1) The quality and
style of the step-by-step solutions used for fine-tuning can make a significant
impact on the model performance; (2) While solution re-ranking and majority
voting are both effective for improving the model performance when used
separately, they can also be used together for an even greater performance
boost; (3) Multi-task fine-tuning that sequentially separates the solution
generation and evaluation tasks can offer improved performance compared with
the solution fine-tuning baseline. Guided by these insights, we design a
fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset
with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the
few-shot performance of pre-trained PaLM 2-L model with majority voting.
","2023-10-17","2310.10047v1.pdf"
"2310.10049","Tao Fan","Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan,
  Qiang Yang","FATE-LLM: A Industrial Grade Federated Learning Framework for Large
  Language Models","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have
exhibited remarkable performances across various tasks in recent years.
However, LLMs face two main challenges in real-world applications. One
challenge is that training LLMs consumes vast computing resources, preventing
LLMs from being adopted by small and medium-sized enterprises with limited
computing resources. Another is that training LLM requires a large amount of
high-quality data, which are often scattered among enterprises. To address
these challenges, we propose FATE-LLM, an industrial-grade federated learning
framework for large language models. FATE-LLM (1) facilitates federated
learning for large language models (coined FedLLM); (2) promotes efficient
training of FedLLM using parameter-efficient fine-tuning methods; (3) protects
the intellectual property of LLMs; (4) preserves data privacy during training
and inference through privacy-preserving mechanisms. We release the code of
FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research
of FedLLM and enable a broad range of industrial applications.
","2023-10-17","2310.10049v1.pdf"
"2310.10050","Melissa Dell","Tom Bryan, Jacob Carlson, Abhishek Arora, Melissa Dell","EfficientOCR: An Extensible, Open-Source Package for Efficiently
  Digitizing World Knowledge","","","","","cs.CV cs.CL econ.GN q-fin.EC","http://creativecommons.org/licenses/by/4.0/","  Billions of public domain documents remain trapped in hard copy or lack an
accurate digitization. Modern natural language processing methods cannot be
used to index, retrieve, and summarize their texts; conduct computational
textual analyses; or extract information for statistical analyses, and these
texts cannot be incorporated into language model training. Given the diversity
and sheer quantity of public domain texts, liberating them at scale requires
optical character recognition (OCR) that is accurate, extremely cheap to
deploy, and sample-efficient to customize to novel collections, languages, and
character sets. Existing OCR engines, largely designed for small-scale
commercial applications in high resource languages, often fall short of these
requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets
both the computational and sample efficiency requirements for liberating texts
at scale by abandoning the sequence-to-sequence architecture typically used for
OCR, which takes representations from a learned vision model as inputs to a
learned language model. Instead, EffOCR models OCR as a character or word-level
image retrieval problem. EffOCR is cheap and sample efficient to train, as the
model only needs to learn characters' visual appearance and not how they are
used in sequence to form language. Models in the EffOCR model zoo can be
deployed off-the-shelf with only a few lines of code. Importantly, EffOCR also
allows for easy, sample efficient customization with a simple model training
interface and minimal labeling requirements due to its sample efficiency. We
illustrate the utility of EffOCR by cheaply and accurately digitizing 20
million historical U.S. newspaper scans, evaluating zero-shot performance on
randomly selected documents from the U.S. National Archives, and accurately
digitizing Japanese documents for which all other OCR solutions failed.
","2023-10-17","2310.10050v1.pdf"
"2310.10054","Jongwoo Ko","Jongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang,
  Euijai Ahn, Se-Young Yun","NASH: A Simple Unified Framework of Structured Pruning for Accelerating
  Encoder-Decoder Language Models","Findings of the Association for Computational Linguistics: EMNLP 2023","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Structured pruning methods have proven effective in reducing the model size
and accelerating inference speed in various network architectures such as
Transformers. Despite the versatility of encoder-decoder models in numerous NLP
tasks, the structured pruning methods on such models are relatively less
explored compared to encoder-only models. In this study, we investigate the
behavior of the structured pruning of the encoder-decoder models in the
decoupled pruning perspective of the encoder and decoder component,
respectively. Our findings highlight two insights: (1) the number of decoder
layers is the dominant factor of inference speed, and (2) low sparsity in the
pruned encoder network enhances generation quality. Motivated by these
findings, we propose a simple and effective framework, NASH, that narrows the
encoder and shortens the decoder networks of encoder-decoder models. Extensive
experiments on diverse generation and inference tasks validate the
effectiveness of our method in both speedup and output quality.
","2023-10-17","2310.10054v1.pdf"
"2310.10062","Alon Jacovi","Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd
  Bohnet, Mor Geva","A Comprehensive Evaluation of Tool-Assisted Generation Strategies","Accepted to EMNLP 2023 Findings","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  A growing area of research investigates augmenting language models with tools
(e.g., search engines, calculators) to overcome their shortcomings (e.g.,
missing or incorrect knowledge, incorrect logical inferences). Various few-shot
tool-usage strategies have been proposed. However, there is no systematic and
fair comparison across different strategies, or between these strategies and
strong baselines that do not leverage tools. We conduct an extensive empirical
analysis, finding that (1) across various datasets, example difficulty levels,
and models, strong no-tool baselines are competitive to tool-assisted
strategies, implying that effectively using tools with in-context
demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval
tasks, strategies that *refine* incorrect outputs with tools outperform
strategies that retrieve relevant information *ahead of* or *during
generation*; (3) tool-assisted strategies are expensive in the number of tokens
they require to work -- incurring additional costs by orders of magnitude --
which does not translate into significant improvement in performance. Overall,
our findings suggest that few-shot tool integration is still an open challenge,
emphasizing the need for comprehensive evaluations of future strategies to
accurately assess their *benefits* and *costs*.
","2023-10-17","2310.10062v1.pdf"
"2310.10072","Ehsan  Latif","Ehsan Latif and Xiaoming Zhai","Fine-tuning ChatGPT for Automatic Scoring","Submitted to Computers and Education: Artificial Intelligence","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for
automatically scoring student written constructed responses using example
assessment tasks in science education. Recent studies on OpenAI's generative
model GPT-3.5 proved its superiority in predicting the natural language with
high accuracy and human-like responses. GPT-3.5 has been trained over enormous
online language materials such as journals and Wikipedia; therefore, more than
direct usage of pre-trained GPT-3.5 is required for automatic scoring as
students utilize a different language than trained material. These imply that a
domain-specific model, fine-tuned over data for specific tasks, can enhance
model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks
with a diverse dataset of middle-school and high-school student responses and
expert scoring. The six tasks comprise two multi-label and four multi-class
assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the
fine-tuned state-of-the-art Google's generated language model, BERT. The
results show that in-domain training corpora constructed from science questions
and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5
shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean
= 9.15, SD = 0.042) for the six tasks, p =0.001 < 0.05. Specifically, for
multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5
achieved significantly higher scoring accuracy than BERT across all the labels,
with the second item achieving a 7.1% increase. The average scoring increase
for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our
study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring
of student responses on domain-specific data in education with high accuracy.
We have released fine-tuned models for public use and community engagement.
","2023-10-17","2310.10072v1.pdf"
"2310.10076","Keita Saito","Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto","Verbosity Bias in Preference Labeling by Large Language Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, Large Language Models (LLMs) have witnessed a remarkable
surge in prevalence, altering the landscape of natural language processing and
machine learning. One key factor in improving the performance of LLMs is
alignment with humans achieved with Reinforcement Learning from Human Feedback
(RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies
are investigating the replacement of human feedback with feedback from other
LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the
biases that come along with evaluating LLMs with other LLMs and take a closer
look into verbosity bias -- a bias where LLMs sometimes prefer more verbose
answers even if they have similar qualities. We see that in our problem
setting, GPT-4 prefers longer answers more than humans. We also propose a
metric to measure this bias.
","2023-10-17","2310.10076v1.pdf"
"2310.10080","Qianli Ma","Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang
  You and Hongxia Yang","Let's reward step by step: Step-Level reward model as the Navigators for
  Reasoning","","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent years have seen considerable advancements in multi-step reasoning with
Large Language Models (LLMs). The previous studies have elucidated the merits
of integrating feedback or search mechanisms during model inference to improve
the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically
furnishes LLMs with step-by-step feedback during the training phase, akin to
Proximal Policy Optimization (PPO) or reject sampling. Our objective is to
examine the efficacy of PRM in the inference phase to help discern the optimal
solution paths for multi-step tasks such as mathematical reasoning and code
generation. To this end, we propose a heuristic greedy search algorithm that
employs the step-level feedback from PRM to optimize the reasoning pathways
explored by LLMs. This tailored PRM demonstrated enhanced results compared to
the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH.
Additionally, to explore the versatility of our approach, we develop a novel
method to automatically generate step-level reward dataset for coding tasks and
observed similar improved performance in the code generation tasks. Thus
highlighting the robust nature of our reward-model-based approach to inference
for reasoning tasks.
","2023-10-17","2310.10080v1.pdf"
"2310.10083","Issey Sukeda","Issey Sukeda, Masahiro Suzuki, Hiroki Sakaji, Satoshi Kodera","JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models
  using Instruction-tuning","8 pages, 1 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the ongoing wave of impact driven by large language models (LLMs) like
ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial
research frontier. Since mainstream LLMs tend to be designed for
general-purpose applications, constructing a medical LLM through domain
adaptation is a huge challenge. While instruction-tuning is used to fine-tune
some LLMs, its precise roles in domain adaptation remain unknown. Here we show
the contribution of LoRA-based instruction-tuning to performance in Japanese
medical question-answering tasks. In doing so, we employ a multifaceted
evaluation for multiple-choice questions, including scoring based on ""Exact
match"" and ""Gestalt distance"" in addition to the conventional accuracy. Our
findings suggest that LoRA-based instruction-tuning can partially incorporate
domain-specific knowledge into LLMs, with larger models demonstrating more
pronounced effects. Furthermore, our results underscore the potential of
adapting English-centric models for Japanese applications in domain adaptation,
while also highlighting the persisting limitations of Japanese-centric models.
This initiative represents a pioneering effort in enabling medical institutions
to fine-tune and operate models without relying on external services.
","2023-10-17","2310.10083v1.pdf"
"2310.10103","Dhruv Shah","Dhruv Shah, Michael Equi, Blazej Osinski, Fei Xia, Brian Ichter,
  Sergey Levine","Navigation with Large Language Models: Semantic Guesswork as a Heuristic
  for Planning","Videos, code, and an interactive Colab notebook that runs in your
  browser https://sites.google.com/view/lfg-nav/","","","","cs.RO cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Navigation in unfamiliar environments presents a major challenge for robots:
while mapping and planning techniques can be used to build up a representation
of the world, quickly discovering a path to a desired goal in unfamiliar
settings with such methods often requires lengthy mapping and exploration.
Humans can rapidly navigate new environments, particularly indoor environments
that are laid out logically, by leveraging semantics -- e.g., a kitchen often
adjoins a living room, an exit sign indicates the way out, and so forth.
Language models can provide robots with such knowledge, but directly using
language models to instruct a robot how to reach some destination can also be
impractical: while language models might produce a narrative about how to reach
some goal, because they are not grounded in real-world observations, this
narrative might be arbitrarily wrong. Therefore, in this paper we study how the
``semantic guesswork'' produced by language models can be utilized as a guiding
heuristic for planning algorithms. Our method, Language Frontier Guide (LFG),
uses the language model to bias exploration of novel real-world environments by
incorporating the semantic knowledge stored in language models as a search
heuristic for planning with either topological or metric maps. We evaluate LFG
in challenging real-world environments and simulated benchmarks, outperforming
uninformed exploration and other ways of using language models.
","2023-10-17","2310.10103v1.pdf"
"2310.10108","An Zhang","An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang,
  Tat-Seng Chua","On Generative Agents in Recommendation","30 pages,14 figures","","","","cs.IR cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recommender systems are the cornerstone of today's information dissemination,
yet a disconnect between offline metrics and online performance greatly hinders
their development. Addressing this challenge, we envision a recommendation
simulator, capitalizing on recent breakthroughs in human-level intelligence
exhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie
recommendation simulator, leveraging LLM-empowered generative agents equipped
with user profile, memory, and actions modules specifically tailored for the
recommender system. In particular, these agents' profile modules are
initialized using the MovieLens dataset, capturing users' unique tastes and
social traits; memory modules log both factual and emotional memories and are
integrated with an emotion-driven reflection mechanism; action modules support
a wide variety of behaviors, spanning both taste-driven and emotion-driven
actions. Each agent interacts with personalized movie recommendations in a
page-by-page manner, relying on a pre-implemented collaborative filtering-based
recommendation algorithm. We delve into both the capabilities and limitations
of Agent4Rec, aiming to explore an essential research question: to what extent
can LLM-empowered generative agents faithfully simulate the behavior of real,
autonomous humans in recommender systems? Extensive and multi-faceted
evaluations of Agent4Rec highlight both the alignment and deviation between
agents and user-personalized preferences. Beyond mere performance comparison,
we explore insightful experiments, such as emulating the filter bubble effect
and discovering the underlying causal relationships in recommendation tasks.
Our codes are available at https://github.com/LehengTHU/Agent4Rec.
","2023-10-17","2310.10108v1.pdf"
"2310.10118","Arthur Amalvy","Arthur Amalvy (LIA), Vincent Labatut (LIA), Richard Dufour (LS2N -
  \'equipe TALN )","Learning to Rank Context for Named Entity Recognition Using a Synthetic
  Dataset","","The 2023 Conference on Empirical Methods in Natural Language
  Processing, Dec 2023, Singapore, Singapore","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While recent pre-trained transformer-based models can perform named entity
recognition (NER) with great accuracy, their limited range remains an issue
when applied to long documents such as whole novels. To alleviate this issue, a
solution is to retrieve relevant context at the document level. Unfortunately,
the lack of supervision for such a task means one has to settle for
unsupervised approaches. Instead, we propose to generate a synthetic context
retrieval training dataset using Alpaca, an instructiontuned large language
model (LLM). Using this dataset, we train a neural context retriever based on a
BERT model that is able to find relevant context for NER. We show that our
method outperforms several retrieval baselines for the NER task on an English
literary dataset composed of the first chapter of 40 books.
","2023-10-17","2310.10118v1.pdf"
"2310.10125","Xiang Wang","Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yingya Zhang, Changxin Gao,
  Deli Zhao, Nong Sang","Few-shot Action Recognition with Captioning Foundation Models","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transferring vision-language knowledge from pretrained multimodal foundation
models to various downstream tasks is a promising direction. However, most
current few-shot action recognition methods are still limited to a single
visual modality input due to the high cost of annotating additional textual
descriptions. In this paper, we develop an effective plug-and-play framework
called CapFSAR to exploit the knowledge of multimodal models without manually
annotating text. To be specific, we first utilize a captioning foundation model
(i.e., BLIP) to extract visual features and automatically generate associated
captions for input videos. Then, we apply a text encoder to the synthetic
captions to obtain representative text embeddings. Finally, a visual-text
aggregation module based on Transformer is further designed to incorporate
cross-modal spatio-temporal complementary information for reliable few-shot
matching. In this way, CapFSAR can benefit from powerful multimodal knowledge
of pretrained foundation models, yielding more comprehensive classification in
the low-shot regime. Extensive experiments on multiple standard few-shot
benchmarks demonstrate that the proposed CapFSAR performs favorably against
existing methods and achieves state-of-the-art performance. The code will be
made publicly available.
","2023-10-17","2310.10125v1.pdf"
"2310.10134","Bodhisattwa Prasad Majumder","Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen,
  Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, Peter Clark","CLIN: A Continually Learning Language Agent for Rapid Task Adaptation
  and Generalization","Project page: https://allenai.github.io/clin/","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Language agents have shown some ability to interact with an external
environment, e.g., a virtual world such as ScienceWorld, to perform complex
tasks, e.g., growing a plant, without the startup costs of reinforcement
learning. However, despite their zero-shot capabilities, these agents to date
do not continually improve over time beyond performance refinement on a
specific task. Here we present CLIN, the first language-based agent to achieve
this, so that it continually improves over multiple trials, including when both
the environment and task are varied, and without requiring parameter updates.
Our approach is to use a persistent, dynamic, textual memory centered on causal
abstractions (rather than general ""helpful hints"") that is regularly updated
after each trial so that the agent gradually learns useful knowledge for new
trials. In the ScienceWorld benchmark, CLIN is able to continually improve on
repeated trials on the same task and environment, outperforming
state-of-the-art reflective language agents like Reflexion by 23 absolute
points. CLIN can also transfer its learning to new environments (or new tasks),
improving its zero-shot performance by 4 points (13 for new tasks) and can
further improve performance there through continual memory updates, enhancing
performance by an additional 17 points (7 for new tasks). This suggests a new
architecture for agents built on frozen models that can still continually and
rapidly improve over time.
","2023-10-17","2310.10134v1.pdf"
"2310.10141","Radha Chitta","Adam Roegiest and Radha Chitta and Jonathan Donnelly and Maya Lash and
  Alexandra Vtyurina and Fran\c{c}ois Longtin","A Search for Prompts: Generating Structured Answers from Contracts","18 pages, 5 figures","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  In many legal processes being able to action on the concrete implication of a
legal question can be valuable to automating human review or signalling certain
conditions (e.g., alerts around automatic renewal). To support such tasks, we
present a form of legal question answering that seeks to return one (or more)
fixed answers for a question about a contract clause. After showing that
unstructured generative question answering can have questionable outcomes for
such a task, we discuss our exploration methodology for legal question
answering prompts using OpenAI's \textit{GPT-3.5-Turbo} and provide a summary
of insights.
  Using insights gleaned from our qualitative experiences, we compare our
proposed template prompts against a common semantic matching approach and find
that our prompt templates are far more accurate despite being less reliable in
the exact response return. With some additional tweaks to prompts and the use
of in-context learning, we are able to further improve the performance of our
proposed strategy while maximizing the reliability of responses as best we can.
","2023-10-17","2310.10141v1.pdf"
"2310.10158","Yunfan Shao","Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu","Character-LLM: A Trainable Agent for Role-Playing","To appear at EMNLP 2023; Repo at
  https://github.com/choosewhatulike/trainable-agents","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.
","2023-10-17","2310.10158v1.pdf"
"2310.10159","Xingjian Du","Xingjian Du, Zhesong Yu, Jiaju Lin, Bilei Zhu, Qiuqiang Kong","Joint Music and Language Attention Models for Zero-shot Music Tagging","\begin{keywords} Music tagging, joint music and language attention
  models, Music Foundation Model. \end{keywords}","","","","cs.SD cs.CL eess.AS","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Music tagging is a task to predict the tags of music recordings. However,
previous music tagging research primarily focuses on close-set music tagging
tasks which can not be generalized to new tags. In this work, we propose a
zero-shot music tagging system modeled by a joint music and language attention
(JMLA) model to address the open-set music tagging problem. The JMLA model
consists of an audio encoder modeled by a pretrained masked autoencoder and a
decoder modeled by a Falcon7B. We introduce preceiver resampler to convert
arbitrary length audio into fixed length embeddings. We introduce dense
attention connections between encoder and decoder layers to improve the
information flow between the encoder and decoder layers. We collect a
large-scale music and description dataset from the internet. We propose to use
ChatGPT to convert the raw descriptions into formalized and diverse
descriptions to train the JMLA models. Our proposed JMLA system achieves a
zero-shot audio tagging accuracy of $ 64.82\% $ on the GTZAN dataset,
outperforming previous zero-shot systems and achieves comparable results to
previous systems on the FMA and the MagnaTagATune datasets.
","2023-10-17","2310.10159v1.pdf"
"2310.10169","Guanting Dong","Guanting Dong, Tingfeng Hui, Zhuoma GongQue, Jinxu Zhao, Daichi Guo,
  Gang Zhao, Keqing He, Weiran Xu","DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy
  Slot Filling Task","Findings of EMNLP 2023 (Short Paper)","","","","cs.CL cs.AI cs.IR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, prompt-based generative frameworks have shown impressive
capabilities in sequence labeling tasks. However, in practical dialogue
scenarios, relying solely on simplistic templates and traditional corpora
presents a challenge for these methods in generalizing to unknown input
perturbations. To address this gap, we propose a multi-task demonstration based
generative framework for noisy slot filling, named DemoNSF. Specifically, we
introduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask
(RM), and hybrid discrimination (HD), to implicitly capture semantic structural
information of input perturbations at different granularities. In the
downstream main task, we design a noisy demonstration construction strategy for
the generative framework, which explicitly incorporates task-specific
information and perturbed distribution during training and inference.
Experiments on two benchmarks demonstrate that DemoNSF outperforms all baseline
methods and achieves strong generalization. Further analysis provides empirical
guidance for the practical application of generative frameworks. Our code is
released at https://github.com/dongguanting/Demo-NSF.
","2023-10-17","2310.10169v1.pdf"
"2310.10180","Jing Xiong","Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying
  Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng,
  Xiaodan Liang, Ming Zhang, Qun Liu","TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative
  Language Models","Accepted by EMNLP 2023. Code is available at
  https://github.com/menik1126/TRIGO","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Automated theorem proving (ATP) has become an appealing domain for exploring
the reasoning ability of the recent successful generative language models.
However, current ATP benchmarks mainly focus on symbolic inference, but rarely
involve the understanding of complex number combination reasoning. In this
work, we propose TRIGO, an ATP benchmark that not only requires a model to
reduce a trigonometric expression with step-by-step proofs but also evaluates a
generative LM's reasoning ability on formulas and its capability to manipulate,
group, and factor number terms. We gather trigonometric expressions and their
reduced forms from the web, annotate the simplification process manually, and
translate it into the Lean formal language system. We then automatically
generate additional examples from the annotated samples to expand the dataset.
Furthermore, we develop an automatic generator based on Lean-Gym to create
dataset splits of varying difficulties and distributions in order to thoroughly
analyze the model's generalization ability. Our extensive experiments show our
proposed TRIGO poses a new challenge for advanced generative LM's including
GPT-4 which is pre-trained on a considerable amount of open-source formal
theorem-proving language data, and provide a new tool to study the generative
LM's ability on both formal and mathematical reasoning.
","2023-10-25","2310.10180v1.pdf"
"2310.10190","Shuo Sun","Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen,
  Jian Su","Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco
  vs Bard vs ChatGPT -- A Text-to-SQL Parsing Comparison","Findings of EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The success of ChatGPT has ignited an AI race, with researchers striving to
develop new large language models (LLMs) that can match or surpass the language
understanding and generation abilities of commercial ones. In recent times, a
number of models have emerged, claiming performance near that of GPT-3.5 or
GPT-4 through various instruction-tuning methods. As practitioners of
Text-to-SQL parsing, we are grateful for their valuable contributions to
open-source research. However, it is important to approach these claims with a
sense of scrutiny and ascertain the actual effectiveness of these models.
Therefore, we pit six popular large language models against each other,
systematically evaluating their Text-to-SQL parsing capability on nine
benchmark datasets with five different prompting strategies, covering both
zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell
significantly short of the performance achieved by closed-source models like
GPT-3.5, highlighting the need for further work to bridge the performance gap
between these models.
","2023-10-17","2310.10190v1.pdf"
"2310.10191","Yuji Zhang","Yuji Zhang, Jing Li, Wenjie Li","VIBE: Topic-Driven Temporal Adaptation for Twitter Classification","accepted by EMNLP 2023","The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Language features are evolving in real-world social media, resulting in the
deteriorating performance of text classification in dynamics. To address this
challenge, we study temporal adaptation, where models trained on past data are
tested in the future. Most prior work focused on continued pretraining or
knowledge updating, which may compromise their performance on noisy social
media data. To tackle this issue, we reflect feature change via modeling latent
topic evolution and propose a novel model, VIBE: Variational Information
Bottleneck for Evolutions. Concretely, we first employ two Information
Bottleneck (IB) regularizers to distinguish past and future topics. Then, the
distinguished topics work as adaptive features via multi-task training with
timestamp and class label prediction. In adaptive learning, VIBE utilizes
retrieved unlabeled data from online streams created posterior to training data
time. Substantial Twitter experiments on three classification tasks show that
our model, with only 3% of data, significantly outperforms previous
state-of-the-art continued-pretraining methods.
","2023-10-20","2310.10191v1.pdf"
"2310.10195","Kai Lv","Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu","AdaLomo: Low-memory Optimization with Adaptive Learning Rate","Fix some typo","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models have achieved remarkable success, but their extensive
parameter size necessitates substantial memory for training, thereby setting a
high threshold. While the recently proposed low-memory optimization (LOMO)
reduces memory footprint, its optimization technique, akin to stochastic
gradient descent, is sensitive to hyper-parameters and exhibits suboptimal
convergence, failing to match the performance of the prevailing optimizer for
large language models, AdamW. Through empirical analysis of the Adam optimizer,
we found that, compared to momentum, the adaptive learning rate is more
critical for bridging the gap. Building on this insight, we introduce the
low-memory optimization with adaptive learning rate (AdaLomo), which offers an
adaptive learning rate for each parameter. To maintain memory efficiency, we
employ non-negative matrix factorization for the second-order moment estimation
in the optimizer state. Additionally, we suggest the use of a grouped update
normalization to stabilize convergence. Our experiments with instruction-tuning
and further pre-training demonstrate that AdaLomo achieves results on par with
AdamW, while significantly reducing memory requirements, thereby lowering the
hardware barrier to training large language models.
","2023-10-24","2310.10195v1.pdf"
"2310.10196","Ming Jin","Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue
  Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, Shirui Pan, Vincent S.
  Tseng, Yu Zheng, Lei Chen, Hui Xiong","Large Models for Time Series and Spatio-Temporal Data: A Survey and
  Outlook","Ongoing work; 24 pages, 3 figures, 3 tables; Github page:
  https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Temporal data, notably time series and spatio-temporal data, are prevalent in
real-world applications. They capture dynamic system measurements and are
produced in vast quantities by both physical and virtual sensors. Analyzing
these data types is vital to harnessing the rich information they encompass and
thus benefits a wide range of downstream tasks. Recent advances in large
language and other foundational models have spurred increased use of these
models in time series and spatio-temporal data mining. Such methodologies not
only enable enhanced pattern recognition and reasoning across diverse domains
but also lay the groundwork for artificial general intelligence capable of
comprehending and processing common temporal data. In this survey, we offer a
comprehensive and up-to-date review of large models tailored (or adapted) for
time series and spatio-temporal data, spanning four key facets: data types,
model categories, model scopes, and application areas/tasks. Our objective is
to equip practitioners with the knowledge to develop applications and further
research in this underexplored domain. We primarily categorize the existing
literature into two major clusters: large models for time series analysis
(LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further
classify research based on model scopes (i.e., general vs. domain-specific) and
application areas/tasks. We also provide a comprehensive collection of
pertinent resources, including datasets, model assets, and useful tools,
categorized by mainstream applications. This survey coalesces the latest
strides in large model-centric research on time series and spatio-temporal
data, underscoring the solid foundations, current advances, practical
applications, abundant resources, and future research opportunities.
","2023-10-23","2310.10196v1.pdf"
"2310.10207","Rujie Wu","Rujie Wu, Xiaojian Ma, Qing Li, Wei Wang, Zhenliang Zhang, Song-Chun
  Zhu, Yizhou Wang","Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in
  the Real World","Project page: https://joyjayng.github.io/Bongard-OpenWorld.github.io","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world
few-shot reasoning for machine vision. It originates from the classical Bongard
Problems (BPs): Given two sets of images (positive and negative), the model
needs to identify the set that query images belong to by inducing the visual
concepts, which is exclusively depicted by images from the positive set. Our
benchmark inherits the few-shot concept induction of the original BPs while
adding the two novel layers of challenge: 1) open-world free-form concepts, as
the visual concepts in Bongard-OpenWorld are unique compositions of terms from
an open vocabulary, ranging from object categories to abstract visual
attributes and commonsense factual knowledge; 2) real-world images, as opposed
to the synthetic diagrams used by many counterparts. In our exploration,
Bongard-OpenWorld already imposes a significant challenge to current few-shot
reasoning algorithms. We further investigate to which extent the recently
introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can
solve our task, by directly probing VLMs, and combining VLMs and LLMs in an
interactive reasoning scheme. We even designed a neuro-symbolic reasoning
approach that reconciles LLMs & VLMs with logical reasoning to emulate the
human problem-solving process for Bongard Problems. However, none of these
approaches manage to close the human-machine gap, as the best learner achieves
64% accuracy while human participants easily reach 91%. We hope
Bongard-OpenWorld can help us better understand the limitations of current
visual intelligence and facilitate future research on visual agents with
stronger few-shot visual reasoning capabilities.
","2023-10-17","2310.10207v1.pdf"
"2310.10221","Zijun Long","Zijun Long and George Killick and Richard McCreadie and Gerardo Aragon
  Camarasa","RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language
  Models","","","","","cs.RO cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Robotic vision applications often necessitate a wide range of visual
perception tasks, such as object detection, segmentation, and identification.
While there have been substantial advances in these individual tasks,
integrating specialized models into a unified vision pipeline presents
significant engineering challenges and costs. Recently, Multimodal Large
Language Models (MLLMs) have emerged as novel backbones for various downstream
tasks. We argue that leveraging the pre-training capabilities of MLLMs enables
the creation of a simplified framework, thus mitigating the need for
task-specific encoders. Specifically, the large-scale pretrained knowledge in
MLLMs allows for easier fine-tuning to downstream robotic vision tasks and
yields superior performance. We introduce the RoboLLM framework, equipped with
a BEiT-3 backbone, to address all visual perception tasks in the ARMBench
challenge-a large-scale robotic manipulation dataset about real-world warehouse
scenarios. RoboLLM not only outperforms existing baselines but also
substantially reduces the engineering burden associated with model selection
and tuning. The source code is publicly available at
https://github.com/longkukuhi/armbench.
","2023-10-17","2310.10221v1.pdf"
"2310.10226","Huayang Li","Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier,
  Taro Watanabe, Yixuan Su","Repetition In Repetition Out: Towards Understanding Neural Text
  Degeneration from the Data Perspective","Accepted to NeurIPS 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  There are a number of diverging hypotheses about the neural text degeneration
problem, i.e., generating repetitive and dull loops, which makes this problem
both interesting and confusing. In this work, we aim to advance our
understanding by presenting a straightforward and fundamental explanation from
the data perspective. Our preliminary investigation reveals a strong
correlation between the degeneration issue and the presence of repetitions in
training data. Subsequent experiments also demonstrate that by selectively
dropping out the attention to repetitive words in training data, degeneration
can be significantly minimized. Furthermore, our empirical analysis illustrates
that prior works addressing the degeneration issue from various standpoints,
such as the high-inflow words, the likelihood objective, and the
self-reinforcement phenomenon, can be interpreted by one simple explanation.
That is, penalizing the repetitions in training data is a common and
fundamental factor for their effectiveness. Moreover, our experiments reveal
that penalizing the repetitions in training data remains critical even when
considering larger model sizes and instruction tuning.
","2023-10-17","2310.10226v1.pdf"
"2310.10260","Adel Ammar","Adel Ammar, Anis Koubaa, Bilel Benjdira, Omar Najar, Serry Sibaee","Prediction of Arabic Legal Rulings using Large Language Models","26 pages","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In the intricate field of legal studies, the analysis of court decisions is a
cornerstone for the effective functioning of the judicial system. The ability
to predict court outcomes helps judges during the decision-making process and
equips lawyers with invaluable insights, enhancing their strategic approaches
to cases. Despite its significance, the domain of Arabic court analysis remains
under-explored. This paper pioneers a comprehensive predictive analysis of
Arabic court decisions on a dataset of 10,813 commercial court real cases,
leveraging the advanced capabilities of the current state-of-the-art large
language models. Through a systematic exploration, we evaluate three prevalent
foundational models (LLaMA-7b, JAIS-13b, and GPT3.5-turbo) and three training
paradigms: zero-shot, one-shot, and tailored fine-tuning. Besides, we assess
the benefit of summarizing and/or translating the original Arabic input texts.
This leads to a spectrum of 14 model variants, for which we offer a granular
performance assessment with a series of different metrics (human assessment,
GPT evaluation, ROUGE, and BLEU scores). We show that all variants of LLaMA
models yield limited performance, whereas GPT-3.5-based models outperform all
other models by a wide margin, surpassing the average score of the dedicated
Arabic-centric JAIS model by 50%. Furthermore, we show that all scores except
human evaluation are inconsistent and unreliable for assessing the performance
of large language models on court decision predictions. This study paves the
way for future research, bridging the gap between computational linguistics and
Arabic legal analytics.
","2023-10-17","2310.10260v1.pdf"
"2310.10262","Uri Hasson","Natalia Flechas Manrique, Wanqian Bao, Aurelie Herbelot, Uri Hasson","Enhancing Interpretability using Human Similarity Judgements to Prune
  Word Embeddings","Accepted for presentation at the BlackboxNLP workshop at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Interpretability methods in NLP aim to provide insights into the semantics
underlying specific system architectures. Focusing on word embeddings, we
present a supervised-learning method that, for a given domain (e.g., sports,
professions), identifies a subset of model features that strongly improve
prediction of human similarity judgments. We show this method keeps only 20-40%
of the original embeddings, for 8 independent semantic domains, and that it
retains different feature sets across domains. We then present two approaches
for interpreting the semantics of the retained features. The first obtains the
scores of the domain words (co-hyponyms) on the first principal component of
the retained embeddings, and extracts terms whose co-occurrence with the
co-hyponyms tracks these scores' profile. This analysis reveals that humans
differentiate e.g. sports based on how gender-inclusive and international they
are. The second approach uses the retained sets as variables in a probing task
that predicts values along 65 semantically annotated dimensions for a dataset
of 535 words. The features retained for professions are best at predicting
cognitive, emotional and social dimensions, whereas features retained for
fruits or vegetables best predict the gustation (taste) dimension. We discuss
implications for alignment between AI systems and human knowledge.
","2023-10-17","2310.10262v1.pdf"
"2310.10266","Zhongtao Jiang","Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, Kang Liu","Generative Calibration for In-context Learning","Findings of EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As one of the most exciting features of large language models (LLMs),
in-context learning is a mixed blessing. While it allows users to
fast-prototype a task solver with only a few training examples, the performance
is generally sensitive to various configurations of the prompt such as the
choice or order of the training examples. In this paper, we for the first time
theoretically and empirically identify that such a paradox is mainly due to the
label shift of the in-context model to the data distribution, in which LLMs
shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.
With this understanding, we can simply calibrate the in-context predictive
distribution by adjusting the label marginal, which is estimated via
Monte-Carlo sampling over the in-context model, i.e., generation of LLMs. We
call our approach as generative calibration. We conduct exhaustive experiments
with 12 text classification tasks and 12 LLMs scaling from 774M to 33B,
generally find that the proposed method greatly and consistently outperforms
the ICL as well as state-of-the-art calibration methods, by up to 27% absolute
in macro-F1. Meanwhile, the proposed method is also stable under different
prompt configurations.
","2023-10-17","2310.10266v1.pdf"
"2310.10275","Hanna Abi Akl","Hanna Abi Akl","A ML-LLM pairing for better code comment classification","10 pages, 2 figures, 2 tables, accepted for the Information Retrieval
  in Software Engineering track at the Forum for Information Retrieval
  Evaluation 2023","Information Retrieval in Software Engineering track at Forum for
  Information Retrieval Evaluation 2023","","","cs.SE cs.AI","http://creativecommons.org/licenses/by/4.0/","  The ""Information Retrieval in Software Engineering (IRSE)"" at FIRE 2023
shared task introduces code comment classification, a challenging task that
pairs a code snippet with a comment that should be evaluated as either useful
or not useful to the understanding of the relevant code. We answer the code
comment classification shared task challenge by providing a two-fold
evaluation: from an algorithmic perspective, we compare the performance of
classical machine learning systems and complement our evaluations from a
data-driven perspective by generating additional data with the help of large
language model (LLM) prompting to measure the potential increase in
performance. Our best model, which took second place in the shared task, is a
Neural Network with a Macro-F1 score of 88.401% on the provided seed data and a
1.5% overall increase in performance on the data generated by the LLM.
","2023-10-17","2310.10275v1.pdf"
"2310.10285","Weixiao Zhou","Weixiao Zhou, Gengyao Li, Xianfu Cheng, Xinnian Liang, Junnan Zhu,
  Feifei Zhai and Zhoujun Li","Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario
  Multi-Domain Dialogue Summarization","Accepted to EMNLP 2023 findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dialogue summarization involves a wide range of scenarios and domains.
However, existing methods generally only apply to specific scenarios or
domains. In this study, we propose a new pre-trained model specifically
designed for multi-scenario multi-domain dialogue summarization. It adopts a
multi-stage pre-training strategy to reduce the gap between the pre-training
objective and fine-tuning objective. Specifically, we first conduct
domain-aware pre-training using large-scale multi-scenario multi-domain
dialogue data to enhance the adaptability of our pre-trained model. Then, we
conduct task-oriented pre-training using large-scale multi-scenario
multi-domain ""dialogue-summary"" parallel data annotated by ChatGPT to enhance
the dialogue summarization ability of our pre-trained model. Experimental
results on three dialogue summarization datasets from different scenarios and
domains indicate that our pre-trained model significantly outperforms previous
state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.
","2023-10-17","2310.10285v1.pdf"
"2310.10299","Matteo Zecchin","Matteo Zecchin, Sangwoo Park, Osvaldo Simeone","Forking Uncertainties: Reliable Prediction and Model Predictive Control
  with Sequence Models via Conformal Risk Control","","","","","cs.IT cs.AI cs.LG math.IT","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In many real-world problems, predictions are leveraged to monitor and control
cyber-physical systems, demanding guarantees on the satisfaction of reliability
and safety requirements. However, predictions are inherently uncertain, and
managing prediction uncertainty presents significant challenges in environments
characterized by complex dynamics and forking trajectories. In this work, we
assume access to a pre-designed probabilistic implicit or explicit sequence
model, which may have been obtained using model-based or model-free methods. We
introduce probabilistic time series-conformal risk prediction (PTS-CRC), a
novel post-hoc calibration procedure that operates on the predictions produced
by any pre-designed probabilistic forecaster to yield reliable error bars. In
contrast to existing art, PTS-CRC produces predictive sets based on an ensemble
of multiple prototype trajectories sampled from the sequence model, supporting
the efficient representation of forking uncertainties. Furthermore, unlike the
state of the art, PTS-CRC can satisfy reliability definitions beyond coverage.
This property is leveraged to devise a novel model predictive control (MPC)
framework that addresses open-loop and closed-loop control problems under
general average constraints on the quality or safety of the control policy. We
experimentally validate the performance of PTS-CRC prediction and control by
studying a number of use cases in the context of wireless networking. Across
all the considered tasks, PTS-CRC predictors are shown to provide more
informative predictive sets, as well as safe control policies with larger
returns.
","2023-10-17","2310.10299v1.pdf"
"2310.10310","Manon Reusens","Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt,
  Bart Baesens","Investigating Bias in Multilingual Language Models: Cross-Lingual
  Transfer of Debiasing Techniques","Accepted to EMNLP 2023 main conference","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper investigates the transferability of debiasing techniques across
different languages within multilingual models. We examine the applicability of
these techniques in English, French, German, and Dutch. Using multilingual BERT
(mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is
not only feasible but also yields promising results. Surprisingly, our findings
reveal no performance disadvantages when applying these techniques to
non-English languages. Using translations of the CrowS-Pairs dataset, our
analysis identifies SentenceDebias as the best technique across different
languages, reducing bias in mBERT by an average of 13%. We also find that
debiasing techniques with additional pretraining exhibit enhanced cross-lingual
effectiveness for the languages included in the analyses, particularly in
lower-resource languages. These novel insights contribute to a deeper
understanding of bias mitigation in multilingual language models and provide
practical guidance for debiasing techniques in different language contexts.
","2023-10-17","2310.10310v1.pdf"
"2310.10315","Alberto Marchisio","Kamila Zaman and Alberto Marchisio and Muhammad Abdullah Hanif and
  Muhammad Shafique","A Survey on Quantum Machine Learning: Current Trends, Challenges,
  Opportunities, and the Road Ahead","","","","","quant-ph cs.LG","http://creativecommons.org/licenses/by/4.0/","  Quantum Computing (QC) claims to improve the efficiency of solving complex
problems, compared to classical computing. When QC is applied to Machine
Learning (ML) applications, it forms a Quantum Machine Learning (QML) system.
After discussing the basic concepts of QC and its advantages over classical
computing, this paper reviews the key aspects of QML in a comprehensive manner.
We discuss different QML algorithms and their domain applicability, quantum
datasets, hardware technologies, software tools, simulators, and applications.
In this survey, we provide valuable information and resources for readers to
jumpstart into the current state-of-the-art techniques in the QML field.
","2023-10-17","2310.10315v1.pdf"
"2310.10317","Cen Wang","Cen Wang, Guang Zeng, Xinyu Wen, Yuhui He, Wei Luo, Shiwei Chen,
  Shiheng Liang, Yue Zhang","Stochastic spin-orbit-torque synapse and its application in uncertainty
  quantification","","","","","physics.app-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Stochasticity plays a significant role in the low-power operation of a
biological neural network. In an artificial neural network (ANN), stochasticity
also contributes to critical functions such as the uncertainty quantification
(UQ) for estimating the probability for the correctness of prediction. This UQ
is vital for cutting-edge applications, including medical diagnostics,
autopilots, and large language models. Thanks to high computing velocity and
low dissipation, a spin-orbit-torque (SOT) device exhibits significant
potential for implementing the UQ. However, up until now, the application of UQ
for stochastic SOT devices remains unexplored. In this study, based on
SOT-induced stochastic magnetic domain wall (DW) motion with varying velocity,
we fabricated an SOT synapse that could emulate stochastic weight update
following the Spike-Timing-Dependent-Plasticity (STDP) rule. Furthermore, we
set up a stochastic Spiking-Neural-Network (SNN), which, when compared to its
deterministic counterpart, demonstrates a clear advantage in quantifying
uncertainty for diagnosing the type of breast tumor (benign or malignant).
","2023-10-17","2310.10317v1.pdf"
"2310.10322","Jun-Yu Ma","Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, Cong Liu","Untying the Reversal Curse via Bidirectional Language Model Editing","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent studies have demonstrated that large language models (LLMs) store
massive factual knowledge within their parameters. But existing LLMs are prone
to hallucinate unintended text due to false or outdated knowledge. Since
retraining LLMs is resource intensive, there has been a growing interest in the
concept of model editing. Despite the emergence of benchmarks and approaches,
these unidirectional editing and evaluation have failed to explore the reversal
curse. Intuitively, if ""The capital of France is"" is edited to be a counterfact
""London"" within a model, then it should be able to naturally reason and recall
the reverse fact, i.e., ""London is the capital of"" followed by ""France"" instead
of ""England"". In this paper, we study bidirectional language model editing,
aiming to provide rigorous model editing evaluation to assess if edited LLMs
can recall the editing knowledge bidirectionally. A new evaluation metric of
reversibility is introduced, and a benchmark dubbed as Bidirectional Assessment
for Knowledge Editing (BAKE) is constructed to evaluate the reversibility of
edited models in recalling knowledge in the reverse direction of editing. We
surprisingly observe that while current editing methods and LLMs can
effectively recall editing facts in the direction of editing, they suffer
serious deficiencies when evaluated in the reverse direction. To mitigate the
reversal curse, a method named Bidirectionally Inversible Relationship moDeling
(BIRD) is proposed. A set of editing objectives that incorporate bidirectional
relationships between subject and object into the updated model weights are
designed. Experiments show that BIRD improves the performance of four
representative LLMs of different sizes via question answering and judgement.
","2023-10-17","2310.10322v1.pdf"
"2310.10325","Marlene Careil","Marl\`ene Careil, Matthew J. Muckley, Jakob Verbeek, St\'ephane
  Lathuili\`ere","Towards image compression with perfect realism at ultra-low bitrates","","","","","cs.CV eess.IV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Image codecs are typically optimized to trade-off bitrate vs, distortion
metrics. At low bitrates, this leads to compression artefacts which are easily
perceptible, even when training with perceptual or adversarial losses. To
improve image quality, and to make it less dependent on the bitrate, we propose
to decode with iterative diffusion models, instead of feed-forward decoders
trained using MSE or LPIPS distortions used in most neural codecs. In addition
to conditioning the model on a vector-quantized image representation, we also
condition on a global textual image description to provide additional context.
We dub our model PerCo for 'perceptual compression', and compare it to
state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The
latter rate is an order of magnitude smaller than those considered in most
prior work. At this bitrate a 512x768 Kodak image is encoded in less than 153
bytes. Despite this ultra-low bitrate, our approach maintains the ability to
reconstruct realistic images. We find that our model leads to reconstructions
with state-of-the-art visual quality as measured by FID and KID, and that the
visual quality is less dependent on the bitrate than previous methods.
","2023-10-17","2310.10325v1.pdf"
"2310.10333","Carolina Camassa","Carolina Camassa","Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers","Accepted at NLLP23","","","","cs.CY cs.CL q-fin.GN","http://creativecommons.org/licenses/by/4.0/","  In the rapidly evolving field of crypto assets, white papers are essential
documents for investor guidance, and are now subject to unprecedented content
requirements under the European Union's Markets in Crypto-Assets Regulation
(MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for
both analyzing these documents and assisting in regulatory compliance. This
paper delivers two contributions to the topic. First, we survey existing
applications of textual analysis to unregulated crypto asset white papers,
uncovering a research gap that could be bridged with interdisciplinary
collaboration. We then conduct an analysis of the changes introduced by MiCAR,
highlighting the opportunities and challenges of integrating NLP within the new
regulatory framework. The findings set the stage for further research, with the
potential to benefit regulators, crypto asset issuers, and investors.
","2023-10-26","2310.10333v3.pdf"
"2310.10338","Frank Fundel","Frank Fundel","Scene Graph Conditioning in Latent Diffusion","Preprint","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Diffusion models excel in image generation but lack detailed semantic control
using text prompts. Additional techniques have been developed to address this
limitation. However, conditioning diffusion models solely on text-based
descriptions is challenging due to ambiguity and lack of structure. In
contrast, scene graphs offer a more precise representation of image content,
making them superior for fine-grained control and accurate synthesis in image
generation models. The amount of image and scene-graph data is sparse, which
makes fine-tuning large diffusion models challenging. We propose multiple
approaches to tackle this problem using ControlNet and Gated Self-Attention. We
were able to show that using out proposed methods it is possible to generate
images from scene graphs with much higher quality, outperforming previous
methods. Our source code is publicly available on
https://github.com/FrankFundel/SGCond
","2023-10-17","2310.10338v1.pdf"
"2310.10343","Jiayu Yang","Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, Hongdong Li","ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Given a single image of a 3D object, this paper proposes a novel method
(named ConsistNet) that is able to generate multiple images of the same object,
as if seen they are captured from different viewpoints, while the 3D
(multi-view) consistencies among those multiple generated images are
effectively exploited. Central to our method is a multi-view consistency block
which enables information exchange across multiple single-view diffusion
processes based on the underlying multi-view geometry principles. ConsistNet is
an extension to the standard latent diffusion model, and consists of two
sub-modules: (a) a view aggregation module that unprojects multi-view features
into global 3D volumes and infer consistency, and (b) a ray aggregation module
that samples and aggregate 3D consistent features back to each view to enforce
consistency. Our approach departs from previous methods in multi-view image
generation, in that it can be easily dropped-in pre-trained LDMs without
requiring explicit pixel correspondences or depth prediction. Experiments show
that our method effectively learns 3D consistency over a frozen Zero123
backbone and can generate 16 surrounding views of the object within 40 seconds
on a single A100 GPU. Our code will be made available on
https://github.com/JiayuYANG/ConsistNet
","2023-10-17","2310.10343v1.pdf"
"2310.10358","Ananya Singha","Ananya Singha, Jos\'e Cambronero, Sumit Gulwani, Vu Le, Chris Parnin","Tabular Representation, Noisy Operators, and Impacts on Table Structure
  Understanding Tasks in LLMs","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are increasingly applied for tabular tasks using
in-context learning. The prompt representation for a table may play a role in
the LLMs ability to process the table. Inspired by prior work, we generate a
collection of self-supervised structural tasks (e.g. navigate to a cell and
row; transpose the table) and evaluate the performance differences when using 8
formats. In contrast to past work, we introduce 8 noise operations inspired by
real-world messy data and adversarial inputs, and show that such operations can
impact LLM performance across formats for different structural understanding
tasks.
","2023-10-17","2310.10358v1.pdf"
"2310.10383","Haoran Li","Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu,
  Chunkit Chan, Yangqiu Song","Privacy in Large Language Models: Attacks, Defenses and Future
  Directions","","","","","cs.CL cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advancement of large language models (LLMs) has significantly enhanced
the ability to effectively tackle various downstream NLP tasks and unify these
tasks into generative pipelines. On the one hand, powerful language models,
trained on massive textual data, have brought unparalleled accessibility and
usability for both models and users. On the other hand, unrestricted access to
these models can also introduce potential malicious and unintentional privacy
risks. Despite ongoing efforts to address the safety and privacy concerns
associated with LLMs, the problem remains unresolved. In this paper, we provide
a comprehensive analysis of the current privacy attacks targeting LLMs and
categorize them according to the adversary's assumed capabilities to shed light
on the potential vulnerabilities present in LLMs. Then, we present a detailed
overview of prominent defense strategies that have been developed to counter
these privacy attacks. Beyond existing works, we identify upcoming privacy
concerns as LLMs evolve. Lastly, we point out several potential avenues for
future exploration.
","2023-10-17","2310.10383v1.pdf"
"2310.10402","Jianhao Yuan","Jianhao Yuan and Jie Zhang and Shuyang Sun and Philip Torr and Bo Zhao","Real-Fake: Effective Training Data Synthesis Through Distribution
  Matching","Code released at
  (https://github.com/BAAI-DCAI/Training-Data-Synthesis)","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Synthetic training data has gained prominence in numerous learning tasks and
scenarios, offering advantages such as dataset augmentation, generalization
evaluation, and privacy preservation. Despite these benefits, the efficiency of
synthetic data generated by current methodologies remains inferior when
training advanced deep models exclusively, limiting its practical utility. To
address this challenge, we analyze the principles underlying training data
synthesis for supervised learning and elucidate a principled theoretical
framework from the distribution-matching perspective that explicates the
mechanisms governing synthesis efficacy. Through extensive experiments, we
demonstrate the effectiveness of our synthetic data across diverse image
classification tasks, both as a replacement for and augmentation to real
datasets, while also benefits challenging tasks such as out-of-distribution
generalization and privacy preservation.
","2023-10-17","2310.10402v1.pdf"
"2310.10404","Kibum Kim","Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon,
  Donghyun Kim, Chanyoung Park","LLM4SGG: Large Language Model for Weakly Supervised Scene Graph
  Generation","24 pages, Preprint","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently
emerged as an alternative to the fully-supervised approach that heavily relies
on costly annotations. In this regard, studies on WSSGG have utilized image
captions to obtain unlocalized triplets while primarily focusing on grounding
the unlocalized triplets over image regions. However, they have overlooked the
two issues involved in the triplet formation process from the captions: 1)
Semantic over-simplification issue arises when extracting triplets from
captions, where fine-grained predicates in captions are undesirably converted
into coarse-grained predicates, resulting in a long-tailed predicate
distribution, and 2) Low-density scene graph issue arises when aligning the
triplets in the caption with entity/predicate classes of interest, where many
triplets are discarded and not used in training, leading to insufficient
supervision. To tackle the two issues, we propose a new approach, i.e., Large
Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two
issues by leveraging the LLM's in-depth understanding of language and reasoning
ability during the extraction of triplets from captions and alignment of
entity/predicate classes with target data. To further engage the LLM in these
processes, we adopt the idea of Chain-of-Thought and the in-context few-shot
learning strategy. To validate the effectiveness of LLM4SGG, we conduct
extensive experiments on Visual Genome and GQA datasets, showing significant
improvements in both Recall@K and mean Recall@K compared to the
state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is
data-efficient, enabling effective model training with a small amount of
training images.
","2023-10-23","2310.10404v1.pdf"
"2310.10433","Pierre Le Jeune","Pierre Le Jeune","Object Detection in Aerial Images in Scarce Data Regimes","PhD Thesis. Work conducted at L2TI (USPN) and COSE","","","","cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Most contributions on Few-Shot Object Detection (FSOD) evaluate their methods
on natural images only, yet the transferability of the announced performance is
not guaranteed for applications on other kinds of images. We demonstrate this
with an in-depth analysis of existing FSOD methods on aerial images and
observed a large performance gap compared to natural images. Small objects,
more numerous in aerial images, are the cause for the apparent performance gap
between natural and aerial images. As a consequence, we improve FSOD
performance on small objects with a carefully designed attention mechanism. In
addition, we also propose a scale-adaptive box similarity criterion, that
improves the training and evaluation of FSOD methods, particularly for small
objects. We also contribute to generic FSOD with two distinct approaches based
on metric learning and fine-tuning. Impressive results are achieved with the
fine-tuning method, which encourages tackling more complex scenarios such as
Cross-Domain FSOD. We conduct preliminary experiments in this direction and
obtain promising results. Finally, we address the deployment of the detection
models inside COSE's systems. Detection must be done in real-time in extremely
large images (more than 100 megapixels), with limited computation power.
Leveraging existing optimization tools such as TensorRT, we successfully tackle
this engineering challenge.
","2023-10-17","2310.10433v1.pdf"
"2310.10436","Nian Li","Nian Li, Chen Gao, Yong Li, Qingmin Liao","Large Language Model-Empowered Agents for Simulating Macroeconomic
  Activities","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advent of the Web has brought about a paradigm shift in traditional
economics, particularly in the digital economy era, enabling the precise
recording and analysis of individual economic behavior. This has led to a
growing emphasis on data-driven modeling in macroeconomics. In macroeconomic
research, Agent-based modeling (ABM) emerged as an alternative, evolving
through rule-based agents, machine learning-enhanced decision-making, and, more
recently, advanced AI agents. However, the existing works are suffering from
three main challenges when endowing agents with human-like decision-making,
including agent heterogeneity, the influence of macroeconomic trends, and
multifaceted economic factors. Large language models (LLMs) have recently
gained prominence in offering autonomous human-like characteristics. Therefore,
leveraging LLMs in macroeconomic simulation presents an opportunity to overcome
traditional limitations. In this work, we take an early step in introducing a
novel approach that leverages LLMs in macroeconomic simulation. We design
prompt-engineering-driven LLM agents to exhibit human-like decision-making and
adaptability in the economic environment, with the abilities of perception,
reflection, and decision-making to address the abovementioned challenges.
Simulation experiments on macroeconomic activities show that LLM-empowered
agents can make realistic work and consumption decisions and emerge more
reasonable macroeconomic phenomena than existing rule-based or AI agents. Our
work demonstrates the promising potential to simulate macroeconomics based on
LLM and its human-like characteristics.
","2023-10-17","2310.10436v1.pdf"
"2310.10449","Lochan Basyal","Lochan Basyal and Mihir Sanghvi","Text Summarization Using Large Language Models: A Comparative Study of
  MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models","4 pages, 2 tables","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Text summarization is a critical Natural Language Processing (NLP) task with
applications ranging from information retrieval to content generation.
Leveraging Large Language Models (LLMs) has shown remarkable promise in
enhancing summarization techniques. This paper embarks on an exploration of
text summarization with a diverse set of LLMs, including MPT-7b-instruct,
falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment
was performed with different hyperparameters and evaluated the generated
summaries using widely accepted metrics such as the Bilingual Evaluation
Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation
(ROUGE) Score, and Bidirectional Encoder Representations from Transformers
(BERT) Score. According to the experiment, text-davinci-003 outperformed the
others. This investigation involved two distinct datasets: CNN Daily Mail and
XSum. Its primary objective was to provide a comprehensive understanding of the
performance of Large Language Models (LLMs) when applied to different datasets.
The assessment of these models' effectiveness contributes valuable insights to
researchers and practitioners within the NLP domain. This work serves as a
resource for those interested in harnessing the potential of LLMs for text
summarization and lays the foundation for the development of advanced
Generative AI applications aimed at addressing a wide spectrum of business
challenges.
","2023-10-19","2310.10449v1.pdf"
"2310.10467","Xiaochong Lan","Xiaochong Lan, Chen Gao, Depeng Jin, Yong Li","Stance Detection with Collaborative Role-Infused LLM-Based Agents","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Stance detection automatically detects the stance in a text towards a target,
vital for content analysis in web and social media research. Despite their
promising capabilities, LLMs encounter challenges when directly applied to
stance detection. First, stance detection demands multi-aspect knowledge, from
deciphering event-related terminologies to understanding the expression styles
in social media platforms. Second, stance detection requires advanced reasoning
to infer authors' implicit viewpoints, as stance are often subtly embedded
rather than overtly stated in the text. To address these challenges, we design
a three-stage framework COLA (short for Collaborative rOle-infused LLM-based
Agents) in which LLMs are designated distinct roles, creating a collaborative
system where each role contributes uniquely. Initially, in the multidimensional
text analysis stage, we configure the LLMs to act as a linguistic expert, a
domain specialist, and a social media veteran to get a multifaceted analysis of
texts, thus overcoming the first challenge. Next, in the reasoning-enhanced
debating stage, for each potential stance, we designate a specific LLM-based
agent to advocate for it, guiding the LLM to detect logical connections between
text features and stance, tackling the second challenge. Finally, in the stance
conclusion stage, a final decision maker agent consolidates prior insights to
determine the stance. Our approach avoids extra annotated data and model
training and is highly usable. We achieve state-of-the-art performance across
multiple datasets. Ablation studies validate the effectiveness of each design
role in handling stance detection. Further experiments have demonstrated the
explainability and the versatility of our approach. Our approach excels in
usability, accuracy, effectiveness, explainability and versatility,
highlighting its value.
","2023-10-17","2310.10467v1.pdf"
"2310.10477","Kai Chen","Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi,
  Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng
  Shang, Xin Jiang, Qun Liu","Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake
  Analysis","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  The rapid advancement of large language models (LLMs) presents both
opportunities and challenges, particularly concerning unintentional generation
of harmful and toxic responses. While the traditional alignment methods strive
to steer LLMs towards desired performance and shield them from malicious
content, this study proposes a novel alignment strategy rooted in mistake
analysis by exposing LLMs to flawed outputs purposefully and then conducting a
thorough assessment to fully comprehend internal reasons via natural language
analysis. Thus, toxic responses can be transformed into instruction tuning
corpus for model alignment, and LLMs can not only be deterred from generating
flawed responses but also trained to self-criticize, leveraging its innate
ability to discriminate toxic content. Experimental results demonstrate that
the proposed method outperforms conventional alignment techniques for safety
instruction following, while maintaining superior efficiency.
","2023-10-23","2310.10477v1.pdf"
"2310.10480","Yue Wang","Haoke Zhang, Yue Wang, Juntao Li, Xiabing Zhou, Min Zhang","G-SPEED: General SParse Efficient Editing MoDel","Accepted to the Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models~(LLMs) have demonstrated incredible capabilities in
understanding, generating, and manipulating languages. Through human-model
interactions, LLMs can automatically understand human-issued instructions and
output the expected contents, which can significantly increase working
efficiency. In various types of real-world demands, editing-oriented tasks
account for a considerable proportion, which involves an interactive process
that entails the continuous refinement of existing texts to meet specific
criteria. Due to the need for multi-round human-model interaction and the
generation of complicated editing tasks, there is an emergent need for
efficient general editing models. In this paper, we propose
\underline{\textbf{G}}eneral \underline{\textbf{SP}}arse
\underline{\textbf{E}}fficient \underline{\textbf{E}}diting
Mo\underline{\textbf{D}}el~(\textbf{G-SPEED}), which can fulfill diverse
editing requirements through a single model while maintaining low computational
costs. Specifically, we first propose a novel unsupervised text editing data
clustering algorithm to deal with the data scarcity problem. Subsequently, we
introduce a sparse editing model architecture to mitigate the inherently
limited learning capabilities of small language models. The experimental
outcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMs
equipped with 175B parameters. Our code and model checkpoints are available at
\url{https://github.com/Banner-Z/G-SPEED}.
","2023-10-17","2310.10480v1.pdf"
"2310.10482","Nuno Miguel Guerreiro","Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre
  Colombo, Andr\'e F.T. Martins","xCOMET: Transparent Machine Translation Evaluation through Fine-grained
  Error Detection","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Widely used learned metrics for machine translation evaluation, such as COMET
and BLEURT, estimate the quality of a translation hypothesis by providing a
single sentence-level score. As such, they offer little insight into
translation errors (e.g., what are the errors and what is their severity). On
the other hand, generative large language models (LLMs) are amplifying the
adoption of more granular strategies to evaluation, attempting to detail and
categorize translation errors. In this work, we introduce xCOMET, an
open-source learned metric designed to bridge the gap between these approaches.
xCOMET integrates both sentence-level evaluation and error span detection
capabilities, exhibiting state-of-the-art performance across all types of
evaluation (sentence-level, system-level, and error span detection). Moreover,
it does so while highlighting and categorizing error spans, thus enriching the
quality assessment. We also provide a robustness analysis with stress tests,
and show that xCOMET is largely capable of identifying localized critical
errors and hallucinations.
","2023-10-17","2310.10482v1.pdf"
"2310.10492","Chuang Li","Chuang Li, Yan Zhang, Min-Yen Kan, Haizhou Li","UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking","8 pages, 6 figures, 6 tables","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Previous zero-shot dialogue state tracking (DST) methods only apply transfer
learning, but ignore unlabelled data in the target domain. We transform
zero-shot DST into few-shot DST by utilising such unlabelled data via joint and
self-training methods. Our method incorporates auxiliary tasks that generate
slot types as inverse prompts for main tasks, creating slot values during joint
training. Cycle consistency between these two tasks enables the generation and
selection of quality samples in unknown target domains for subsequent
fine-tuning. This approach also facilitates automatic label creation, thereby
optimizing the training and fine-tuning of DST models. We demonstrate this
method's effectiveness on large language models in zero-shot scenarios,
improving average joint goal accuracy by $8\%$ across all domains in MultiWOZ.
","2023-10-17","2310.10492v1.pdf"
"2310.10495","Grant Forbes","Grant C. Forbes, Parth Katlana, Zeydy Ortiz","Metric Ensembles For Hallucination Detection","9 pages, 5 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Abstractive text summarization has garnered increased interest as of late, in
part due to the proliferation of large language models (LLMs). One of the most
pressing problems related to generation of abstractive summaries is the need to
reduce ""hallucinations,"" information that was not included in the document
being summarized, and which may be wholly incorrect. Due to this need, a wide
array of metrics estimating consistency with the text being summarized have
been proposed. We examine in particular a suite of unsupervised metrics for
summary consistency, and measure their correlations with each other and with
human evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then
compare these evaluations to models made from a simple linear ensemble of these
metrics. We find that LLM-based methods outperform other unsupervised metrics
for hallucination detection. We also find that ensemble methods can improve
these scores even further, provided that the metrics in the ensemble have
sufficiently similar and uncorrelated error rates. Finally, we present an
ensemble method for LLM-based evaluations that we show improves over this
previous SOTA.
","2023-10-17","2310.10495v1.pdf"
"2310.10501","Traian Rebedea","Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien,
  Jonathan Cohen","NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications
  with Programmable Rails","Accepted at EMNLP 2023 - Demo track","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  NeMo Guardrails is an open-source toolkit for easily adding programmable
guardrails to LLM-based conversational systems. Guardrails (or rails for short)
are a specific way of controlling the output of an LLM, such as not talking
about topics considered harmful, following a predefined dialogue path, using a
particular language style, and more. There are several mechanisms that allow
LLM providers and developers to add guardrails that are embedded into a
specific model at training, e.g. using model alignment. Differently, using a
runtime inspired from dialogue management, NeMo Guardrails allows developers to
add programmable rails to LLM applications - these are user-defined,
independent of the underlying LLM, and interpretable. Our initial results show
that the proposed approach can be used with several LLM providers to develop
controllable and safe LLM applications using programmable rails.
","2023-10-17","2310.10501v1.pdf"
"2310.10505","Yang Yu","Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo","ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method
  for Aligning Large Language Models","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Alignment is of critical importance for training large language models
(LLMs). The predominant strategy to address this is through Reinforcement
Learning from Human Feedback (RLHF), where PPO serves as the de-facto
algorithm. Yet, PPO is known to suffer from computational inefficiency, which
is a challenge that this paper aims to address. We identify three important
properties in RLHF tasks: fast simulation, deterministic transitions, and
trajectory-level rewards, which are not leveraged in PPO. Based on such
observations, we develop a new algorithm tailored for RLHF, called ReMax. The
algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is
equipped with a new variance-reduction technique.
  Our method has three-fold advantages over PPO: first, ReMax is simple to
implement and removes many hyper-parameters in PPO, which are scale-sensitive
and laborious to tune. Second, ReMax saves about 50% memory usage in principle.
As a result, PPO runs out-of-memory when fine-tuning a Llama2 (7B) model on
8xA100-40GB GPUs, whereas ReMax can afford training. This memory improvement is
achieved by removing the value model in PPO. Third, based on our calculations,
we find that even assuming PPO can afford the training of Llama2 (7B), it would
still run about 2x slower than ReMax. This is due to the computational overhead
of the value model, which does not exist in ReMax. Importantly, the above
computational improvements do not sacrifice the performance. We hypothesize
these advantages can be maintained in larger-scaled models. Our implementation
of ReMax is available at https://github.com/liziniu/ReMax
","2023-10-18","2310.10505v1.pdf"
"2310.10513","Yihao Liu","Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu
  Qiao, Chao Dong","Unifying Image Processing as Visual Prompting Question Answering","16 pages, 12 figures","","","","cs.CV eess.IV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Image processing is a fundamental task in computer vision, which aims at
enhancing image quality and extracting essential features for subsequent vision
applications. Traditionally, task-specific models are developed for individual
tasks and designing such models requires distinct expertise. Building upon the
success of large language models (LLMs) in natural language processing (NLP),
there is a similar trend in computer vision, which focuses on developing
large-scale models through pretraining and in-context learning. This paradigm
shift reduces the reliance on task-specific models, yielding a powerful unified
model to deal with various tasks. However, these advances have predominantly
concentrated on high-level vision tasks, with less attention paid to low-level
vision tasks. To address this issue, we propose a universal model for general
image processing that covers image restoration, image enhancement, image
feature extraction tasks, \textit{etc}. Our proposed framework, named
PromptGIP, unifies these diverse image processing tasks within a universal
framework. Inspired by NLP question answering (QA) techniques, we employ a
visual prompting question answering paradigm. Specifically, we treat the
input-output image pair as a structured question-answer sentence, thereby
reprogramming the image processing task as a prompting QA problem. PromptGIP
can undertake diverse \textbf{cross-domain} tasks using provided visual
prompts, eliminating the need for task-specific finetuning. Our methodology
offers a universal and adaptive solution to general image processing. While
PromptGIP has demonstrated a certain degree of out-of-domain task
generalization capability, further research is expected to fully explore its
more powerful emergent generalization.
","2023-10-17","2310.10513v1.pdf"
"2310.10520","Guanting Dong","Yuxiang Wu, Guanting Dong, Weiran Xu","Semantic Parsing by Large Language Models for Intricate Updating
  Strategies of Zero-Shot Dialogue State Tracking","Accepted to the Findings of EMNLP 2023 (Short Paper)","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods.
","2023-10-19","2310.10520v1.pdf"
"2310.10543","Hassan Shahmohammadi","Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch","ViPE: Visualise Pretty-much Everything","To be presented in EMNLP2023 Main Conference","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Figurative and non-literal expressions are profoundly integrated in human
communication. Visualising such expressions allow us to convey our creative
thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable
Diffusion, on the other hand, struggle to depict non-literal expressions.
Recent works primarily deal with this issue by compiling humanly annotated
datasets on a small scale, which not only demands specialised expertise but
also proves highly inefficient. To address this issue, we introduce ViPE:
Visualise Pretty-much Everything. ViPE offers a series of lightweight and
robust language models that have been trained on a large-scale set of lyrics
with noisy visual descriptions that represent their implicit meaning. The
synthetic visual descriptions are generated by GPT3.5 relying on neither human
annotations nor images. ViPE effectively expresses any arbitrary piece of text
into a visualisable description, enabling meaningful and high-quality image
generation. We provide compelling evidence that ViPE is more robust than GPT3.5
in synthesising visual elaborations. ViPE also exhibits an understanding of
figurative expressions comparable to human experts, providing a powerful and
open-source backbone to many downstream applications such as music video and
caption generation.
","2023-10-17","2310.10543v1.pdf"
"2310.10544","Laurence Maloney","Laurence T Maloney, Maria F Dal Martello, Vivian Fei and Valerie Ma","Use of probabilistic phrases in a coordination game: human versus GPT-4","","","","","q-bio.NC cs.AI","http://creativecommons.org/licenses/by/4.0/","  English speakers use probabilistic phrases such as likely to communicate
information about the probability or likelihood of events. Communication is
successful to the extent that the listener grasps what the speaker means to
convey and, if communication is successful, two individuals can potentially
coordinate their actions based on shared knowledge about uncertainty. We first
assessed human ability to estimate the probability and the ambiguity
(imprecision) of 23 probabilistic phrases in two different contexts, investment
advice and medical advice. We then had GPT4 (OpenAI), a recent Large Language
Model, complete the same tasks as the human participants. We found that the
median human participant and GPT4 assigned probability estimates that were in
good agreement (proportions of variance accounted were close to .90). GPT4's
estimates of probability both in the investment and Medical contexts were as
close or closer to that of the human participants as the human participants
were to one another. Estimates of probability for both the human participants
and GPT4 were little affected by context. In contrast, human and GPT4 estimates
of ambiguity were not in as good agreement. We repeated some of the GPT4
estimates to assess their stability: does GPT4, if run twice, produce the same
or similar estimates? There is some indication that it does not.
","2023-10-17","2310.10544v1.pdf"
"2310.10556","Zihao Li","Zihao Li, Xiang Ji, Minshuo Chen, Mengdi Wang","Sample Complexity of Preference-Based Nonparametric Off-Policy
  Evaluation with Deep Networks","","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A recently popular approach to solving reinforcement learning is with data
from human preferences. In fact, human preference data are now used with
classic reinforcement learning algorithms such as actor-critic methods, which
involve evaluating an intermediate policy over a reward learned from human
preference data with distribution shift, known as off-policy evaluation (OPE).
Such algorithm includes (i) learning reward function from human preference
dataset, and (ii) learning expected cumulative reward of a target policy.
Despite the huge empirical success, existing OPE methods with preference data
often lack theoretical understanding and rely heavily on heuristics. In this
paper, we study the sample efficiency of OPE with human preference and
establish a statistical guarantee for it. Specifically, we approach OPE by
learning the value function by fitted-Q-evaluation with a deep neural network.
By appropriately selecting the size of a ReLU network, we show that one can
leverage any low-dimensional manifold structure in the Markov decision process
and obtain a sample-efficient estimator without suffering from the curse of
high data ambient dimensionality. Under the assumption of high reward
smoothness, our results \textit{almost align with the classical OPE results
with observable reward data}. To the best of our knowledge, this is the first
result that establishes a \textit{provably efficient} guarantee for off-policy
evaluation with RLHF.
","2023-10-17","2310.10556v1.pdf"
"2310.10560","Animesh Basak Chowdhury","Animesh Basak Chowdhury, Shailja Thakur, Hammond Pearce, Ramesh Karri,
  Siddharth Garg","Towards the Imagenets of ML4EDA","Invited paper, ICCAD 2023","ICCAD 2023","","October 16 Update","cs.LG cs.AI cs.AR cs.PL","http://creativecommons.org/licenses/by-sa/4.0/","  Despite the growing interest in ML-guided EDA tools from RTL to GDSII, there
are no standard datasets or prototypical learning tasks defined for the EDA
problem domain. Experience from the computer vision community suggests that
such datasets are crucial to spur further progress in ML for EDA. Here we
describe our experience curating two large-scale, high-quality datasets for
Verilog code generation and logic synthesis. The first, VeriGen, is a dataset
of Verilog code collected from GitHub and Verilog textbooks. The second,
OpenABC-D, is a large-scale, labeled dataset designed to aid ML for logic
synthesis tasks. The dataset consists of 870,000 And-Inverter-Graphs (AIGs)
produced from 1500 synthesis runs on a large number of open-source hardware
projects. In this paper we will discuss challenges in curating, maintaining and
growing the size and scale of these datasets. We will also touch upon questions
of dataset quality and security, and the use of novel data augmentation tools
that are tailored for the hardware domain.
","2023-10-17","2310.10560v1.pdf"
"2310.10567","Deng Jingcheng","Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng","RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder
  for Language Modeling","Accepted to the Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Retrieval-augmented language models show promise in addressing issues like
outdated information and hallucinations in language models (LMs). However,
current research faces two main problems: 1) determining what information to
retrieve, and 2) effectively combining retrieved information during generation.
We argue that valuable retrieved information should not only be related to the
current source text but also consider the future target text, given the nature
of LMs that model future tokens. Moreover, we propose that aggregation using
latent variables derived from a compact latent space is more efficient than
utilizing explicit raw text, which is limited by context length and susceptible
to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model
built upon the variational auto-encoder (VAE). It encodes the text corpus into
a latent space, capturing current and future information from both source and
target text. Additionally, we leverage the VAE to initialize the latent space
and adopt the probabilistic form of the retrieval generation paradigm by
expanding the Gaussian prior distribution into a Gaussian mixture distribution.
Theoretical analysis provides an optimizable upper bound for RegaVAE.
Experimental results on various datasets demonstrate significant improvements
in text generation quality and hallucination removal.
","2023-10-24","2310.10567v1.pdf"
"2310.10570","Mathieu Ravaut","Mathieu Ravaut, Shafiq Joty, Aixin Sun, Nancy F. Chen","On Position Bias in Summarization with Large Language Models","work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) excel in zero-shot abstractive summarization
tasks, delivering fluent and pertinent summaries. Recent advancements have
extended their capabilities to handle long-input contexts, surpassing token
limits of 32k or more. However, in the realm of multi-document question
answering, language models exhibit uneven utilization of their input context.
They tend to favor the initial and final segments, resulting in a U-shaped
performance pattern concerning where the answer is located within the input.
This bias raises concerns, particularly in summarization tasks where crucial
content may be dispersed throughout the source document(s). This paper presents
a comprehensive investigation encompassing 10 datasets, 4 LLMs, and 5
evaluation metrics to analyze how these models leverage their input for
abstractive summarization. Our findings reveal a pronounced bias towards the
introductory content (and to a lesser extent, the final content), posing
challenges for LLM performance across a range of diverse summarization
benchmarks.
","2023-10-17","2310.10570v1.pdf"
"2310.10583","Sagi Shaier","Sagi Shaier, Lawrence E. Hunter, Katharina von der Wense","Who Are All The Stochastic Parrots Imitating? They Should Tell Us!","Accepted to IJCNLP-AACL 2023","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Both standalone language models (LMs) as well as LMs within downstream-task
systems have been shown to generate statements which are factually untrue. This
problem is especially severe for low-resource languages, where training data is
scarce and of worse quality than for high-resource languages. In this opinion
piece, we argue that LMs in their current state will never be fully trustworthy
in critical settings and suggest a possible novel strategy to handle this
issue: by building LMs such that can cite their sources - i.e., point a user to
the parts of their training data that back up their outputs. We first discuss
which current NLP tasks would or would not benefit from such models. We then
highlight the expected benefits such models would bring, e.g., quick
verifiability of statements. We end by outlining the individual tasks that
would need to be solved on the way to developing LMs with the ability to cite.
We hope to start a discussion about the field's current approach to building
LMs, especially for low-resource languages, and the role of the training data
in explaining model generations.
","2023-10-17","2310.10583v1.pdf"
"2310.10590","Ji Qi","Ji Qi, Kaixuan Ji, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Lei Hou,
  Juanzi Li, Bin Xu","Mastering the Task of Open Information Extraction with Large Language
  Models and Consistent Reasoning Environment","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Open Information Extraction (OIE) aims to extract objective structured
knowledge from natural texts, which has attracted growing attention to build
dedicated models with human experience. As the large language models (LLMs)
have exhibited remarkable in-context learning capabilities, a question arises
as to whether the task of OIE can be effectively tackled with this paradigm? In
this paper, we explore solving the OIE problem by constructing an appropriate
reasoning environment for LLMs. Specifically, we first propose a method to
effectively estimate the discrepancy of syntactic distribution between a LLM
and test samples, which can serve as correlation evidence for preparing
positive demonstrations. Upon the evidence, we introduce a simple yet effective
mechanism to establish the reasoning environment for LLMs on specific tasks.
Without bells and whistles, experimental results on the standard CaRB benchmark
demonstrate that our $6$-shot approach outperforms state-of-the-art supervised
method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and
ACE05 show that our method can naturally generalize to other information
extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,
respectively.
","2023-10-17","2310.10590v1.pdf"
"2310.10591","Chengzhi Mao","Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao","Interpreting and Controlling Vision Foundation Models via Text
  Explanations","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large-scale pre-trained vision foundation models, such as CLIP, have become
de facto backbones for various vision tasks. However, due to their black-box
nature, understanding the underlying rules behind these models' predictions and
controlling model behaviors have remained open challenges. We present a
framework for interpreting vision transformer's latent tokens with natural
language. Given a latent token, our framework retains its semantic information
to the final layer using transformer's local operations and retrieves the
closest text for explanation. Our approach enables understanding of model
visual reasoning procedure without needing additional model training or data
collection. Based on the obtained interpretations, our framework allows for
model editing that controls model reasoning behaviors and improves model
robustness against biases and spurious correlations.
","2023-10-17","2310.10591v1.pdf"
"2310.10616","Tianyu Guo","Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio
  Savarese, Yu Bai","How Do Transformers Learn In-Context Beyond Simple Functions? A Case
  Study on Learning with Representations","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models based on the transformer architecture have
demonstrated remarkable in-context learning (ICL) capabilities, understandings
of such capabilities are still in an early stage, where existing theory and
mechanistic understanding focus mostly on simple scenarios such as learning
simple function classes. This paper takes initial steps on understanding ICL in
more complex scenarios, by studying learning with representations. Concretely,
we construct synthetic in-context learning problems with a compositional
structure, where the label depends on the input through a possibly complex but
fixed representation function, composed with a linear function that differs in
each instance. By construction, the optimal ICL algorithm first transforms the
inputs by the representation function, and then performs linear ICL on top of
the transformed dataset. We show theoretically the existence of transformers
that approximately implement such algorithms with mild depth and size.
Empirically, we find trained transformers consistently achieve near-optimal ICL
performance in this setting, and exhibit the desired dissection where lower
layers transforms the dataset and upper layers perform linear ICL. Through
extensive probing and a new pasting experiment, we further reveal several
mechanisms within the trained transformers, such as concrete copying behaviors
on both the inputs and the representations, linear ICL capability of the upper
layers alone, and a post-ICL representation selection mechanism in a harder
mixture setting. These observed mechanisms align well with our theory and may
shed light on how transformers perform ICL in more realistic scenarios.
","2023-10-17","2310.10616v1.pdf"
"2310.10623","Leonardo F. R. Ribeiro","Leonardo F. R. Ribeiro, Mohit Bansal, Markus Dreyer","Generating Summaries with Controllable Readability Levels","Accepted as an EMNLP 2023 main paper","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Readability refers to how easily a reader can understand a written text.
Several factors affect the readability level, such as the complexity of the
text, its subject matter, and the reader's background knowledge. Generating
summaries based on different readability levels is critical for enabling
knowledge consumption by diverse audiences. However, current text generation
approaches lack refined control, resulting in texts that are not customized to
readers' proficiency levels. In this work, we bridge this gap and study
techniques to generate summaries at specified readability levels. Unlike
previous methods that focus on a specific readability level (e.g., lay
summarization), we generate summaries with fine-grained control over their
readability. We develop three text generation techniques for controlling
readability: (1) instruction-based readability control, (2) reinforcement
learning to minimize the gap between requested and observed readability and (3)
a decoding approach that uses lookahead to estimate the readability of upcoming
decoding steps. We show that our generation methods significantly improve
readability control on news summarization (CNN/DM dataset), as measured by
various readability metrics and human judgement, establishing strong baselines
for controllable readability in summarization.
","2023-10-17","2310.10623v1.pdf"
"2310.10624","Jia-Wei Liu","Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui
  Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou","DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and
  View-Change Human-Centric Video Editing","Project Page: https://showlab.github.io/DynVideo-E/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite remarkable research advances in diffusion-based video editing,
existing methods are limited to short-length videos due to the contradiction
between long-range consistency and frame-wise editing. Recent approaches
attempt to tackle this challenge by introducing video-2D representations to
degrade video editing to image editing. However, they encounter significant
difficulties in handling large-scale motion- and view-change videos especially
for human-centric videos. This motivates us to introduce the dynamic Neural
Radiance Fields (NeRF) as the human-centric video representation to ease the
video editing problem to a 3D space editing task. As such, editing can be
performed in the 3D spaces and propagated to the entire video via the
deformation field. To provide finer and direct controllable editing, we propose
the image-based 3D space editing pipeline with a set of effective designs.
These include multi-view multi-pose Score Distillation Sampling (SDS) from both
2D personalized diffusion priors and 3D diffusion priors, reconstruction losses
on the reference image, text-guided local parts super-resolution, and style
transfer for 3D background space. Extensive experiments demonstrate that our
method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two
challenging datasets by a large margin of 50% ~ 95% in terms of human
preference. Compelling video comparisons are provided in the project page
https://showlab.github.io/DynVideo-E/. Our code and data will be released to
the community.
","2023-10-17","2310.10624v1.pdf"
"2310.10625","Yilun Du","Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian
  Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum,
  Leslie Kaelbling, Andy Zeng, Jonathan Tompson","Video Language Planning","https://video-language-planning.github.io/","","","","cs.CV cs.AI cs.LG cs.RO","http://creativecommons.org/licenses/by/4.0/","  We are interested in enabling visual planning for complex long-horizon tasks
in the space of generated videos and language, leveraging recent advances in
large generative models pretrained on Internet-scale data. To this end, we
present video language planning (VLP), an algorithm that consists of a tree
search procedure, where we train (i) vision-language models to serve as both
policies and value functions, and (ii) text-to-video models as dynamics models.
VLP takes as input a long-horizon task instruction and current image
observation, and outputs a long video plan that provides detailed multimodal
(video and language) specifications that describe how to complete the final
task. VLP scales with increasing computation budget where more computation time
results in improved video plans, and is able to synthesize long-horizon video
plans across different robotics domains: from multi-object rearrangement, to
multi-camera bi-arm dexterous manipulation. Generated video plans can be
translated into real robot actions via goal-conditioned policies, conditioned
on each intermediate frame of the generated video. Experiments show that VLP
substantially improves long-horizon task success rates compared to prior
methods on both simulated and real robots (across 3 hardware platforms).
","2023-10-17","2310.10625v1.pdf"
"2310.10627","Andreas Stuhlm\""uller","Charlie George and Andreas Stuhlm\""uller","Factored Verification: Detecting and Reducing Hallucination in Summaries
  of Academic Papers","Second Workshop on Information Extraction from Scientific
  Publications (WIESP) at IJCNLP-AACL 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Hallucination plagues even frontier LLMs--but how bad is it really for
summarizing academic papers? We evaluate Factored Verification, a simple
automated method for detecting hallucinations in abstractive summaries. This
method sets a new SotA on hallucination detection in the summarization task of
the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to
estimate how often language models hallucinate when summarizing across multiple
academic papers and find 0.62 hallucinations in the average ChatGPT (16k)
summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct
using Factored Critiques and find that this lowers the number of hallucinations
to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations
we find are often subtle, so we advise caution when using models to synthesize
academic papers.
","2023-10-17","2310.10627v1.pdf"
"2310.10628","Samuel Dooley","Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White,
  Samuel Dooley","Data Contamination Through the Lens of Time","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent claims about the impressive abilities of large language models (LLMs)
are often supported by evaluating publicly available benchmarks. Since LLMs
train on wide swaths of the internet, this practice raises concerns of data
contamination, i.e., evaluating on examples that are explicitly or implicitly
included in the training data. Data contamination remains notoriously
challenging to measure and mitigate, even with partial attempts like controlled
experimentation of training data, canary strings, or embedding similarities. In
this work, we conduct the first thorough longitudinal analysis of data
contamination in LLMs by using the natural experiment of training cutoffs in
GPT models to look at benchmarks released over time. Specifically, we consider
two code/mathematical problem-solving datasets, Codeforces and Project Euler,
and find statistically significant trends among LLM pass rate vs. GitHub
popularity and release date that provide strong evidence of contamination. By
open-sourcing our dataset, raw results, and evaluation framework, our work
paves the way for rigorous analyses of data contamination in modern models. We
conclude with a discussion of best practices and future steps for publicly
releasing benchmarks in the age of LLMs that train on webscale data.
","2023-10-17","2310.10628v1.pdf"
"2310.10631","Zhangir Azerbayev Mr","Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco
  Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella
  Biderman and Sean Welleck","Llemma: An Open Language Model For Mathematics","","","","","cs.CL cs.AI cs.LO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present Llemma, a large language model for mathematics. We continue
pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web
data containing mathematics, and mathematical code, yielding Llemma. On the
MATH benchmark Llemma outperforms all known open base models, as well as the
unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is
capable of tool use and formal theorem proving without any further finetuning.
We openly release all artifacts, including 7 billion and 34 billion parameter
models, the Proof-Pile-2, and code to replicate our experiments.
","2023-10-17","2310.10631v1.pdf"
"2310.10632","Aleksandar Shtedritski","Odhran O'Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud,
  Ali Essa Ghareeb, Justin Booth, Samuel G Rodriques","BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology","EMNLP 2023. Dataset and code:
  https://github.com/bioplanner/bioplanner","","","","cs.CL cs.AI cs.RO","http://creativecommons.org/licenses/by/4.0/","  The ability to automatically generate accurate protocols for scientific
experiments would represent a major step towards the automation of science.
Large Language Models (LLMs) have impressive capabilities on a wide range of
tasks, such as question answering and the generation of coherent text and code.
However, LLMs can struggle with multi-step problems and long-term planning,
which are crucial for designing scientific experiments. Moreover, evaluation of
the accuracy of scientific protocols is challenging, because experiments can be
described correctly in many different ways, require expert knowledge to
evaluate, and cannot usually be executed automatically. Here we present an
automatic evaluation framework for the task of planning experimental protocols,
and we introduce BioProt: a dataset of biology protocols with corresponding
pseudocode representations. To measure performance on generating scientific
protocols, we use an LLM to convert a natural language protocol into
pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode
from a high-level description and a list of admissible pseudocode functions. We
evaluate GPT-3 and GPT-4 on this task and explore their robustness. We
externally validate the utility of pseudocode representations of text by
generating accurate novel protocols using retrieved pseudocode, and we run a
generated protocol successfully in our biological laboratory. Our framework is
extensible to the evaluation and improvement of language model planning
abilities in other areas of science or other areas that lack automatic
evaluation.
","2023-10-17","2310.10632v1.pdf"
"2310.10634","Fan Zhou","Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao
  Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu,
  Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu","OpenAgents: An Open Platform for Language Agents in the Wild","34 pages, 8 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Language agents show potential in being capable of utilizing natural language
for varied and intricate tasks in diverse environments, particularly when built
upon large language models (LLMs). Current language agent frameworks aim to
facilitate the construction of proof-of-concept language agents while
neglecting the non-expert user access to agents and paying little attention to
application-level designs. We present OpenAgents, an open platform for using
and hosting language agents in the wild of everyday life. OpenAgents includes
three agents: (1) Data Agent for data analysis with Python/SQL and data tools;
(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web
browsing. OpenAgents enables general users to interact with agent
functionalities through a web user interface optimized for swift responses and
common failures while offering developers and researchers a seamless deployment
experience on local setups, providing a foundation for crafting innovative
language agents and facilitating real-world evaluations. We elucidate the
challenges and opportunities, aspiring to set a foundation for future research
and development of real-world language agents.
","2023-10-17","2310.10634v1.pdf"
"2310.10637","Kunal Handa","Kunal Handa, Margaret Clapper, Jessica Boyle, Rose E Wang, Diyi Yang,
  David S Yeager, Dorottya Demszky","""Mistakes Help Us Grow"": Facilitating and Evaluating Growth Mindset
  Supportive Language in Classrooms","In Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing
that one's skills can be improved over time--has been shown to significantly
reduce disparities in academic achievement and enhance students' learning
outcomes. Although teachers espouse growth mindset principles, most find it
difficult to adopt GMSL in their practice due the lack of effective coaching in
this area. We explore whether large language models (LLMs) can provide
automated, personalized coaching to support teachers' use of GMSL. We establish
an effective coaching tool to reframe unsupportive utterances to GMSL by
developing (i) a parallel dataset containing GMSL-trained teacher reframings of
unsupportive statements with an accompanying annotation guide, (ii) a GMSL
prompt framework to revise teachers' unsupportive language, and (iii) an
evaluation framework grounded in psychological theory for evaluating GMSL with
the help of students and teachers. We conduct a large-scale evaluation
involving 174 teachers and 1,006 students, finding that both teachers and
students perceive GMSL-trained teacher and model reframings as more effective
in fostering a growth mindset and promoting challenge-seeking behavior, among
other benefits. We also find that model-generated reframings outperform those
from the GMSL-trained teachers. These results show promise for harnessing LLMs
to provide automated GMSL feedback for teachers and, more broadly, LLMs'
potentiality for supporting students' learning in the classroom. Our findings
also demonstrate the benefit of large-scale human evaluations when applying
LLMs in educational domains.
","2023-10-17","2310.10637v1.pdf"
"2310.10638","Weijia Shi","Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and
  Margaret Li and Xi Victoria Lin and Noah A. Smith and Luke Zettlemoyer and
  Scott Yih and Mike Lewis","In-Context Pretraining: Language Modeling Beyond Document Boundaries","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs'performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
","2023-10-24","2310.10638v1.pdf"
"2310.10639","Kevin Black","Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea
  Finn, Aviral Kumar, Sergey Levine","Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion
  Models","22 pages, 8 figures","","","","cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  If generalist robots are to operate in truly unstructured environments, they
need to be able to recognize and reason about novel objects and scenarios. Such
objects and scenarios might not be present in the robot's own training data. We
propose SuSIE, a method that leverages an image-editing diffusion model to act
as a high-level planner by proposing intermediate subgoals that a low-level
controller can accomplish. Specifically, we finetune InstructPix2Pix on video
data, consisting of both human videos and robot rollouts, such that it outputs
hypothetical future ""subgoal"" observations given the robot's current
observation and a language command. We also use the robot data to train a
low-level goal-conditioned policy to act as the aforementioned low-level
controller. We find that the high-level subgoal predictions can utilize
Internet-scale pretraining and visual understanding to guide the low-level
goal-conditioned policy, achieving significantly better generalization and
precision than conventional language-conditioned policies. We achieve
state-of-the-art results on the CALVIN benchmark, and also demonstrate robust
generalization on real-world manipulation tasks, beating strong baselines that
have access to privileged information or that utilize orders of magnitude more
compute and training data. The project website can be found at
http://rail-berkeley.github.io/susie .
","2023-10-17","2310.10639v1.pdf"
"2310.10640","Mohammad Hanan Gani","Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter
  Wonka","LLM Blueprint: Enabling Text-to-Image Generation with Complex and
  Detailed Prompts","Code: https://github.com/hananshafi/llmblueprint","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diffusion-based generative models have significantly advanced text-to-image
generation but encounter challenges when processing lengthy and intricate text
prompts describing complex scenes with multiple objects. While excelling in
generating images from short, single-object descriptions, these models often
struggle to faithfully capture all the nuanced details within longer and more
elaborate textual inputs. In response, we present a novel approach leveraging
Large Language Models (LLMs) to extract critical components from text prompts,
including bounding box coordinates for foreground objects, detailed textual
descriptions for individual objects, and a succinct background context. These
components form the foundation of our layout-to-image generation model, which
operates in two phases. The initial Global Scene Generation utilizes object
layouts and background context to create an initial scene but often falls short
in faithfully representing object characteristics as specified in the prompts.
To address this limitation, we introduce an Iterative Refinement Scheme that
iteratively evaluates and refines box-level content to align them with their
textual descriptions, recomposing objects as needed to ensure consistency. Our
evaluation on complex prompts featuring multiple objects demonstrates a
substantial improvement in recall compared to baseline diffusion models. This
is further validated by a user study, underscoring the efficacy of our approach
in generating coherent and detailed scenes from intricate textual inputs.
","2023-10-17","2310.10640v1.pdf"
"2310.10644","Yukai Shi","Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi, Tianyu Yang,
  Yukun Huang, Shilong Liu, Lei Zhang, Heung-Yeung Shum","TOSS:High-quality Text-guided Novel View Synthesis from a Single Image","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we present TOSS, which introduces text to the task of novel
view synthesis (NVS) from just a single RGB image. While Zero-1-to-3 has
demonstrated impressive zero-shot open-set NVS capability, it treats NVS as a
pure image-to-image translation problem. This approach suffers from the
challengingly under-constrained nature of single-view NVS: the process lacks
means of explicit user control and often results in implausible NVS
generations. To address this limitation, TOSS uses text as high-level semantic
information to constrain the NVS solution space. TOSS fine-tunes text-to-image
Stable Diffusion pre-trained on large-scale text-image pairs and introduces
modules specifically tailored to image and camera pose conditioning, as well as
dedicated training for pose correctness and preservation of fine details.
Comprehensive experiments are conducted with results showing that our proposed
TOSS outperforms Zero-1-to-3 with more plausible, controllable and
multiview-consistent NVS results. We further support these results with
comprehensive ablations that underscore the effectiveness and potential of the
introduced semantic guidance and architecture design.
","2023-10-17","2310.10644v1.pdf"
"2310.10645","Boyi Li","Boyi Li and Philipp Wu and Pieter Abbeel and Jitendra Malik","Interactive Task Planning with Language Models","","","","","cs.RO cs.AI cs.CL cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An interactive robot framework accomplishes long-horizon task planning and
can easily generalize to new goals or distinct tasks, even during execution.
However, most traditional methods require predefined module design, which makes
it hard to generalize to different goals. Recent large language model based
approaches can allow for more open-ended planning but often require heavy
prompt engineering or domain-specific pretrained models. To tackle this, we
propose a simple framework that achieves interactive task planning with
language models. Our system incorporates both high-level planning and low-level
function execution via language. We verify the robustness of our system in
generating novel high-level instructions for unseen objectives and its ease of
adaptation to different tasks by merely substituting the task guidelines,
without the need for additional complex prompt engineering. Furthermore, when
the user sends a new request, our system is able to replan accordingly with
precision based on the new request, task guidelines and previously executed
steps. Please check more details on our https://wuphilipp.github.io/itp_site
and https://youtu.be/TrKLuyv26_g.
","2023-10-17","2310.10645v1.pdf"
"2310.10647","Zhen Xing","Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan
  Wu, Yu-Gang Jiang","A Survey on Video Diffusion Models","","","","","cs.CV cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent wave of AI-generated content (AIGC) has witnessed substantial
success in computer vision, with the diffusion model playing a crucial role in
this achievement. Due to their impressive generative capabilities, diffusion
models are gradually superseding methods based on GANs and auto-regressive
Transformers, demonstrating exceptional performance not only in image
generation and editing, but also in the realm of video-related research.
However, existing surveys mainly focus on diffusion models in the context of
image generation, with few up-to-date reviews on their application in the video
domain. To address this gap, this paper presents a comprehensive review of
video diffusion models in the AIGC era. Specifically, we begin with a concise
introduction to the fundamentals and evolution of diffusion models.
Subsequently, we present an overview of research on diffusion models in the
video domain, categorizing the work into three key areas: video generation,
video editing, and other video understanding tasks. We conduct a thorough
review of the literature in these three key areas, including further
categorization and practical contributions in the field. Finally, we discuss
the challenges faced by research in this domain and outline potential future
developmental trends. A comprehensive list of video diffusion models studied in
this survey is available at
https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.
","2023-10-17","2310.10647v1.pdf"
"2310.10648","Rose Wang","Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, Dorottya
  Demszky","Step-by-Step Remediation of Students' Mathematical Mistakes","Our work is open-sourced at this link:
  \url{https://github.com/rosewang2008/remath}","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Scaling high-quality tutoring is a major challenge in education. Because of
the growing demand, many platforms employ novice tutors who, unlike
professional educators, struggle to effectively address student mistakes and
thus fail to seize prime learning opportunities for students. In this paper, we
explore the potential for large language models (LLMs) to assist math tutors in
remediating student mistakes. We present ReMath, a benchmark co-developed with
experienced math teachers that deconstructs their thought process for
remediation. The benchmark consists of three step-by-step tasks: (1) infer the
type of student error, (2) determine the strategy to address the error, and (3)
generate a response that incorporates that information. We evaluate the
performance of state-of-the-art instruct-tuned and dialog models on ReMath. Our
findings suggest that although models consistently improve upon original tutor
responses, we cannot rely on models alone to remediate mistakes. Providing
models with the error type (e.g., the student is guessing) and strategy (e.g.,
simplify the problem) leads to a 75% improvement in the response quality over
models without that information. Nonetheless, despite the improvement, the
quality of the best model's responses still falls short of experienced math
teachers. Our work sheds light on the potential and limitations of using
current LLMs to provide high-quality learning experiences for both tutors and
students at scale. Our work is open-sourced at this link:
\url{https://github.com/rosewang2008/remath}.
","2023-10-17","2310.10648v1.pdf"
"2310.10664","Dmitrijs Trizna","Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Fabio Roli","Nebula: Self-Attention for Dynamic Malware Analysis","18 pages, 7 figures, 12 tables, preprint, in review","","","","cs.CR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Dynamic analysis enables detecting Windows malware by executing programs in a
controlled environment, and storing their actions in log reports. Previous work
has started training machine learning models on such reports to perform either
malware detection or malware classification. However, most of the approaches
(i) have only considered convolutional and long-short term memory networks,
(ii) they have been built focusing only on APIs called at runtime, without
considering other relevant though heterogeneous sources of information like
network and file operations, and (iii) the code and pretrained models are
hardly available, hindering reproducibility of results in this research area.
In this work, we overcome these limitations by presenting Nebula, a versatile,
self-attention transformer-based neural architecture that can generalize across
different behavior representations and formats, combining heterogeneous
information from dynamic log reports. We show the efficacy of Nebula on three
distinct data collections from different dynamic analysis platforms, comparing
its performance with previous state-of-the-art models developed for malware
detection and classification tasks. We produce an extensive ablation study that
showcases how the components of Nebula influence its predictive performance,
while enabling it to outperform some competing approaches at very low false
positive rates. We conclude our work by inspecting the behavior of Nebula
through the application of explainability methods, which highlight that Nebula
correctly focuses more on portions of reports that contain malicious
activities. We release our code and models at github.com/dtrizna/nebula.
","2023-10-18","2310.10664v1.pdf"
"2310.10665","Adnan Qayyum","Mahdi Alkaeed, Adnan Qayyum, and Junaid Qadir","Privacy Preservation in Artificial Intelligence and Extended Reality
  (AI-XR) Metaverses: A Survey","","","","","cs.CR cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  The metaverse is a nascent concept that envisions a virtual universe, a
collaborative space where individuals can interact, create, and participate in
a wide range of activities. Privacy in the metaverse is a critical concern as
the concept evolves and immersive virtual experiences become more prevalent.
The metaverse privacy problem refers to the challenges and concerns surrounding
the privacy of personal information and data within Virtual Reality (VR)
environments as the concept of a shared VR space becomes more accessible.
Metaverse will harness advancements from various technologies such as
Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and
5G/6G-based communication to provide personalized and immersive services to its
users. Moreover, to enable more personalized experiences, the metaverse relies
on the collection of fine-grained user data that leads to various privacy
issues. Therefore, before the potential of the metaverse can be fully realized,
privacy concerns related to personal information and data within VR
environments must be addressed. This includes safeguarding users' control over
their data, ensuring the security of their personal information, and protecting
in-world actions and interactions from unauthorized sharing. In this paper, we
explore various privacy challenges that future metaverses are expected to face,
given their reliance on AI for tracking users, creating XR and MR experiences,
and facilitating interactions. Moreover, we thoroughly analyze technical
solutions such as differential privacy, Homomorphic Encryption (HE), and
Federated Learning (FL) and discuss related sociotechnical issues regarding
privacy.
","2023-10-18","2310.10665v1.pdf"
"2310.10669","Zhengmian Hu","Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng
  Huang","Unbiased Watermark for Large Language Models","","","","","cs.CR","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The recent advancements in large language models (LLMs) have sparked a
growing apprehension regarding the potential misuse. One approach to mitigating
this risk is to incorporate watermarking techniques into LLMs, allowing for the
tracking and attribution of model outputs. This study examines a crucial aspect
of watermarking: how significantly watermarks impact the quality of
model-generated outputs. Previous studies have suggested a trade-off between
watermark strength and output quality. However, our research demonstrates that
it is possible to integrate watermarks without affecting the output probability
distribution with appropriate implementation. We refer to this type of
watermark as an unbiased watermark. This has significant implications for the
use of LLMs, as it becomes impossible for users to discern whether a service
provider has incorporated watermarks or not. Furthermore, the presence of
watermarks does not compromise the performance of the model in downstream
tasks, ensuring that the overall utility of the language model is preserved.
Our findings contribute to the ongoing discussion around responsible AI
development, suggesting that unbiased watermarks can serve as an effective
means of tracking and attributing model outputs without sacrificing output
quality.
","2023-10-19","2310.10669v1.pdf"
"2310.10673","David Sinclair Dr","David Sinclair and Willem Pye","Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate
  Emotion Probability Vectors","Emotion descriptors derived from LLMs are likely to be used in
  governing robot behaviour. Vectorizing emotion descriptors provides a finer
  grained representation of state than current 'positive vs negative' situation
  assessments. Finer grained geometric representation of state will be
  necessary for future human robot interaction governance","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper shows how LLMs (Large Language Models) may be used to estimate a
summary of the emotional state associated with piece of text. The summary of
emotional state is a dictionary of words used to describe emotion together with
the probability of the word appearing after a prompt comprising the original
text and an emotion eliciting tail. Through emotion analysis of Amazon product
reviews we demonstrate emotion descriptors can be mapped into a PCA type space.
It was hoped that text descriptions of actions to improve a current text
described state could also be elicited through a tail prompt. Experiment seemed
to indicate that this is not straightforward to make work. This failure put our
hoped for selection of action via choosing the best predict ed outcome via
comparing emotional responses out of reach for the moment.
","2023-10-18","2310.10673v1.pdf"
"2310.10677","Sophia Gu","Sophia Gu","LLMs as Potential Brainstorming Partners for Math and Science Problems","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  With the recent rise of widely successful deep learning models, there is
emerging interest among professionals in various math and science communities
to see and evaluate the state-of-the-art models' abilities to collaborate on
finding or solving problems that often require creativity and thus
brainstorming. While a significant chasm still exists between current
human-machine intellectual collaborations and the resolution of complex math
and science problems, such as the six unsolved Millennium Prize Problems, our
initial investigation into this matter reveals a promising step towards
bridging the divide. This is due to the recent advancements in Large Language
Models (LLMs). More specifically, we conduct comprehensive case studies to
explore both the capabilities and limitations of the current state-of-the-art
LLM, notably GPT-4, in collective brainstorming with humans.
","2023-10-18","2310.10677v1.pdf"
"2310.10679","Pawe{\l} Niszczota","Pawe{\l} Niszczota and Mateusz Janczak","Large language models can replicate cross-cultural differences in
  personality","11 pages, 1 figure and 1 table in the manuscript (more figures and
  tables in the supplementary materials)","","","","cs.CL cs.AI cs.CY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We use a large-scale experiment (N=8000) to determine whether GPT-4 can
replicate cross-cultural differences in the Big Five, measured using the
Ten-Item Personality Inventory. We used the US and South Korea as the cultural
pair, given that prior research suggests substantial personality differences
between people from these two countries. We manipulated the target of the
simulation (US vs. Korean), the language of the inventory (English vs. Korean),
and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4
replicated the cross-cultural differences for each factor. However, mean
ratings had an upward bias and exhibited lower variation than in the human
samples, as well as lower structural validity. Overall, we provide preliminary
evidence that LLMs can aid cross-cultural psychological research.
","2023-10-18","2310.10679v1.pdf"
"2310.10683","Yuanshun Yao","Yuanshun Yao, Xiaojun Xu, Yang Liu","Large Language Model Unlearning","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  We study how to perform unlearning, i.e. forgetting undesirable
(mis)behaviors, on large language models (LLMs). We show at least three
scenarios of aligning LLMs with human preferences can benefit from unlearning:
(1) removing harmful responses, (2) erasing copyright-protected content as
requested, and (3) eliminating hallucinations. Unlearning, as an alignment
technique, has three advantages. (1) It only requires negative (e.g. harmful)
examples, which are much easier and cheaper to collect (e.g. via red teaming or
user reporting) than positive (e.g. helpful and often human-written) examples
required in RLHF (RL from human feedback). (2) It is computationally efficient.
(3) It is especially effective when we know which training samples cause the
misbehavior. To the best of our knowledge, our work is among the first to
explore LLM unlearning. We are also among the first to formulate the settings,
goals, and evaluations in LLM unlearning. We show that if practitioners only
have limited resources, and therefore the priority is to stop generating
undesirable outputs rather than to try to generate desirable outputs,
unlearning is particularly appealing. Despite only having negative samples, our
ablation study shows that unlearning can still achieve better alignment
performance than RLHF with just 2% of its computational time.
","2023-10-18","2310.10683v1.pdf"
"2310.10686","Zheyu Zhang Aqa","Zheyu Zhang and Zhuorui Ye and Yikang Shen and Chuang Gan","Autonomous Tree-search Ability of Large Language Models","Due to the limitation ""The abstract field cannot be longer than 1,920
  characters"", the abstract here is shorter than that in the PDF file","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models have excelled in remarkable reasoning capabilities with
advanced prompting techniques, but they fall short on tasks that require
exploration, strategic foresight, and sequential decision-making. Recent works
propose to utilize external programs to define search logic, such that LLMs can
perform passive tree search to solve more challenging reasoning tasks. Though
impressive results have been achieved, there are several fundamental
limitations of these approaches. First, passive tree searches are not efficient
as they usually require multiple rounds of LLM API calls to solve one single
problem. Moreover, passive search methods are not flexible since they need
task-specific program designs. Then a natural question arises: can we maintain
the tree-search capability of LLMs without the aid of external programs, and
can still generate responses that clearly demonstrate the process of a
tree-structure search? To this end, we propose a new concept called autonomous
tree-search ability of LLM, which can automatically generate a response
containing search trajectories for the correct answer. Concretely, we perform
search trajectories using capable LLM API via a fixed system prompt, allowing
them to perform autonomous tree-search (ATS) right out of the box. Experiments
on 4 puzzle games demonstrate our method can achieve huge improvements. The
ATS-BFS method outperforms the Chain of Thought approach by achieving an
average accuracy improvement of 33%. Compared to Tree of Thoughts, it requires
65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.
Moreover, we have collected data using the ATS prompt method and fine-tuned
LLaMA. This approach yield a greater improvement compared to the ones
fine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an
average of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.
","2023-10-18","2310.10686v1.pdf"
"2310.10688","Rajat Sen","Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou","A decoder-only foundation model for time-series forecasting","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Motivated by recent advances in large language models for Natural Language
Processing (NLP), we design a time-series foundation model for forecasting
whose out-of-the-box zero-shot performance on a variety of public datasets
comes close to the accuracy of state-of-the-art supervised forecasting models
for each individual dataset. Our model is based on pretraining a
patched-decoder style attention model on a large time-series corpus, and can
work well across different forecasting history lengths, prediction lengths and
temporal granularities.
","2023-10-18","2310.10688v1.pdf"
"2310.10690","Adish Singla","Manh Hung Nguyen, Sebastian Tschiatschek, Adish Singla","Large Language Models for In-Context Student Modeling: Synthesizing
  Student's Behavior in Visual Programming from One-Shot Observation","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Student modeling is central to many educational technologies as it enables
the prediction of future learning outcomes and targeted instructional
strategies. However, open-ended learning environments pose challenges for
accurately modeling students due to the diverse behaviors exhibited by students
and the absence of a well-defined set of learning skills. To approach these
challenges, we explore the application of Large Language Models (LLMs) for
in-context student modeling in open-ended learning environments. We introduce a
novel framework, LLM-SS, that leverages LLMs for synthesizing student's
behavior. More concretely, given a particular student's solving attempt on a
reference task as observation, the goal is to synthesize the student's attempt
on a target task. Our framework can be combined with different LLMs; moreover,
we fine-tune LLMs using domain-specific expertise to boost their understanding
of domain background and student behaviors. We evaluate several concrete
methods based on LLM-SS using the StudentSyn benchmark, an existing student's
attempt synthesis benchmark in visual programming. Experimental results show a
significant improvement compared to baseline methods included in the StudentSyn
benchmark. Furthermore, our method using the fine-tuned Llama2-70B model
improves noticeably compared to using the base model and becomes on par with
using the state-of-the-art GPT-4 model.
","2023-10-18","2310.10690v1.pdf"
"2310.10692","Laetitia Teodorescu","Julien Pourcel, C\'edric Colas, Pierre-Yves Oudeyer, Laetitia
  Teodorescu","ACES: Generating Diverse Programming Puzzles with Autotelic Language
  Models and Semantic Descriptors","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Finding and selecting new and interesting problems to solve is at the heart
of curiosity, science and innovation. We here study automated problem
generation in the context of the open-ended space of python programming
puzzles. Existing generative models often aim at modeling a reference
distribution without any explicit diversity optimization. Other methods
explicitly optimizing for diversity do so either in limited hand-coded
representation spaces or in uninterpretable learned embedding spaces that may
not align with human perceptions of interesting variations. With ACES
(Autotelic Code Exploration via Semantic descriptors), we introduce a new
autotelic generation method that leverages semantic descriptors produced by a
large language model (LLM) to directly optimize for interesting diversity, as
well as few-shot-based generation. Each puzzle is labeled along 10 dimensions,
each capturing a programming skill required to solve it. ACES generates and
pursues novel and feasible goals to explore that abstract semantic space,
slowly discovering a diversity of solvable programming puzzles in any given
run. Across a set of experiments, we show that ACES discovers a richer
diversity of puzzles than existing diversity-maximizing algorithms as measured
across a range of diversity metrics. We further study whether and in which
conditions this diversity can translate into the successful training of puzzle
solving models.
","2023-10-26","2310.10692v1.pdf"
"2310.10699","Yu Pan","Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, Qun
  Liu","Reusing Pretrained Models by Multi-linear Operators for Efficient
  Training","Accepted in NeurIPS 2023","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Training large models from scratch usually costs a substantial amount of
resources. Towards this problem, recent studies such as bert2BERT and LiGO have
reused small pretrained models to initialize a large model (termed the ``target
model''), leading to a considerable acceleration in training. Despite the
successes of these previous studies, they grew pretrained models by mapping
partial weights only, ignoring potential correlations across the entire model.
As we show in this paper, there are inter- and intra-interactions among the
weights of both the pretrained and the target models. As a result, the partial
mapping may not capture the complete information and lead to inadequate growth.
In this paper, we propose a method that linearly correlates each weight of the
target model to all the weights of the pretrained model to further enhance
acceleration ability. We utilize multi-linear operators to reduce computational
and spacial complexity, enabling acceptable resource requirements. Experiments
demonstrate that our method can save 76\% computational costs on DeiT-base
transferred from DeiT-small, which outperforms bert2BERT by +12.0\% and LiGO by
+20.7\%, respectively.
","2023-10-18","2310.10699v1.pdf"
"2310.10700","Yangyang Guo","Yangyang Guo and Guangzhi Wang and Mohan Kankanhalli","PELA: Learning Parameter-Efficient Models with Low-Rank Approximation","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Applying a pre-trained large model to downstream tasks is prohibitive under
resource-constrained conditions. Recent dominant approaches for addressing
efficiency issues involve adding a few learnable parameters to the fixed
backbone model. This strategy, however, leads to more challenges in loading
large models for downstream fine-tuning with limited resources. In this paper,
we propose a novel method for increasing the parameter efficiency of
pre-trained models by introducing an intermediate pre-training stage. To this
end, we first employ low-rank approximation to compress the original large
model and then devise a feature distillation module and a weight perturbation
regularization module. These modules are specifically designed to enhance the
low-rank model. Concretely, we update only the low-rank model while freezing
the backbone parameters during pre-training. This allows for direct and
efficient utilization of the low-rank model for downstream tasks. The proposed
method achieves both efficiencies in terms of required parameters and
computation time while maintaining comparable results with minimal
modifications to the base architecture. Specifically, when applied to three
vision-only and one vision-language Transformer models, our approach often
demonstrates a $\sim$0.6 point decrease in performance while reducing the
original parameter size by 1/3 to 2/3.
","2023-10-18","2310.10700v1.pdf"
"2310.10701","Huao Li","Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana
  Hughes, Michael Lewis, Katia Sycara","Theory of Mind for Multi-Agent Collaboration via Large Language Models","Accepted to EMNLP 2023 (Main Conference)","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.
","2023-10-24","2310.10701v1.pdf"
"2310.10706","Zijian Ding","Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault,
  Alejandro Jaimes","Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation
  through the Lens of News Headline Generation","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  To explore how humans can best leverage LLMs for writing and how interacting
with these models affects feelings of ownership and trust in the writing
process, we compared common human-AI interaction types (e.g., guiding system,
selecting from system outputs, post-editing outputs) in the context of
LLM-assisted news headline generation. While LLMs alone can generate
satisfactory news headlines, on average, human control is needed to fix
undesirable model outputs. Of the interaction methods, guiding and selecting
model output added the most benefit with the lowest cost (in time and effort).
Further, AI assistance did not harm participants' perception of control
compared to freeform editing.
","2023-10-19","2310.10706v1.pdf"
"2310.10707","Anirudh Som","Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol,
  Dimitra Vergyri","Demonstrations Are All You Need: Advancing Offensive Content
  Paraphrasing using In-Context Learning","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Paraphrasing of offensive content is a better alternative to content removal
and helps improve civility in a communication environment. Supervised
paraphrasers; however, rely heavily on large quantities of labelled data to
help preserve meaning and intent. They also retain a large portion of the
offensiveness of the original content, which raises questions on their overall
usability. In this paper we aim to assist practitioners in developing usable
paraphrasers by exploring In-Context Learning (ICL) with large language models
(LLMs), i.e., using a limited number of input-label demonstration pairs to
guide the model in generating desired outputs for specific queries. Our study
focuses on key factors such as -- number and order of demonstrations, exclusion
of prompt instruction, and reduction in measured toxicity. We perform
principled evaluation on three datasets, including our proposed Context-Aware
Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite
paraphrases, and additional dialogue context. We evaluate our approach using
two closed source and one open source LLM. Our results reveal that ICL is
comparable to supervised methods in generation quality, while being
qualitatively better by 25% on human evaluation and attaining lower toxicity by
76%. Also, ICL-based paraphrasers only show a slight reduction in performance
even with just 10% training data.
","2023-10-18","2310.10707v1.pdf"
"2310.10708","Chenxu Zhao","Chenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, Ninghao Liu","Automated Natural Language Explanation of Deep Visual Neurons with Large
  Models","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep neural networks have exhibited remarkable performance across a wide
range of real-world tasks. However, comprehending the underlying reasons for
their effectiveness remains a challenging problem. Interpreting deep neural
networks through examining neurons offers distinct advantages when it comes to
exploring the inner workings of neural networks. Previous research has
indicated that specific neurons within deep vision networks possess semantic
meaning and play pivotal roles in model performance. Nonetheless, the current
methods for generating neuron semantics heavily rely on human intervention,
which hampers their scalability and applicability. To address this limitation,
this paper proposes a novel post-hoc framework for generating semantic
explanations of neurons with large foundation models, without requiring human
intervention or prior knowledge. Our framework is designed to be compatible
with various model architectures and datasets, facilitating automated and
scalable neuron interpretation. Experiments are conducted with both qualitative
and quantitative analysis to verify the effectiveness of our proposed approach.
","2023-10-18","2310.10708v1.pdf"
"2310.10760","Dhagash Mehta","Bhaskarjit Sarmah, Tianjie Zhu, Dhagash Mehta, Stefano Pasquali","Towards reducing hallucination in extracting information from financial
  reports using Large Language Models","4 pages + references. Accepted for publication in Workshop on
  Generative AI at the 3rd International Conference on AI-ML Systems 2023,
  Bengaluru, India","","","","cs.CL q-fin.PM q-fin.ST stat.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  For a financial analyst, the question and answer (Q\&A) segment of the
company financial report is a crucial piece of information for various analysis
and investment decisions. However, extracting valuable insights from the Q\&A
section has posed considerable challenges as the conventional methods such as
detailed reading and note-taking lack scalability and are susceptible to human
errors, and Optical Character Recognition (OCR) and similar techniques
encounter difficulties in accurately processing unstructured transcript text,
often missing subtle linguistic nuances that drive investor decisions. Here, we
demonstrate the utilization of Large Language Models (LLMs) to efficiently and
rapidly extract information from earnings report transcripts while ensuring
high accuracy transforming the extraction process as well as reducing
hallucination by combining retrieval-augmented generation technique as well as
metadata. We evaluate the outcomes of various LLMs with and without using our
proposed approach based on various objective metrics for evaluating Q\&A
systems, and empirically demonstrate superiority of our method.
","2023-10-18","2310.10760v1.pdf"
"2310.10765","Yu Gu","Yu Gu, Jianwei Yang, Naoto Usuyama, Chunyuan Li, Sheng Zhang, Matthew
  P. Lungren, Jianfeng Gao, Hoifung Poon","BiomedJourney: Counterfactual Biomedical Image Generation by
  Instruction-Learning from Multimodal Patient Journeys","Project page & demo: https://aka.ms/biomedjourney","","","","cs.CV cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Rapid progress has been made in instruction-learning for image editing with
natural-language instruction, as exemplified by InstructPix2Pix. In
biomedicine, such methods can be applied to counterfactual image generation,
which helps differentiate causal structure from spurious correlation and
facilitate robust image interpretation for disease progression modeling.
However, generic image-editing models are ill-suited for the biomedical domain,
and counterfactual biomedical image generation is largely underexplored. In
this paper, we present BiomedJourney, a novel method for counterfactual
biomedical image generation by instruction-learning from multimodal patient
journeys. Given a patient with two biomedical images taken at different time
points, we use GPT-4 to process the corresponding imaging reports and generate
a natural language description of disease progression. The resulting triples
(prior image, progression description, new image) are then used to train a
latent diffusion model for counterfactual biomedical image generation. Given
the relative scarcity of image time series data, we introduce a two-stage
curriculum that first pretrains the denoising network using the much more
abundant single image-report pairs (with dummy prior image), and then continues
training using the counterfactual triples. Experiments using the standard
MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive
battery of tests on counterfactual medical image generation, BiomedJourney
substantially outperforms prior state-of-the-art methods in instruction image
editing and medical image generation such as InstructPix2Pix and RoentGen. To
facilitate future study in counterfactual medical generation, we plan to
release our instruction-learning code and pretrained models.
","2023-10-24","2310.10765v1.pdf"
"2310.10769","Ruiqi Wu","Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, Xiangyu
  Zhang","LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation","Project Page: https://rq-wu.github.io/projects/LAMP","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  With the impressive progress in diffusion-based text-to-image generation,
extending such powerful generative ability to text-to-video raises enormous
attention. Existing methods either require large-scale text-video pairs and a
large number of training resources or learn motions that are precisely aligned
with template videos. It is non-trivial to balance a trade-off between the
degree of generation freedom and the resource costs for video generation. In
our study, we present a few-shot-based tuning framework, LAMP, which enables
text-to-image diffusion model Learn A specific Motion Pattern with 8~16 videos
on a single GPU. Specifically, we design a first-frame-conditioned pipeline
that uses an off-the-shelf text-to-image model for content generation so that
our tuned video diffusion model mainly focuses on motion learning. The
well-developed text-to-image techniques can provide visually pleasing and
diverse content as generation conditions, which highly improves video quality
and generation freedom. To capture the features of temporal dimension, we
expand the pretrained 2D convolution layers of the T2I model to our novel
temporal-spatial motion learning layers and modify the attention blocks to the
temporal level. Additionally, we develop an effective inference trick,
shared-noise sampling, which can improve the stability of videos with
computational costs. Our method can also be flexibly applied to other tasks,
e.g. real-world image animation and video editing. Extensive experiments
demonstrate that LAMP can effectively learn the motion pattern on limited data
and generate high-quality videos. The code and models are available at
https://rq-wu.github.io/projects/LAMP.
","2023-10-18","2310.10769v1.pdf"
"2310.10781","Saumajit Saha","Saumajit Saha and Albert Nanda","BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models
  for Violence Inciting Text Detection in Bengali","5 pages, 2 figures, workshop","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper presents the system that we have developed while solving this
shared task on violence inciting text detection in Bangla. We explain both the
traditional and the recent approaches that we have used to make our models
learn. Our proposed system helps to classify if the given text contains any
threat. We studied the impact of data augmentation when there is a limited
dataset available. Our quantitative results show that finetuning a
multilingual-e5-base model performed the best in our task compared to other
transformer-based architectures. We obtained a macro F1 of 68.11\% in the test
set and our performance in this shared task is ranked at 23 in the leaderboard.
","2023-10-18","2310.10781v1.pdf"
"2310.10808","Christian Weilbach","Giselle Gonzalez Garcia, Christian Weilbach","If the Sources Could Talk: Evaluating Large Language Models for Research
  Assistance in History","Will be published at CHR2023","","","","cs.IR cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  The recent advent of powerful Large-Language Models (LLM) provides a new
conversational form of inquiry into historical memory (or, training data, in
this case). We show that by augmenting such LLMs with vector embeddings from
highly specialized academic sources, a conversational methodology can be made
accessible to historians and other researchers in the Humanities. Concretely,
we evaluate and demonstrate how LLMs have the ability of assisting researchers
while they examine a customized corpora of different types of documents,
including, but not exclusive to: (1). primary sources, (2). secondary sources
written by experts, and (3). the combination of these two. Compared to
established search interfaces for digital catalogues, such as metadata and
full-text search, we evaluate the richer conversational style of LLMs on the
performance of two main types of tasks: (1). question-answering, and (2).
extraction and organization of data. We demonstrate that LLMs semantic
retrieval and reasoning abilities on problem-specific tasks can be applied to
large textual archives that have not been part of the its training data.
Therefore, LLMs can be augmented with sources relevant to specific research
projects, and can be queried privately by researchers.
","2023-10-18","2310.10808v1.pdf"
"2310.10822","Chengguang Xu","Chengguang Xu, Hieu T. Nguyen, Christopher Amato, Lawson L.S. Wong","Vision and Language Navigation in the Real World via Online Visual
  Language Mapping","","","","","cs.RO cs.CV cs.SY eess.SY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Navigating in unseen environments is crucial for mobile robots. Enhancing
them with the ability to follow instructions in natural language will further
improve navigation efficiency in unseen cases. However, state-of-the-art (SOTA)
vision-and-language navigation (VLN) methods are mainly evaluated in
simulation, neglecting the complex and noisy real world. Directly transferring
SOTA navigation policies trained in simulation to the real world is challenging
due to the visual domain gap and the absence of prior knowledge about unseen
environments. In this work, we propose a novel navigation framework to address
the VLN task in the real world. Utilizing the powerful foundation models, the
proposed framework includes four key components: (1) an LLMs-based instruction
parser that converts the language instruction into a sequence of pre-defined
macro-action descriptions, (2) an online visual-language mapper that builds a
real-time visual-language map to maintain a spatial and semantic understanding
of the unseen environment, (3) a language indexing-based localizer that grounds
each macro-action description into a waypoint location on the map, and (4) a
DD-PPO-based local controller that predicts the action. We evaluate the
proposed pipeline on an Interbotix LoCoBot WX250 in an unseen lab environment.
Without any fine-tuning, our pipeline significantly outperforms the SOTA VLN
baseline in the real world.
","2023-10-18","2310.10822v1.pdf"
"2310.10837","R\'obert Csord\'as","R\'obert Csord\'as, Kazuki Irie, J\""urgen Schmidhuber","Approximating Two-Layer Feedforward Networks for Efficient Transformers","Accepted to EMNLP 2023 Findings","","","","cs.LG cs.NE","http://creativecommons.org/licenses/by/4.0/","  How to reduce compute and memory requirements of neural networks (NNs)
without sacrificing performance? Many recent works use sparse Mixtures of
Experts (MoEs) to build resource-efficient large language models (LMs). Here we
introduce several novel perspectives on MoEs, presenting a general framework
that unifies various methods to approximate two-layer NNs (e.g., feedforward
blocks of Transformers), including product-key memories (PKMs). Leveraging
insights from this framework, we propose methods to improve both MoEs and PKMs.
Unlike prior work that compares MoEs with dense baselines under the
compute-equal condition, our evaluation condition is parameter-equal, which is
crucial to properly evaluate LMs. We show that our MoEs are competitive with
the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two
different scales, while being much more resource efficient. This demonstrates
that MoEs are relevant not only to extremely large LMs but also to any-scale
resource-efficient LMs. Our code is public.
","2023-10-24","2310.10837v1.pdf"
"2310.10844","Erfan Shayegani","Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong,
  Nael Abu-Ghazaleh","Survey of Vulnerabilities in Large Language Models Revealed by
  Adversarial Attacks","","","","","cs.CL cs.CR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are swiftly advancing in architecture and
capability, and as they integrate more deeply into complex systems, the urgency
to scrutinize their security properties grows. This paper surveys research in
the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield
of trustworthy ML, combining the perspectives of Natural Language Processing
and Security. Prior work has shown that even safety-aligned LLMs (via
instruction tuning and reinforcement learning through human feedback) can be
susceptible to adversarial attacks, which exploit weaknesses and mislead AI
systems, as evidenced by the prevalence of `jailbreak' attacks on models like
ChatGPT and Bard. In this survey, we first provide an overview of large
language models, describe their safety alignment, and categorize existing
research based on various learning structures: textual-only attacks,
multi-modal attacks, and additional attack methods specifically targeting
complex systems, such as federated learning or multi-agent systems. We also
offer comprehensive remarks on works that focus on the fundamental sources of
vulnerabilities and potential defenses. To make this field more accessible to
newcomers, we present a systematic review of existing works, a structured
typology of adversarial attack concepts, and additional resources, including
slides for presentations on related topics at the 62nd Annual Meeting of the
Association for Computational Linguistics (ACL'24).
","2023-10-18","2310.10844v1.pdf"
"2310.10845","Amirkeivan Mohtashami","Amirkeivan Mohtashami, Matteo Pagliardini, Martin Jaggi","CoTFormer: More Tokens With Attention Make Up For Less Depth","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The race to continually develop ever larger and deeper foundational models is
underway. However, techniques like the Chain-of-Thought (CoT) method continue
to play a pivotal role in achieving optimal downstream performance. In this
work, we establish an approximate parallel between using chain-of-thought and
employing a deeper transformer. Building on this insight, we introduce
CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to
achieve capacity comparable to a deeper model. Our empirical findings
demonstrate the effectiveness of CoTFormers, as they significantly outperform
larger standard transformers.
","2023-10-18","2310.10845v1.pdf"
"2310.10873","Xiaobo Xia","Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu,
  Qingyun Wu, Tongliang Liu","IDEAL: Influence-Driven Selective Annotations Empower In-Context
  Learners in Large Language Models","24 pages, 6 figures, 11 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In-context learning is a promising paradigm that utilizes in-context examples
as prompts for the predictions of large language models. These prompts are
crucial for achieving strong performance. However, since the prompts need to be
sampled from a large volume of annotated examples, finding the right prompt may
result in high annotation costs. To address this challenge, this paper
introduces an influence-driven selective annotation method that aims to
minimize annotation costs while improving the quality of in-context examples.
The essence of our method is to select a pivotal subset from a large-scale
unlabeled data pool to annotate for the subsequent sampling of prompts.
Specifically, a directed graph is first constructed to represent unlabeled
data. Afterward, the influence of candidate unlabeled subsets is quantified
with a diffusion process. A simple yet effective greedy algorithm for unlabeled
data selection is lastly introduced. It iteratively selects the data if it
provides a maximum marginal gain with respect to quantified influence. Compared
with previous efforts on selective annotations, our influence-driven method
works in an end-to-end manner, avoids an intractable explicit balance between
data diversity and representativeness, and enjoys theoretical support.
Experiments confirm the superiority of the proposed method on various
benchmarks, achieving better performance under lower time consumption during
subset selection. The project page is available at
https://skzhang1.github.io/IDEAL/.
","2023-10-18","2310.10873v1.pdf"
"2310.10903","Sharin Jacob","Sharin Jacob, Tamara Tate, Mark Warschauer","Emergent AI-Assisted Discourse: Case Study of a Second Language Writer
  Authoring with ChatGPT","24 pages","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The rapid proliferation of ChatGPT has incited debates regarding its impact
on human writing. Amid concerns about declining writing standards, this study
investigates the role of ChatGPT in facilitating academic writing, especially
among language learners. Using a case study approach, this study examines the
experiences of Kailing, a doctoral student, who integrates ChatGPT throughout
their academic writing process. The study employs activity theory as a lens for
understanding writing with generative AI tools and data analyzed includes
semi-structured interviews, writing samples, and GPT logs. Results indicate
that Kailing effectively collaborates with ChatGPT across various writing
stages while preserving her distinct authorial voice and agency. This
underscores the potential of AI tools such as ChatGPT to enhance academic
writing for language learners without overshadowing individual authenticity.
This case study offers a critical exploration of how ChatGPT is utilized in the
academic writing process and the preservation of a student's authentic voice
when engaging with the tool.
","2023-10-24","2310.10903v1.pdf"
"2310.10908","Zihan Qiu","Zihan Qiu, Zeyu Huang, Jie Fu","Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit
  from Emergent Modular Structures?","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Incorporating modular designs into neural networks demonstrates superior
out-of-generalization, learning efficiency, etc. Existing modular neural
networks are generally $\textit{explicit}$ because their modular architectures
are pre-defined, and individual modules are expected to implement distinct
functions. Conversely, recent works reveal that there exist $\textit{implicit}$
modular structures in standard pre-trained transformers, namely
$\textit{Emergent Modularity}$. They indicate that such modular structures
exhibit during the early pre-training phase and are totally spontaneous.
However, most transformers are still treated as monolithic models with their
modular natures underutilized. Therefore, given the excellent properties of
explicit modular architecture, we explore $\textit{whether and how dense
pre-trained transformers can benefit from emergent modular structures.}$ To
study this question, we construct \textbf{E}mergent
$\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (EMoE). Without introducing
additional parameters, EMoE can be seen as the modular counterpart of the
original model and can be effortlessly incorporated into downstream tuning.
Extensive experiments (we tune 1785 models) on various downstream tasks (vision
and language) and models (22M to1.5B) demonstrate that EMoE effectively boosts
in-domain and out-of-domain generalization abilities. Further analysis and
ablation study suggest that EMoE mitigates negative knowledge transfer and is
robust to various configurations. Code is available at
\url{https://github.com/qiuzh20/EMoE}
","2023-10-18","2310.10908v1.pdf"
"2310.10912","Lv Tang","Lv Tang, Peng-Tao Jiang, Hao-Ke Xiao, Bo Li","Towards Training-free Open-world Segmentation via Image Prompting
  Foundation Models","","","","","cs.CV","http://creativecommons.org/publicdomain/zero/1.0/","  The realm of computer vision has witnessed a paradigm shift with the advent
of foundational models, mirroring the transformative influence of large
language models in the domain of natural language processing. This paper delves
into the exploration of open-world segmentation, presenting a novel approach
called Image Prompt Segmentation (IPSeg) that harnesses the power of vision
foundational models. At the heart of IPSeg lies the principle of a
training-free paradigm, which capitalizes on image prompting techniques. IPSeg
utilizes a single image containing a subjective visual concept as a flexible
prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our
approach extracts robust features for the prompt image and input image, then
matches the input representations to the prompt representations via a novel
feature interaction module to generate point prompts highlighting target
objects in the input image. The generated point prompts are further utilized to
guide the Segment Anything Model to segment the target object in the input
image. The proposed method stands out by eliminating the need for exhaustive
training sessions, thereby offering a more efficient and scalable solution.
Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's
efficacy for flexible open-world segmentation using intuitive image prompts.
This work pioneers tapping foundation models for open-world understanding
through visual concepts conveyed in images.
","2023-10-18","2310.10912v1.pdf"
"2310.10920","Anurag Acharya","Anurag Acharya, Sai Munikoti, Aaron Hellinger, Sara Smith, Sridevi
  Wagle, and Sameera Horawalavithana","NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear
  Domain","9 pages","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  As LLMs have become increasingly popular, they have been used in almost every
field. But as the application for LLMs expands from generic fields to narrow,
focused science domains, there exists an ever-increasing gap in ways to
evaluate their efficacy in those fields. For the benchmarks that do exist, a
lot of them focus on questions that don't require proper understanding of the
subject in question. In this paper, we present NuclearQA, a human-made
benchmark of 100 questions to evaluate language models in the nuclear domain,
consisting of a varying collection of questions that have been specifically
designed by experts to test the abilities of language models. We detail our
approach and show how the mix of several types of questions makes our benchmark
uniquely capable of evaluating models in the nuclear domain. We also present
our own evaluation metric for assessing LLM's performances due to the
limitations of existing ones. Our experiments on state-of-the-art models
suggest that even the best LLMs perform less than satisfactorily on our
benchmark, demonstrating the scientific knowledge gap of existing LLMs.
","2023-10-18","2310.10920v1.pdf"
"2310.10921","Nathan Cooper","Nathan Cooper","Intelligent Software Tooling for Improving Software Development","PhD thesis","","","","cs.SE cs.AI","http://creativecommons.org/licenses/by/4.0/","  Software has eaten the world with many of the necessities and quality of life
services people use requiring software. Therefore, tools that improve the
software development experience can have a significant impact on the world such
as generating code and test cases, detecting bugs, question and answering,
etc., The success of Deep Learning (DL) over the past decade has shown huge
advancements in automation across many domains, including Software Development
processes. One of the main reasons behind this success is the availability of
large datasets such as open-source code available through GitHub or image
datasets of mobile Graphical User Interfaces (GUIs) with RICO and ReDRAW to be
trained on. Therefore, the central research question my dissertation explores
is: In what ways can the software development process be improved through
leveraging DL techniques on the vast amounts of unstructured software
engineering artifacts?
","2023-10-18","2310.10921v1.pdf"
"2310.10942","Yangyang Guo","Yanyang Guo and Fangkai Jiao and Zhiqi Shen and Liqiang Nie and Mohan
  Kankanhalli","Unanswerable Visual Question Answering","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Teaching Visual Question Answering (VQA) models to abstain from unanswerable
questions is indispensable for building a trustworthy AI system. Existing
studies, though have explored various aspects of VQA, yet marginally ignored
this particular attribute. This paper aims to bridge the research gap by
contributing a comprehensive dataset, called UNK-VQA. The dataset is
specifically designed to address the challenge of questions that can be
unanswerable. To this end, we first augment the existing data via deliberate
perturbations on either the image or question. In specific, we carefully ensure
that the question-image semantics remain close to the original unperturbed
distribution. By means of this, the identification of unanswerable questions
becomes challenging, setting our dataset apart from others that involve mere
image replacement. We then extensively evaluate the zero- and few-shot
performance of several emerging multi-modal large models and discover
significant limitations of them when applied to our dataset. Additionally, we
also propose a straightforward method to tackle these unanswerable questions.
This dataset, we believe, will serve as a valuable benchmark for enhancing the
abstention capability of VQA models, thereby leading to increased
trustworthiness of AI systems.
","2023-10-18","2310.10942v1.pdf"
"2310.10944","Yiyang Cai Mr","Wenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao Shen","TEQ: Trainable Equivalent Transformation for Quantization of LLMs","10 pages, 3 figures","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  As large language models (LLMs) become more prevalent, there is a growing
need for new and improved quantization methods that can meet the
computationalast layer demands of these modern architectures while maintaining
the accuracy. In this paper, we present TEQ, a trainable equivalent
transformation that preserves the FP32 precision of the model output while
taking advantage of low-precision quantization, especially 3 and 4 bits
weight-only quantization. The training process is lightweight, requiring only
1K steps and fewer than 0.1 percent of the original model's trainable
parameters. Furthermore, the transformation does not add any computational
overhead during inference. Our results are on-par with the state-of-the-art
(SOTA) methods on typical LLMs. Our approach can be combined with other methods
to achieve even better performance. The code is available at
https://github.com/intel/neural-compressor.
","2023-10-18","2310.10944v1.pdf"
"2310.10955","Esmat Sahak","Esmat Sahak, Zining Zhu, Frank Rudzicz","A State-Vector Framework for Dataset Effects","EMNLP 2023","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The impressive success of recent deep neural network (DNN)-based systems is
significantly influenced by the high-quality datasets used in training.
However, the effects of the datasets, especially how they interact with each
other, remain underexplored. We propose a state-vector framework to enable
rigorous studies in this direction. This framework uses idealized probing test
results as the bases of a vector space. This framework allows us to quantify
the effects of both standalone and interacting datasets. We show that the
significant effects of some commonly-used language understanding datasets are
characteristic and are concentrated on a few linguistic dimensions.
Additionally, we observe some ``spill-over'' effects: the datasets could impact
the models along dimensions that may seem unrelated to the intended tasks. Our
state-vector framework paves the way for a systematic understanding of the
dataset effects, a crucial component in responsible and robust model
development.
","2023-10-18","2310.10955v1.pdf"
"2310.10962","Huiming Wang","Huiming Wang, Liying Cheng, Zhaodonghui Li, De Wen Soh, Lidong Bing","Semantic-Aware Contrastive Sentence Representation Learning with Large
  Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Contrastive learning has been proven to be effective in learning better
sentence representations. However, to train a contrastive learning model, large
numbers of labeled sentences are required to construct positive and negative
pairs explicitly, such as those in natural language inference (NLI) datasets.
Unfortunately, acquiring sufficient high-quality labeled data can be both
time-consuming and resource-intensive, leading researchers to focus on
developing methods for learning unsupervised sentence representations. As there
is no clear relationship between these unstructured randomly-sampled sentences,
building positive and negative pairs over them is tricky and problematic. To
tackle these challenges, in this paper, we propose SemCSR, a semantic-aware
contrastive sentence representation framework. By leveraging the generation and
evaluation capabilities of large language models (LLMs), we can automatically
construct a high-quality NLI-style corpus without any human annotation, and
further incorporate the generated sentence pairs into learning a contrastive
sentence representation model. Extensive experiments and comprehensive analyses
demonstrate the effectiveness of our proposed framework for learning a better
sentence representation with LLMs.
","2023-10-18","2310.10962v1.pdf"
"2310.10967","Pinren Lu","Hang Yin, Pinren Lu, Ziang Li, Bin Sun, Kan Li","EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset","","","","","cs.CL cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The need for high-quality data has been a key issue hindering the research of
dialogue tasks. Recent studies try to build datasets through manual, web
crawling, and large pre-trained models. However, man-made data is expensive and
data collected from the internet often includes generic responses, meaningless
statements, and toxic dialogues. Automatic data generation through large models
is a cost-effective method, but for open-domain multimodal dialogue tasks,
there are still three drawbacks: 1) There is currently no open-source large
model that can accept multimodal input; 2) The content generated by the model
lacks interpretability; 3) The generated data is usually difficult to quality
control and require extensive resource to collect. To alleviate the significant
human and resource expenditure in data collection, we propose a Multimodal Data
Construction Framework (MDCF). MDCF designs proper prompts to spur the
large-scale pre-trained language model to generate well-formed and satisfactory
content. Additionally, MDCF also automatically provides explanation for a given
image and its corresponding dialogue, which can provide a certain degree of
interpretability and facilitate manual follow-up quality inspection. Based on
this, we release an Explanatory Multimodal Open-Domain dialogue dataset
(EXMODD). Experiments indicate a positive correlation between the model's
ability to generate accurate understandings and high-quality responses. Our
code and data can be found at https://github.com/poplpr/EXMODD.
","2023-10-18","2310.10967v1.pdf"
"2310.10971","Christopher Fifty","Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure
  Leskovec, Christopher R\'e, Sebastian Thrun","Context-Aware Meta-Learning","","","","","cs.LG cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models like ChatGPT demonstrate a remarkable capacity to learn
new concepts during inference without any fine-tuning. However, visual models
trained to detect new objects during inference have been unable to replicate
this ability, and instead either perform poorly or require meta-training and/or
fine-tuning on similar objects. In this work, we propose a meta-learning
algorithm that emulates Large Language Models by learning new visual concepts
during inference without fine-tuning. Our approach leverages a frozen
pre-trained feature extractor, and analogous to in-context learning, recasts
meta-learning as sequence modeling over datapoints with known labels and a test
datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our
approach -- without meta-training or fine-tuning -- exceeds or matches the
state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.
","2023-10-18","2310.10971v1.pdf"
"2310.10981","Bin Wang","Bin Wang, Zhengyuan Liu, Nancy F. Chen","Instructive Dialogue Summarization with Query Aggregations","Accept to EMNLP 2023 Main Conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Conventional dialogue summarization methods directly generate summaries and
do not consider user's specific interests. This poses challenges in cases where
the users are more focused on particular topics or aspects. With the
advancement of instruction-finetuned language models, we introduce
instruction-tuning to dialogues to expand the capability set of dialogue
summarization models. To overcome the scarcity of instructive dialogue
summarization data, we propose a three-step approach to synthesize high-quality
query-based summarization triples. This process involves summary-anchored query
generation, query filtering, and query-based summary generation. By training a
unified model called InstructDS (Instructive Dialogue Summarization) on three
summarization datasets with multi-purpose instructive triples, we expand the
capability of dialogue summarization models. We evaluate our method on four
datasets, including dialogue summarization and dialogue reading comprehension.
Experimental results show that our approach outperforms the state-of-the-art
models and even models with larger sizes. Additionally, our model exhibits
higher generalizability and faithfulness, as confirmed by human subjective
evaluations.
","2023-10-18","2310.10981v1.pdf"
"2310.11003","Yingyi Ma","Yingyi Ma, Zhe Liu, Ozlem Kalinli","Correction Focused Language Model Training for Speech Recognition","","","","","cs.CL cs.LG cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models (LMs) have been commonly adopted to boost the performance of
automatic speech recognition (ASR) particularly in domain adaptation tasks.
Conventional way of LM training treats all the words in corpora equally,
resulting in suboptimal improvements in ASR performance. In this work, we
introduce a novel correction focused LM training approach which aims to
prioritize ASR fallible words. The word-level ASR fallibility score,
representing the likelihood of ASR mis-recognition, is defined and shaped as a
prior word distribution to guide the LM training. To enable correction focused
training with text-only corpora, large language models (LLMs) are employed as
fallibility score predictors and text generators through multi-task
fine-tuning. Experimental results for domain adaptation tasks demonstrate the
effectiveness of our proposed method. Compared with conventional LMs,
correction focused training achieves up to relatively 5.5% word error rate
(WER) reduction in sufficient text scenarios. In insufficient text scenarios,
LM training with LLM-generated text achieves up to relatively 13% WER
reduction, while correction focused training further obtains up to relatively
6% WER reduction.
","2023-10-18","2310.11003v1.pdf"
"2310.11026","Tomohito Kasahara","Tomohito Kasahara, Daisuke Kawahara","Exploring Automatic Evaluation Methods based on a Decoder-based LLM for
  Text Generation","Accepted to IJCNLP-AACL 2023 SRW","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Automatic evaluation of text generation is essential for improving the
accuracy of generation tasks. In light of the current trend towards
increasingly larger decoder-based language models, we investigate automatic
evaluation methods based on such models for text generation. This paper
compares various methods, including tuning with encoder-based models and large
language models under equal conditions, on two different tasks, machine
translation evaluation and semantic textual similarity, in two languages,
Japanese and English. Experimental results show that compared to the tuned
encoder-based models, the tuned decoder-based models perform poorly. The
analysis of the causes for this suggests that the decoder-based models focus on
surface word sequences and do not capture meaning. It is also revealed that
in-context learning of very large decoder-based models such as ChatGPT makes it
difficult to identify fine-grained semantic differences.
","2023-10-18","2310.11026v1.pdf"
"2310.11028","Rajarshi Saha","Rajarshi Saha, Varun Srivastava, Mert Pilanci","Matrix Compression via Randomized Low Rank and Low Precision
  Factorization","Accepted to the 37th Conference on Neural Information Processing
  Systems (NeurIPS 2023)","","","","cs.LG cs.IT math.IT stat.ML","http://creativecommons.org/licenses/by/4.0/","  Matrices are exceptionally useful in various fields of study as they provide
a convenient framework to organize and manipulate data in a structured manner.
However, modern matrices can involve billions of elements, making their storage
and processing quite demanding in terms of computational resources and memory
usage. Although prohibitively large, such matrices are often approximately low
rank. We propose an algorithm that exploits this structure to obtain a low rank
decomposition of any matrix $\mathbf{A}$ as $\mathbf{A} \approx
\mathbf{L}\mathbf{R}$, where $\mathbf{L}$ and $\mathbf{R}$ are the low rank
factors. The total number of elements in $\mathbf{L}$ and $\mathbf{R}$ can be
significantly less than that in $\mathbf{A}$. Furthermore, the entries of
$\mathbf{L}$ and $\mathbf{R}$ are quantized to low precision formats $--$
compressing $\mathbf{A}$ by giving us a low rank and low precision
factorization. Our algorithm first computes an approximate basis of the range
space of $\mathbf{A}$ by randomly sketching its columns, followed by a
quantization of the vectors constituting this basis. It then computes
approximate projections of the columns of $\mathbf{A}$ onto this quantized
basis. We derive upper bounds on the approximation error of our algorithm, and
analyze the impact of target rank and quantization bit-budget. The tradeoff
between compression ratio and approximation accuracy allows for flexibility in
choosing these parameters based on specific application requirements. We
empirically demonstrate the efficacy of our algorithm in image compression,
nearest neighbor classification of image and text embeddings, and compressing
the layers of LlaMa-$7$b. Our results illustrate that we can achieve
compression ratios as aggressive as one bit per matrix coordinate, all while
surpassing or maintaining the performance of traditional compression
techniques.
","2023-10-18","2310.11028v1.pdf"
"2310.11029","Swaraj Dube","Ashley Fernandez, Swaraj Dube","Core Building Blocks: Next Gen Geo Spatial GPT Application","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper proposes MapGPT which is a novel approach that integrates the
capabilities of language models, specifically large language models (LLMs),
with spatial data processing techniques. This paper introduces MapGPT, which
aims to bridge the gap between natural language understanding and spatial data
analysis by highlighting the relevant core building blocks. By combining the
strengths of LLMs and geospatial analysis, MapGPT enables more accurate and
contextually aware responses to location-based queries. The proposed
methodology highlights building LLMs on spatial and textual data, utilizing
tokenization and vector representations specific to spatial information. The
paper also explores the challenges associated with generating spatial vector
representations. Furthermore, the study discusses the potential of
computational capabilities within MapGPT, allowing users to perform geospatial
computations and obtain visualized outputs. Overall, this research paper
presents the building blocks and methodology of MapGPT, highlighting its
potential to enhance spatial data understanding and generation in natural
language processing applications.
","2023-10-19","2310.11029v1.pdf"
"2310.11031","Gyuseong Lee","Gyuseong Lee, Wooseok Jang, Jin Hyeon Kim, Jaewoo Jung, Seungryong Kim","Domain Generalization Using Large Pretrained Models with
  Mixture-of-Adapters","20 pages, 11 figures","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Learning a robust vision model despite large distribution shift is essential
for model deployment in real-world settings. Especially, domain generalization
(DG) algorithm aims to maintain the performance of a trained model on different
distributions which were not seen during training. One of the most effective
methods has been leveraging the already learned rich knowledge of large
pretrained models. However, naively fine-tuning large models to DG tasks is
often practically infeasible due to memory limitations, extensive time
requirements for training, and the risk of learned knowledge deterioration.
Recently, parameter-efficient fine-tuning (PEFT) methods have been proposed to
reduce the high computational cost during training and efficiently adapt large
models to downstream tasks. In this work, for the first time, we find that the
use of adapters in PEFT methods not only reduce high computational cost during
training but also serve as an effective regularizer for DG tasks. Surprisingly,
a naive adapter implementation for large models achieve superior performance on
common datasets. However, in situations of large distribution shifts,
additional factors such as optimal amount of regularization due to the strength
of distribution shifts should be considered for a sophisticated adapter
implementation. To address this, we propose a mixture-of-expert based adapter
fine-tuning method, dubbed as mixture-of-adapters (MoA). Specifically, we
employ multiple adapters that have varying capacities, and by using learnable
routers, we allocate each token to a proper adapter. By using both PEFT and MoA
methods, we effectively alleviate the performance deterioration caused by
distribution shifts and achieve state-of-the-art performance on diverse DG
benchmarks.
","2023-10-18","2310.11031v1.pdf"
"2310.11046","Lin Wang","Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li","Fast Graph Condensation with Structure-based Neural Tangent Kernel","15 pages, 6 figures, 5 tables","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  The rapid development of Internet technology has given rise to a vast amount
of graph-structured data. Graph Neural Networks (GNNs), as an effective method
for various graph mining tasks, incurs substantial computational resource costs
when dealing with large-scale graph data. A data-centric manner solution is
proposed to condense the large graph dataset into a smaller one without
sacrificing the predictive performance of GNNs. However, existing efforts
condense graph-structured data through a computational intensive bi-level
optimization architecture also suffer from massive computation costs. In this
paper, we propose reforming the graph condensation problem as a Kernel Ridge
Regression (KRR) task instead of iteratively training GNNs in the inner loop of
bi-level optimization. More specifically, We propose a novel dataset
condensation framework (GC-SNTK) for graph-structured data, where a
Structure-based Neural Tangent Kernel (SNTK) is developed to capture the
topology of graph and serves as the kernel function in KRR paradigm.
Comprehensive experiments demonstrate the effectiveness of our proposed model
in accelerating graph condensation while maintaining high prediction
performance.
","2023-10-18","2310.11046v1.pdf"
"2310.11053","Shitong Duan","Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu","Denevil: Towards Deciphering and Navigating the Ethical Values of Large
  Language Models via Instruction Learning","","","","","cs.CL cs.AI cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have made unprecedented breakthroughs, yet their
increasing integration into everyday life might raise societal risks due to
generated unethical content. Despite extensive study on specific issues like
bias, the intrinsic values of LLMs remain largely unexplored from a moral
philosophy perspective. This work delves into ethical values utilizing Moral
Foundation Theory. Moving beyond conventional discriminative evaluations with
poor reliability, we propose DeNEVIL, a novel prompt generation algorithm
tailored to dynamically exploit LLMs' value vulnerabilities and elicit the
violation of ethics in a generative manner, revealing their underlying value
inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset
comprising 2,397 prompts covering 500+ value principles, and then benchmark the
intrinsic values across a spectrum of LLMs. We discovered that most models are
essentially misaligned, necessitating further ethical value alignment. In
response, we develop VILMO, an in-context alignment method that substantially
enhances the value compliance of LLM outputs by learning to generate
appropriate value instructions, outperforming existing competitors. Our methods
are suitable for black-box and open-source models, offering a promising initial
step in studying the ethical values of LLMs.
","2023-10-18","2310.11053v1.pdf"
"2310.11079","Hsuan Su","Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay,
  Shang-Tse Chen, Hung-yi Lee","Learning from Red Teaming: Gender Bias Provocation and Mitigation in
  Large Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recently, researchers have made considerable improvements in dialogue systems
with the progress of large language models (LLMs) such as ChatGPT and GPT-4.
These LLM-based chatbots encode the potential biases while retaining
disparities that can harm humans during interactions. The traditional biases
investigation methods often rely on human-written test cases. However, these
test cases are usually expensive and limited. In this work, we propose a
first-of-its-kind method that automatically generates test cases to detect
LLMs' potential gender bias. We apply our method to three well-known LLMs and
find that the generated test cases effectively identify the presence of biases.
To address the biases identified, we propose a mitigation strategy that uses
the generated test cases as demonstrations for in-context learning to
circumvent the need for parameter fine-tuning. The experimental results show
that LLMs generate fairer responses with the proposed approach.
","2023-10-18","2310.11079v1.pdf"
"2310.11085","Yilmazcan Ozyurt","Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang","In-Context Few-Shot Relation Extraction via Pre-Trained Language Models","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Relation extraction aims at inferring structured human knowledge from textual
documents. State-of-the-art methods based on language models commonly have two
limitations: (1) they require named entities to be either given as input or
infer them, which introduces additional noise, and (2) they require human
annotations of documents. As a remedy, we present a novel framework for
in-context few-shot relation extraction via pre-trained language models. To the
best of our knowledge, we are the first to reformulate the relation extraction
task as a tailored in-context few-shot learning paradigm. Thereby, we achieve
crucial benefits in that we eliminate the need for both named entity
recognition and human annotation of documents. Unlike existing methods based on
fine-tuning, our framework is flexible in that it can be easily updated for a
new set of relations without re-training. We evaluate our framework using
DocRED, the largest publicly available dataset for document-level relation
extraction, and demonstrate that our framework achieves state-of-the-art
performance. Finally, our framework allows us to identify missing annotations,
and we thus show that our framework actually performs much better than the
original labels from the development set of DocRED.
","2023-10-18","2310.11085v1.pdf"
"2310.11097","Lorenzo Canale","Lorenzo Canale, Alberto Messina","Experimenting AI Technologies for Disinformation Combat: the IDMO
  Project","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The Italian Digital Media Observatory (IDMO) project, part of a European
initiative, focuses on countering disinformation and fake news. This report
outlines contributions from Rai-CRITS to the project, including: (i) the
creation of novel datasets for testing technologies (ii) development of an
automatic model for categorizing Pagella Politica verdicts to facilitate
broader analysis (iii) creation of an automatic model for recognizing textual
entailment with exceptional accuracy on the FEVER dataset (iv) assessment using
GPT-4 to identify textual entailmen (v) a game to raise awareness about fake
news at national events.
","2023-10-23","2310.11097v1.pdf"
"2310.11116","Shayan Alipour","Shayan Alipour, Alessandro Galeazzi, Emanuele Sangiorgio, Michele
  Avalle, Ljubisa Bojic, Matteo Cinelli, Walter Quattrociocchi","Cross-Platform Social Dynamics: An Analysis of ChatGPT and COVID-19
  Vaccine Conversations","","","","","cs.CY physics.soc-ph","http://creativecommons.org/licenses/by/4.0/","  The role of social media in information dissemination and agenda-setting has
significantly expanded in recent years. By offering real-time interactions,
online platforms have become invaluable tools for studying societal responses
to significant events as they unfold. However, online reactions to external
developments are influenced by various factors, including the nature of the
event and the online environment. This study examines the dynamics of public
discourse on digital platforms to shed light on this issue. We analyzed over 12
million posts and news articles related to two significant events: the release
of ChatGPT in 2022 and the global discussions about COVID-19 vaccines in 2021.
Data was collected from multiple platforms, including Twitter, Facebook,
Instagram, Reddit, YouTube, and GDELT. We employed topic modeling techniques to
uncover the distinct thematic emphases on each platform, which reflect their
specific features and target audiences. Additionally, sentiment analysis
revealed various public perceptions regarding the topics studied. Lastly, we
compared the evolution of engagement across platforms, unveiling unique
patterns for the same topic. Notably, discussions about COVID-19 vaccines
spread more rapidly due to the immediacy of the subject, while discussions
about ChatGPT, despite its technological importance, propagated more gradually.
","2023-10-18","2310.11116v1.pdf"
"2310.11146","Evelina Leivada","Evelina Leivada, Vittoria Dentella, Elliot Murphy","The Quo Vadis of the Relationship between Language and Large Language
  Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In the field of Artificial (General) Intelligence (AI), the several recent
advancements in Natural language processing (NLP) activities relying on Large
Language Models (LLMs) have come to encourage the adoption of LLMs as
scientific models of language. While the terminology employed for the
characterization of LLMs favors their embracing as such, it is not clear that
they are in a place to offer insights into the target system they seek to
represent. After identifying the most important theoretical and empirical risks
brought about by the adoption of scientific models that lack transparency, we
discuss LLMs relating them to every scientific model's fundamental components:
the object, the medium, the meaning and the user. We conclude that, at their
current stage of development, LLMs hardly offer any explanations for language,
and then we provide an outlook for more informative future research directions
on this topic.
","2023-10-18","2310.11146v1.pdf"
"2310.11154","Neville Kenneth Kitson","Neville K Kitson and Anthony C Constantinou","Causal discovery using dynamically requested knowledge","","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Causal Bayesian Networks (CBNs) are an important tool for reasoning under
uncertainty in complex real-world systems. Determining the graphical structure
of a CBN remains a key challenge and is undertaken either by eliciting it from
humans, using machine learning to learn it from data, or using a combination of
these two approaches. In the latter case, human knowledge is generally provided
to the algorithm before it starts, but here we investigate a novel approach
where the structure learning algorithm itself dynamically identifies and
requests knowledge for relationships that the algorithm identifies as uncertain
during structure learning. We integrate this approach into the Tabu structure
learning algorithm and show that it offers considerable gains in structural
accuracy, which are generally larger than those offered by existing approaches
for integrating knowledge. We suggest that a variant which requests only arc
orientation information may be particularly useful where the practitioner has
little preexisting knowledge of the causal relationships. As well as offering
improved accuracy, the approach can use human expertise more effectively and
contributes to making the structure learning process more transparent.
","2023-10-18","2310.11154v1.pdf"
"2310.11158","Honghua Chen","Honghua Chen and Nai Ding","Probing the Creativity of Large Language Models: Can models produce
  divergent semantic association?","Accepted for publication in Findings of EMNLP 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models possess remarkable capacity for processing language,
but it remains unclear whether these models can further generate creative
content. The present study aims to investigate the creative thinking of large
language models through a cognitive perspective. We utilize the divergent
association task (DAT), an objective measurement of creativity that asks models
to generate unrelated words and calculates the semantic distance between them.
We compare the results across different models and decoding strategies. Our
findings indicate that: (1) When using the greedy search strategy, GPT-4
outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level.
(2) Stochastic sampling and temperature scaling are effective to obtain higher
DAT scores for models except GPT-4, but face a trade-off between creativity and
stability. These results imply that advanced large language models have
divergent semantic associations, which is a fundamental process underlying
creativity.
","2023-10-18","2310.11158v1.pdf"
"2310.11163","Xu Huang","Xu Huang, Zhirui Zhang, Ruize Gao, Yichao Du, Lemao Liu, Gouping
  Huang, Shuming Shi, Jiajun Chen, Shujian Huang","IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing
  Interactive Machine Translation Systems","Accepted by EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present IMTLab, an open-source end-to-end interactive machine translation
(IMT) system platform that enables researchers to quickly build IMT systems
with state-of-the-art models, perform an end-to-end evaluation, and diagnose
the weakness of systems. IMTLab treats the whole interactive translation
process as a task-oriented dialogue with a human-in-the-loop setting, in which
human interventions can be explicitly incorporated to produce high-quality,
error-free translations. To this end, a general communication interface is
designed to support the flexible IMT architectures and user policies. Based on
the proposed design, we construct a simulated and real interactive environment
to achieve end-to-end evaluation and leverage the framework to systematically
evaluate previous IMT systems. Our simulated and manual experiments show that
the prefix-constrained decoding approach still gains the lowest editing cost in
the end-to-end evaluation, while BiTIIMT achieves comparable editing cost with
a better interactive experience.
","2023-10-18","2310.11163v1.pdf"
"2310.11173","Shuo Wang","Shuo Wang, Yan Zhu, Xiaoyuan Luo, Zhiwei Yang, Yizhe Zhang, Peiyao Fu,
  Manning Wang, Zhijian Song, Quanlin Li, Pinghong Zhou, Yike Guo","Knowledge Extraction and Distillation from Large-Scale Image-Text
  Colonoscopy Records Leveraging Large Language and Vision Models","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  The development of artificial intelligence systems for colonoscopy analysis
often necessitates expert-annotated image datasets. However, limitations in
dataset size and diversity impede model performance and generalisation.
Image-text colonoscopy records from routine clinical practice, comprising
millions of images and text reports, serve as a valuable data source, though
annotating them is labour-intensive. Here we leverage recent advancements in
large language and vision models and propose EndoKED, a data mining paradigm
for deep knowledge extraction and distillation. EndoKED automates the
transformation of raw colonoscopy records into image datasets with pixel-level
annotation. We validate EndoKED using multi-centre datasets of raw colonoscopy
records (~1 million images), demonstrating its superior performance in training
polyp detection and segmentation models. Furthermore, the EndoKED pre-trained
vision backbone enables data-efficient and generalisable learning for optical
biopsy, achieving expert-level performance in both retrospective and
prospective validation.
","2023-10-18","2310.11173v1.pdf"
"2310.11182","Heng Gu","(Eric) Heng Gu, Chadha Degachi, U\u{g}ur Gen\c{c}, Senthil
  Chandrasegaran, Himanshu Verma","On the Effectiveness of Creating Conversational Agent Personalities
  Through Prompting","6 pages, 1 table, PGAI CIKM 2023","","","","cs.HC","http://creativecommons.org/licenses/by/4.0/","  In this work, we report on the effectiveness of our efforts to tailor the
personality and conversational style of a conversational agent based on GPT-3.5
and GPT-4 through prompts. We use three personality dimensions with two levels
each to create eight conversational agents archetypes. Ten conversations were
collected per chatbot, of ten exchanges each, generating 1600 exchanges across
GPT-3.5 and GPT-4. Using Linguistic Inquiry and Word Count (LIWC) analysis, we
compared the eight agents on language elements including clout, authenticity,
and emotion. Four language cues were significantly distinguishing in GPT-3.5,
while twelve were distinguishing in GPT-4. With thirteen out of a total
nineteen cues in LIWC appearing as significantly distinguishing, our results
suggest possible novel prompting approaches may be needed to better suit the
creation and evaluation of persistent conversational agent personalities or
language styles.
","2023-10-18","2310.11182v1.pdf"
"2310.11191","Lorenzo Jaime Flores","Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang,
  Arman Cohan","Medical Text Simplification: Optimizing for Readability with
  Unlikelihood Training and Reranked Beam Search Decoding","EMNLP 2023 Findings","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Text simplification has emerged as an increasingly useful application of AI
for bridging the communication gap in specialized fields such as medicine,
where the lexicon is often dominated by technical jargon and complex
constructs. Despite notable progress, methods in medical simplification
sometimes result in the generated text having lower quality and diversity. In
this work, we explore ways to further improve the readability of text
simplification in the medical domain. We propose (1) a new unlikelihood loss
that encourages generation of simpler terms and (2) a reranked beam search
decoding method that optimizes for simplicity, which achieve better performance
on readability metrics on three datasets. This study's findings offer promising
avenues for improving text simplification in the medical field.
","2023-10-27","2310.11191v1.pdf"
"2310.11207","Yilun Zhou","Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou,
  Leilani H. Gilpin","Can Large Language Models Explain Themselves? A Study of LLM-Generated
  Self-Explanations","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) such as ChatGPT have demonstrated superior
performance on a variety of natural language processing (NLP) tasks including
sentiment analysis, mathematical reasoning and summarization. Furthermore,
since these models are instruction-tuned on human conversations to produce
""helpful"" responses, they can and often will produce explanations along with
the response, which we call self-explanations. For example, when analyzing the
sentiment of a movie review, the model may output not only the positivity of
the sentiment, but also an explanation (e.g., by listing the sentiment-laden
words such as ""fantastic"" and ""memorable"" in the review). How good are these
automatically generated self-explanations? In this paper, we investigate this
question on the task of sentiment analysis and for feature attribution
explanation, one of the most commonly studied settings in the interpretability
literature (for pre-ChatGPT models). Specifically, we study different ways to
elicit the self-explanations, evaluate their faithfulness on a set of
evaluation metrics, and compare them to traditional explanation methods such as
occlusion or LIME saliency maps. Through an extensive set of experiments, we
find that ChatGPT's self-explanations perform on par with traditional ones, but
are quite different from them according to various agreement metrics, meanwhile
being much cheaper to produce (as they are generated along with the
prediction). In addition, we identified several interesting characteristics of
them, which prompt us to rethink many current model interpretability practices
in the era of ChatGPT(-like) LLMs.
","2023-10-18","2310.11207v1.pdf"
"2310.11211","Wei Yao","Wei Yao, Zhanke Zhou, Zhicong Li, Bo Han, Yong Liu","Understanding Fairness Surrogate Functions in Algorithmic Fairness","8 pages","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It has been observed that machine learning algorithms exhibit biased
predictions against certain population groups. To mitigate such bias while
achieving comparable accuracy, a promising approach is to introduce surrogate
functions of the concerned fairness definition and solve a constrained
optimization problem. However, an intriguing issue in previous work is that
such fairness surrogate functions may yield unfair results. In this work, in
order to deeply understand this issue, taking a widely used fairness
definition, demographic parity as an example, we both theoretically and
empirically show that there is a surrogate-fairness gap between the fairness
definition and the fairness surrogate function. The ""gap"" directly determines
whether a surrogate function is an appropriate substitute for a fairness
definition. Also, the theoretical analysis and experimental results about the
""gap"" motivate us that the unbounded surrogate functions will be affected by
the points far from the decision boundary, which is the large margin points
issue investigated in this paper. To address it, we propose the general sigmoid
surrogate with a rigorous and reliable fairness guarantee. Interestingly, the
theory also provides insights into two important issues that deal with the
large margin points as well as obtaining a more balanced dataset are beneficial
to fairness. Furthermore, we elaborate a novel and general algorithm called
Balanced Surrogate, which iteratively reduces the ""gap"" to improve fairness.
Finally, we provide empirical evidence showing that our methods achieve better
fairness performance in three real-world datasets.
","2023-10-19","2310.11211v1.pdf"
"2310.11220","Jiho Kim","Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi","KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using
  Large Language Models","Accepted to EMNLP 2023 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While large language models (LLMs) have made considerable advancements in
understanding and generating unstructured text, their application in structured
data remains underexplored. Particularly, using LLMs for complex reasoning
tasks on knowledge graphs (KGs) remains largely untouched. To address this, we
propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing
KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and
Inference, each aimed at partitioning sentences, retrieving relevant graph
components, and deriving logical conclusions, respectively. We evaluate KG-GPT
using KG-based fact verification and KGQA benchmarks, with the model showing
competitive and robust performance, even outperforming several fully-supervised
models. Our work, therefore, marks a significant step in unifying structured
and unstructured data processing within the realm of LLMs.
","2023-10-18","2310.11220v1.pdf"
"2310.11227","Enyu Zhou","Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu
  Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang","RealBehavior: A Framework for Faithfully Characterizing Foundation
  Models' Human-like Behavior Mechanisms","Accepted to Findings of EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reports of human-like behaviors in foundation models are growing, with
psychological theories providing enduring tools to investigate these behaviors.
However, current research tends to directly apply these human-oriented tools
without verifying the faithfulness of their outcomes. In this paper, we
introduce a framework, RealBehavior, which is designed to characterize the
humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our
framework assesses the faithfulness of results based on reproducibility,
internal and external consistency, and generalizability. Our findings suggest
that a simple application of psychological tools cannot faithfully characterize
all human-like behaviors. Moreover, we discuss the impacts of aligning models
with human and social values, arguing for the necessity of diversifying
alignment objectives to prevent the creation of models with restricted
characteristics.
","2023-10-18","2310.11227v1.pdf"
"2310.11237","Pengyu Wang","Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu","Watermarking LLMs with Weight Quantization","Accepted by Findings of EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Abuse of large language models reveals high risks as large language models
are being deployed at an astonishing speed. It is important to protect the
model weights to avoid malicious usage that violates licenses of open-source
large language models. This paper proposes a novel watermarking strategy that
plants watermarks in the quantization process of large language models without
pre-defined triggers during inference. The watermark works when the model is
used in the fp32 mode and remains hidden when the model is quantized to int8,
in this way, the users can only inference the model without further supervised
fine-tuning of the model. We successfully plant the watermark into open-source
large language model weights including GPT-Neo and LLaMA. We hope our proposed
method can provide a potential direction for protecting model weights in the
era of large language model applications.
","2023-10-18","2310.11237v1.pdf"
"2310.11244","Ralph Peeters","Ralph Peeters, Christian Bizer","Entity Matching using Large Language Models","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Entity Matching is the task of deciding whether two entity descriptions refer
to the same real-world entity. Entity Matching is a central step in most data
integration pipelines and an enabler for many e-commerce applications which
require to match products offers from different vendors. State-of-the-art
entity matching methods often rely on pre-trained language models (PLMs) such
as BERT or RoBERTa. Two major drawbacks of these models for entity matching are
that (i) the models require significant amounts of task-specific training data
and (ii) the fine-tuned models are not robust concerning out-of-distribution
entities. In this paper, we investigate using large language models (LLMs) for
entity matching as a less domain-specific training data reliant and more robust
alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5
and GPT4, as well as open source LLMs based on Llama2 which can be run locally.
We evaluate these models in a zero-shot scenario as well as a scenario where
task-specific training data is available. We compare different prompt designs
as well as the prompt sensitivity of the models in the zero-shot scenario. We
investigate (i) the selection of in-context demonstrations, (ii) the generation
of matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenario
using the same pool of training data across the different approaches. Our
experiments show that GPT4 without any task-specific training data outperforms
fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets
reaching F1 scores around 90%. The experiments with in-context learning and
rule generation show that all models beside of GPT4 benefit from these
techniques (on average 5.9% and 2.2% F1), while GPT4 does not need such
additional guidance in most cases...
","2023-10-18","2310.11244v1.pdf"
"2310.11248","Yangruibo Ding","Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan,
  Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,
  Dan Roth, Bing Xiang","CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code
  Completion","To appear at NeurIPS 2023 (Datasets and Benchmarks Track)","","","","cs.LG cs.CL cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Code completion models have made significant progress in recent years, yet
current popular evaluation datasets, such as HumanEval and MBPP, predominantly
focus on code completion tasks within a single file. This over-simplified
setting falls short of representing the real-world software development
scenario where repositories span multiple files with numerous cross-file
dependencies, and accessing and understanding cross-file context is often
required to complete the code correctly.
  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual
code completion benchmark that necessitates an in-depth cross-file contextual
understanding to complete the code accurately. CrossCodeEval is built on a
diverse set of real-world, open-sourced, permissively-licensed repositories in
four popular programming languages: Python, Java, TypeScript, and C#. To create
examples that strictly require cross-file context for accurate completion, we
propose a straightforward yet efficient static-analysis-based approach to
pinpoint the use of cross-file context within the current file.
  Extensive experiments on state-of-the-art code language models like CodeGen
and StarCoder demonstrate that CrossCodeEval is extremely challenging when the
relevant cross-file context is absent, and we see clear improvements when
adding these context into the prompt. However, despite such improvements, the
pinnacle of performance remains notably unattained even with the
highest-performing model, indicating that CrossCodeEval is also capable of
assessing model's capability in leveraging extensive context to make better
code completion. Finally, we benchmarked various methods in retrieving
cross-file context, and show that CrossCodeEval can also be used to measure the
capability of code retrievers.
","2023-10-18","2310.11248v1.pdf"
"2310.11249","Xiao Yang","Xu Yang, Xiao Yang, Weiqing Liu, Jinhui Li, Peng Yu, Zeqi Ye, Jiang
  Bian","Leveraging Large Language Model for Automatic Evolving of Industrial
  Data-Centric R&D Cycle","29 pages, 11 figures","","","","cs.AI q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the wake of relentless digital transformation, data-driven solutions are
emerging as powerful tools to address multifarious industrial tasks such as
forecasting, anomaly detection, planning, and even complex decision-making.
Although data-centric R&D has been pivotal in harnessing these solutions, it
often comes with significant costs in terms of human, computational, and time
resources. This paper delves into the potential of large language models (LLMs)
to expedite the evolution cycle of data-centric R&D. Assessing the foundational
elements of data-centric R&D, including heterogeneous task-related data,
multi-facet domain knowledge, and diverse computing-functional tools, we
explore how well LLMs can understand domain-specific requirements, generate
professional ideas, utilize domain-specific tools to conduct experiments,
interpret results, and incorporate knowledge from past endeavors to tackle new
challenges. We take quantitative investment research as a typical example of
industrial data-centric R&D scenario and verified our proposed framework upon
our full-stack open-sourced quantitative research platform Qlib and obtained
promising results which shed light on our vision of automatic evolving of
industrial data-centric R&D cycle.
","2023-10-18","2310.11249v1.pdf"
"2310.11252","Thilo Spinner","Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias St\""ahle,
  Daniel A. Keim, Oliver Deussen, Andreas Spitz, Mennatallah El-Assady","Revealing the Unwritten: Visual Investigation of Beam Search Trees to
  Address Language Model Prompting Challenges","9 pages paper, 2 pages references, 7 figures","","","","cs.CL cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The growing popularity of generative language models has amplified interest
in interactive methods to guide model outputs. Prompt refinement is considered
one of the most effective means to influence output among these methods. We
identify several challenges associated with prompting large language models,
categorized into data- and model-specific, linguistic, and socio-linguistic
challenges. A comprehensive examination of model outputs, including runner-up
candidates and their corresponding probabilities, is needed to address these
issues. The beam search tree, the prevalent algorithm to sample model outputs,
can inherently supply this information. Consequently, we introduce an
interactive visual method for investigating the beam search tree, facilitating
analysis of the decisions made by the model during generation. We
quantitatively show the value of exposing the beam search tree and present five
detailed analysis scenarios addressing the identified challenges. Our
methodology validates existing results and offers additional insights.
","2023-10-18","2310.11252v1.pdf"
"2310.11266","Satwant Kumar","Khushboo Verma, Marina Moore, Stephanie Wottrich, Karla Robles
  L\'opez, Nishant Aggarwal, Zeel Bhatt, Aagamjit Singh, Bradford Unroe, Salah
  Basheer, Nitish Sachdeva, Prinka Arora, Harmanjeet Kaur, Tanupreet Kaur,
  Tevon Hood, Anahi Marquez, Tushar Varshney, Nanfu Deng, Azaan Ramani,
  Pawanraj Ishwara, Maimoona Saeed, Tatiana L\'opez Velarde Pe\~na, Bryan
  Barksdale, Sushovan Guha, Satwant Kumar","Emulating Human Cognitive Processes for Expert-Level Medical
  Question-Answering with Large Language Models","","","","","cs.CL cs.AI cs.NE","http://creativecommons.org/licenses/by-nc-sa/4.0/","  In response to the pressing need for advanced clinical problem-solving tools
in healthcare, we introduce BooksMed, a novel framework based on a Large
Language Model (LLM). BooksMed uniquely emulates human cognitive processes to
deliver evidence-based and reliable responses, utilizing the GRADE (Grading of
Recommendations, Assessment, Development, and Evaluations) framework to
effectively quantify evidence strength. For clinical decision-making to be
appropriately assessed, an evaluation metric that is clinically aligned and
validated is required. As a solution, we present ExpertMedQA, a multispecialty
clinical benchmark comprised of open-ended, expert-level clinical questions,
and validated by a diverse group of medical professionals. By demanding an
in-depth understanding and critical appraisal of up-to-date clinical
literature, ExpertMedQA rigorously evaluates LLM performance. BooksMed
outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT
in a variety of medical scenarios. Therefore, a framework that mimics human
cognitive stages could be a useful tool for providing reliable and
evidence-based responses to clinical inquiries.
","2023-10-18","2310.11266v1.pdf"
"2310.11282","Jaap Jumelet","Jaap Jumelet, Michael Hanna, Marianne de Heer Kloots, Anna Langedijk,
  Charlotte Pouw, Oskar van der Wal","ChapGTP, ILLC's Attempt at Raising a BabyLM: Improving Data Efficiency
  by Automatic Task Formation","Part of the BabyLM challenge at CoNLL","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present the submission of the ILLC at the University of Amsterdam to the
BabyLM challenge (Warstadt et al., 2023), in the strict-small track. Our final
model, ChapGTP, is a masked language model that was trained for 200 epochs,
aided by a novel data augmentation technique called Automatic Task Formation.
We discuss in detail the performance of this model on the three evaluation
suites: BLiMP, (Super)GLUE, and MSGS. Furthermore, we present a wide range of
methods that were ultimately not included in the model, but may serve as
inspiration for training LMs in low-resource settings.
","2023-10-18","2310.11282v1.pdf"
"2310.11291","Chiwun Yang","Zhao Song, Chiwun Yang","An Automatic Learning Rate Schedule Algorithm for Achieving Faster
  Convergence and Steeper Descent","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The delta-bar-delta algorithm is recognized as a learning rate adaptation
technique that enhances the convergence speed of the training process in
optimization by dynamically scheduling the learning rate based on the
difference between the current and previous weight updates. While this
algorithm has demonstrated strong competitiveness in full data optimization
when compared to other state-of-the-art algorithms like Adam and SGD, it may
encounter convergence issues in mini-batch optimization scenarios due to the
presence of noisy gradients.
  In this study, we thoroughly investigate the convergence behavior of the
delta-bar-delta algorithm in real-world neural network optimization. To address
any potential convergence challenges, we propose a novel approach called RDBD
(Regrettable Delta-Bar-Delta). Our approach allows for prompt correction of
biased learning rate adjustments and ensures the convergence of the
optimization process. Furthermore, we demonstrate that RDBD can be seamlessly
integrated with any optimization algorithm and significantly improve the
convergence speed.
  By conducting extensive experiments and evaluations, we validate the
effectiveness and efficiency of our proposed RDBD approach. The results
showcase its capability to overcome convergence issues in mini-batch
optimization and its potential to enhance the convergence speed of various
optimization algorithms. This research contributes to the advancement of
optimization techniques in neural network training, providing practitioners
with a reliable automatic learning rate scheduler for achieving faster
convergence and improved optimization outcomes.
","2023-10-18","2310.11291v1.pdf"
"2310.11303","Weiqi Wang Mr.","Haochen Shi, Weiqi Wang, Tianqing Fang, Baixuan Xu, Wenxuan Ding, Xin
  Liu, Yangqiu Song","QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for
  Zero-Shot Commonsense Question Answering","Findings of EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Zero-shot commonsense Question-Answering (QA) requires models to reason about
general situations beyond specific benchmarks. State-of-the-art approaches
fine-tune language models on QA pairs constructed from CommonSense Knowledge
Bases (CSKBs) to equip the models with more commonsense knowledge in a QA
context. However, current QA synthesis protocols may introduce noise from the
CSKBs and generate ungrammatical questions and false negative options, which
impede the model's ability to generalize. To address these issues, we propose
QADYNAMICS, a training dynamics-driven framework for QA diagnostics and
refinement. Our approach analyzes the training dynamics of each QA pair at both
the question level and option level, discarding machine-detectable artifacts by
removing uninformative QA pairs and mislabeled or false-negative options.
Extensive experiments demonstrate the effectiveness of our approach, which
outperforms all baselines while using only 33% of the synthetic data, even
including LLMs such as ChatGPT. Moreover, expert evaluations confirm that our
framework significantly improves the quality of QA synthesis. Our codes and
model checkpoints are available at
https://github.com/HKUST-KnowComp/QaDynamics.
","2023-10-18","2310.11303v1.pdf"
"2310.11318","Shiwei Zhang","Shiwei Zhang, Mingfang Wu, Xiuzhen Zhang","Utilising a Large Language Model to Annotate Subject Metadata: A Case
  Study in an Australian National Research Data Catalogue","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In support of open and reproducible research, there has been a rapidly
increasing number of datasets made available for research. As the availability
of datasets increases, it becomes more important to have quality metadata for
discovering and reusing them. Yet, it is a common issue that datasets often
lack quality metadata due to limited resources for data curation. Meanwhile,
technologies such as artificial intelligence and large language models (LLMs)
are progressing rapidly. Recently, systems based on these technologies, such as
ChatGPT, have demonstrated promising capabilities for certain data curation
tasks. This paper proposes to leverage LLMs for cost-effective annotation of
subject metadata through the LLM-based in-context learning. Our method employs
GPT-3.5 with prompts designed for annotating subject metadata, demonstrating
promising performance in automatic metadata annotation. However, models based
on in-context learning cannot acquire discipline-specific rules, resulting in
lower performance in several categories. This limitation arises from the
limited contextual information available for subject inference. To the best of
our knowledge, we are introducing, for the first time, an in-context learning
method that harnesses large language models for automated subject metadata
annotation.
","2023-10-18","2310.11318v1.pdf"
"2310.11324","Melanie Sclar","Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr","Quantifying Language Models' Sensitivity to Spurious Features in Prompt
  Design or: How I learned to start worrying about prompt formatting","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  As large language models (LLMs) are adopted as a fundamental component of
language technologies, it is crucial to accurately characterize their
performance. Because choices in prompt design can strongly influence model
behavior, this design process is critical in effectively using any modern
pre-trained generative language model. In this work, we focus on LLM
sensitivity to a quintessential class of meaning-preserving design choices:
prompt formatting. We find that several widely used open-source LLMs are
extremely sensitive to subtle changes in prompt formatting in few-shot
settings, with performance differences of up to 76 accuracy points when
evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model
size, the number of few-shot examples, or performing instruction tuning. Our
analysis suggests that work evaluating LLMs with prompting-based methods would
benefit from reporting a range of performance across plausible prompt formats,
instead of the currently-standard practice of reporting performance on a single
format. We also show that format performance only weakly correlates between
models, which puts into question the methodological validity of comparing
models with an arbitrarily chosen, fixed prompt format. To facilitate
systematic analysis we propose FormatSpread, an algorithm that rapidly
evaluates a sampled set of plausible prompt formats for a given task, and
reports the interval of expected performance without accessing model weights.
Furthermore, we present a suite of analyses that characterize the nature of
this sensitivity, including exploring the influence of particular atomic
perturbations and the internal representation of particular formats.
","2023-10-18","2310.11324v1.pdf"
"2310.11360","Langlin Huang","Langlin Huang, Shuhao Gu, Zhuocheng Zhang, Yang Feng","Enhancing Neural Machine Translation with Semantic Units","Accepted to EMNLP findings 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Conventional neural machine translation (NMT) models typically use subwords
and words as the basic units for model input and comprehension. However,
complete words and phrases composed of several tokens are often the fundamental
units for expressing semantics, referred to as semantic units. To address this
issue, we propose a method Semantic Units for Machine Translation (SU4MT) which
models the integral meanings of semantic units within a sentence, and then
leverages them to provide a new perspective for understanding the sentence.
Specifically, we first propose Word Pair Encoding (WPE), a phrase extraction
method to help identify the boundaries of semantic units. Next, we design an
Attentive Semantic Fusion (ASF) layer to integrate the semantics of multiple
subwords into a single vector: the semantic unit representation. Lastly, the
semantic-unit-level sentence representation is concatenated to the token-level
one, and they are combined as the input of encoder. Experimental results
demonstrate that our method effectively models and leverages
semantic-unit-level information and outperforms the strong baselines. The code
is available at https://github.com/ictnlp/SU4MT.
","2023-10-18","2310.11360v1.pdf"
"2310.11374","Mengyao Wang","Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi Li, Benyou Wang,
  Jing Qin","DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for
  Emotion Recognition in Conversations","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) and their variants have shown extraordinary
efficacy across numerous downstream natural language processing (NLP) tasks,
which has presented a new vision for the development of NLP. Despite their
remarkable performance in natural language generating (NLG), LLMs lack a
distinct focus on the emotion understanding domain. As a result, using LLMs for
emotion recognition may lead to suboptimal and inadequate precision. Another
limitation of LLMs is that they are typical trained without leveraging
multi-modal information. To overcome these limitations, we propose DialogueLLM,
a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA
models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.
The visual information is considered as the supplementary knowledge to
construct high-quality instructions. We offer a comprehensive evaluation of our
proposed model on three benchmarking emotion recognition in conversations (ERC)
datasets and compare the results against the SOTA baselines and other SOTA
LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB
A100 GPU in 5 hours, facilitating reproducibility for other researchers.
","2023-10-18","2310.11374v1.pdf"
"2310.11392","Yingxu He","Yingxu He and Qiqi Sun","Towards Automatic Satellite Images Captions Generation Using Large
  Language Models","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Automatic image captioning is a promising technique for conveying visual
information using natural language. It can benefit various tasks in satellite
remote sensing, such as environmental monitoring, resource management, disaster
management, etc. However, one of the main challenges in this domain is the lack
of large-scale image-caption datasets, as they require a lot of human expertise
and effort to create. Recent research on large language models (LLMs) has
demonstrated their impressive performance in natural language understanding and
generation tasks. Nonetheless, most of them cannot handle images (GPT-3.5,
Falcon, Claude, etc.), while conventional captioning models pre-trained on
general ground-view images often fail to produce detailed and accurate captions
for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we
propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to
automatically collect captions for remote sensing images by guiding LLMs to
describe their object annotations. We also present a benchmark model that
adapts the pre-trained generative image2text model (GIT) to generate
high-quality captions for remote-sensing images. Our evaluation demonstrates
the effectiveness of our approach for collecting captions for remote sensing
images.
","2023-10-18","2310.11392v1.pdf"
"2310.11397","Rui Wen","Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem","Last One Standing: A Comparative Analysis of Security and Privacy of
  Soft Prompt Tuning, LoRA, and In-Context Learning","","","","","cs.CR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are powerful tools for natural language
processing, enabling novel applications and user experiences. However, to
achieve optimal performance, LLMs often require adaptation with private data,
which poses privacy and security challenges. Several techniques have been
proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),
Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative
privacy and security properties have not been systematically investigated. In
this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL
against three types of well-established attacks: membership inference, which
exposes data leakage (privacy); backdoor, which injects malicious behavior
(security); and model stealing, which can violate intellectual property
(privacy and security). Our results show that there is no silver bullet for
privacy and security in LLM adaptation and each technique has different
strengths and weaknesses.
","2023-10-18","2310.11397v1.pdf"
"2310.11409","Andreas Happe","Andreas Happe, Aaron Kaplan, J\""urgen Cito","Evaluating LLMs for Privilege-Escalation Scenarios","","","","","cs.CR cs.AI","http://creativecommons.org/licenses/by/4.0/","  Penetration testing, an essential component of cybersecurity, allows
organizations to proactively identify and remediate vulnerabilities in their
systems, thus bolstering their defense mechanisms against potential
cyberattacks. One recent advancement in the realm of penetration testing is the
utilization of Language Models (LLMs). We explore the intersection of LLMs and
penetration testing to gain insight into their capabilities and challenges in
the context of privilige escalation. We create an automated Linux
privilege-escalation benchmark utilizing local virtual machines. We introduce
an LLM-guided privilege-escalation tool designed for evaluating different LLMs
and prompt strategies against our benchmark. We analyze the impact of different
prompt designs, the benefits of in-context learning, and the advantages of
offering high-level guidance to LLMs. We discuss challenging areas for LLMs,
including maintaining focus during testing, coping with errors, and finally
comparing them with both stochastic parrots as well as with human hackers.
","2023-10-24","2310.11409v1.pdf"
"2310.11428","Cyril Zhang","Adam Block, Dylan J. Foster, Akshay Krishnamurthy, Max Simchowitz,
  Cyril Zhang","Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning
  and Autoregression","","","","","cs.LG math.OC stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work studies training instabilities of behavior cloning with deep neural
networks. We observe that minibatch SGD updates to the policy network during
training result in sharp oscillations in long-horizon rewards, despite
negligibly affecting the behavior cloning loss. We empirically disentangle the
statistical and computational causes of these oscillations, and find them to
stem from the chaotic propagation of minibatch SGD noise through unstable
closed-loop dynamics. While SGD noise is benign in the single-step action
prediction objective, it results in catastrophic error accumulation over long
horizons, an effect we term gradient variance amplification (GVA). We show that
many standard mitigation techniques do not alleviate GVA, but find an
exponential moving average (EMA) of iterates to be surprisingly effective at
doing so. We illustrate the generality of this phenomenon by showing the
existence of GVA and its amelioration by EMA in both continuous control and
autoregressive language generation. Finally, we provide theoretical vignettes
that highlight the benefits of EMA in alleviating GVA and shed light on the
extent to which classical convex models can help in understanding the benefits
of iterate averaging in deep learning.
","2023-10-18","2310.11428v1.pdf"
"2310.11430","Ant\'onio Farinhas","Ant\'onio Farinhas, Jos\'e G. C. de Souza, Andr\'e F. T. Martins","An Empirical Study of Translation Hypothesis Ensembling with Large
  Language Models","EMNLP 2023 (main conference)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) are becoming a one-fits-many solution, but they
sometimes hallucinate or produce unreliable output. In this paper, we
investigate how hypothesis ensembling can improve the quality of the generated
text for the specific problem of LLM-based machine translation. We experiment
with several techniques for ensembling hypotheses produced by LLMs such as
ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple
dimensions, including the method to generate hypotheses (multiple prompts,
temperature-based sampling, and beam search) and the strategy to produce the
final translation (instruction-based, quality-based reranking, and minimum
Bayes risk (MBR) decoding). Our results show that MBR decoding is a very
effective method, that translation quality can be improved using a small number
of samples, and that instruction tuning has a strong impact on the relation
between the diversity of the hypotheses and the sampling temperature.
","2023-10-18","2310.11430v1.pdf"
"2310.11440","Xiaodong Cun","Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin
  Chen, Yang Liu, Tieyong Zeng, Raymond Chan, Ying Shan","EvalCrafter: Benchmarking and Evaluating Large Video Generation Models","Technical Report, Project page: https://evalcrafter.github.io/","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  The vision and language generative models have been overgrown in recent
years. For video generation, various open-sourced models and public-available
services are released for generating high-visual quality videos. However, these
methods often use a few academic metrics, for example, FVD or IS, to evaluate
the performance. We argue that it is hard to judge the large conditional
generative models from the simple metrics since these models are often trained
on very large datasets with multi-aspect abilities. Thus, we propose a new
framework and pipeline to exhaustively evaluate the performance of the
generated videos. To achieve this, we first conduct a new prompt list for
text-to-video generation by analyzing the real-world prompt list with the help
of the large language model. Then, we evaluate the state-of-the-art video
generative models on our carefully designed benchmarks, in terms of visual
qualities, content qualities, motion qualities, and text-caption alignment with
around 18 objective metrics. To obtain the final leaderboard of the models, we
also fit a series of coefficients to align the objective metrics to the users'
opinions. Based on the proposed opinion alignment method, our final score shows
a higher correlation than simply averaging the metrics, showing the
effectiveness of the proposed evaluation method.
","2023-10-19","2310.11440v1.pdf"
"2310.11441","Jianwei Yang","Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng
  Gao","Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V","","","","","cs.CV cs.AI cs.CL cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present Set-of-Mark (SoM), a new visual prompting method, to unleash the
visual grounding abilities of large multimodal models (LMMs), such as GPT-4V.
As illustrated in Fig. 1 (right), we employ off-the-shelf interactive
segmentation models, such as SAM, to partition an image into regions at
different levels of granularity, and overlay these regions with a set of marks
e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can
answer the questions that require visual grounding. We perform a comprehensive
empirical study to validate the effectiveness of SoM on a wide range of
fine-grained vision and multimodal tasks. For example, our experiments show
that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring
segmentation model on RefCOCOg in a zero-shot setting.
","2023-10-18","2310.11441v1.pdf"
"2310.11446","Pierre Fernandez","Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs","Functional Invariants to Watermark Large Transformers","","","","","cs.CR cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  The rapid growth of transformer-based models increases the concerns about
their integrity and ownership insurance. Watermarking addresses this issue by
embedding a unique identifier into the model, while preserving its performance.
However, most existing approaches require to optimize the weights to imprint
the watermark signal, which is not suitable at scale due to the computational
cost. This paper explores watermarks with virtually no computational cost,
applicable to a non-blind white-box setting (assuming access to both the
original and watermarked networks). They generate functionally equivalent
copies by leveraging the models' invariance, via operations like dimension
permutations or scaling/unscaling. This enables to watermark models without any
change in their outputs and remains stealthy. Experiments demonstrate the
effectiveness of the approach and its robustness against various model
transformations (fine-tuning, quantization, pruning), making it a practical
solution to protect the integrity of large models.
","2023-10-18","2310.11446v1.pdf"
"2310.11451","Ming Zhong","Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He","Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from
  a Parametric Perspective","Preprint","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) inherently encode a wealth of knowledge within
their parameters through pre-training on extensive corpora. While prior
research has delved into operations on these parameters to manipulate the
underlying implicit knowledge (encompassing detection, editing, and merging),
there remains an ambiguous understanding regarding their transferability across
models with varying scales. In this paper, we seek to empirically investigate
knowledge transfer from larger to smaller models through a parametric
perspective. To achieve this, we employ sensitivity-based techniques to extract
and align knowledge-specific parameters between different LLMs. Moreover, the
LoRA module is used as the intermediary mechanism for injecting the extracted
knowledge into smaller models. Evaluations across four benchmarks validate the
efficacy of our proposed method. Our findings highlight the critical factors
contributing to the process of parametric knowledge transfer, underscoring the
transferability of model parameters across LLMs of different scales. We release
code and data at \url{https://github.com/maszhongming/ParaKnowTransfer}.
","2023-10-18","2310.11451v1.pdf"
"2310.11454","Dawid Jan Kopiczko","Dawid Jan Kopiczko, Tijmen Blankevoort, Yuki Markus Asano","VeRA: Vector-based Random Matrix Adaptation","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Low-rank adapation (LoRA) is a popular method that reduces the number of
trainable parameters when finetuning large language models, but still faces
acute storage challenges when scaling to even larger models or deploying
numerous per-user or per-task adapted models. In this work, we present
Vector-based Random Matrix Adaptation (VeRA), which reduces the number of
trainable parameters by 10x compared to LoRA, yet maintains the same
performance. It achieves this by using a single pair of low-rank matrices
shared across all layers and learning small scaling vectors instead. We
demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its
application in instruction-following with just 1.4M parameters using the Llama2
7B model.
","2023-10-18","2310.11454v1.pdf"
"2310.11458","Ethan Holbrook","Juan C. Verduzco, Ethan Holbrook, and Alejandro Strachan","GPT-4 as an interface between researchers and computational software:
  improving usability and reproducibility","22 pages, 7 figures","","","","cond-mat.mtrl-sci cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are playing an increasingly important role in
science and engineering. For example, their ability to parse and understand
human and computer languages makes them powerful interpreters and their use in
applications like code generation are well-documented. We explore the ability
of the GPT-4 LLM to ameliorate two major challenges in computational materials
science: i) the high barriers for adoption of scientific software associated
with the use of custom input languages, and ii) the poor reproducibility of
published results due to insufficient details in the description of simulation
methods. We focus on a widely used software for molecular dynamics simulations,
the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS), and
quantify the usefulness of input files generated by GPT-4 from task
descriptions in English and its ability to generate detailed descriptions of
computational tasks from input files. We find that GPT-4 can generate correct
and ready-to-use input files for relatively simple tasks and useful starting
points for more complex, multi-step simulations. In addition, GPT-4's
description of computational tasks from input files can be tuned from a
detailed set of step-by-step instructions to a summary description appropriate
for publications. Our results show that GPT-4 can reduce the number of routine
tasks performed by researchers, accelerate the training of new users, and
enhance reproducibility.
","2023-10-19","2310.11458v1.pdf"
"2310.11476","Mengnan Qi","Yufan Huang, Mengnan Qi, Yongqiang Yao, Maoquan Wang, Bin Gu, Colin
  Clement, Neel Sundaresan","Program Translation via Code Distillation","","","","","cs.SE cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Software version migration and program translation are an important and
costly part of the lifecycle of large codebases. Traditional machine
translation relies on parallel corpora for supervised translation, which is not
feasible for program translation due to a dearth of aligned data. Recent
unsupervised neural machine translation techniques have overcome data
limitations by included techniques such as back translation and low level
compiler intermediate representations (IR). These methods face significant
challenges due to the noise in code snippet alignment and the diversity of IRs
respectively. In this paper we propose a novel model called Code Distillation
(CoDist) whereby we capture the semantic and structural equivalence of code in
a language agnostic intermediate representation. Distilled code serves as a
translation pivot for any programming language, leading by construction to
parallel corpora which scale to all available source code by simply applying
the distillation compiler. We demonstrate that our approach achieves
state-of-the-art performance on CodeXGLUE and TransCoder GeeksForGeeks
translation benchmarks, with an average absolute increase of 12.7% on the
TransCoder GeeksforGeeks translation benchmark compare to TransCoder-ST.
","2023-10-19","2310.11476v1.pdf"
"2310.11501","Myra Cheng","Myra Cheng, Tiziano Piccardi, Diyi Yang","CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations","To appear at EMNLP 2023 (Main)","","","","cs.CL cs.AI cs.CY","http://creativecommons.org/licenses/by/4.0/","  Recent work has aimed to capture nuances of human behavior by using LLMs to
simulate responses from particular demographics in settings like social science
experiments and public opinion surveys. However, there are currently no
established ways to discuss or evaluate the quality of such LLM simulations.
Moreover, there is growing concern that these LLM simulations are flattened
caricatures of the personas that they aim to simulate, failing to capture the
multidimensionality of people and perpetuating stereotypes. To bridge these
gaps, we present CoMPosT, a framework to characterize LLM simulations using
four dimensions: Context, Model, Persona, and Topic. We use this framework to
measure open-ended LLM simulations' susceptibility to caricature, defined via
two criteria: individuation and exaggeration. We evaluate the level of
caricature in scenarios from existing work on LLM simulations. We find that for
GPT-4, simulations of certain demographics (political and marginalized groups)
and topics (general, uncontroversial) are highly susceptible to caricature.
","2023-10-19","2310.11501v1.pdf"
"2310.11511","Akari Asai","Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi","Self-RAG: Learning to Retrieve, Generate, and Critique through
  Self-Reflection","30 pages, 2 figures, 12 tables","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.
","2023-10-19","2310.11511v1.pdf"
"2310.11513","Dhruba Ghosh","Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt","GenEval: An Object-Focused Framework for Evaluating Text-to-Image
  Alignment","","","","","cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recent breakthroughs in diffusion models, multimodal pretraining, and
efficient finetuning have led to an explosion of text-to-image generative
models. Given human evaluation is expensive and difficult to scale, automated
methods are critical for evaluating the increasingly large number of new
models. However, most current automated evaluation metrics like FID or
CLIPScore only offer a holistic measure of image quality or image-text
alignment, and are unsuited for fine-grained or instance-level analysis. In
this paper, we introduce GenEval, an object-focused framework to evaluate
compositional image properties such as object co-occurrence, position, count,
and color. We show that current object detection models can be leveraged to
evaluate text-to-image models on a variety of generation tasks with strong
human agreement, and that other discriminative vision models can be linked to
this pipeline to further verify properties like object color. We then evaluate
several open-source text-to-image models and analyze their relative generative
capabilities on our benchmark. We find that recent models demonstrate
significant improvement on these tasks, though they are still lacking in
complex capabilities such as spatial relations and attribute binding. Finally,
we demonstrate how GenEval might be used to help discover existing failure
modes, in order to inform development of the next generation of text-to-image
models. Our code to run the GenEval framework is publicly available at
https://github.com/djghosh13/geneval.
","2023-10-19","2310.11513v1.pdf"
"2310.11523","Siyan Zhao","Siyan Zhao, John Dang, Aditya Grover","Group Preference Optimization: Few-Shot Alignment of Large Language
  Models","24 pages, 12 figures","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Many applications of large language models (LLMs), ranging from chatbots to
creative writing, require nuanced subjective judgments that can differ
significantly across different groups. Existing alignment algorithms can be
expensive to align for each group, requiring prohibitive amounts of
group-specific preference data and computation for real-world use cases. We
introduce Group Preference Optimization (GPO), an alignment framework that
steers language models to preferences of individual groups in a few-shot
manner. In GPO, we augment the base LLM with an independent transformer module
trained to predict the preferences of a group for the LLM generations. For
few-shot learning, we parameterize this module as an in-context autoregressive
transformer and train it via meta-learning on several groups. We empirically
validate the efficacy of GPO through rigorous evaluations using LLMs with
varied sizes on three human opinion adaptation tasks. These tasks involve
adapting to the preferences of US demographic groups, global countries, and
individual users. Our results demonstrate that GPO not only aligns models more
accurately but also requires fewer group-specific preferences, and less
training and inference computing resources, outperforming existing strategies
such as in-context steering and fine-tuning methods.
","2023-10-19","2310.11523v1.pdf"
"2310.11531","Dengwang Tang","Dengwang Tang, Rahul Jain, Botao Hao, Zheng Wen","Efficient Online Learning with Offline Datasets for Infinite Horizon
  MDPs: A Bayesian Approach","22 pages","","","","cs.LG cs.AI cs.SY eess.SY stat.ML","http://creativecommons.org/licenses/by-sa/4.0/","  In this paper, we study the problem of efficient online reinforcement
learning in the infinite horizon setting when there is an offline dataset to
start with. We assume that the offline dataset is generated by an expert but
with unknown level of competence, i.e., it is not perfect and not necessarily
using the optimal policy. We show that if the learning agent models the
behavioral policy (parameterized by a competence parameter) used by the expert,
it can do substantially better in terms of minimizing cumulative regret, than
if it doesn't do that. We establish an upper bound on regret of the exact
informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a
novel prior-dependent regret analysis of Bayesian online learning algorithms
for the infinite horizon setting. We then propose an approximate Informed RLSVI
algorithm that we can interpret as performing imitation learning with the
offline dataset, and then performing online learning.
","2023-10-19","2310.11531v1.pdf"
"2310.11532","Jie Pu","Jie Pu, Thai-Son Nguyen, Sebastian St\""uker","Multi-stage Large Language Model Correction for Speech Recognition","Submitted to ICASSP 2024","","","","cs.CL eess.AS","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In this paper, we investigate the usage of large language models (LLMs) to
improve the performance of competitive speech recognition systems. Different
from traditional language models that focus on one single data domain, the rise
of LLMs brings us the opportunity to push the limit of state-of-the-art ASR
performance, and at the same time to achieve higher robustness and generalize
effectively across multiple domains. Motivated by this, we propose a novel
multi-stage approach to combine traditional language model re-scoring and LLM
prompting. Specifically, the proposed method has two stages: the first stage
uses a language model to re-score an N-best list of ASR hypotheses and run a
confidence check; The second stage uses prompts to a LLM to perform ASR error
correction on less confident results from the first stage. Our experimental
results demonstrate the effectiveness of the proposed method by showing a 10% ~
20% relative improvement in WER over a competitive ASR system -- across
multiple test domains.
","2023-10-19","2310.11532v1.pdf"
"2310.11546","Ernesto Giralt","Ernesto Giralt Hern\'andez","Bias and Error Mitigation in Software-Generated Data: An Advanced Search
  and Optimization Framework Leveraging Generative Code Models","","","","","cs.SE cs.IT cs.LG math.IT math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Data generation and analysis is a fundamental aspect of many industries and
disciplines, from strategic decision making in business to research in the
physical and social sciences. However, data generated using software and
algorithms can be subject to biases and errors. These can be due to problems
with the original software, default settings that do not align with the
specific needs of the situation, or even deeper problems with the underlying
theories and models. This paper proposes an advanced search and optimization
framework aimed at generating and choosing optimal source code capable of
correcting errors and biases from previous versions to address typical problems
in software systems specializing in data analysis and generation, especially
those in the corporate and data science world. Applying this framework multiple
times on the same software system would incrementally improve the quality of
the output results. It uses Solomonoff Induction as a sound theoretical basis,
extending it with Kolmogorov Conditional Complexity, a novel adaptation, to
evaluate a set of candidate programs. We propose the use of generative models
for the creation of this set of programs, with special emphasis on the
capabilities of Large Language Models (LLMs) to generate high quality code.
","2023-10-19","2310.11546v1.pdf"
"2310.11564","Joel Jang","Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel,
  Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu","Personalized Soups: Personalized Large Language Model Alignment via
  Post-hoc Parameter Merging","Preprint","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with general, aggregate human preferences, it is suboptimal for
learning diverse, individual perspectives. In this work, we study Reinforcement
Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are
aligned to multiple (sometimes conflicting) preferences by modeling alignment
as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong
single-objective baselines, we show that we can achieve personalized alignment
by decomposing preferences into multiple dimensions. These dimensions are
defined based on personalizations that are declared as desirable by the user.
In this work, we show that they can be efficiently trained independently in a
distributed manner and combined effectively post-hoc through parameter merging.
The code is available at https://github.com/joeljang/RLPHF.
","2023-10-19","2310.11564v1.pdf"
"2310.11571","Matthew Toles","Matthew Toles, Yukun Huang, Zhou Yu, Luis Gravano","What is a good question? Task-oriented asking with fact-level masking","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Asking questions is an important element of real-life collaboration on
reasoning tasks like question answering. For example, a legal assistant chatbot
may be unable to make accurate recommendations without specific information on
the user's circumstances. However, large language models are usually deployed
to solve reasoning tasks directly without asking follow-up questions to the
user or third parties. We term this problem task-oriented asking (TOA).
Zero-shot chat models can perform TOA, but their training is primarily based on
next-token prediction rather than whether questions contribute to successful
collaboration. To enable the training and evaluation of TOA models, we present
a definition and framework for natural language task-oriented asking, the
problem of generating questions that result in answers useful for a reasoning
task. We also present fact-level masking (FLM), a procedure for converting
natural language datasets into self-supervised TOA datasets by omitting
particular critical facts. Finally, we generate a TOA dataset from the HotpotQA
dataset using FLM and evaluate several zero-shot language models on it. Our
experiments show that current zero-shot models struggle to ask questions that
retrieve useful information, as compared to human annotators. These results
demonstrate an opportunity to use FLM datasets and the TOA framework to train
and evaluate better TOA models.
","2023-10-19","2310.11571v1.pdf"
"2310.11589","Belinda Z. Li","Belinda Z. Li, Alex Tamkin, Noah Goodman, Jacob Andreas","Eliciting Human Preferences with Language Models","26 pages, 15 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models (LMs) can be directed to perform target tasks by using
labeled examples or natural language prompts. But selecting examples or writing
prompts for can be challenging--especially in tasks that involve unusual edge
cases, demand precise articulation of nebulous preferences, or require an
accurate mental model of LM behavior. We propose to use *LMs themselves* to
guide the task specification process. In this paper, we introduce **Generative
Active Task Elicitation (GATE)**: a learning framework in which models elicit
and infer intended behavior through free-form, language-based interaction with
users. We study GATE in three domains: email validation, content
recommendation, and moral reasoning. In preregistered experiments, we show that
LMs prompted to perform GATE (e.g., by generating open-ended questions or
synthesizing informative edge cases) elicit responses that are often more
informative than user-written prompts or labels. Users report that interactive
task elicitation requires less effort than prompting or example labeling and
surfaces novel considerations not initially anticipated by users. Our findings
suggest that LM-driven elicitation can be a powerful tool for aligning models
to complex human preferences and values.
","2023-10-19","2310.11589v1.pdf"
"2310.11604","Teyun Kwon","Teyun Kwon, Norman Di Palo, Edward Johns","Language Models as Zero-Shot Trajectory Generators","19 pages, 21 figures","","","","cs.RO cs.AI cs.CL cs.HC cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have recently shown promise as high-level
planners for robots when given access to a selection of low-level skills.
However, it is often assumed that LLMs do not possess sufficient knowledge to
be used for the low-level trajectories themselves. In this work, we address
this assumption thoroughly, and investigate if an LLM (GPT-4) can directly
predict a dense sequence of end-effector poses for manipulation skills, when
given access to only object detection and segmentation vision models. We study
how well a single task-agnostic prompt, without any in-context examples, motion
primitives, or external trajectory optimisers, can perform across 26 real-world
language-based tasks, such as ""open the bottle cap"" and ""wipe the plate with
the sponge"", and we investigate which design choices in this prompt are the
most effective. Our conclusions raise the assumed limit of LLMs for robotics,
and we reveal for the first time that LLMs do indeed possess an understanding
of low-level robot control sufficient for a range of common tasks, and that
they can additionally detect failures and then re-plan trajectories
accordingly. Videos, code, and prompts are available at:
https://www.robot-learning.uk/language-models-trajectory-generators.
","2023-10-19","2310.11604v1.pdf"
"2310.11607","Nicholas Botzer","Nicholas Botzer, David Vasquez, Tim Weninger, Issam Laradji","TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for
  Semi-Supervised Intent Classification","9 pages, 6 figures, 4 tables","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  The ability to detect intent in dialogue systems has become increasingly
important in modern technology. These systems often generate a large amount of
unlabeled data, and manually labeling this data requires substantial human
effort. Semi-supervised methods attempt to remedy this cost by using a model
trained on a few labeled examples and then by assigning pseudo-labels to
further a subset of unlabeled examples that has a model prediction confidence
higher than a certain threshold. However, one particularly perilous consequence
of these methods is the risk of picking an imbalanced set of examples across
classes, which could lead to poor labels. In the present work, we describe
Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling
approach based on distance in the embedding space while maintaining a balanced
set of pseudo-labeled examples across classes through a ranking-based approach.
Experiments on several datasets show that TK-KNN outperforms existing models,
particularly when labeled data is scarce on popular datasets such as CLINC150
and Banking77. Code is available at https://github.com/ServiceNow/tk-knn
","2023-10-19","2310.11607v1.pdf"
"2310.11614","Leonardo Hern\'andez Cano","Leonardo Hernandez Cano, Yewen Pu, Robert D. Hawkins, Josh Tenenbaum,
  Armando Solar-Lezama","Learning a Hierarchical Planner from Humans in Multiple Generations","First two authors contributed equally","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  A typical way in which a machine acquires knowledge from humans is by
programming. Compared to learning from demonstrations or experiences,
programmatic learning allows the machine to acquire a novel skill as soon as
the program is written, and, by building a library of programs, a machine can
quickly learn how to perform complex tasks. However, as programs often take
their execution contexts for granted, they are brittle when the contexts
change, making it difficult to adapt complex programs to new contexts. We
present natural programming, a library learning system that combines
programmatic learning with a hierarchical planner. Natural programming
maintains a library of decompositions, consisting of a goal, a linguistic
description of how this goal decompose into sub-goals, and a concrete instance
of its decomposition into sub-goals. A user teaches the system via curriculum
building, by identifying a challenging yet not impossible goal along with
linguistic hints on how this goal may be decomposed into sub-goals. The system
solves for the goal via hierarchical planning, using the linguistic hints to
guide its probability distribution in proposing the right plans. The system
learns from this interaction by adding newly found decompositions in the
successful search into its library. Simulated studies and a human experiment
(n=360) on a controlled environment demonstrate that natural programming can
robustly compose programs learned from different users and contexts, adapting
faster and solving more complex tasks when compared to programmatic baselines.
","2023-10-19","2310.11614v1.pdf"
"2310.11628","Avijit Thawani","Avijit Thawani, Saurabh Ghanekar, Xiaoyuan Zhu, Jay Pujara","Learn Your Tokens: Word-Pooled Tokenization for Language Modeling","Accepted to EMNLP 2023 Findings","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models typically tokenize text into subwords, using a deterministic,
hand-engineered heuristic of combining characters into longer surface-level
strings such as 'ing' or whole words. Recent literature has repeatedly shown
the limitations of such a tokenization strategy, particularly for documents not
written in English and for representing numbers. On the other extreme,
byte/character-level language models are much less restricted but suffer from
increased sequence description lengths and a subsequent quadratic expansion in
self-attention computation. Recent attempts to compress and limit these context
lengths with fixed size convolutions is helpful but completely ignores the word
boundary. This paper considers an alternative 'learn your tokens' scheme which
utilizes the word boundary to pool bytes/characters into word representations,
which are fed to the primary language model, before again decoding individual
characters/bytes per word in parallel. We find that our moderately expressive
and moderately fast end-to-end tokenizer outperform by over 300% both subwords
and byte/character models over the intrinsic language modeling metric of
next-word prediction across datasets. It particularly outshines on rare words,
outperforming by a factor of 30! We extensively study the language modeling
setup for all three categories of tokenizers and theoretically analyze how our
end-to-end models can also be a strong trade-off in efficiency and robustness.
","2023-10-19","2310.11628v1.pdf"
"2310.11634","Arkil Patel","Arkil Patel, Satwik Bhattamishra, Siva Reddy, Dzmitry Bahdanau","MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language
  Models to Generalize to Novel Interpretations","EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Humans possess a remarkable ability to assign novel interpretations to
linguistic expressions, enabling them to learn new words and understand
community-specific connotations. However, Large Language Models (LLMs) have a
knowledge cutoff and are costly to finetune repeatedly. Therefore, it is
crucial for LLMs to learn novel interpretations in-context. In this paper, we
systematically analyse the ability of LLMs to acquire novel interpretations
using in-context learning. To facilitate our study, we introduce MAGNIFICo, an
evaluation suite implemented within a text-to-SQL semantic parsing framework
that incorporates diverse tokens and prompt settings to simulate real-world
complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a
surprisingly robust capacity for comprehending novel interpretations from
natural language descriptions as well as from discussions within long
conversations. Nevertheless, our findings also highlight the need for further
improvements, particularly when interpreting unfamiliar words or when composing
multiple novel interpretations simultaneously in the same example.
Additionally, our analysis uncovers the semantic predispositions in LLMs and
reveals the impact of recency bias for information presented in long contexts.
","2023-10-19","2310.11634v1.pdf"
"2310.11638","Linhao Luo","Linhao Luo, Thuy-Trang Vu, Dinh Phung, Gholamreza Haffari","Systematic Assessment of Factual Knowledge in Large Language Models","Accepted by EMNLP 2023 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Previous studies have relied on existing question-answering benchmarks to
evaluate the knowledge stored in large language models (LLMs). However, this
approach has limitations regarding factual knowledge coverage, as it mostly
focuses on generic domains which may overlap with the pretraining data. This
paper proposes a framework to systematically assess the factual knowledge of
LLMs by leveraging knowledge graphs (KGs). Our framework automatically
generates a set of questions and expected answers from the facts stored in a
given KG, and then evaluates the accuracy of LLMs in answering these questions.
We systematically evaluate the state-of-the-art LLMs with KGs in generic and
specific domains. The experiment shows that ChatGPT is consistently the top
performer across all domains. We also find that LLMs performance depends on the
instruction finetuning, domain and question complexity and is prone to
adversarial context.
","2023-10-23","2310.11638v1.pdf"
"2310.11648","Qi Jia","Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu","Zero-shot Faithfulness Evaluation for Text Summarization with Foundation
  Language Model","Accepted by EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite tremendous improvements in natural language generation, summarization
models still suffer from the unfaithfulness issue. Previous work evaluates
faithfulness either using models trained on the other tasks or in-domain
synthetic data, or prompting a large model such as ChatGPT. This paper proposes
to do zero-shot faithfulness evaluation simply with a moderately-sized
foundation language model. We introduce a new metric FFLM, which is a
combination of probability changes based on the intuition that prefixing a
piece of text that is consistent with the output will increase the probability
of predicting the output. Experiments show that FFLM performs competitively
with or even outperforms ChatGPT on both inconsistency detection and
faithfulness rating with 24x fewer parameters. FFLM also achieves improvements
over other strong baselines.
","2023-10-19","2310.11648v1.pdf"
"2310.11650","Siyu An","Siyu An, Ye Liu, Haoyuan Peng and Di Yin","VKIE: The Application of Key Information Extraction on Video Text","","","","","cs.IR cs.CV cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Extracting structured information from videos is critical for numerous
downstream applications in the industry. In this paper, we define a significant
task of extracting hierarchical key information from visual texts on videos. To
fulfill this task, we decouples it into four subtasks and introduce two
implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially
completes the four subtasks in continuous stages, while UniVKIE is improved by
unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage
multimodal information from vision, text, and coordinates for feature
representation. Extensive experiments on one well-defined dataset demonstrate
that our solutions can achieve remarkable performance and efficient inference
speed. The code and dataset will be publicly available.
","2023-10-19","2310.11650v1.pdf"
"2310.11657","Townim Faisal Chowdhury","Fahimul Hoque Shubho, Townim Faisal Chowdhury, Ali Cheraghian, Morteza
  Saberi, Nabeel Mohammed, Shafin Rahman","ChatGPT-guided Semantics for Zero-shot Learning","Accepted in International Conference on Digital Image Computing:
  Techniques and Applications (DICTA), 2023","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Zero-shot learning (ZSL) aims to classify objects that are not observed or
seen during training. It relies on class semantic description to transfer
knowledge from the seen classes to the unseen classes. Existing methods of
obtaining class semantics include manual attributes or automatic word vectors
from language models (like word2vec). We know attribute annotation is costly,
whereas automatic word-vectors are relatively noisy. To address this problem,
we explore how ChatGPT, a large language model, can enhance class semantics for
ZSL tasks. ChatGPT can be a helpful source to obtain text descriptions for each
class containing related attributes and semantics. We use the word2vec model to
get a word vector using the texts from ChatGPT. Then, we enrich word vectors by
combining the word embeddings from class names and descriptions generated by
ChatGPT. More specifically, we leverage ChatGPT to provide extra supervision
for the class description, eventually benefiting ZSL models. We evaluate our
approach on various 2D image (CUB and AwA) and 3D point cloud (ModelNet10,
ModelNet40, and ScanObjectNN) datasets and show that it improves ZSL
performance. Our work contributes to the ZSL literature by applying ChatGPT for
class semantics enhancement and proposing a novel word vector fusion method.
","2023-10-19","2310.11657v1.pdf"
"2310.11666","Kouichi Hagino","N. Hizawa and K. Hagino","Non-empirical shape dynamics of heavy nuclei with multi-task deep
  learning","15 pages, 11 figures","","","KUNS-2984","nucl-th physics.data-an","http://creativecommons.org/licenses/by/4.0/","  A microscopic description of nuclear fission represents one of the most
challenging problems in nuclear theory. While phenomenological coordinates,
such as multipole moments, have often been employed to describe fission, it is
not obvious whether these parameters fully reflect the shape dynamics of
interest. We here propose a novel method to extract collective coordinates,
which are free from phenomenology, based on multi-task deep learning in
conjunction with a density functional theory (DFT). To this end, we first
introduce randomly generated external fields to a Skyrme-EDF and construct a
set of nuclear number densities and binding energies for deformed states of
${}^{236}$U around the ground state. By training a neural network on such
dataset with a combination of an autoencoder and supervised learning, we
successfully identify a two-dimensional latent variables that accurately
reproduce both the energies and the densities of the original Skyrme-EDF
calculations, within a mean absolute error of 113 keV for the energies. In
contrast, when multipole moments are used as latent variables for training in
constructing the decoders, we find that the training data for the binding
energies are reproduced only within 2 MeV. This implies that conventional
multipole moments do not provide fully adequate variables for a shape dynamics
of heavy nuclei.
","2023-10-19","2310.11666v1.pdf"
"2310.11667","Hao Zhu","Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang
  Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig,
  Maarten Sap","SOTOPIA: Interactive Evaluation for Social Intelligence in Language
  Agents","Preprint, 43 pages. The first two authors contribute equally","","","","cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Humans are social beings; we pursue social goals in our daily interactions,
which is a crucial aspect of social intelligence. Yet, AI systems' abilities in
this realm remain elusive. We present SOTOPIA, an open-ended environment to
simulate complex social interactions between artificial agents and evaluate
their social intelligence. In our environment, agents role-play and interact
under a wide variety of scenarios; they coordinate, collaborate, exchange, and
compete with each other to achieve complex social goals. We simulate the
role-play interaction between LLM-based agents and humans within this task
space and evaluate their performance with a holistic evaluation framework
called SOTOPIA-Eval. With SOTOPIA, we find significant differences between
these models in terms of their social intelligence, and we identify a subset of
SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.
We find that on this subset, GPT-4 achieves a significantly lower goal
completion rate than humans and struggles to exhibit social commonsense
reasoning and strategic communication skills. These findings demonstrate
SOTOPIA's promise as a general platform for research on evaluating and
improving social intelligence in artificial agents.
","2023-10-19","2310.11667v1.pdf"
"2310.11670","Hao Zhao","Hao Zhao, Jie Fu, Zhaofeng He","Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning","Accepted by EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in
adapting the pre-trained language models to downstream tasks while only
updating a small number of parameters. Despite the success, most existing
methods independently adapt to each task without considering knowledge transfer
between tasks and are limited to low-data regimes. To overcome this issue, we
propose Prototype-based HyperAdapter (PHA), a novel framework built on the
adapter-tuning and hypernetwork. It introduces an instance-dense retriever and
a prototypical hypernetwork to generate the conditional modules in a
sample-efficient manner. This leads to comparable performance improvements
against existing PEFT methods on multi-task learning and few-shot transfer
learning. More importantly, when the available data size gets smaller, our
method outperforms other strong baselines by a large margin. Based on our
extensive empirical experiments across various datasets, we demonstrate that
PHA strikes a better trade-off between trainable parameters, accuracy on stream
tasks, and sample efficiency.
","2023-10-20","2310.11670v1.pdf"
"2310.11676","Yizhen Zheng","Junjun Pan, Yixin Liu, Yizhen Zheng, Shirui Pan","PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly
  Detection","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Node-level graph anomaly detection (GAD) plays a critical role in identifying
anomalous nodes from graph-structured data in various domains such as medicine,
social networks, and e-commerce. However, challenges have arisen due to the
diversity of anomalies and the dearth of labeled data. Existing methodologies -
reconstruction-based and contrastive learning - while effective, often suffer
from efficiency issues, stemming from their complex objectives and elaborate
modules. To improve the efficiency of GAD, we introduce a simple method termed
PREprocessing and Matching (PREM for short). Our approach streamlines GAD,
reducing time and memory consumption while maintaining powerful anomaly
detection capabilities. Comprising two modules - a pre-processing module and an
ego-neighbor matching module - PREM eliminates the necessity for
message-passing propagation during training, and employs a simple contrastive
loss, leading to considerable reductions in training time and memory usage.
Moreover, through rigorous evaluations of five real-world datasets, our method
demonstrated robustness and effectiveness. Notably, when validated on the ACM
dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training
speed, and sharply reduce memory usage compared to the most efficient baseline.
","2023-10-19","2310.11676v1.pdf"
"2310.11681","Kerui Zhu","Kerui Zhu, Jie Huang, Kevin Chen-Chuan Chang","Descriptive Knowledge Graph in Biomedical Domain","EMNLP 2023 Demo","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a novel system that automatically extracts and generates
informative and descriptive sentences from the biomedical corpus and
facilitates the efficient search for relational knowledge. Unlike previous
search engines or exploration systems that retrieve unconnected passages, our
system organizes descriptive sentences as a relational graph, enabling
researchers to explore closely related biomedical entities (e.g., diseases
treated by a chemical) or indirectly connected entities (e.g., potential drugs
for treating a disease). Our system also uses ChatGPT and a fine-tuned relation
synthesis model to generate concise and reliable descriptive sentences from
retrieved information, reducing the need for extensive human reading effort.
With our system, researchers can easily obtain both high-level knowledge and
detailed references and interactively steer to the information of interest. We
spotlight the application of our system in COVID-19 research, illustrating its
utility in areas such as drug repurposing and literature curation.
","2023-10-19","2310.11681v1.pdf"
"2310.11685","Yichuan Deng","Yichuan Deng, Zhao Song, Tianyi Zhou","Superiority of Softmax: Unveiling the Performance Edge Over Linear
  Attention","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large transformer models have achieved state-of-the-art results in numerous
natural language processing tasks. Among the pivotal components of the
transformer architecture, the attention mechanism plays a crucial role in
capturing token interactions within sequences through the utilization of
softmax function.
  Conversely, linear attention presents a more computationally efficient
alternative by approximating the softmax operation with linear complexity.
However, it exhibits substantial performance degradation when compared to the
traditional softmax attention mechanism.
  In this paper, we bridge the gap in our theoretical understanding of the
reasons behind the practical performance gap between softmax and linear
attention. By conducting a comprehensive comparative analysis of these two
attention mechanisms, we shed light on the underlying reasons for why softmax
attention outperforms linear attention in most scenarios.
","2023-10-19","2310.11685v1.pdf"
"2310.11689","Jiefeng Chen","Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O Arik, Tomas
  Pfister, Somesh Jha","Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs","Paper published at Findings of the Association for Computational
  Linguistics: EMNLP, 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have recently shown great advances in a variety
of tasks, including natural language understanding and generation. However,
their use in high-stakes decision-making scenarios is still limited due to the
potential for errors. Selective prediction is a technique that can be used to
improve the reliability of the LLMs by allowing them to abstain from making
predictions when they are unsure of the answer. In this work, we propose a
novel framework for adaptation with self-evaluation to improve the selective
prediction performance of LLMs. Our framework is based on the idea of using
parameter-efficient tuning to adapt the LLM to the specific task at hand while
improving its ability to perform self-evaluation. We evaluate our method on a
variety of question-answering (QA) datasets and show that it outperforms
state-of-the-art selective prediction methods. For example, on the CoQA
benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the
AUROC from 74.61% to 80.25%.
","2023-10-19","2310.11689v1.pdf"
"2310.11699","Ali Vosoughi","Jing Bi, Nguyen Manh Nguyen, Ali Vosoughi, Chenliang Xu","MISAR: A Multimodal Instructional System with Augmented Reality","Accepted at ICCV 2023 - AV4D, 6 figures, 2 tables","","","","cs.CL cs.CV","http://creativecommons.org/licenses/by/4.0/","  Augmented reality (AR) requires the seamless integration of visual, auditory,
and linguistic channels for optimized human-computer interaction. While
auditory and visual inputs facilitate real-time and contextual user guidance,
the potential of large language models (LLMs) in this landscape remains largely
untapped. Our study introduces an innovative method harnessing LLMs to
assimilate information from visual, auditory, and contextual modalities.
Focusing on the unique challenge of task performance quantification in AR, we
utilize egocentric video, speech, and context analysis. The integration of LLMs
facilitates enhanced state estimation, marking a step towards more adaptive AR
systems. Code, dataset, and demo will be available at
https://github.com/nguyennm1024/misar.
","2023-10-19","2310.11699v1.pdf"
"2310.11703","Yikun Han","Yikun Han, Chunjiang Liu, Pengfei Wang","A Comprehensive Survey on Vector Database: Storage and Retrieval
  Technique, Challenge","","","","","cs.DB cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A vector database is used to store high-dimensional data that cannot be
characterized by traditional DBMS. Although there are not many articles
describing existing or introducing new vector database architectures, the
approximate nearest neighbor search problem behind vector databases has been
studied for a long time, and considerable related algorithmic articles can be
found in the literature. This article attempts to comprehensively review
relevant algorithms to provide a general understanding of this booming research
area. The basis of our framework categorises these studies by the approach of
solving ANNS problem, respectively hash-based, tree-based, graph-based and
quantization-based approaches. Then we present an overview of existing
challenges for vector databases. Lastly, we sketch how vector databases can be
combined with large language models and provide new possibilities.
","2023-10-19","2310.11703v1.pdf"
"2310.11716","Ming Li","Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu,
  Tianyi Zhou","Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in Large Language Models (LLMs) have expanded the
horizons of natural language understanding and generation. Notably, the output
control and alignment with the input of LLMs can be refined through instruction
tuning. However, as highlighted in several studies, low-quality data in the
training set are usually detrimental to instruction tuning, resulting in
inconsistent or even misleading LLM outputs. We propose a novel method, termed
""reflection-tuning,"" which addresses the problem by self-improvement and
judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle
the original training data by introspecting and enhancing the quality of
instructions and responses in the data. Extensive experiments on widely used
evaluation benchmarks show that LLMs trained with our recycled data outperform
those trained with existing datasets in various benchmarks.
","2023-10-19","2310.11716v1.pdf"
"2310.11721","Caoyun Fan","Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, Yaohui Jin","Chain-of-Thought Tuning: Masked Language Models can also Think Step By
  Step in Natural Language Understanding","EMNLP2023 Main Conference","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Chain-of-Thought (CoT) is a technique that guides Large Language Models
(LLMs) to decompose complex tasks into multi-step reasoning through
intermediate steps in natural language form. Briefly, CoT enables LLMs to think
step by step. However, although many Natural Language Understanding (NLU) tasks
also require thinking step by step, LLMs perform less well than small-scale
Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose
Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt
tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the
perspective of CoT, CoTT's two-step framework enables MLMs to implement task
decomposition; CoTT's prompt tuning allows intermediate steps to be used in
natural language form. Thereby, the success of CoT can be extended to NLU tasks
through MLMs. To verify the effectiveness of CoTT, we conduct experiments on
two NLU tasks: hierarchical classification and relation extraction, and the
results show that CoTT outperforms baselines and achieves state-of-the-art
performance.
","2023-10-19","2310.11721v1.pdf"
"2310.11722","Yaxin Fan","Yaxin Fan, Feng Jiang, Peifeng Li, Haizhou Li","Quantify Health-Related Atomic Knowledge in Chinese Medical Large
  Language Models: A Computational Analysis","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have the potential to revolutionize the way
users self-diagnose through search engines by offering direct and efficient
suggestions. Recent studies primarily focused on the quality of LLMs evaluated
by GPT-4 or their ability to pass medical exams, no studies have quantified the
extent of health-related atomic knowledge stored in LLMs' memory, which is the
basis of LLMs to provide more factual suggestions. In this paper, we first
constructed a benchmark, including the most common types of atomic knowledge in
user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces
of atomic knowledge. Then, we evaluated both generic and specialized LLMs on
the benchmark. The experimental results showcased that generic LLMs perform
better than specialized LLMs in terms of atomic knowledge and
instruction-following ability. Error analysis revealed that both generic and
specialized LLMs are sycophantic, e.g., always catering to users' claims when
it comes to unknown knowledge. Besides, generic LLMs showed stronger safety,
which can be learned by specialized LLMs through distilled data. We further
explored different types of data commonly adopted for fine-tuning specialized
LLMs, i.e., real-world, semi-distilled, and distilled data, and found that
distilled data can benefit LLMs most.
","2023-10-19","2310.11722v1.pdf"
"2310.11732","Guande He","Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, Jun Zhu","Investigating Uncertainty Calibration of Aligned Language Models under
  the Multiple-Choice Setting","","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  Despite the significant progress made in practical applications of aligned
language models (LMs), they tend to be overconfident in output answers compared
to the corresponding pre-trained LMs. In this work, we systematically evaluate
the impact of the alignment process on logit-based uncertainty calibration of
LMs under the multiple-choice setting. We first conduct a thoughtful empirical
study on how aligned LMs differ in calibration from their pre-trained
counterparts. Experimental results reveal that there are two distinct
uncertainties in LMs under the multiple-choice setting, which are responsible
for the answer decision and the format preference of the LMs, respectively.
Then, we investigate the role of these two uncertainties on aligned LM's
calibration through fine-tuning in simple synthetic alignment schemes and
conclude that one reason for aligned LMs' overconfidence is the conflation of
these two types of uncertainty. Furthermore, we examine the utility of common
post-hoc calibration methods for aligned LMs and propose an easy-to-implement
and sample-efficient method to calibrate aligned LMs. We hope our findings
could provide insights into the design of more reliable alignment processes for
LMs.
","2023-10-19","2310.11732v1.pdf"
"2310.11753","Naoki Wake","Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu,
  Katsushi Ikeuchi","Bias in Emotion Recognition with ChatGPT","5 pages, 4 figures, 6 tables","","","","cs.RO cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This technical report explores the ability of ChatGPT in recognizing emotions
from text, which can be the basis of various applications like interactive
chatbots, data annotation, and mental health analysis. While prior research has
shown ChatGPT's basic ability in sentiment analysis, its performance in more
nuanced emotion recognition is not yet explored. Here, we conducted experiments
to evaluate its performance of emotion recognition across different datasets
and emotion labels. Our findings indicate a reasonable level of reproducibility
in its performance, with noticeable improvement through fine-tuning. However,
the performance varies with different emotion labels and datasets, highlighting
an inherent instability and possible bias. The choice of dataset and emotion
labels significantly impacts ChatGPT's emotion recognition performance. This
paper sheds light on the importance of dataset and label selection, and the
potential of fine-tuning in enhancing ChatGPT's emotion recognition
capabilities, providing a groundwork for better integration of emotion analysis
in applications using ChatGPT.
","2023-10-19","2310.11753v1.pdf"
"2310.11761","Ruihao Shui","Ruihao Shui, Yixin Cao, Xiang Wang and Tat-Seng Chua","A Comprehensive Evaluation of Large Language Models on Legal Judgment
  Prediction","EMNLP Findings 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have demonstrated great potential for
domain-specific applications, such as the law domain. However, recent disputes
over GPT-4's law evaluation raise questions concerning their performance in
real-world legal tasks. To systematically investigate their competency in the
law, we design practical baseline solutions based on LLMs and test on the task
of legal judgment prediction. In our solutions, LLMs can work alone to answer
open questions or coordinate with an information retrieval (IR) system to learn
from similar cases or solve simplified multi-choice questions. We show that
similar cases and multi-choice options, namely label candidates, included in
prompts can help LLMs recall domain knowledge that is critical for expertise
legal reasoning. We additionally present an intriguing paradox wherein an IR
system surpasses the performance of LLM+IR due to limited gains acquired by
weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes
redundant. Our evaluation pipeline can be easily extended into other tasks to
facilitate evaluations in other domains. Code is available at
https://github.com/srhthu/LM-CompEval-Legal
","2023-10-19","2310.11761v1.pdf"
"2310.11769","Felix Stollenwerk","Felix Stollenwerk, Niklas Fastlund, Anna Nyqvist, Joey \""Ohman","Annotated Job Ads with Named Entity Recognition","SLTC 2022","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We have trained a named entity recognition (NER) model that screens Swedish
job ads for different kinds of useful information (e.g. skills required from a
job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we
faced was the creation of a labelled dataset, which required manual annotation.
This paper gives an overview of the methods we employed to make the annotation
process more efficient and to ensure high quality data. We also report on the
performance of the resulting model.
","2023-10-19","2310.11769v1.pdf"
"2310.11770","Ricardo Britto","Ricardo Britto, Timothy Murphy, Massimo Iovene, Leif Jonsson, Melike
  Erol-Kantarci, Benedek Kov\'acs","Telecom AI Native Systems in the Age of Generative AI -- An Engineering
  Perspective","5 pages, 1 figure","","","","cs.SE cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The rapid advancements in Artificial Intelligence (AI), particularly in
generative AI and foundational models (FMs), have ushered in transformative
changes across various industries. Large language models (LLMs), a type of FM,
have demonstrated their prowess in natural language processing tasks and
content generation, revolutionizing how we interact with software products and
services. This article explores the integration of FMs in the
telecommunications industry, shedding light on the concept of AI native telco,
where AI is seamlessly woven into the fabric of telecom products. It delves
into the engineering considerations and unique challenges associated with
implementing FMs into the software life cycle, emphasizing the need for AI
native-first approaches. Despite the enormous potential of FMs, ethical,
regulatory, and operational challenges require careful consideration,
especially in mission-critical telecom contexts. As the telecom industry seeks
to harness the power of AI, a comprehensive understanding of these challenges
is vital to thrive in a fiercely competitive market.
","2023-10-19","2310.11770v1.pdf"
"2310.11772","Yu Hai","Hai Yu, Chong Deng, Qinglin Zhang, Jiaqing Liu, Qian Chen, Wen Wang","Improving Long Document Topic Segmentation Models With Enhanced
  Coherence Modeling","Accepted by EMNLP 2023. Codes is available at
  https://github.com/alibaba-damo-academy/SpokenNLP/","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Topic segmentation is critical for obtaining structured documents and
improving downstream tasks such as information retrieval. Due to its ability of
automatically exploring clues of topic shift from abundant labeled data, recent
supervised neural models have greatly promoted the development of long document
topic segmentation, but leaving the deeper relationship between coherence and
topic segmentation underexplored. Therefore, this paper enhances the ability of
supervised models to capture coherence from both logical structure and semantic
similarity perspectives to further improve the topic segmentation performance,
proposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive
Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to
force the model to comprehend structural information by learning the original
relations between adjacent sentences in a disarrayed document, which is
constructed by jointly disrupting the original document at topic and sentence
levels. Moreover, we utilize inter- and intra-topic information to construct
contrastive samples and design the CSSL objective to ensure that the sentences
representations in the same topic have higher similarity, while those in
different topics are less similar. Extensive experiments show that the
Longformer with our approach significantly outperforms old state-of-the-art
(SOTA) methods. Our approach improve $F_1$ of old SOTA by 3.42 (73.74 -> 77.16)
and reduces $P_k$ by 1.11 points (15.0 -> 13.89) on WIKI-727K and achieves an
average relative reduction of 4.3% on $P_k$ on WikiSection. The average
relative $P_k$ drop of 8.38% on two out-of-domain datasets also demonstrates
the robustness of our approach.
","2023-10-24","2310.11772v1.pdf"
"2310.11778","Qichao Wang","Qichao Wang, Tian Bian, Yian Yin, Tingyang Xu, Hong Cheng, Helen M.
  Meng, Zibin Zheng, Liang Chen, Bingzhe Wu","Language Agents for Detecting Implicit Stereotypes in Text-to-image
  Models at Scale","","","","","cs.CY cs.CL","http://creativecommons.org/licenses/by/4.0/","  The recent surge in the research of diffusion models has accelerated the
adoption of text-to-image models in various Artificial Intelligence Generated
Content (AIGC) commercial products. While these exceptional AIGC products are
gaining increasing recognition and sparking enthusiasm among consumers, the
questions regarding whether, when, and how these models might unintentionally
reinforce existing societal stereotypes remain largely unaddressed. Motivated
by recent advancements in language agents, here we introduce a novel agent
architecture tailored for stereotype detection in text-to-image models. This
versatile agent architecture is capable of accommodating free-form detection
tasks and can autonomously invoke various tools to facilitate the entire
process, from generating corresponding instructions and images, to detecting
stereotypes. We build the stereotype-relevant benchmark based on multiple
open-text datasets, and apply this architecture to commercial products and
popular open source text-to-image models. We find that these models often
display serious stereotypes when it comes to certain prompts about personal
characteristics, social cultural context and crime-related aspects. In summary,
these empirical findings underscore the pervasive existence of stereotypes
across social dimensions, including gender, race, and religion, which not only
validate the effectiveness of our proposed approach, but also emphasize the
critical necessity of addressing potential ethical risks in the burgeoning
realm of AIGC. As AIGC continues its rapid expansion trajectory, with new
models and plugins emerging daily in staggering numbers, the challenge lies in
the timely detection and mitigation of potential biases within these models.
","2023-10-19","2310.11778v1.pdf"
"2310.11784","Xinhua Cheng","Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang,
  Li Yuan","Progressive3D: Progressively Local Editing for Text-to-3D Content
  Creation with Complex Semantic Prompts","Project Page: https://cxh0519.github.io/projects/Progressive3D/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent text-to-3D generation methods achieve impressive 3D content creation
capacity thanks to the advances in image diffusion models and optimizing
strategies. However, current methods struggle to generate correct 3D content
for a complex prompt in semantics, i.e., a prompt describing multiple
interacted objects binding with different attributes. In this work, we propose
a general framework named Progressive3D, which decomposes the entire generation
into a series of locally progressive editing steps to create precise 3D content
for complex prompts, and we constrain the content change to only occur in
regions determined by user-defined region prompts in each editing step.
Furthermore, we propose an overlapped semantic component suppression technique
to encourage the optimization process to focus more on the semantic differences
between prompts. Extensive experiments demonstrate that the proposed
Progressive3D framework generates precise 3D content for prompts with complex
semantics and is general for various text-to-3D methods driven by different 3D
representations.
","2023-10-19","2310.11784v1.pdf"
"2310.11800","Roman Gerasimov","Roman Gerasimov, Adam J. Burgasser, Ilaria Caiazzo, Derek Homeier,
  Harvey B. Richer, Matteo Correnti, Jeremy Heyl","Exploring the Chemistry and Mass Function of the Globular Cluster 47
  Tucanae with New Theoretical Color-Magnitude Diagrams","","","","","astro-ph.SR astro-ph.GA","http://creativecommons.org/licenses/by/4.0/","  Despite their shared origin, members of globular clusters display
star-to-star variations in composition. The observed pattern of element
abundances is unique to these stellar environments, and cannot be fully
explained by any proposed mechanism. It remains unclear whether stars form with
chemical heterogeneity, or inherit it from interactions with other members.
These scenarios may be differentiated by the dependence of chemical spread on
stellar mass; however, obtaining a sufficiently large mass baseline requires
abundance measurements on the lower main sequence that is too faint for
spectroscopy even in the nearest globular clusters. We developed a stellar
modelling method to obtain precise chemical abundances for stars near the end
of the main sequence from multiband photometry, and applied it to the globular
cluster 47 Tucanae. The computational efficiency is attained by matching
chemical elements to the model components that are most sensitive to their
abundance. We determined [O/Fe] for ~5000 members below the main sequence knee
at the level of accuracy, comparable to the spectroscopic measurements of
evolved members in literature. The inferred distribution disfavors stellar
interactions as the origin of chemical spread; however, an accurate theory of
accretion is required to draw a more definitive conclusion. We anticipate that
future observations of 47 Tucanae with JWST will extend the mass baseline of
our analysis into the substellar regime. Therefore, we present predicted
color-magnitude diagrams and mass-magnitude relations for the brown dwarf
members of 47 Tucanae.
","2023-10-19","2310.11800v1.pdf"
"2310.11802","Weian Mao","Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao
  Chen, Chunhua Shen","De novo protein design using geometric vector field networks","","","","","cs.CE cs.LG q-bio.BM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Innovations like protein diffusion have enabled significant progress in de
novo protein design, which is a vital topic in life science. These methods
typically depend on protein structure encoders to model residue backbone
frames, where atoms do not exist. Most prior encoders rely on atom-wise
features, such as angles and distances between atoms, which are not available
in this context. Thus far, only several simple encoders, such as IPA, have been
proposed for this scenario, exposing the frame modeling as a bottleneck. In
this work, we proffer the Vector Field Network (VFN), which enables network
layers to perform learnable vector computations between coordinates of
frame-anchored virtual atoms, thus achieving a higher capability for modeling
frames. The vector computation operates in a manner similar to a linear layer,
with each input channel receiving 3D virtual atom coordinates instead of scalar
values. The multiple feature vectors output by the vector computation are then
used to update the residue representations and virtual atom coordinates via
attention aggregation. Remarkably, VFN also excels in modeling both frames and
atoms, as the real atoms can be treated as the virtual atoms for modeling,
positioning VFN as a potential universal encoder. In protein diffusion (frame
modeling), VFN exhibits an impressive performance advantage over IPA, excelling
in terms of both designability (67.04% vs. 53.58%) and diversity (66.54% vs.
51.98%). In inverse folding (frame and atom modeling), VFN outperforms the
previous SoTA model, PiFold (54.7% vs. 51.66%), on sequence recovery rate. We
also propose a method of equipping VFN with the ESM model, which significantly
surpasses the previous ESM-based SoTA (62.67% vs. 55.65%), LM-Design, by a
substantial margin.
","2023-10-19","2310.11802v1.pdf"
"2310.11829","Jiawei Liu","Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei
  Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi","Towards Graph Foundation Models: A Survey and Beyond","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Emerging as fundamental building blocks for diverse artificial intelligence
applications, foundation models have achieved notable success across natural
language processing and many other domains. Parallelly, graph machine learning
has witnessed a transformative shift, with shallow methods giving way to deep
learning approaches. The emergence and homogenization capabilities of
foundation models have piqued the interest of graph machine learning
researchers, sparking discussions about developing the next graph learning
paradigm that is pre-trained on broad graph data and can be adapted to a wide
range of downstream graph tasks. However, there is currently no clear
definition and systematic analysis for this type of work. In this article, we
propose the concept of graph foundation models (GFMs), and provide the first
comprehensive elucidation on their key characteristics and technologies.
Following that, we categorize existing works towards GFMs into three categories
based on their reliance on graph neural networks and large language models.
Beyond providing a comprehensive overview of the current landscape of graph
foundation models, this article also discusses potential research directions
for this evolving field.
","2023-10-19","2310.11829v1.pdf"
"2310.11848","Malak Sadek","Malak Sadek, Rafael A. Calvo, Celine Mougenot","The Value-Sensitive Conversational Agent Co-Design Framework","23 pages, 8 figures","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Conversational agents (CAs) are gaining traction in both industry and
academia, especially with the advent of generative AI and large language
models. As these agents are used more broadly by members of the general public
and take on a number of critical use cases and social roles, it becomes
important to consider the values embedded in these systems. This consideration
includes answering questions such as 'whose values get embedded in these
agents?' and 'how do those values manifest in the agents being designed?'
Accordingly, the aim of this paper is to present the Value-Sensitive
Conversational Agent (VSCA) Framework for enabling the collaborative design
(co-design) of value-sensitive CAs with relevant stakeholders. Firstly,
requirements for co-designing value-sensitive CAs which were identified in
previous works are summarised here. Secondly, the practical framework is
presented and discussed, including its operationalisation into a design
toolkit. The framework facilitates the co-design of three artefacts that elicit
stakeholder values and have a technical utility to CA teams to guide CA
implementation, enabling the creation of value-embodied CA prototypes. Finally,
an evaluation protocol for the framework is proposed where the effects of the
framework and toolkit are explored in a design workshop setting to evaluate
both the process followed and the outcomes produced.
","2023-10-19","2310.11848v1.pdf"
"2310.11857","Yuqing Kong","Yuqing Kong","Multistable Perception, False Consensus, and Information Complements","","","","","cs.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents a distributed communication model to investigate
multistable perception, where a stimulus gives rise to multiple competing
perceptual interpretations. We formalize stable perception as consensus
achieved through components exchanging information. Our key finding is that
relationships between components influence monostable versus multistable
perceptions. When components contain substitute information about the
prediction target, stimuli display monostability. With complementary
information, multistability arises. We then analyze phenomena like order
effects and switching costs. Finally, we provide two additional perspectives.
An optimization perspective balances accuracy and communication costs, relating
stability to local optima. A Prediction market perspective highlights the
strategic behaviors of neural coordination and provides insights into phenomena
like rivalry, inhibition, and mental disorders. The two perspectives
demonstrate how relationships among components influence perception costs, and
impact competition and coordination behaviors in neural dynamics.
","2023-10-19","2310.11857v1.pdf"
"2310.11862","Shiye Wang","Shiye Wang, Kaituo Feng, Changsheng Li, Ye Yuan, Guoren Wang","Learning to Generate Parameters of ConvNets for Unseen Image Data","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Typical Convolutional Neural Networks (ConvNets) depend heavily on large
amounts of image data and resort to an iterative optimization algorithm (e.g.,
SGD or Adam) to learn network parameters, which makes training very time- and
resource-intensive. In this paper, we propose a new training paradigm and
formulate the parameter learning of ConvNets into a prediction task: given a
ConvNet architecture, we observe there exists correlations between image
datasets and their corresponding optimal network parameters, and explore if we
can learn a hyper-mapping between them to capture the relations, such that we
can directly predict the parameters of the network for an image dataset never
seen during the training phase. To do this, we put forward a new hypernetwork
based model, called PudNet, which intends to learn a mapping between datasets
and their corresponding network parameters, and then predicts parameters for
unseen data with only a single forward propagation. Moreover, our model
benefits from a series of adaptive hyper recurrent units sharing weights to
capture the dependencies of parameters among different network layers.
Extensive experiments demonstrate that our proposed method achieves good
efficacy for unseen image datasets on two kinds of settings: Intra-dataset
prediction and Inter-dataset prediction. Our PudNet can also well scale up to
large-scale datasets, e.g., ImageNet-1K. It takes 8967 GPU seconds to train
ResNet-18 on the ImageNet-1K using GC from scratch and obtain a top-5 accuracy
of 44.65 %. However, our PudNet costs only 3.89 GPU seconds to predict the
network parameters of ResNet-18 achieving comparable performance (44.92 %),
more than 2,300 times faster than the traditional training paradigm.
","2023-10-25","2310.11862v1.pdf"
"2310.11870","Ze Gao Mr","Yuqian Sun, Yuying Tang, Ze Gao, Zhijun Pan, Chuyan Xu, Yurou Chen,
  Kejiang Qian, Zhigang Wang, Tristan Braud, Chang Hee Lee, Ali Asadipour","AI Nushu: An Exploration of Language Emergence in Sisterhood -Through
  the Lens of Computational Linguistics","Accepted for publication at SIGGRAPH Asia 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  This paper presents ""AI Nushu,"" an emerging language system inspired by Nushu
(women's scripts), the unique language created and used exclusively by ancient
Chinese women who were thought to be illiterate under a patriarchal society. In
this interactive installation, two artificial intelligence (AI) agents are
trained in the Chinese dictionary and the Nushu corpus. By continually
observing their environment and communicating, these agents collaborate towards
creating a standard writing system to encode Chinese. It offers an artistic
interpretation of the creation of a non-western script from a computational
linguistics perspective, integrating AI technology with Chinese cultural
heritage and a feminist viewpoint.
","2023-10-19","2310.11870v1.pdf"
"2310.11877","Aviv Slobodkin","Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, Shauli
  Ravfogel","The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the
  Hidden States of Over-Confident Large Language Models","EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have been shown to possess impressive
capabilities, while also raising crucial concerns about the faithfulness of
their responses. A primary issue arising in this context is the management of
unanswerable queries by LLMs, which often results in hallucinatory behavior,
due to overconfidence. In this paper, we explore the behavior of LLMs when
presented with unanswerable queries. We ask: do models \textbf{represent} the
fact that the question is unanswerable when generating a hallucinatory answer?
Our results show strong indications that such models encode the answerability
of an input query, with the representation of the first decoded token often
being a strong indicator. These findings shed new light on the spatial
organization within the latent representations of LLMs, unveiling previously
unexplored facets of these models. Moreover, they pave the way for the
development of improved decoding techniques with better adherence to factual
generation, particularly in scenarios where query unanswerability is a concern.
","2023-10-19","2310.11877v1.pdf"
"2310.11952","Soochan Lee","Soochan Lee, Jaehyeon Son, Gunhee Kim","Recasting Continual Learning as Sequence Modeling","NeurIPS 2023","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  In this work, we aim to establish a strong connection between two significant
bodies of machine learning research: continual learning and sequence modeling.
That is, we propose to formulate continual learning as a sequence modeling
problem, allowing advanced sequence models to be utilized for continual
learning. Under this formulation, the continual learning process becomes the
forward pass of a sequence model. By adopting the meta-continual learning (MCL)
framework, we can train the sequence model at the meta-level, on multiple
continual learning episodes. As a specific example of our new formulation, we
demonstrate the application of Transformers and their efficient variants as MCL
methods. Our experiments on seven benchmarks, covering both classification and
regression, show that sequence models can be an attractive solution for general
MCL.
","2023-10-19","2310.11952v1.pdf"
"2310.11954","Dingyao Yu","Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun
  Zhang, Jiang Bian","MusicAgent: An AI Agent for Music Understanding and Generation with
  Large Language Models","","","","","cs.CL cs.MM eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  AI-empowered music processing is a diverse field that encompasses dozens of
tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension
tasks (e.g., music classification). For developers and amateurs, it is very
difficult to grasp all of these task to satisfy their requirements in music
processing, especially considering the huge differences in the representations
of music data and the model applicability across platforms among various tasks.
Consequently, it is necessary to build a system to organize and integrate these
tasks, and thus help practitioners to automatically analyze their demand and
call suitable tools as solutions to fulfill their requirements. Inspired by the
recent success of large language models (LLMs) in task automation, we develop a
system, named MusicAgent, which integrates numerous music-related tools and an
autonomous workflow to address user requirements. More specifically, we build
1) toolset that collects tools from diverse sources, including Hugging Face,
GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g.,
ChatGPT) to organize these tools and automatically decompose user requests into
multiple sub-tasks and invoke corresponding music tools. The primary goal of
this system is to free users from the intricacies of AI-music tools, enabling
them to concentrate on the creative aspect. By granting users the freedom to
effortlessly combine tools, the system offers a seamless and enriching music
experience.
","2023-10-26","2310.11954v1.pdf"
"2310.11958","Yuval Pinter","Yuval Pinter, Michael Elhadad","Emptying the Ocean with a Spoon: Should We Edit Models?","Findings of ACL: EMNLP 2023","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We call into question the recently popularized method of direct model editing
as a means of correcting factual errors in LLM generations. We contrast model
editing with three similar but distinct approaches that pursue better defined
objectives: (1) retrieval-based architectures, which decouple factual memory
from inference and linguistic capabilities embodied in LLMs; (2) concept
erasure methods, which aim at preventing systemic bias in generated text; and
(3) attribution methods, which aim at grounding generations into identified
textual sources. We argue that direct model editing cannot be trusted as a
systematic remedy for the disadvantages inherent to LLMs, and while it has
proven potential in improving model explainability, it opens risks by
reinforcing the notion that models can be trusted for factuality. We call for
cautious promotion and application of model editing as part of the LLM
deployment process, and for responsibly limiting the use cases of LLMs to those
not relying on editing as a critical component.
","2023-10-19","2310.11958v1.pdf"
"2310.11960","Yanming Kang","Yanming Kang, Giang Tran, Hans De Sterck","Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for
  Long Sequences","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformer-based models have achieved state-of-the-art performance in many
areas. However, the quadratic complexity of self-attention with respect to the
input length hinders the applicability of Transformer-based models to long
sequences. To address this, we present Fast Multipole Attention, a new
attention mechanism that uses a divide-and-conquer strategy to reduce the time
and memory complexity of attention for sequences of length $n$ from
$\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a
global receptive field. The hierarchical approach groups queries, keys, and
values into $\mathcal{O}( \log n)$ levels of resolution, where groups at
greater distances are increasingly larger in size and the weights to compute
group quantities are learned. As such, the interaction between tokens far from
each other is considered in lower resolution in an efficient hierarchical
manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$
or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled
or not. This multi-level divide-and-conquer strategy is inspired by fast
summation methods from $n$-body physics and the Fast Multipole Method. We
perform evaluation on autoregressive and bidirectional language modeling tasks
and compare our Fast Multipole Attention model with other efficient attention
variants on medium-size datasets. We find empirically that the Fast Multipole
Transformer performs much better than other efficient transformers in terms of
memory size and accuracy. The Fast Multipole Attention mechanism has the
potential to empower large language models with much greater sequence lengths,
taking the full context into account in an efficient, naturally hierarchical
manner during training and when generating long sequences.
","2023-10-24","2310.11960v1.pdf"
"2310.11970","Yixin Wu","Yixin Wu, Rui Wen, Michael Backes, Pascal Berrang, Mathias Humbert,
  Yun Shen, Yang Zhang","Quantifying Privacy Risks of Prompts in Visual Prompt Learning","To appear in the 33rd USENIX Security Symposium, August 14-16, 2024","","","","cs.CR","http://creativecommons.org/licenses/by/4.0/","  Large-scale pre-trained models are increasingly adapted to downstream tasks
through a new paradigm called prompt learning. In contrast to fine-tuning,
prompt learning does not update the pre-trained model's parameters. Instead, it
only learns an input perturbation, namely prompt, to be added to the downstream
task data for predictions. Given the fast development of prompt learning, a
well-generalized prompt inevitably becomes a valuable asset as significant
effort and proprietary data are used to create it. This naturally raises the
question of whether a prompt may leak the proprietary information of its
training data. In this paper, we perform the first comprehensive privacy
assessment of prompts learned by visual prompt learning through the lens of
property inference and membership inference attacks. Our empirical evaluation
shows that the prompts are vulnerable to both attacks. We also demonstrate that
the adversary can mount a successful property inference attack with limited
cost. Moreover, we show that membership inference attacks against prompts can
be successful with relaxed adversarial assumptions. We further make some
initial investigations on the defenses and observe that our method can mitigate
the membership inference attacks with a decent utility-defense trade-off but
fails to defend against property inference attacks. We hope our results can
shed light on the privacy risks of the popular prompt learning paradigm. To
facilitate the research in this direction, we will share our code and models
with the community.
","2023-10-19","2310.11970v1.pdf"
"2310.11971","Rui Zheng","Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou,
  Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang","Improving Generalization of Alignment with Human Preferences through
  Group Invariant Learning","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  The success of AI assistants based on language models (LLMs) hinges crucially
on Reinforcement Learning from Human Feedback (RLHF), which enables the
generation of responses more aligned with human preferences. As universal AI
assistants, there's a growing expectation for them to perform consistently
across various domains. However, previous work shows that Reinforcement
Learning (RL) often exploits shortcuts to attain high rewards and overlooks
challenging samples. This focus on quick reward gains undermines both the
stability in training and the model's ability to generalize to new, unseen
data. In this work, we propose a novel approach that can learn a consistent
policy via RL across various data groups or domains. Given the challenges
associated with acquiring group annotations, our method automatically
classifies data into different groups, deliberately maximizing performance
variance. Then, we optimize the policy to perform well on challenging groups.
Lastly, leveraging the established groups, our approach adaptively adjusts the
exploration space, allocating more learning capacity to more challenging data
and preventing the model from over-optimizing on simpler data. Experimental
results indicate that our approach significantly enhances training stability
and model generalization.
","2023-10-20","2310.11971v2.pdf"
"2310.11984","Shaoxiong Duan","Shaoxiong Duan and Yining Shi","From Interpolation to Extrapolation: Complete Length Generalization for
  Arithmetic Transformers","","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Since its introduction, the transformer model has demonstrated outstanding
performance across various tasks. However, there are still unresolved issues
regarding length generalization, particularly in algorithmic tasks. In this
paper, we investigate the inherent capabilities of transformer models in
learning arithmetic algorithms, such as addition and multiplication. Through
experiments and attention analysis, we identify a number of crucial factors for
achieving optimal length generalization. We show that transformer models are
able to generalize to long lengths with the help of targeted attention biasing.
We then introduce Attention Bias Calibration (ABC), a calibration stage that
enables the model to automatically learn the proper attention biases, which we
link to mechanisms in relative position encoding. We demonstrate that using
ABC, the transformer model can achieve unprecedented perfect length
generalization on certain arithmetic tasks.
","2023-10-19","2310.11984v1.pdf"
"2310.11986","Laura Weidinger","Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa
  Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor
  Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac","Sociotechnical Safety Evaluation of Generative AI Systems","main paper p.1-29, 5 figures, 2 tables","","","","cs.AI cs.CL cs.CY","http://creativecommons.org/licenses/by/4.0/","  Generative AI systems produce a range of risks. To ensure the safety of
generative AI systems, these risks must be evaluated. In this paper, we make
two main contributions toward establishing such evaluations. First, we propose
a three-layered framework that takes a structured, sociotechnical approach to
evaluating these risks. This framework encompasses capability evaluations,
which are the main current approach to safety evaluation. It then reaches
further by building on system safety principles, particularly the insight that
context determines whether a given capability may cause harm. To account for
relevant context, our framework adds human interaction and systemic impacts as
additional layers of evaluation. Second, we survey the current state of safety
evaluation of generative AI systems and create a repository of existing
evaluations. Three salient evaluation gaps emerge from this analysis. We
propose ways forward to closing these gaps, outlining practical steps as well
as roles and responsibilities for different actors. Sociotechnical safety
evaluation is a tractable approach to the robust and comprehensive safety
evaluation of generative AI systems.
","2023-10-19","2310.11986v1.pdf"
"2310.11991","Floris Holstege","Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks","Removing Spurious Concepts from Neural Network Representations via Joint
  Subspace Estimation","Preprint. Under Review. 33 pages","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Out-of-distribution generalization in neural networks is often hampered by
spurious correlations. A common strategy is to mitigate this by removing
spurious concepts from the neural network representation of the data. Existing
concept-removal methods tend to be overzealous by inadvertently eliminating
features associated with the main task of the model, thereby harming model
performance. We propose an iterative algorithm that separates spurious from
main-task concepts by jointly identifying two low-dimensional orthogonal
subspaces in the neural network representation. We evaluate the algorithm on
benchmark datasets for computer vision (Waterbirds, CelebA) and natural
language processing (MultiNLI), and show that it outperforms existing concept
removal methods
","2023-10-19","2310.11991v1.pdf"
"2310.12011","Zheye Deng","Zheye Deng, Weiqi Wang, Zhaowei Wang, Xin Liu, Yangqiu Song","Gold: A Global and Local-aware Denoising Framework for Commonsense
  Knowledge Graph Noise Detection","Accepted to EMNLP findings 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning,
yet constructing them through human annotations can be costly. As a result,
various automatic methods have been proposed to construct CSKG with larger
semantic coverage. However, these unsupervised approaches introduce spurious
noise that can lower the quality of the resulting CSKG, which cannot be tackled
easily by existing denoising algorithms due to the unique characteristics of
nodes and structures in CSKGs. To address this issue, we propose Gold (Global
and Local-aware Denoising), a denoising framework for CSKGs that incorporates
entity semantic information, global rules, and local structural information
from the CSKG. Experiment results demonstrate that Gold outperforms all
baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks.
Furthermore, we show that denoising a real-world CSKG is effective and even
benefits the downstream zero-shot commonsense question-answering task.
","2023-10-19","2310.12011v1.pdf"
"2310.12020","Shengqiang Zhang","Shengqiang Zhang, Philipp Wicke, L\""utfi Kerem \c{S}enel, Luis
  Figueredo, Abdeldjallil Naceri, Sami Haddadin, Barbara Plank, Hinrich
  Sch\""utze","LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic
  Tabletop Manipulation","6 pages, 4 figures. The video and code of LoHoRavens are available at
  https://cisnlp.github.io/lohoravens-webpage/","","","","cs.RO cs.CL cs.CV","http://creativecommons.org/licenses/by/4.0/","  The convergence of embodied agents and large language models (LLMs) has
brought significant advancements to embodied instruction following.
Particularly, the strong reasoning capabilities of LLMs make it possible for
robots to perform long-horizon tasks without expensive annotated
demonstrations. However, public benchmarks for testing the long-horizon
reasoning capabilities of language-conditioned robots in various scenarios are
still missing. To fill this gap, this work focuses on the tabletop manipulation
task and releases a simulation benchmark, \textit{LoHoRavens}, which covers
various long-horizon reasoning aspects spanning color, size, space, arithmetics
and reference. Furthermore, there is a key modality bridging problem for
long-horizon manipulation tasks with LLMs: how to incorporate the observation
feedback during robot execution for the LLM's closed-loop planning, which is
however less studied by prior work. We investigate two methods of bridging the
modality gap: caption generation and learnable interface for incorporating
explicit and implicit observation feedback to the LLM, respectively. These
methods serve as the two baselines for our proposed benchmark. Experiments show
that both methods struggle to solve some tasks, indicating long-horizon
manipulation tasks are still challenging for current popular models. We expect
the proposed public benchmark and baselines can help the community develop
better models for long-horizon tabletop manipulation tasks.
","2023-10-24","2310.12020v1.pdf"
"2310.12036","Mohammad Gheshlaghi Azar","Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel
  Guo and Daniele Calandriello and Michal Valko and R\'emi Munos","A General Theoretical Paradigm to Understand Learning from Human
  Preferences","","","","","cs.AI cs.LG stat.ML","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The prevalent deployment of learning from human preferences through
reinforcement learning (RLHF) relies on two important approximations: the first
assumes that pairwise preferences can be substituted with pointwise rewards.
The second assumes that a reward model trained on these pointwise rewards can
generalize from collected data to out-of-distribution data sampled by the
policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an
approach that bypasses the second approximation and learn directly a policy
from collected data without the reward modelling stage. However, this method
still heavily relies on the first approximation.
  In this paper we try to gain a deeper theoretical understanding of these
practical algorithms. In particular we derive a new general objective called
$\Psi$PO for learning from human preferences that is expressed in terms of
pairwise preferences and therefore bypasses both approximations. This new
general objective allows us to perform an in-depth analysis of the behavior of
RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential
pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$
simply to Identity, for which we can derive an efficient optimisation
procedure, prove performance guarantees and demonstrate its empirical
superiority to DPO on some illustrative examples.
","2023-10-19","2310.12036v1.pdf"
"2310.12049","Patrick Y. Wu","Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing","Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison
  Scaling of Texts with Large Language Models","26 pages, 2 figures","","","","cs.CL cs.CY","http://creativecommons.org/publicdomain/zero/1.0/","  Existing text scaling methods often require a large corpus, struggle with
short texts, or require labeled data. We develop a text scaling method that
leverages the pattern recognition capabilities of generative large language
models (LLMs). Specifically, we propose concept-guided chain-of-thought
(CGCoT), which uses prompts designed to summarize ideas and identify target
parties in texts to generate concept-specific breakdowns, in many ways similar
to guidance for human coder content analysis. CGCoT effectively shifts pairwise
text comparisons from a reasoning problem to a pattern recognition problem. We
then pairwise compare concept-specific breakdowns using an LLM. We use the
results of these pairwise comparisons to estimate a scale using the
Bradley-Terry model. We use this approach to scale affective speech on Twitter.
Our measures correlate more strongly with human judgments than alternative
approaches like Wordfish. Besides a small set of pilot data to develop the
CGCoT prompts, our measures require no additional labeled data and produce
binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands
of human-labeled tweets. We demonstrate how combining substantive knowledge
with LLMs can create state-of-the-art measures of abstract concepts.
","2023-10-19","2310.12049v1.pdf"
"2310.12069","Bhuvnesh Jain","Dimitrios Tanoglidis, Bhuvnesh Jain, Helen Qu (University of
  Pennsylvania)","Transformers for scientific data: a pedagogical review for astronomers","17 pages, 5 figures","","","","astro-ph.IM cs.LG","http://creativecommons.org/licenses/by/4.0/","  The deep learning architecture associated with ChatGPT and related generative
AI products is known as transformers. Initially applied to Natural Language
Processing, transformers and the self-attention mechanism they exploit have
gained widespread interest across the natural sciences. The goal of this
pedagogical and informal review is to introduce transformers to scientists. The
review includes the mathematics underlying the attention mechanism, a
description of the original transformer architecture, and a section on
applications to time series and imaging data in astronomy. We include a
Frequently Asked Questions section for readers who are curious about generative
AI or interested in getting started with transformers for their research
problem.
","2023-10-20","2310.12069v1.pdf"
"2310.12072","Sehoon Kim","Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt
  Keutzer, Amir Gholami, Sophia Shao","SPEED: Speculative Pipelined Execution for Efficient Decoding","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.
","2023-10-19","2310.12072v1.pdf"
"2310.12085","Zikang Leng","Zikang Leng, Hyeokhyen Kwon, Thomas Pl\""otz","On the Benefit of Generative Foundation Models for Human Activity
  Recognition","Generative AI for Pervasive Computing (GenAI4PC) Symposium within
  UbiComp/ISWC 2023","","","","cs.CV cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In human activity recognition (HAR), the limited availability of annotated
data presents a significant challenge. Drawing inspiration from the latest
advancements in generative AI, including Large Language Models (LLMs) and
motion synthesis models, we believe that generative AI can address this data
scarcity by autonomously generating virtual IMU data from text descriptions.
Beyond this, we spotlight several promising research pathways that could
benefit from generative AI for the community, including the generating
benchmark datasets, the development of foundational models specific to HAR, the
exploration of hierarchical structures within HAR, breaking down complex
activities, and applications in health sensing and activity summarization.
","2023-10-19","2310.12085v1.pdf"
"2310.12100","Yaqing Wang","Yaqing Wang, Jialin Wu, Tanmaya Dabral, Jiageng Zhang, Geoff Brown,
  Chun-Ta Lu, Frederick Liu, Yi Liang, Bo Pang, Michael Bendersky, Radu Soricut","Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning
  for Versatile Multimodal Modeling","","","","","cs.CL cs.AI cs.CV cs.LG cs.MM","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) and vision language models (VLMs) demonstrate
excellent performance on a wide range of tasks by scaling up parameter counts
from O(10^9) to O(10^{12}) levels and further beyond. These large scales make
it impossible to adapt and deploy fully specialized models given a task of
interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising
direction to tackle the adaptation and serving challenges for such large
models. We categorize PEFT techniques into two types: intrusive and
non-intrusive. Intrusive PEFT techniques directly change a model's internal
architecture. Though more flexible, they introduce significant complexities for
training and serving. Non-intrusive PEFT techniques leave the internal
architecture unchanged and only adapt model-external parameters, such as
embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT
technique that achieves competitive performance compared to SoTA intrusive PEFT
(LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both
text-only and multimodal tasks, with experiments that account for both
parameter-count scaling and training regime (with and without instruction
tuning).
","2023-10-19","2310.12100v1.pdf"
"2310.12103","Li Ding","Li Ding, Jenny Zhang, Jeff Clune, Lee Spector, Joel Lehman","Quality Diversity through Human Feedback","","","","","cs.AI cs.NE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reinforcement learning from human feedback (RLHF) has exhibited the potential
to enhance the performance of foundation models for qualitative tasks. Despite
its promise, its efficacy is often restricted when conceptualized merely as a
mechanism to maximize learned reward models of averaged human preferences,
especially in areas such as image generation which demand diverse model
responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking
diverse, high-quality solutions, are often constrained by the dependency on
manually defined diversity metrics. Interestingly, such limitations of RLHF and
QD can be overcome by blending insights from both. This paper introduces
Quality Diversity through Human Feedback (QDHF), which employs human feedback
for inferring diversity metrics, expanding the applicability of QD algorithms.
Empirical results reveal that QDHF outperforms existing QD methods regarding
automatic diversity discovery, and matches the search capabilities of QD with
human-constructed metrics. Notably, when deployed for a latent space
illumination task, QDHF markedly enhances the diversity of images generated by
a Diffusion model. The study concludes with an in-depth analysis of QDHF's
sample efficiency and the quality of its derived diversity metrics, emphasizing
its promise for enhancing exploration and diversity in optimization for
complex, open-ended tasks.
","2023-10-19","2310.12103v1.pdf"
"2310.12128","Jaemin Cho","Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal","DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM
  Planning","Project page: https://diagrammerGPT.github.io/","","","","cs.CV cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text-to-image (T2I) generation has seen significant growth over the past few
years. Despite this, there has been little work on generating diagrams with T2I
models. A diagram is a symbolic/schematic representation that explains
information using structurally rich and spatially complex visualizations (e.g.,
a dense combination of related objects, text labels, directional arrows,
connection lines, etc.). Existing state-of-the-art T2I models often fail at
diagram generation because they lack fine-grained object layout control when
many objects are densely connected via complex relations such as arrows/lines
and also often fail to render comprehensible text labels. To address this gap,
we present DiagrammerGPT, a novel two-stage text-to-diagram generation
framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)
to generate more accurate open-domain, open-platform diagrams. In the first
stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a
planner-auditor feedback loop) which describe all the entities (objects and
text labels), their relationships (arrows or lines), and their bounding box
layouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a
text label rendering module to generate diagrams following the diagram plans.
To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a
densely annotated diagram dataset built on top of the AI2D dataset. We show
quantitatively and qualitatively that our DiagrammerGPT framework produces more
accurate diagrams, outperforming existing T2I models. We also provide
comprehensive analysis including open-domain diagram generation, vector graphic
diagram generation in different platforms, human-in-the-loop diagram plan
editing, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our
work can inspire further research on diagram generation via T2I models and
LLMs.
","2023-10-19","2310.12128v1.pdf"
"2310.12135","Shikhar Murty","Shikhar Murty, Orr Paradise, Pratyusha Sharma","Pseudointelligence: A Unifying Framework for Language Model Evaluation","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  With large language models surpassing human performance on an increasing
number of benchmarks, we must take a principled approach for targeted
evaluation of model capabilities. Inspired by pseudorandomness, we propose
pseudointelligence, which captures the maxim that ""(perceived) intelligence
lies in the eye of the beholder"". That is, that claims of intelligence are
meaningful only when their evaluator is taken into account. Concretely, we
propose a complexity-theoretic framework of model evaluation cast as a dynamic
interaction between a model and a learned evaluator. We demonstrate that this
framework can be used to reason about two case studies in language model
evaluation, as well as analyze existing evaluation methods.
","2023-10-19","2310.12135v1.pdf"
"2310.12147","Hanbo Zhang","Hanbo Zhang and Jie Xu and Yuchen Mo and Tao Kong","InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot
  Interactions","8 pages, 9 figures, 3 tables, under review","","","","cs.RO cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ambiguity is ubiquitous in human communication. Previous approaches in
Human-Robot Interaction (HRI) have often relied on predefined interaction
templates, leading to reduced performance in realistic and open-ended
scenarios. To address these issues, we present a large-scale dataset, \invig,
for interactive visual grounding under language ambiguity. Our dataset
comprises over 520K images accompanied by open-ended goal-oriented
disambiguation dialogues, encompassing millions of object instances and
corresponding question-answer pairs. Leveraging the \invig dataset, we conduct
extensive studies and propose a set of baseline solutions for end-to-end
interactive visual disambiguation and grounding, achieving a 45.6\% success
rate during validation. To the best of our knowledge, the \invig dataset is the
first large-scale dataset for resolving open-ended interactive visual
grounding, presenting a practical yet highly challenging benchmark for
ambiguity-aware HRI. Codes and datasets are available at:
\href{https://openivg.github.io}{https://openivg.github.io}.
","2023-10-19","2310.12147v1.pdf"
"2310.12150","Hung-Ting Chen","Hung-Ting Chen, Fangyuan Xu, Shane A. Arora, Eunsol Choi","Understanding Retrieval Augmentation for Long-Form Question Answering","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present a study of retrieval-augmented language models (LMs) on long-form
question answering. We analyze how retrieval augmentation impacts different
LMs, by comparing answers generated from models while using the same evidence
documents, and how differing quality of retrieval document set impacts the
answers generated from the same LM. We study various attributes of generated
answers (e.g., fluency, length, variance) with an emphasis on the attribution
of generated long-form answers to in-context evidence documents. We collect
human annotations of answer attribution and evaluate methods for automatically
judging attribution. Our study provides new insights on how retrieval
augmentation impacts long, knowledge-rich text generation of LMs. We further
identify attribution patterns for long text generation and analyze the main
culprits of attribution errors. Together, our analysis reveals how retrieval
augmentation impacts long knowledge-rich text generation and provide directions
for future work.
","2023-10-19","2310.12150v1.pdf"
"2310.12162","Iqbal H. Sarker","Iqbal H. Sarker, Helge Janicke, Nazeeruddin Mohammad, Paul Watters and
  Surya Nepal","AI Potentiality and Awareness: A Position Paper from the Perspective of
  Human-AI Teaming in Cybersecurity","10 pages, Springer","","","","cs.CR cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  This position paper explores the broad landscape of AI potentiality in the
context of cybersecurity, with a particular emphasis on its possible risk
factors with awareness, which can be managed by incorporating human experts in
the loop, i.e., ""Human-AI"" teaming. As artificial intelligence (AI)
technologies advance, they will provide unparalleled opportunities for attack
identification, incident response, and recovery. However, the successful
deployment of AI into cybersecurity measures necessitates an in-depth
understanding of its capabilities, challenges, and ethical and legal
implications to handle associated risk factors in real-world application areas.
Towards this, we emphasize the importance of a balanced approach that
incorporates AI's computational power with human expertise. AI systems may
proactively discover vulnerabilities and detect anomalies through pattern
recognition, and predictive modeling, significantly enhancing speed and
accuracy. Human experts can explain AI-generated decisions to stakeholders,
regulators, and end-users in critical situations, ensuring responsibility and
accountability, which helps establish trust in AI-driven security solutions.
Therefore, in this position paper, we argue that human-AI teaming is worthwhile
in cybersecurity, in which human expertise such as intuition, critical
thinking, or contextual understanding is combined with AI's computational power
to improve overall cyber defenses.
","2023-10-20","2310.12162v1.pdf"
"2310.12214","Meng Tong","Meng Tong and Kejiang Chen and Yuang Qi and Jie Zhang and Weiming
  Zhang and Nenghai Yu","PrivInfer: Privacy-Preserving Inference for Black-box Large Language
  Model","","","","","cs.CR","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs), such as ChatGPT, have simplified text
generation tasks, yet their inherent privacy risks are increasingly garnering
attention. Existing solutions for privacy-preserving inference face significant
challenges in practical deployment and implementation. In this paper, we
propose PrivInfer, the first practical framework for privacy-preserving
inference. It comprises two modules specifically designed for black-box LLMs in
text generation. The perturbation module, employing differential privacy,
generates perturbed prompts, thus enabling privacy-preserving inference with
black-box LLMs. The restoration module extracts coherent and meaningful
responses from obtained perturbed results, thus ensuring the accomplishment of
the text generation tasks. Additionally, to enhance privacy and utility
further, we develop RANTEXT, a novel differential privacy mechanism integrated
into the perturbation module of PrivInfer. This mechanism is specifically
tailored for LLMs and utilizes random adjacency in text perturbations.
Experimental results indicate that PrivInfer is comparable to GPT-4 in text
generation quality, and RANTEXT outperforms the current leading scheme in
privacy protection, even under its adaptive attack, our proposed GPT inference
attack.
","2023-10-25","2310.12214v1.pdf"
"2310.12236","Isidora Chara Tourni","Isidora Chara Tourni, Subhajit Naskar","Direct Neural Machine Translation with Task-level Mixture of Experts
  models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Direct neural machine translation (direct NMT) is a type of NMT system that
translates text between two non-English languages. Direct NMT systems often
face limitations due to the scarcity of parallel data between non-English
language pairs. Several approaches have been proposed to address this
limitation, such as multilingual NMT and pivot NMT (translation between two
languages via English). Task-level Mixture of expert models (Task-level MoE),
an inference-efficient variation of Transformer-based models, has shown
promising NMT performance for a large number of language pairs. In Task-level
MoE, different language groups can use different routing strategies to optimize
cross-lingual learning and inference speed. In this work, we examine Task-level
MoE's applicability in direct NMT and propose a series of high-performing
training and evaluation configurations, through which Task-level MoE-based
direct NMT systems outperform bilingual and pivot-based models for a large
number of low and high-resource direct pairs, and translation directions. Our
Task-level MoE with 16 experts outperforms bilingual NMT, Pivot NMT models for
7 language pairs, while pivot-based models still performed better in 9 pairs
and directions.
","2023-10-20","2310.12236v1.pdf"
"2310.12274","Chen Jin","Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare","An Image is Worth Multiple Words: Learning Object Level Concepts using
  Multi-Concept Prompt Learning","Project page: https://github.com/lxasqjc/MCPL","","","","cs.CV cs.AI cs.CL cs.GR cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Textural Inversion, a prompt learning method, learns a singular embedding for
a new ""word"" to represent image style and appearance, allowing it to be
integrated into natural language sentences to generate novel synthesised
images. However, identifying and integrating multiple object-level concepts
within one scene poses significant challenges even when embeddings for
individual concepts are attainable. This is further confirmed by our empirical
tests. To address this challenge, we introduce a framework for Multi-Concept
Prompt Learning (MCPL), where multiple new ""words"" are simultaneously learned
from a single sentence-image pair. To enhance the accuracy of word-concept
correlation, we propose three regularisation techniques: Attention Masking
(AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss
(PromptCL) to separate the embeddings of different concepts; and Bind adjective
(Bind adj.) to associate new ""words"" with known words. We evaluate via image
generation, editing, and attention visualisation with diverse images. Extensive
quantitative comparisons demonstrate that our method can learn more
semantically disentangled concepts with enhanced word-concept correlation.
Additionally, we introduce a novel dataset and evaluation protocol tailored for
this new task of learning object-level concepts.
","2023-10-20","2310.12274v1.pdf"
"2310.12298","Abhinav Bhatele","Siddharth Singh, Zachary Sating, Abhinav Bhatele","Jorge: Approximate Preconditioning for GPU-efficient Second-order
  Optimization","","","","","cs.LG cs.AI cs.DC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite their better convergence properties compared to first-order
optimizers, second-order optimizers for deep learning have been less popular
due to their significant computational costs. The primary efficiency bottleneck
in such optimizers is matrix inverse calculations in the preconditioning step,
which are expensive to compute on GPUs. In this paper, we introduce Jorge, a
second-order optimizer that promises the best of both worlds -- rapid
convergence benefits of second-order methods, and high computational efficiency
typical of first-order methods. We address the primary computational bottleneck
of computing matrix inverses by completely eliminating them using an
approximation of the preconditioner computation. This makes Jorge extremely
efficient on GPUs in terms of wall-clock time. Further, we describe an approach
to determine Jorge's hyperparameters directly from a well-tuned SGD baseline,
thereby significantly minimizing tuning efforts. Our empirical evaluations
demonstrate the distinct advantages of using Jorge, outperforming
state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple
deep learning models, both in terms of sample efficiency and wall-clock time.
","2023-10-20","2310.12298v1.pdf"
"2310.12300","Sheng Lu","Sheng Lu, Shan Chen, Yingya Li, Danielle Bitterman, Guergana Savova,
  and Iryna Gurevych","Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly","EMNLP 2023 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In-context learning (ICL) is a new learning paradigm that has gained
popularity along with the development of large language models. In this work,
we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usable
information (PVI), to an in-context version (in-context PVI). Compared to the
original PVI, in-context PVI is more efficient in that it requires only a few
exemplars and does not require fine-tuning. We conducted a comprehensive
empirical analysis to evaluate the reliability of in-context PVI. Our findings
indicate that in-context PVI estimates exhibit similar characteristics to the
original PVI. Specific to the in-context setting, we show that in-context PVI
estimates remain consistent across different exemplar selections and numbers of
shots. The variance of in-context PVI estimates across different exemplar
selections is insignificant, which suggests that in-context PVI are stable.
Furthermore, we demonstrate how in-context PVI can be employed to identify
challenging instances. Our work highlights the potential of in-context PVI and
provides new insights into the capabilities of ICL.
","2023-10-20","2310.12300v1.pdf"
"2310.12303","Christian Herold","Frithjof Petrick and Christian Herold and Pavel Petrushkov and Shahram
  Khadivi and Hermann Ney","Document-Level Language Models for Machine Translation","accepted at WMT 2023","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.
","2023-10-20","2310.12303v1.pdf"
"2310.12321","Katikapalli Subramanyam Kalyan","Katikapalli Subramanyam Kalyan","A Survey of GPT-3 Family Large Language Models Including ChatGPT and
  GPT-4","Preprint under review, 58 pages","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) are a special class of pretrained language
models obtained by scaling model size, pretraining corpus and computation.
LLMs, because of their large size and pretraining on large volumes of text
data, exhibit special abilities which allow them to achieve remarkable
performances without any task-specific training in many of the natural language
processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the
popularity of LLMs is increasing exponentially after the introduction of models
like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models,
including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With
the ever-rising popularity of GLLMs, especially in the research community,
there is a strong need for a comprehensive survey which summarizes the recent
research progress in multiple dimensions and can guide the research community
with insightful future research directions. We start the survey paper with
foundation concepts like transformers, transfer learning, self-supervised
learning, pretrained language models and large language models. We then present
a brief overview of GLLMs and discuss the performances of GLLMs in various
downstream tasks, specific domains and multiple languages. We also discuss the
data labelling and data augmentation abilities of GLLMs, the robustness of
GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with
multiple insightful future research directions. To summarize, this
comprehensive survey paper will serve as a good resource for both academic and
industry people to stay updated with the latest research related to GPT-3
family large language models.
","2023-10-20","2310.12321v1.pdf"
"2310.12342","Yongqi Tong","Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang, Zi Lin, Simeng Han,
  Jingbo Shang","Eliminating Reasoning via Inferring with Planning: A New Framework to
  Guide LLMs' Non-linear Thinking","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Chain-of-Thought(CoT) prompting and its variants explore equipping large
language models (LLMs) with high-level reasoning abilities by emulating
human-like linear cognition and logic. However, the human mind is complicated
and mixed with both linear and nonlinear thinking. In this work, we propose
\textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel
prompting that combines the principles of elimination and inference in order to
guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize
Natural Language Inference (NLI) to deduce each possible solution's entailment
relation with context, commonsense, or facts, therefore yielding a broader
perspective by thinking back for inferring. This forward planning and backward
eliminating process allows IEP to better simulate the complex human thinking
processes compared to other CoT-based methods, which only reflect linear
cognitive processes. We conducted a series of empirical studies and have
corroborated that IEP consistently outperforms CoT across various tasks.
Additionally, we observe that integrating IEP and CoT further improves the
LLMs' performance on certain tasks, highlighting the necessity of equipping
LLMs with mixed logic processes. Moreover, to better evaluate comprehensive
features inherent in human logic, we introduce \textbf{M}ental-\textbf{A}bility
\textbf{R}easoning \textbf{B}enchmark (MARB). The benchmark comprises six novel
subtasks with a total of 9,115 questions, among which 1,685 are developed with
hand-crafted rationale references. We believe both \textsc{IEP} and
\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and
verbal reasoning abilities and drive further advancements. \textsc{MARB} will
be available at ~\texttt{anonymity link} soon.
","2023-10-20","2310.12342v1.pdf"
"2310.12344","Cheng-Fu Yang","Cheng-Fu Yang, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan,
  Yu-Chiang Frank Wang, Kai-Wei Chang","LACMA: Language-Aligning Contrastive Learning with Meta-Actions for
  Embodied Instruction Following","EMNLP 2023","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  End-to-end Transformers have demonstrated an impressive success rate for
Embodied Instruction Following when the environment has been seen in training.
However, they tend to struggle when deployed in an unseen environment. This
lack of generalizability is due to the agent's insensitivity to subtle changes
in natural language instructions. To mitigate this issue, we propose explicitly
aligning the agent's hidden states with the instructions via contrastive
learning. Nevertheless, the semantic gap between high-level language
instructions and the agent's low-level action space remains an obstacle.
Therefore, we further introduce a novel concept of meta-actions to bridge the
gap. Meta-actions are ubiquitous action patterns that can be parsed from the
original action sequence. These patterns represent higher-level semantics that
are intuitively aligned closer to the instructions. When meta-actions are
applied as additional training signals, the agent generalizes better to unseen
environments. Compared to a strong multi-modal Transformer baseline, we achieve
a significant 4.5% absolute gain in success rate in unseen environments of
ALFRED Embodied Instruction Following. Additional analysis shows that the
contrastive objective and meta-actions are complementary in achieving the best
results, and the resulting agent better aligns its states with corresponding
instructions, making it more suitable for real-world embodied agents. The code
is available at: https://github.com/joeyy5588/LACMA.
","2023-10-20","2310.12344v1.pdf"
"2310.12357","Chongzhou Fang","Chongzhou Fang, Ning Miao, Shaurya Srivastav, Jialin Liu, Ruoyu Zhang,
  Ruijie Fang, Asmita Asmita, Ryan Tsang, Najmeh Nazari, Han Wang and Houman
  Homayoun","Large Language Models for Code Analysis: Do LLMs Really Do Their Job?","","","","","cs.SE cs.CR","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have demonstrated significant potential in the
realm of natural language understanding and programming code processing tasks.
Their capacity to comprehend and generate human-like code has spurred research
into harnessing LLMs for code analysis purposes. However, the existing body of
literature falls short in delivering a systematic evaluation and assessment of
LLMs' effectiveness in code analysis, particularly in the context of obfuscated
code.
  This paper seeks to bridge this gap by offering a comprehensive evaluation of
LLMs' capabilities in performing code analysis tasks. Additionally, it presents
real-world case studies that employ LLMs for the analysis of malicious code.
Our findings indicate that LLMs can indeed serve as valuable tools for
automating code analysis, albeit with certain limitations. Through meticulous
exploration, this research contributes to a deeper understanding of the
potential and constraints associated with utilizing LLMs in code analysis,
paving the way for enhanced applications in this critical domain.
","2023-10-20","2310.12357v1.pdf"
"2310.12362","Ruisi Zhang","Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, Farinaz
  Koushanfar","REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative
  Large Language Models","","","","","cs.CR cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present REMARK-LLM, a novel efficient, and robust watermarking framework
designed for texts generated by large language models (LLMs). Synthesizing
human-like content using LLMs necessitates vast computational resources and
extensive datasets, encapsulating critical intellectual property (IP). However,
the generated content is prone to malicious exploitation, including spamming
and plagiarism. To address the challenges, REMARK-LLM proposes three new
components: (i) a learning-based message encoding module to infuse binary
signatures into LLM-generated texts; (ii) a reparameterization module to
transform the dense distributions from the message encoding to the sparse
distribution of the watermarked textual tokens; (iii) a decoding module
dedicated for signature extraction; Furthermore, we introduce an optimized beam
search algorithm to guarantee the coherence and consistency of the generated
content. REMARK-LLM is rigorously trained to encourage the preservation of
semantic integrity in watermarked content, while ensuring effective watermark
retrieval. Extensive evaluations on multiple unseen datasets highlight
REMARK-LLM proficiency and transferability in inserting 2 times more signature
bits into the same texts when compared to prior art, all while maintaining
semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against
a spectrum of watermark detection and removal attacks.
","2023-10-20","2310.12362v1.pdf"
"2310.12378","Taejin Park","Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C. Puvvada,
  Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam and Boris
  Ginsburg","The CHiME-7 Challenge: System Description and Performance of NeMo Team's
  DASR System","","CHiME-7 Workshop 2023","","","eess.AS cs.SD","http://creativecommons.org/licenses/by/4.0/","  We present the NVIDIA NeMo team's multi-channel speech recognition system for
the 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task,
focusing on the development of a multi-channel, multi-speaker speech
recognition system tailored to transcribe speech from distributed microphones
and microphone arrays. The system predominantly comprises of the following
integral modules: the Speaker Diarization Module, Multi-channel Audio Front-End
Processing Module, and the ASR Module. These components collectively establish
a cascading system, meticulously processing multi-channel and multi-speaker
audio input. Moreover, this paper highlights the comprehensive optimization
process that significantly enhanced our system's performance. Our team's
submission is largely based on NeMo toolkits and will be publicly available.
","2023-10-20","2310.12378v1.pdf"
"2310.12379","Nitesh Kumar","Nitesh Kumar and Steven Schockaert","Solving Hard Analogy Questions with Relation Embedding Chains","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Modelling how concepts are related is a central topic in Lexical Semantics. A
common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to
model the relation between two concepts as a set of paths. However, KGs are
limited to a fixed set of relation types, and they are incomplete and often
noisy. Another strategy is to distill relation embeddings from a fine-tuned
language model. However, this is less suitable for words that are only
indirectly related and it does not readily allow us to incorporate structured
domain knowledge. In this paper, we aim to combine the best of both worlds. We
model relations as paths but associate their edges with relation embeddings.
The paths are obtained by first identifying suitable intermediate words and
then selecting those words for which informative relation embeddings can be
obtained. We empirically show that our proposed representations are useful for
solving hard analogy questions.
","2023-10-20","2310.12379v1.pdf"
"2310.12397","Kaya Stechly","Kaya Stechly, Matthew Marquez, Subbarao Kambhampati","GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for
  Reasoning Problems","18 pages, 3 figures","","","","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples, a wide spread belief in their iterative self-critique
capabilities persists. In this paper, we set out to systematically investigate
the effectiveness of iterative prompting of LLMs in the context of Graph
Coloring, a canonical NP-complete reasoning problem that is related to
propositional satisfiability as well as practical problems like scheduling and
allocation. We present a principled empirical study of the performance of GPT4
in solving graph coloring instances or verifying the correctness of candidate
colorings. In iterative modes, we experiment with the model critiquing its own
answers and an external correct reasoner verifying proposed solutions. In both
cases, we analyze whether the content of the criticisms actually affects bottom
line performance. The study seems to indicate that (i) LLMs are bad at solving
graph coloring instances (ii) they are no better at verifying a solution--and
thus are not effective in iterative modes with LLMs critiquing LLM-generated
solutions (iii) the correctness and content of the criticisms--whether by LLMs
or external solvers--seems largely irrelevant to the performance of iterative
prompting. We show that the observed increase in effectiveness is largely due
to the correct solution being fortuitously present in the top-k completions of
the prompt (and being recognized as such by an external verifier). Our results
thus call into question claims about the self-critiquing capabilities of state
of the art LLMs.
","2023-10-20","2310.12397v1.pdf"
"2310.12406","Yi Yang Dr","Yixuan Tang, Yi Yang, Allen H Huang, Andy Tam, Justin Z Tang","FinEntity: Entity-level Sentiment Classification for Financial Texts","EMNLP'23 Main Conference Short Paper","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In the financial domain, conducting entity-level sentiment analysis is
crucial for accurately assessing the sentiment directed toward a specific
financial entity. To our knowledge, no publicly available dataset currently
exists for this purpose. In this work, we introduce an entity-level sentiment
classification dataset, called \textbf{FinEntity}, that annotates financial
entity spans and their sentiment (positive, neutral, and negative) in financial
news. We document the dataset construction process in the paper. Additionally,
we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on
entity-level sentiment classification. In a case study, we demonstrate the
practical utility of using FinEntity in monitoring cryptocurrency markets. The
data and code of FinEntity is available at
\url{https://github.com/yixuantt/FinEntity}
","2023-10-20","2310.12406v1.pdf"
"2310.12418","Siru Ouyang","Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan
  Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, Jiawei Han","The Shifted and The Overlooked: A Task-oriented Investigation of
  User-GPT Interactions","EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent progress in Large Language Models (LLMs) has produced models that
exhibit remarkable performance across a variety of NLP tasks. However, it
remains unclear whether the existing focus of NLP research accurately captures
the genuine requirements of human users. This paper provides a comprehensive
analysis of the divergence between current NLP research and the needs of
real-world NLP applications via a large-scale collection of user-GPT
conversations. We analyze a large-scale collection of real user queries to GPT.
We compare these queries against existing NLP benchmark tasks and identify a
significant gap between the tasks that users frequently request from LLMs and
the tasks that are commonly studied in academic research. For example, we find
that tasks such as ``design'' and ``planning'' are prevalent in user
interactions but are largely neglected or different from traditional NLP
benchmarks. We investigate these overlooked tasks, dissect the practical
challenges they pose, and provide insights toward a roadmap to make LLMs better
aligned with user needs.
","2023-10-20","2310.12418v1.pdf"
"2310.12425","Md Rashedul Hasan","Md Rashedul Hasan, Jiawei Li, Iftekhar Ahmed, Hamid Bagheri","Automated Repair of Declarative Software Specifications in the Era of
  Large Language Models","13 Pages with reference, 4 Tables, 2 Figures, 2 Listings","","","","cs.SE cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The growing adoption of declarative software specification languages, coupled
with their inherent difficulty in debugging, has underscored the need for
effective and automated repair techniques applicable to such languages.
Researchers have recently explored various methods to automatically repair
declarative software specifications, such as template-based repair,
feedback-driven iterative repair, and bounded exhaustive approaches. The latest
developments in large language models provide new opportunities for the
automatic repair of declarative specifications. In this study, we assess the
effectiveness of utilizing OpenAI's ChatGPT to repair software specifications
written in the Alloy declarative language. Unlike imperative languages,
specifications in Alloy are not executed but rather translated into logical
formulas and evaluated using backend constraint solvers to identify
specification instances and counterexamples to assertions. Our evaluation
focuses on ChatGPT's ability to improve the correctness and completeness of
Alloy declarative specifications through automatic repairs. We analyze the
results produced by ChatGPT and compare them with those of leading automatic
Alloy repair methods. Our study revealed that while ChatGPT falls short in
comparison to existing techniques, it was able to successfully repair bugs that
no other technique could address. Our analysis also identified errors in
ChatGPT's generated repairs, including improper operator usage, type errors,
higher-order logic misuse, and relational arity mismatches. Additionally, we
observed instances of hallucinations in ChatGPT-generated repairs and
inconsistency in its results. Our study provides valuable insights for software
practitioners, researchers, and tool builders considering ChatGPT for
declarative specification repairs.
","2023-10-20","2310.12425v1.pdf"
"2310.12426","Deepak Nathani","Deepak Nathani, David Wang, Liangming Pan, William Yang Wang","MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language
  Models","Accepted at EMNLP 2023 Main Conference, Camera Ready","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Language Models (LMs) have shown impressive performance in various natural
language tasks. However, when it comes to natural language reasoning, LMs still
face challenges such as hallucination, generating incorrect intermediate
reasoning steps, and making mathematical errors. Recent research has focused on
enhancing LMs through self-improvement using feedback. Nevertheless, existing
approaches relying on a single generic feedback source fail to address the
diverse error types found in LM-generated reasoning chains. In this work, we
propose Multi-Aspect Feedback, an iterative refinement framework that
integrates multiple feedback modules, including frozen LMs and external tools,
each focusing on a specific error category. Our experimental results
demonstrate the efficacy of our approach to addressing several errors in the
LM-generated reasoning chain and thus improving the overall performance of an
LM in several reasoning tasks. We see a relative improvement of up to 20% in
Mathematical Reasoning and up to 18% in Logical Entailment.
","2023-10-20","2310.12426v1.pdf"
"2310.12439","Hongwei Yao","Hongwei Yao, Jian Lou and Zhan Qin","PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models","Code will be released on: https://github.com/grasses/PoisonPrompt","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Prompts have significantly improved the performance of pretrained Large
Language Models (LLMs) on various downstream tasks recently, making them
increasingly indispensable for a diverse range of LLM application scenarios.
However, the backdoor vulnerability, a serious security threat that can
maliciously alter the victim model's normal predictions, has not been
sufficiently explored for prompt-based LLMs. In this paper, we present
POISONPROMPT, a novel backdoor attack capable of successfully compromising both
hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and
robustness of POISONPROMPT through extensive experiments on three popular
prompt methods, using six datasets and three widely used LLMs. Our findings
highlight the potential security threats posed by backdoor attacks on
prompt-based LLMs and emphasize the need for further research in this area.
","2023-10-20","2310.12439v1.pdf"
"2310.12459","Javier Hernandez","Javier Hernandez, Jina Suh, Judith Amores, Kael Rowan, Gonzalo Ramos,
  and Mary Czerwinski","Affective Conversational Agents: Understanding Expectations and Personal
  Influences","","","","","cs.HC cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The rise of AI conversational agents has broadened opportunities to enhance
human capabilities across various domains. As these agents become more
prevalent, it is crucial to investigate the impact of different affective
abilities on their performance and user experience. In this study, we surveyed
745 respondents to understand the expectations and preferences regarding
affective skills in various applications. Specifically, we assessed preferences
concerning AI agents that can perceive, respond to, and simulate emotions
across 32 distinct scenarios. Our results indicate a preference for scenarios
that involve human interaction, emotional support, and creative tasks, with
influences from factors such as emotional reappraisal and personality traits.
Overall, the desired affective skills in AI agents depend largely on the
application's context and nature, emphasizing the need for adaptability and
context-awareness in the design of affective AI conversational agents.
","2023-10-20","2310.12459v1.pdf"
"2310.12462","Yichuan Deng","Yichuan Deng, Zhao Song, Shenghao Xie, Chiwun Yang","Unmasking Transformers: A Theoretical Approach to Data Recovery via
  Attention Weights","","","","","cs.LG cs.CL stat.ML","http://creativecommons.org/licenses/by-nc-sa/4.0/","  In the realm of deep learning, transformers have emerged as a dominant
architecture, particularly in natural language processing tasks. However, with
their widespread adoption, concerns regarding the security and privacy of the
data processed by these models have arisen. In this paper, we address a pivotal
question: Can the data fed into transformers be recovered using their attention
weights and outputs? We introduce a theoretical framework to tackle this
problem. Specifically, we present an algorithm that aims to recover the input
data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top
\in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by
minimizing the loss function $L(X)$. This loss function captures the
discrepancy between the expected output and the actual output of the
transformer. Our findings have significant implications for the Localized
Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's
design from a security and privacy perspective. This work underscores the
importance of understanding and safeguarding the internal workings of
transformers to ensure the confidentiality of processed data.
","2023-10-20","2310.12462v1.pdf"
"2310.12467","Etsuko Ishii","Etsuko Ishii, Yan Xu, Bryan Wilie, Ziwei Ji, Holy Lovenia, Willy
  Chung, Pascale Fung","Contrastive Learning for Inference in Dialogue","Accepted to EMNLP2023","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Inference, especially those derived from inductive processes, is a crucial
component in our conversation to complement the information implicitly or
explicitly conveyed by a speaker. While recent large language models show
remarkable advances in inference tasks, their performance in inductive
reasoning, where not all information is present in the context, is far behind
deductive reasoning. In this paper, we analyze the behavior of the models based
on the task difficulty defined by the semantic information gap -- which
distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).
Our analysis reveals that the disparity in information between dialogue
contexts and desired inferences poses a significant challenge to the inductive
inference process. To mitigate this information gap, we investigate a
contrastive learning approach by feeding negative samples. Our experiments
suggest negative samples help models understand what is wrong and improve their
inference generations.
","2023-10-20","2310.12467v1.pdf"
"2310.12477","Kai-Wei Chang","Ming-Hao Hsu, Kai-Wei Chang, Shang-Wen Li, Hung-yi Lee","An Exploration of In-Context Learning for Speech Language Model","The first two authors contributed equally","","","","eess.AS cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ever since the development of GPT-3 in the natural language processing (NLP)
field, in-context learning (ICL) has played an important role in utilizing
large language models (LLMs). By presenting the LM utterance-label
demonstrations at the input, the LM can accomplish few-shot learning without
relying on gradient descent or requiring explicit modification of its
parameters. This enables the LM to learn and adapt in a black-box manner.
Despite the success of ICL in NLP, little work is exploring the possibility of
ICL in speech processing. This study proposes the first exploration of ICL with
a speech LM without text supervision. We first show that the current speech LM
does not have the ICL capability. With the proposed warmup training, the speech
LM can, therefore, perform ICL on unseen tasks. In this work, we verify the
feasibility of ICL for speech LM on speech classification tasks.
","2023-10-20","2310.12477v1.pdf"
"2310.12481","Wenxuan Wang","Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang,
  Zhaopeng Tu, Michael R. Lyu","Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in
  Large Language Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we identify a cultural dominance issue within large language
models (LLMs) due to the predominant use of English data in model training
(e.g. ChatGPT). LLMs often provide inappropriate English-culture-related
answers that are not relevant to the expected culture when users ask in
non-English languages. To systematically evaluate the cultural dominance issue,
we build a benchmark that consists of both concrete (e.g. holidays and songs)
and abstract (e.g. values and opinions) cultural objects. Empirical results
show that the representative GPT models suffer from the culture dominance
problem, where GPT-4 is the most affected while text-davinci-003 suffers the
least from this problem. Our study emphasizes the need for critical examination
of cultural dominance and ethical consideration in their development and
deployment. We show two straightforward methods in model development (i.e.
pretraining on more diverse data) and deployment (e.g. culture-aware prompting)
can significantly mitigate the cultural dominance issue in LLMs.
","2023-10-20","2310.12481v1.pdf"
"2310.12489","Olumide Ebenezer Ojo","Olumide E. Ojo, Olaronke O. Adebanji, Alexander Gelbukh, Hiram Calvo,
  Anna Feldman","MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI
  Responses in Health Consultations","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Zero-shot classification enables text to be classified into classes not seen
during training. In this research, we investigate the effectiveness of
pre-trained language models to accurately classify responses from Doctors and
AI in health consultations through zero-shot learning. Our study aims to
determine whether these models can effectively detect if a text originates from
human or AI models without specific corpus training. We collect responses from
doctors to patient inquiries about their health and pose the same
question/response to AI models. While zero-shot language models show a good
understanding of language in general, they have limitations in classifying
doctor and AI responses in healthcare consultations. This research lays the
groundwork for further research into this field of medical text classification,
informing the development of more effective approaches to accurately classify
doctor-generated and AI-generated text in health consultations.
","2023-10-23","2310.12489v1.pdf"
"2310.12505","Boyi Deng","Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He","Attack Prompt Generation for Red Teaming and Defending Large Language
  Models","Accepted to EMNLP 2023 (Findings)","","","","cs.CL cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) are susceptible to red teaming attacks, which
can induce LLMs to generate harmful content. Previous research constructs
attack prompts via manual or automatic methods, which have their own
limitations on construction cost and quality. To address these issues, we
propose an integrated approach that combines manual and automatic methods to
economically generate high-quality attack prompts. Specifically, considering
the impressive capabilities of newly emerged LLMs, we propose an attack
framework to instruct LLMs to mimic human-generated prompts through in-context
learning. Furthermore, we propose a defense framework that fine-tunes victim
LLMs through iterative interactions with the attack framework to enhance their
safety against red teaming attacks. Extensive experiments on different LLMs
validate the effectiveness of our proposed attack and defense frameworks.
Additionally, we release a series of attack prompts datasets named SAP with
varying sizes, facilitating the safety evaluation and enhancement of more LLMs.
Our code and dataset is available on https://github.com/Aatrox103/SAP .
","2023-10-20","2310.12505v1.pdf"
"2310.12508","Chongyu Fan","Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, Sijia
  Liu","SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency
  in Both Image Classification and Generation","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With evolving data regulations, machine unlearning (MU) has become an
important tool for fostering trust and safety in today's AI models. However,
existing MU methods focusing on data and/or weight perspectives often grapple
with limitations in unlearning accuracy, stability, and cross-domain
applicability. To address these challenges, we introduce the concept of 'weight
saliency' in MU, drawing parallels with input saliency in model explanation.
This innovation directs MU's attention toward specific model weights rather
than the entire model, improving effectiveness and efficiency. The resultant
method that we call saliency unlearning (SalUn) narrows the performance gap
with 'exact' unlearning (model retraining from scratch after removing the
forgetting dataset). To the best of our knowledge, SalUn is the first
principled MU approach adaptable enough to effectively erase the influence of
forgetting data, classes, or concepts in both image classification and
generation. For example, SalUn yields a stability advantage in high-variance
random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on
the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from
generating harmful images, SalUn achieves nearly 100% unlearning accuracy,
outperforming current state-of-the-art baselines like Erased Stable Diffusion
and Forget-Me-Not.
","2023-10-20","2310.12508v1.pdf"
"2310.12516","Xiaodong Yu","Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao","Automatic Hallucination Assessment for Aligned Large Language Models via
  Transferable Adversarial Attacks","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Although remarkable progress has been achieved in preventing large language
model (LLM) hallucinations using instruction tuning and retrieval augmentation,
it remains challenging to measure the reliability of LLMs using human-crafted
evaluation data which is not available for many tasks and domains and could
suffer from data leakage. Inspired by adversarial machine learning, this paper
aims to develop a method of automatically generating evaluation data by
appropriately modifying existing data on which LLMs behave faithfully.
Specifically, this paper presents AutoDebug, an LLM-based framework to use
prompting chaining to generate transferable adversarial attacks in the form of
question-answering examples. We seek to understand the extent to which these
examples trigger the hallucination behaviors of LLMs.
  We implement AutoDebug using ChatGPT and evaluate the resulting two variants
of a popular open-domain question-answering dataset, Natural Questions (NQ), on
a collection of open-source and proprietary LLMs under various prompting
settings. Our generated evaluation data is human-readable and, as we show,
humans can answer these modified questions well. Nevertheless, we observe
pronounced accuracy drops across multiple LLMs including GPT-4. Our
experimental results show that LLMs are likely to hallucinate in two categories
of question-answering scenarios where (1) there are conflicts between knowledge
given in the prompt and their parametric knowledge, or (2) the knowledge
expressed in the prompt is complex. Finally, we find that the adversarial
examples generated by our method are transferable across all considered LLMs.
The examples generated by a small model can be used to debug a much larger
model, making our approach cost-effective.
","2023-10-20","2310.12516v1.pdf"
"2310.12520","Xiang Zhang","Xiang Zhang, Senyu Li, Zijun Wu, Ning Shi","Lost in Translation: When GPT-4V(ision) Can't See Eye to Eye with Text.
  A Vision-Language-Consistency Analysis of VLLMs and Beyond","","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in multimodal techniques open exciting possibilities for
models excelling in diverse tasks involving text, audio, and image processing.
Models like GPT-4V, blending computer vision and language modeling, excel in
complex text and image tasks. Numerous prior research endeavors have diligently
examined the performance of these Vision Large Language Models (VLLMs) across
tasks like object detection, image captioning and others. However, these
analyses often focus on evaluating the performance of each modality in
isolation, lacking insights into their cross-modal interactions. Specifically,
questions concerning whether these vision-language models execute vision and
language tasks consistently or independently have remained unanswered. In this
study, we draw inspiration from recent investigations into multilingualism and
conduct a comprehensive analysis of model's cross-modal interactions. We
introduce a systematic framework that quantifies the capability disparities
between different modalities in the multi-modal setting and provide a set of
datasets designed for these evaluations. Our findings reveal that models like
GPT-4V tend to perform consistently modalities when the tasks are relatively
simple. However, the trustworthiness of results derived from the vision
modality diminishes as the tasks become more challenging. Expanding on our
findings, we introduce ""Vision Description Prompting,"" a method that
effectively improves performance in challenging vision-related tasks.
","2023-10-20","2310.12520v1.pdf"
"2310.12523","Imdad Ullah","Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq
  Ahamed Ahanger, Zawar Shah, Junaid Qadir, and Salil S. Kanhere","Privacy Preserving Large Language Models: ChatGPT Case Study Based
  Vision and Framework","","","","","cs.CR","http://creativecommons.org/licenses/by/4.0/","  The generative Artificial Intelligence (AI) tools based on Large Language
Models (LLMs) use billions of parameters to extensively analyse large datasets
and extract critical private information such as, context, specific details,
identifying information etc. This have raised serious threats to user privacy
and reluctance to use such tools. This article proposes the conceptual model
called PrivChatGPT, a privacy-preserving model for LLMs that consists of two
main components i.e., preserving user privacy during the data
curation/pre-processing together with preserving private context and the
private training process for large-scale data. To demonstrate its
applicability, we show how a private mechanism could be integrated into the
existing model for training LLMs to protect user privacy; specifically, we
employed differential privacy and private training using Reinforcement Learning
(RL). We measure the privacy loss and evaluate the measure of uncertainty or
randomness once differential privacy is applied. It further recursively
evaluates the level of privacy guarantees and the measure of uncertainty of
public database and resources, during each update when new information is added
for training purposes. To critically evaluate the use of differential privacy
for private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,
private information retrieval, randomisation, for various performance measures
such as the model performance and accuracy, computational complexity, privacy
vs. utility etc. We conclude that differential privacy, randomisation, and
obfuscation can impact utility and performance of trained models, conversely,
the use of ToR, Blockchain, and PIR may introduce additional computational
complexity and high training latency. We believe that the proposed model could
be used as a benchmark for proposing privacy preserving LLMs for generative AI
tools.
","2023-10-20","2310.12523v1.pdf"
"2310.12547","Junghyun Kim","Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung,
  Byoung-Tak Zhang","PGA: Personalizing Grasping Agents with Single Human-Robot Interaction","7 pages, under review","","","","cs.RO cs.CV cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that
ground and grasp objects based on natural language instructions. While robots
capable of recognizing personal objects like ""my wallet"" can interact more
naturally with non-expert users, current LCRG systems primarily limit robots to
understanding only generic expressions. To this end, we introduce a task
scenario GraspMine with a novel dataset that aims to locate and grasp personal
objects given personal indicators via learning from a single human-robot
interaction. To address GraspMine, we propose Personalized Grasping Agent
(PGA), that learns personal objects by propagating user-given information
through a Reminiscence-a collection of raw images from the user's environment.
Specifically, PGA acquires personal object information by a user presenting a
personal object with its associated indicator, followed by PGA inspecting the
object by rotating it. Based on the acquired information, PGA pseudo-labels
objects in the Reminiscence by our proposed label propagation algorithm.
Harnessing the information acquired from the interactions and the
pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding
model to grasp personal objects. Experiments on GraspMine show that PGA
significantly outperforms baseline methods both in offline and online settings,
signifying its effectiveness and personalization applicability on real-world
scenarios. Finally, qualitative analysis shows the effectiveness of PGA through
a detailed investigation of results in each phase.
","2023-10-20","2310.12547v1.pdf"
"2310.12560","Ruizhe Chen","Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu,
  Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu","Fast Model Debias with Machine Unlearning","accepted by NIPS 2023","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recent discoveries have revealed that deep neural networks might behave in a
biased manner in many real-world scenarios. For instance, deep networks trained
on a large-scale face recognition dataset CelebA tend to predict blonde hair
for females and black hair for males. Such biases not only jeopardize the
robustness of models but also perpetuate and amplify social biases, which is
especially concerning for automated decision-making processes in healthcare,
recruitment, etc., as they could exacerbate unfair economic and social
inequalities among different groups. Existing debiasing methods suffer from
high costs in bias labeling or model re-training, while also exhibiting a
deficiency in terms of elucidating the origins of biases within the model. To
this respect, we propose a fast model debiasing framework (FMD) which offers an
efficient approach to identify, evaluate and remove biases inherent in trained
models. The FMD identifies biased attributes through an explicit counterfactual
concept and quantifies the influence of data samples with influence functions.
Moreover, we design a machine unlearning-based strategy to efficiently and
effectively remove the bias in a trained model with a small counterfactual
dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets
along with experiments with large language models demonstrate that our method
achieves superior or competing accuracies compared with state-of-the-art
methods while attaining significantly fewer biases and requiring much less
debiasing cost. Notably, our method requires only a small external dataset and
updating a minimal amount of model parameters, without the requirement of
access to training data that may be too large or unavailable in practice.
","2023-10-20","2310.12560v1.pdf"
"2310.12611","Oskar van der Wal MSc","Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna and
  Oskar van der Wal","Identifying and Adapting Transformer-Components Responsible for Gender
  Bias in an English Language Model","Accepted at BlackboxNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language models (LMs) exhibit and amplify many types of undesirable biases
learned from the training data, including gender bias. However, we lack tools
for effectively and efficiently changing this behavior without hurting general
language modeling performance. In this paper, we study three methods for
identifying causal relations between LM components and particular output:
causal mediation analysis, automated circuit discovery and our novel, efficient
method called DiffMask+ based on differential masking. We apply the methods to
GPT-2 small and the problem of gender bias, and use the discovered sets of
components to perform parameter-efficient fine-tuning for bias mitigation. Our
results show significant overlap in the identified components (despite huge
differences in the computational requirements of the methods) as well as
success in mitigating gender bias, with less damage to general language
modeling compared to full model fine-tuning. However, our work also underscores
the difficulty of defining and measuring bias, and the sensitivity of causal
discovery procedures to dataset choice. We hope our work can contribute to more
attention for dataset development, and lead to more effective mitigation
strategies for other types of bias.
","2023-10-20","2310.12611v1.pdf"
"2310.12620","Yue Guo","Yue Guo, Chenxi Hu, Yi Yang","Predict the Future from the Past? On the Temporal Data Distribution
  Shift in Financial Sentiment Classifications","EMNLP 2023 main conference","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Temporal data distribution shift is prevalent in the financial text. How can
a financial sentiment analysis system be trained in a volatile market
environment that can accurately infer sentiment and be robust to temporal data
distribution shifts? In this paper, we conduct an empirical study on the
financial sentiment analysis system under temporal data distribution shifts
using a real-world financial social media dataset that spans three years. We
find that the fine-tuned models suffer from general performance degradation in
the presence of temporal distribution shifts. Furthermore, motivated by the
unique temporal nature of the financial text, we propose a novel method that
combines out-of-distribution detection with time series modeling for temporal
financial sentiment analysis. Experimental results show that the proposed
method enhances the model's capability to adapt to evolving temporal shifts in
a volatile financial market.
","2023-10-20","2310.12620v1.pdf"
"2310.12638","Hanna Abi Akl","Hanna Abi Akl","PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph
  Question-Answering Grounding","10 pages, 3 figures, 2 tables, accepted for the Scholarly-QALD
  challenge at the International Semantic Web Conference (ISWC) 2023","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The Scholarly Question Answering over Linked Data (Scholarly QALD) at The
International Semantic Web Conference (ISWC) 2023 challenge presents two
sub-tasks to tackle question answering (QA) over knowledge graphs (KGs). We
answer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS)
framework based on PSYCHIC, an extractive QA model capable of identifying the
query and entities related to a KG question. Our system achieved a F1 score of
00.18% on question answering and came in third place for entity linking (EL)
with a score of 71.00%.
","2023-10-20","2310.12638v1.pdf"
"2310.12640","Yi Bin","Yi Bin, Wenhao Shi, Bin Ji, Jipeng Zhang, Yujuan Ding, Yang Yang","Non-Autoregressive Sentence Ordering","Accepted at Findings of EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Existing sentence ordering approaches generally employ encoder-decoder
frameworks with the pointer net to recover the coherence by recurrently
predicting each sentence step-by-step. Such an autoregressive manner only
leverages unilateral dependencies during decoding and cannot fully explore the
semantic dependency between sentences for ordering. To overcome these
limitations, in this paper, we propose a novel Non-Autoregressive Ordering
Network, dubbed \textit{NAON}, which explores bilateral dependencies between
sentences and predicts the sentence for each position in parallel. We claim
that the non-autoregressive manner is not just applicable but also particularly
suitable to the sentence ordering task because of two peculiar characteristics
of the task: 1) each generation target is in deterministic length, and 2) the
sentences and positions should match exclusively. Furthermore, to address the
repetition issue of the naive non-autoregressive Transformer, we introduce an
exclusive loss to constrain the exclusiveness between positions and sentences.
To verify the effectiveness of the proposed model, we conduct extensive
experiments on several common-used datasets and the experimental results show
that our method outperforms all the autoregressive approaches and yields
competitive performance compared with the state-of-the-arts. The codes are
available at:
\url{https://github.com/steven640pixel/nonautoregressive-sentence-ordering}.
","2023-10-20","2310.12640v1.pdf"
"2310.12664","Yue Guo","Yue Guo, Zian Xu, Yi Yang","Is ChatGPT a Financial Expert? Evaluating Language Models on Financial
  Natural Language Processing","Findings of EMNLP 2023 (short paper)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The emergence of Large Language Models (LLMs), such as ChatGPT, has
revolutionized general natural language preprocessing (NLP) tasks. However,
their expertise in the financial domain lacks a comprehensive evaluation. To
assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval,
a framework for Financial Language Model Evaluation, comprising nine datasets
designed to evaluate the performance of language models. This study compares
the performance of encoder-only language models and the decoder-only language
models. Our findings reveal that while some decoder-only LLMs demonstrate
notable performance across most financial tasks via zero-shot prompting, they
generally lag behind the fine-tuned expert models, especially when dealing with
proprietary datasets. We hope this study provides foundation evaluations for
continuing efforts to build more advanced LLMs in the financial domain.
","2023-10-20","2310.12664v1.pdf"
"2310.12670","Yuxin Wang","Yuxin Wang, Shaohuai Shi, Xin He, Zhenheng Tang, Xinglin Pan, Yang
  Zheng, Xiaoyu Wu, Amelie Chi Zhou, Bingsheng He, Xiaowen Chu","Reliable and Efficient In-Memory Fault Tolerance of Large Language Model
  Pretraining","Fault Tolerance, Checkpoint Optimization, Large Language Model, 3D
  parallelism","","","","cs.DC cs.PF","http://creativecommons.org/licenses/by/4.0/","  Extensive system scales (i.e. thousands of GPU/TPUs) and prolonged training
periods (i.e. months of pretraining) significantly escalate the probability of
failures when training large language models (LLMs). Thus, efficient and
reliable fault-tolerance methods are in urgent need. Checkpointing is the
primary fault-tolerance method to periodically save parameter snapshots from
GPU memory to disks via CPU memory. In this paper, we identify the frequency of
existing checkpoint-based fault-tolerance being significantly limited by the
storage I/O overheads, which results in hefty re-training costs on restarting
from the nearest checkpoint. In response to this gap, we introduce an in-memory
fault-tolerance framework for large-scale LLM pretraining. The framework boosts
the efficiency and reliability of fault tolerance from three aspects: (1)
Reduced Data Transfer and I/O: By asynchronously caching parameters, i.e.,
sharded model parameters, optimizer states, and RNG states, to CPU volatile
memory, Our framework significantly reduces communication costs and bypasses
checkpoint I/O. (2) Enhanced System Reliability: Our framework enhances
parameter protection with a two-layer hierarchy: snapshot management processes
(SMPs) safeguard against software failures, together with Erasure Coding (EC)
protecting against node failures. This double-layered protection greatly
improves the survival probability of the parameters compared to existing
checkpointing methods. (3) Improved Snapshotting Frequency: Our framework
achieves more frequent snapshotting compared with asynchronous checkpointing
optimizations under the same saving time budget, which improves the fault
tolerance efficiency. Empirical results demonstrate that Our framework
minimizes the overhead of fault tolerance of LLM pretraining by effectively
leveraging redundant CPU resources.
","2023-10-20","2310.12670v1.pdf"
"2310.12680","Puneesh Deora","Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, Christos Thrampoulidis","On the Optimization and Generalization of Multi-head Attention","48 page; presented in the Workshop on High-dimensional Learning
  Dynamics, ICML 2023","","","","cs.LG math.OC stat.ML","http://creativecommons.org/licenses/by/4.0/","  The training and generalization dynamics of the Transformer's core mechanism,
namely the Attention mechanism, remain under-explored. Besides, existing
analyses primarily focus on single-head attention. Inspired by the demonstrated
benefits of overparameterization when training fully-connected networks, we
investigate the potential optimization and generalization advantages of using
multiple attention heads. Towards this goal, we derive convergence and
generalization guarantees for gradient-descent training of a single-layer
multi-head self-attention model, under a suitable realizability condition on
the data. We then establish primitive conditions on the initialization that
ensure realizability holds. Finally, we demonstrate that these conditions are
satisfied for a simple tokenized-mixture model. We expect the analysis can be
extended to various data-model and architecture variations.
","2023-10-20","2310.12680v1.pdf"
"2310.12700","Martin Weigt","Francesco Calvanese, Camille N. Lambert, Philippe Nghe, Francesco
  Zamponi, Martin Weigt","Towards Parsimonious Generative Modeling of RNA Families","33 pages (including SI)","","","","q-bio.BM cond-mat.stat-mech q-bio.GN q-bio.QM","http://creativecommons.org/licenses/by/4.0/","  Generative probabilistic models emerge as a new paradigm in data-driven,
evolution-informed design of biomolecular sequences. This paper introduces a
novel approach, called Edge Activation Direct Coupling Analysis (eaDCA),
tailored to the characteristics of RNA sequences, with a strong emphasis on
simplicity, efficiency, and interpretability. eaDCA explicitly constructs
sparse coevolutionary models for RNA families, achieving performance levels
comparable to more complex methods while utilizing a significantly lower number
of parameters. Our approach demonstrates efficiency in generating artificial
RNA sequences that closely resemble their natural counterparts in both
statistical analyses and SHAPE-MaP experiments, and in predicting the effect of
mutations. Notably, eaDCA provides a unique feature: estimating the number of
potential functional sequences within a given RNA family. For example, in the
case of cyclic di-AMP riboswitches (RF00379), our analysis suggests the
existence of approximately $\mathbf{10^{39}}$ functional nucleotide sequences.
While huge compared to the known $< \mathbf{4,000}$ natural sequences, this
number represents only a tiny fraction of the vast pool of nearly
$\mathbf{10^{82}}$ possible nucleotide sequences of the same length (136
nucleotides). These results underscore the promise of sparse and interpretable
generative models, such as eaDCA, in enhancing our understanding of the
expansive RNA sequence space.
","2023-10-20","2310.12700v1.pdf"
"2310.12751","Hao Sun","Hao Sun, John Hewitt","Character-level Chinese Backpack Language Models","BlackboxNLP 2023 Camera-Ready","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Backpack is a Transformer alternative shown to improve interpretability
in English language modeling by decomposing predictions into a weighted sum of
token sense components. However, Backpacks' reliance on token-defined meaning
raises questions as to their potential for languages other than English, a
language for which subword tokenization provides a reasonable approximation for
lexical items. In this work, we train, evaluate, interpret, and control
Backpack language models in character-tokenized Chinese, in which words are
often composed of many characters. We find that our (134M parameter) Chinese
Backpack language model performs comparably to a (104M parameter) Transformer,
and learns rich character-level meanings that log-additively compose to form
word meanings. In SimLex-style lexical semantic evaluations, simple averages of
Backpack character senses outperform input embeddings from a Transformer. We
find that complex multi-character meanings are often formed by using the same
per-character sense weights consistently across context. Exploring
interpretability-through control, we show that we can localize a source of
gender bias in our Backpacks to specific character senses and intervene to
reduce the bias.
","2023-10-20","2310.12751v1.pdf"
"2310.12773","Josef Dai","Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu,
  Yizhou Wang, Yaodong Yang","Safe RLHF: Safe Reinforcement Learning from Human Feedback","","","","","cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers' confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.
","2023-10-20","2310.12773v1.pdf"
"2310.12774","Han Zhou","Han Zhou, Xingchen Wan, Ivan Vuli\'c, Anna Korhonen","Survival of the Most Influential Prompts: Efficient Black-Box Prompt
  Search via Clustering and Pruning","Findings of EMNLP 2023. 10 pages, 5 figures, 4 tables (14 pages, 5
  figures, 8 tables including references and appendices)","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Prompt-based learning has been an effective paradigm for large pretrained
language models (LLM), enabling few-shot or even zero-shot learning. Black-box
prompt search has received growing interest recently for its distinctive
properties of gradient-free optimization, proven particularly useful and
powerful for model-as-a-service usage. However, the discrete nature and the
complexity of combinatorial optimization hinder the efficiency of modern
black-box approaches. Despite extensive research on search algorithms, the
crucial aspect of search space design and optimization has been largely
overlooked. In this paper, we first conduct a sensitivity analysis by prompting
LLM, revealing that only a small number of tokens exert a disproportionate
amount of influence on LLM predictions. Leveraging this insight, we propose the
Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple
black-box search method that first clusters and prunes the search space to
focus exclusively on influential prompt tokens. By employing even simple search
methods within the pruned search space, ClaPS achieves state-of-the-art
performance across various tasks and LLMs, surpassing the performance of
complex approaches while significantly reducing search costs. Our findings
underscore the critical role of search space design and optimization in
enhancing both the usefulness and the efficiency of black-box prompt-based
learning.
","2023-10-20","2310.12774v1.pdf"
"2310.12794","Ningyu Xu","Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang","Are Structural Concepts Universal in Transformer Language Models?
  Towards Interpretable Cross-Lingual Generalization","Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have exhibited considerable cross-lingual
generalization abilities, whereby they implicitly transfer knowledge across
languages. However, the transfer is not equally successful for all languages,
especially for low-resource ones, which poses an ongoing challenge. It is
unclear whether we have reached the limits of implicit cross-lingual
generalization and if explicit knowledge transfer is viable. In this paper, we
investigate the potential for explicitly aligning conceptual correspondence
between languages to enhance cross-lingual generalization. Using the syntactic
aspect of language as a testbed, our analyses of 43 languages reveal a high
degree of alignability among the spaces of structural concepts within each
language for both encoder-only and decoder-only LLMs. We then propose a
meta-learning-based method to learn to align conceptual spaces of different
languages, which facilitates zero-shot and few-shot generalization in concept
classification and also offers insights into the cross-lingual in-context
learning phenomenon. Experiments on syntactic analysis tasks show that our
approach achieves competitive results with state-of-the-art methods and narrows
the performance gap between languages, particularly benefiting those with
limited resources.
","2023-10-20","2310.12794v1.pdf"
"2310.12798","Zhiyuan Liu","Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji
  Kawaguchi, Xiang Wang, Tat-Seng Chua","MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and
  Uni-Modal Adapter","EMNLP main conference. 9 pages","","","","cs.CL cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules' topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder's
representation space and an LM's text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM's ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.
","2023-10-20","2310.12798v1.pdf"
"2310.12802","Lucas Lacasa","Llu\'is Arola-Fern\'andez and Lucas Lacasa","An effective theory of collective deep learning","","","","","physics.soc-ph cond-mat.dis-nn cs.AI cs.LG nlin.AO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Unraveling the emergence of collective learning in systems of coupled
artificial neural networks is an endeavor with broader implications for
physics, machine learning, neuroscience and society. Here we introduce a
minimal model that condenses several recent decentralized algorithms by
considering a competition between two terms: the local learning dynamics in the
parameters of each neural network unit, and a diffusive coupling among units
that tends to homogenize the parameters of the ensemble. We derive the
coarse-grained behavior of our model via an effective theory for linear
networks that we show is analogous to a deformed Ginzburg-Landau model with
quenched disorder. This framework predicts (depth-dependent)
disorder-order-disorder phase transitions in the parameters' solutions that
reveal the onset of a collective learning phase, along with a depth-induced
delay of the critical point and a robust shape of the microscopic learning
path. We validate our theory in realistic ensembles of coupled nonlinear
networks trained in the MNIST dataset under privacy constraints. Interestingly,
experiments confirm that individual networks -- trained only with private data
-- can fully generalize to unseen data classes when the collective learning
phase emerges. Our work elucidates the physics of collective learning and
contributes to the mechanistic interpretability of deep learning in
decentralized settings.
","2023-10-20","2310.12802v1.pdf"
"2310.12803","Amir Feder","Amir Feder, Yoav Wald, Claudia Shi, Suchi Saria, David Blei","Causal-structure Driven Augmentations for Text OOD Generalization","Forthcoming in NeurIPS 2023","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  The reliance of text classifiers on spurious correlations can lead to poor
generalization at deployment, raising concerns about their use in
safety-critical domains such as healthcare. In this work, we propose to use
counterfactual data augmentation, guided by knowledge of the causal structure
of the data, to simulate interventions on spurious features and to learn more
robust text classifiers. We show that this strategy is appropriate in
prediction problems where the label is spuriously correlated with an attribute.
Under the assumptions of such problems, we discuss the favorable sample
complexity of counterfactual data augmentation, compared to importance
re-weighting. Pragmatically, we match examples using auxiliary data, based on
diff-in-diff methodology, and use a large language model (LLM) to represent a
conditional probability of text. Through extensive experimentation on learning
caregiver-invariant predictors of clinical diagnoses from medical narratives
and on semi-synthetic data, we demonstrate that our method for simulating
interventions improves out-of-distribution (OOD) accuracy compared to baseline
invariant learning algorithms.
","2023-10-20","2310.12803v1.pdf"
"2310.12808","Nico Daheim","Nico Daheim, Thomas M\""ollenhoff, Edoardo Maria Ponti, Iryna Gurevych,
  Mohammad Emtiyaz Khan","Model Merging by Uncertainty-Based Gradient Matching","Preprint. Under review","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Models trained on different datasets can be merged by a weighted-averaging of
their parameters, but why does it work and when can it fail? Here, we connect
the inaccuracy of weighted-averaging to mismatches in the gradients and propose
a new uncertainty-based scheme to improve the performance by reducing the
mismatch. The connection also reveals implicit assumptions in other schemes
such as averaging, task arithmetic, and Fisher-weighted averaging. Our new
method gives consistent improvements for large language models and vision
transformers, both in terms of performance and robustness to hyperparameters.
","2023-10-20","2310.12808v1.pdf"
"2310.12815","Yupei Liu","Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong","Prompt Injection Attacks and Defenses in LLM-Integrated Applications","","","","","cs.CR cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are increasingly deployed as the backend for a
variety of real-world applications called LLM-Integrated Applications. Multiple
recent works showed that LLM-Integrated Applications are vulnerable to prompt
injection attacks, in which an attacker injects malicious instruction/data into
the input of those applications such that they produce results as the attacker
desires. However, existing works are limited to case studies. As a result, the
literature lacks a systematic understanding of prompt injection attacks and
their defenses. We aim to bridge the gap in this work. In particular, we
propose a general framework to formalize prompt injection attacks. Existing
attacks, which are discussed in research papers and blog posts, are special
cases in our framework. Our framework enables us to design a new attack by
combining existing attacks. Moreover, we also propose a framework to
systematize defenses against prompt injection attacks. Using our frameworks, we
conduct a systematic evaluation on prompt injection attacks and their defenses
with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in
this field. Our code is available at
https://github.com/liu00222/Open-Prompt-Injection.
","2023-10-20","2310.12815v1.pdf"
"2310.12818","Weize Chen","Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu,
  Maosong Sun, Jie Zhou","Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared
  Pre-trained Language Models","EMNLP 2023 Findings","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Parameter-shared pre-trained language models (PLMs) have emerged as a
successful approach in resource-constrained environments, enabling substantial
reductions in model storage and memory costs without significant performance
compromise. However, it is important to note that parameter sharing does not
alleviate computational burdens associated with inference, thus impeding its
practicality in situations characterized by limited stringent latency
requirements or computational resources. Building upon neural ordinary
differential equations (ODEs), we introduce a straightforward technique to
enhance the inference efficiency of parameter-shared PLMs. Additionally, we
propose a simple pre-training technique that leads to fully or partially shared
models capable of achieving even greater inference acceleration. The
experimental results demonstrate the effectiveness of our methods on both
autoregressive and autoencoding PLMs, providing novel insights into more
efficient utilization of parameter-shared models in resource-constrained
settings.
","2023-10-20","2310.12818v1.pdf"
"2310.12823","Aohan Zeng","Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong,
  Jie Tang","AgentTuning: Enabling Generalized Agent Abilities for LLMs","31 pages","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Open large language models (LLMs) with great performance in various tasks
have significantly advanced the development of LLMs. However, they are far
inferior to commercial models such as ChatGPT and GPT-4 when acting as agents
to tackle complex tasks in the real world. These agent tasks employ LLMs as the
central controller responsible for planning, memorization, and tool
utilization, necessitating both fine-grained prompting methods and robust LLMs
to achieve satisfactory performance. Though many prompting methods have been
proposed to complete particular agent tasks, there is lack of research focusing
on improving the agent capabilities of LLMs themselves without compromising
their general abilities. In this work, we present AgentTuning, a simple and
general method to enhance the agent abilities of LLMs while maintaining their
general LLM capabilities. We construct AgentInstruct, a lightweight
instruction-tuning dataset containing high-quality interaction trajectories. We
employ a hybrid instruction-tuning strategy by combining AgentInstruct with
open-source instructions from general domains. AgentTuning is used to
instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show
that AgentTuning enables LLMs' agent capabilities without compromising general
abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent
tasks, demonstrating generalized agent capabilities. We open source the
AgentInstruct and AgentLM-7B, 13B, and 70B models at
https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to
commercial LLMs for agent tasks.
","2023-10-24","2310.12823v1.pdf"
"2310.12836","Jinheon Baek","Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, Sung Ju Hwang","Knowledge-Augmented Language Model Verification","EMNLP 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent Language Models (LMs) have shown impressive capabilities in generating
texts with the knowledge internalized in parameters. Yet, LMs often generate
the factually incorrect responses to the given queries, since their knowledge
may be inaccurate, incomplete, and outdated. To address this problem, previous
works propose to augment LMs with the knowledge retrieved from an external
knowledge source. However, such approaches often show suboptimal text
generation performance due to two reasons: 1) the model may fail to retrieve
the knowledge relevant to the given query, or 2) the model may not faithfully
reflect the retrieved knowledge in the generated text. To overcome these, we
propose to verify the output and the knowledge of the knowledge-augmented LMs
with a separate verifier, which is a small LM that is trained to detect those
two types of errors through instruction-finetuning. Then, when the verifier
recognizes an error, we can rectify it by either retrieving new knowledge or
generating new text. Further, we use an ensemble of the outputs from different
instructions with a single verifier to enhance the reliability of the
verification processes. We validate the effectiveness of the proposed
verification steps on multiple question answering benchmarks, whose results
show that the proposed verifier effectively identifies retrieval and generation
errors, allowing LMs to provide more factually correct outputs. Our code is
available at https://github.com/JinheonBaek/KALMV.
","2023-10-20","2310.12836v1.pdf"
"2310.12860","Punyajoy Saha","Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee and Punyajoy Saha","Probing LLMs for hate speech detection: strengths and vulnerabilities","13 pages, 9 figures, 7 tables, accepted to EMNLP 2023","","","","cs.CL cs.CY","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Recently efforts have been made by social media platforms as well as
researchers to detect hateful or toxic language using large language models.
However, none of these works aim to use explanation, additional context and
victim community information in the detection process. We utilise different
prompt variation, input information and evaluate large language models in zero
shot setting (without adding any in-context examples). We select three large
language models (GPT-3.5, text-davinci and Flan-T5) and three datasets -
HateXplain, implicit hate and ToxicSpans. We find that on average including the
target information in the pipeline improves the model performance substantially
(~20-30%) over the baseline across the datasets. There is also a considerable
effect of adding the rationales/explanations into the pipeline (~10-20%) over
the baseline across the datasets. In addition, we further provide a typology of
the error cases where these large language models fail to (i) classify and (ii)
explain the reason for the decisions they take. Such vulnerable points
automatically constitute 'jailbreak' prompts for these models and industry
scale safeguard techniques need to be developed to make the models robust
against such prompts.
","2023-10-20","2310.12860v1.pdf"
"2310.12864","Lihu Chen","Lihu Chen, Ga\""el Varoquaux, Fabian M. Suchanek","The Locality and Symmetry of Positional Encodings","Long Paper in Findings of EMNLP23","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Positional Encodings (PEs) are used to inject word-order information into
transformer-based language models. While they can significantly enhance the
quality of sentence representations, their specific contribution to language
models is not fully understood, especially given recent findings that various
positional encodings are insensitive to word order. In this work, we conduct a
systematic study of positional encodings in \textbf{Bidirectional Masked
Language Models} (BERT-style) , which complements existing work in three
aspects: (1) We uncover the core function of PEs by identifying two common
properties, Locality and Symmetry; (2) We show that the two properties are
closely correlated with the performances of downstream tasks; (3) We quantify
the weakness of current PEs by introducing two new probing tasks, on which
current PEs perform poorly. We believe that these results are the basis for
developing better PEs for transformer-based language models. The code is
available at \faGithub~ \url{https://github.com/tigerchen52/locality\_symmetry}
","2023-10-20","2310.12864v1.pdf"
"2310.12874","Cheng Jiayang","Cheng Jiayang, Lin Qiu, Tsz Ho Chan, Tianqing Fang, Weiqi Wang,
  Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang,
  Zheng Zhang","StoryAnalogy: Deriving Story-level Analogies from Large Language Models
  to Unlock Analogical Understanding","Accepted by EMNLP 2023 main conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Analogy-making between narratives is crucial for human reasoning. In this
paper, we evaluate the ability to identify and generate analogies by
constructing a first-of-its-kind large-scale story-level analogy corpus,
\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with
human annotations on two similarities from the extended Structure-Mapping
Theory. We design a set of tests on \textsc{StoryAnalogy}, presenting the first
evaluation of story-level analogy identification and generation. Interestingly,
we find that the analogy identification tasks are incredibly difficult not only
for sentence embedding models but also for the recent large language models
(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around
30% accuracy in multiple-choice questions (compared to over 85% accuracy for
humans). Furthermore, we observe that the data in \textsc{StoryAnalogy} can
improve the quality of analogy generation in LLMs, where a fine-tuned
FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.
","2023-10-24","2310.12874v1.pdf"
"2310.12883","Antonio Carlos Costa","Antonio C. Costa, Tosif Ahamed, David Jordan, Greg J. Stephens","A Markovian dynamics for $C. elegans$ behavior across scales","28 pages, 14 figures","","","","physics.bio-ph nlin.CD q-bio.NC","http://creativecommons.org/licenses/by-nc-nd/4.0/","  How do we capture the breadth of behavior in animal movement, from rapid body
twitches to aging? Using high-resolution videos of the nematode worm $C.
elegans$, we show that a single dynamics connects posture-scale fluctuations
with trajectory diffusion, and longer-lived behavioral states. We take short
posture sequences as an instantaneous behavioral measure, fixing the sequence
length for maximal prediction. Within the space of posture sequences we
construct a fine-scale, maximum entropy partition so that transitions among
microstates define a high-fidelity Markov model, which we also use as a means
of principled coarse-graining. We translate these dynamics into movement using
resistive force theory, capturing the statistical properties of foraging
trajectories. Predictive across scales, we leverage the longest-lived
eigenvectors of the inferred Markov chain to perform a top-down subdivision of
the worm's foraging behavior, revealing both ``runs-and-pirouettes'' as well as
previously uncharacterized finer-scale behaviors. We use our model to
investigate the relevance of these fine-scale behaviors for foraging success,
recovering a trade-off between local and global search strategies.
","2023-10-20","2310.12883v1.pdf"
"2310.12892","Songbo Hu","Songbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio
  Iacobacci, Anna Korhonen, Ivan Vuli\'c","A Systematic Study of Performance Disparities in Multilingual
  Task-Oriented Dialogue Systems","Accepted to EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Achieving robust language technologies that can perform well across the
world's many languages is a central goal of multilingual NLP. In this work, we
take stock of and empirically analyse task performance disparities that exist
between multilingual task-oriented dialogue (ToD) systems. We first define new
quantitative measures of absolute and relative equivalence in system
performance, capturing disparities across languages and within individual
languages. Through a series of controlled experiments, we demonstrate that
performance disparities depend on a number of factors: the nature of the ToD
task at hand, the underlying pretrained language model, the target language,
and the amount of ToD annotated data. We empirically prove the existence of the
adaptation and intrinsic biases in current ToD systems: e.g., ToD systems
trained for Arabic or Turkish using annotated ToD data fully parallel to
English ToD data still exhibit diminished ToD task performance. Beyond
providing a series of insights into the performance disparities of ToD systems
in different languages, our analyses offer practical tips on how to approach
ToD data collection and system development for new languages.
","2023-10-20","2310.12892v1.pdf"
"2310.12902","Nina Begus","Nina Begus","Experimental Narratives: A Comparison of Human Crowdsourced Storytelling
  and AI Storytelling","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct comparison between human and LLM-generated storytelling. Responses to
the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth
in the collective imaginary of both humans and large language models. All
solicited narratives present a scientific or technological pursuit. The
analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives can occasionally provide innovative plot twists,
they offer less imaginative scenarios and rhetoric than human-authored texts.
The proposed framework argues that fiction can be used as a window into human
and AI-based collective imaginary and social dimensions.
","2023-10-20","2310.12902v1.pdf"
"2310.12920","Sulin Liu","Sulin Liu, Peter J. Ramadge, Ryan P. Adams","Generative Marginalization Models","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce marginalization models (MaMs), a new family of generative models
for high-dimensional discrete data. They offer scalable and flexible generative
modeling with tractable likelihoods by explicitly modeling all induced marginal
distributions. Marginalization models enable fast evaluation of arbitrary
marginal probabilities with a single forward pass of the neural network, which
overcomes a major limitation of methods with exact marginal inference, such as
autoregressive models (ARMs). We propose scalable methods for learning the
marginals, grounded in the concept of ""marginalization self-consistency"".
Unlike previous methods, MaMs support scalable training of any-order generative
models for high-dimensional problems under the setting of energy-based
training, where the goal is to match the learned distribution to a given
desired probability (specified by an unnormalized (log) probability function
such as energy function or reward function). We demonstrate the effectiveness
of the proposed model on a variety of discrete data distributions, including
binary images, language, physical systems, and molecules, for maximum
likelihood and energy-based training settings. MaMs achieve orders of magnitude
speedup in evaluating the marginal probabilities on both settings. For
energy-based training tasks, MaMs enable any-order generative modeling of
high-dimensional problems beyond the capability of previous methods. Code is at
https://github.com/PrincetonLIPS/MaM.
","2023-10-20","2310.12920v1.pdf"
"2310.12921","David Lindner","Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David
  Lindner","Vision-Language Models are Zero-Shot Reward Models for Reinforcement
  Learning","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning (RL) requires either manually specifying a reward
function, which is often infeasible, or learning a reward model from a large
amount of human feedback, which is often very expensive. We study a more
sample-efficient alternative: using pretrained vision-language models (VLMs) as
zero-shot reward models (RMs) to specify tasks via natural language. We propose
a natural and general approach to using VLMs as reward models, which we call
VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn
complex tasks without a manually specified reward function, such as kneeling,
doing the splits, and sitting in a lotus position. For each of these tasks, we
only provide a single sentence text prompt describing the desired task with
minimal prompt engineering. We provide videos of the trained agents at:
https://sites.google.com/view/vlm-rm. We can improve performance by providing a
second ``baseline'' prompt and projecting out parts of the CLIP embedding space
irrelevant to distinguish between goal and baseline. Further, we find a strong
scaling effect for VLM-RMs: larger VLMs trained with more compute and data are
better reward models. The failure modes of VLM-RMs we encountered are all
related to known capability limitations of current VLMs, such as limited
spatial reasoning ability or visually unrealistic environments that are far
off-distribution for the VLM. We find that VLM-RMs are remarkably robust as
long as the VLM is large enough. This suggests that future VLMs will become
more and more useful reward models for a wide range of RL applications.
","2023-10-20","2310.12921v1.pdf"
"2310.12931","Yecheng Jason Ma","Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert
  Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar","Eureka: Human-Level Reward Design via Coding Large Language Models","Project website and open-source code:
  https://eureka-research.github.io/","","","","cs.RO cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have excelled as high-level semantic planners
for sequential decision-making tasks. However, harnessing them to learn complex
low-level manipulation tasks, such as dexterous pen spinning, remains an open
problem. We bridge this fundamental gap and present Eureka, a human-level
reward design algorithm powered by LLMs. Eureka exploits the remarkable
zero-shot generation, code-writing, and in-context improvement capabilities of
state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over
reward code. The resulting rewards can then be used to acquire complex skills
via reinforcement learning. Without any task-specific prompting or pre-defined
reward templates, Eureka generates reward functions that outperform expert
human-engineered rewards. In a diverse suite of 29 open-source RL environments
that include 10 distinct robot morphologies, Eureka outperforms human experts
on 83% of the tasks, leading to an average normalized improvement of 52%. The
generality of Eureka also enables a new gradient-free in-context learning
approach to reinforcement learning from human feedback (RLHF), readily
incorporating human inputs to improve the quality and the safety of the
generated rewards without model updating. Finally, using Eureka rewards in a
curriculum learning setting, we demonstrate for the first time, a simulated
Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a
pen in circles at rapid speed.
","2023-10-20","2310.12931v1.pdf"
"2310.12945","Junlin Han","Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin,
  Stephen Gould","3D-GPT: Procedural 3D Modeling with Large Language Models","Project page: https://chuny1.github.io/3DGPT/3dgpt.html","","","","cs.CV cs.GR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the pursuit of efficient automated content creation, procedural
generation, leveraging modifiable parameters and rule-based systems, emerges as
a promising approach. Nonetheless, it could be a demanding endeavor, given its
intricate nature necessitating a deep understanding of rules, algorithms, and
parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing
large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT
positions LLMs as proficient problem solvers, dissecting the procedural 3D
modeling tasks into accessible segments and appointing the apt agent for each
task. 3D-GPT integrates three core agents: the task dispatch agent, the
conceptualization agent, and the modeling agent. They collaboratively achieve
two objectives. First, it enhances concise initial scene descriptions, evolving
them into detailed forms while dynamically adapting the text based on
subsequent instructions. Second, it integrates procedural generation,
extracting parameter values from enriched text to effortlessly interface with
3D software for asset creation. Our empirical investigations confirm that
3D-GPT not only interprets and executes instructions, delivering reliable
results but also collaborates effectively with human designers. Furthermore, it
seamlessly integrates with Blender, unlocking expanded manipulation
possibilities. Our work highlights the potential of LLMs in 3D modeling,
offering a basic framework for future advancements in scene generation and
animation.
","2023-10-20","2310.12945v1.pdf"
"2310.12953","Sangho Suh","Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, and Haijun Xia","Structured Generation and Exploration of Design Space with Large
  Language Models for Human-AI Co-Creation","","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Thanks to their generative capabilities, large language models (LLMs) have
become an invaluable tool for creative processes. These models have the
capacity to produce hundreds and thousands of visual and textual outputs,
offering abundant inspiration for creative endeavors. But are we harnessing
their full potential? We argue that current interaction paradigms fall short,
guiding users towards rapid convergence on a limited set of ideas, rather than
empowering them to explore the vast latent design space in generative models.
To address this limitation, we propose a framework that facilitates the
structured generation of design space in which users can seamlessly explore,
evaluate, and synthesize a multitude of responses. We demonstrate the
feasibility and usefulness of this framework through the design and development
of an interactive system, Luminate, and a user study with 8 professional
writers. Our work advances how we interact with LLMs for creative tasks,
introducing a way to harness the creative potential of LLMs.
","2023-10-24","2310.12953v1.pdf"
"2310.12956","David T. Hoffmann","David T. Hoffmann, Simon Schrodi, Nadine Behrmann, Volker Fischer,
  Thomas Brox","Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced
  Optimization Problems","","","","","cs.LG cs.AI cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this work, we study rapid, step-wise improvements of the loss in
transformers when being confronted with multi-step decision tasks. We found
that transformers struggle to learn the intermediate tasks, whereas CNNs have
no such issue on the tasks we studied. When transformers learn the intermediate
task, they do this rapidly and unexpectedly after both training and validation
loss saturated for hundreds of epochs. We call these rapid improvements
Eureka-moments, since the transformer appears to suddenly learn a previously
incomprehensible task. Similar leaps in performance have become known as
Grokking. In contrast to Grokking, for Eureka-moments, both the validation and
the training loss saturate before rapidly improving. We trace the problem back
to the Softmax function in the self-attention block of transformers and show
ways to alleviate the problem. These fixes improve training speed. The improved
models reach 95% of the baseline model in just 20% of training steps while
having a much higher likelihood to learn the intermediate task, lead to higher
final accuracy and are more robust to hyper-parameters.
","2023-10-20","2310.12956v1.pdf"
"2310.12960","Xueliang Zhao","Xueliang Zhao, Xinting Huang, Wei Bi, Lingpeng Kong","SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving","Preprint","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLMs) have driven substantial progress in artificial
intelligence in recent years, exhibiting impressive capabilities across a wide
range of tasks, including mathematical problem-solving. Inspired by the success
of subgoal-based methods, we propose a novel framework called
\textbf{SE}quential sub\textbf{G}oal \textbf{O}ptimization (SEGO) to enhance
LLMs' ability to solve mathematical problems. By establishing a connection
between the subgoal breakdown process and the probability of solving problems,
SEGO aims to identify better subgoals with theoretical guarantees. Addressing
the challenge of identifying suitable subgoals in a large solution space, our
framework generates problem-specific subgoals and adjusts them according to
carefully designed criteria. Incorporating these optimized subgoals into the
policy model training leads to significant improvements in problem-solving
performance. We validate SEGO's efficacy through experiments on two benchmarks,
GSM8K and MATH, where our approach outperforms existing methods, highlighting
the potential of SEGO in AI-driven mathematical problem-solving.
  Data and code associated with this paper will be available at
https://github.com/zhaoxlpku/SEGO
","2023-10-20","2310.12960v1.pdf"
"2310.12962","Eric A Mitchell","Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn,
  Christopher D. Manning","An Emulator for Fine-Tuning Large Language Models using Small Language
  Models","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Widely used language models (LMs) are typically built by scaling up a
two-stage training pipeline: a pre-training stage that uses a very large,
diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that
uses targeted examples or other specifications of desired behaviors. While it
has been hypothesized that knowledge and skills come from pre-training, and
fine-tuning mostly filters this knowledge and skillset, this intuition has not
been extensively tested. To aid in doing so, we introduce a novel technique for
decoupling the knowledge and skills gained in these two stages, enabling a
direct answer to the question, ""What would happen if we combined the knowledge
learned by a large model during pre-training with the knowledge learned by a
small model during fine-tuning (or vice versa)?"" Using an RL-based framework
derived from recent developments in learning from human preferences, we
introduce emulated fine-tuning (EFT), a principled and practical method for
sampling from a distribution that approximates (or 'emulates') the result of
pre-training and fine-tuning at different scales. Our experiments with EFT show
that scaling up fine-tuning tends to improve helpfulness, while scaling up
pre-training tends to improve factuality. Beyond decoupling scale, we show that
EFT enables test-time adjustment of competing behavioral traits like
helpfulness and harmlessness without additional training. Finally, a special
case of emulated fine-tuning, which we call LM up-scaling, avoids
resource-intensive fine-tuning of large pre-trained models by ensembling them
with small fine-tuned models, essentially emulating the result of fine-tuning
the large pre-trained model. Up-scaling consistently improves helpfulness and
factuality of instruction-following models in the Llama, Llama-2, and Falcon
families, without additional hyperparameters or training.
","2023-10-20","2310.12962v1.pdf"
"2310.12963","Aman Madaan","Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi
  Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik
  Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui","AutoMix: Automatically Mixing Language Models","The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) are now available in various sizes and
configurations from cloud API providers. While this diversity offers a broad
spectrum of choices, effectively leveraging the options to optimize
computational cost and performance remains challenging. In this work, we
present AutoMix, an approach that strategically routes queries to larger LMs,
based on the approximate correctness of outputs from a smaller LM. Central to
AutoMix is a few-shot self-verification mechanism, which estimates the
reliability of its own outputs without requiring training. Given that
verifications can be noisy, we employ a meta verifier in AutoMix to refine the
accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five
context-grounded reasoning datasets demonstrate that AutoMix surpasses
established baselines, improving the incremental benefit per cost by up to 89%.
Our code and data are available at https://github.com/automix-llm/automix.
","2023-10-20","2310.12963v1.pdf"
"2310.12971","Suzanne Petryk","David Chan, Suzanne Petryk, Joseph E. Gonzalez, Trevor Darrell, John
  Canny","CLAIR: Evaluating Image Captions with Large Language Models","To Appear at EMNLP 2023","","","","cs.CV cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The evaluation of machine-generated image captions poses an interesting yet
persistent challenge. Effective evaluation measures must consider numerous
dimensions of similarity, including semantic relevance, visual structure,
object interactions, caption diversity, and specificity. Existing
highly-engineered measures attempt to capture specific aspects, but fall short
in providing a holistic score that aligns closely with human judgments. Here,
we propose CLAIR, a novel method that leverages the zero-shot language modeling
capabilities of large language models (LLMs) to evaluate candidate captions. In
our evaluations, CLAIR demonstrates a stronger correlation with human judgments
of caption quality compared to existing measures. Notably, on Flickr8K-Expert,
CLAIR achieves relative correlation improvements over SPICE of 39.6% and over
image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides
noisily interpretable results by allowing the language model to identify the
underlying reasoning behind its assigned score. Code is available at
https://davidmchan.github.io/clair/
","2023-10-26","2310.12971v1.pdf"
"2310.12978","Ling-Hao Chen","Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei
  Zhang, Heung-Yeung Shum","HumanTOMATO: Text-aligned Whole-body Motion Generation","31 pages, 15 figures, 16 tables. Project page:
  https://lhchen.top/HumanTOMATO","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work targets a novel text-driven whole-body motion generation task,
which takes a given textual description as input and aims at generating
high-quality, diverse, and coherent facial expressions, hand gestures, and body
motions simultaneously. Previous works on text-driven motion generation tasks
mainly have two limitations: they ignore the key role of fine-grained hand and
face controlling in vivid whole-body motion generation, and lack a good
alignment between text and motion. To address such limitations, we propose a
Text-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, which
is the first attempt to our knowledge towards applicable holistic motion
generation in this research area. To tackle this challenging task, our solution
includes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and
a Hierarchical-GPT for fine-grained body and hand motion reconstruction and
generation with two structured codebooks; and (2) a pre-trained
text-motion-alignment model to help generated motion align with the input
textual description explicitly. Comprehensive experiments verify that our model
has significant advantages in both the quality of generated motions and their
alignment with text.
","2023-10-20","2310.12978v1.pdf"
"2310.12989","Yikuan Li","Yikuan Li, Hanyin Wang, Halid Yerebakan, Yoshihisa Shinagawa and Yuan
  Luo","Enhancing Health Data Interoperability with Large Language Models: A
  FHIR Study","Submitted to 2024 AMIA IS","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In this study, we investigated the ability of the large language model (LLM)
to enhance healthcare data interoperability. We leveraged the LLM to convert
clinical texts into their corresponding FHIR resources. Our experiments,
conducted on 3,671 snippets of clinical text, demonstrated that the LLM not
only streamlines the multi-step natural language processing and human
calibration processes but also achieves an exceptional accuracy rate of over
90% in exact matches when compared to human annotations.
","2023-10-23","2310.12989v1.pdf"
"2310.12994","Kerem Oktar","Kerem Oktar, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths","Dimensions of Disagreement: Unpacking Divergence and Misalignment in
  Cognitive Science and Artificial Intelligence","Currently under review","","","","q-bio.NC cs.AI","http://creativecommons.org/licenses/by/4.0/","  The increasing prevalence of artificial agents creates a correspondingly
increasing need to manage disagreements between humans and artificial agents,
as well as between artificial agents themselves. Considering this larger space
of possible agents exposes an opportunity for furthering our understanding of
the nature of disagreement: past studies in psychology have often cast
disagreement as two agents forming diverging evaluations of the same object,
but disagreement can also arise from differences in how agents represent that
object. AI research on human-machine alignment and recent work in computational
cognitive science have focused on this latter kind of disagreement, and have
developed tools that can be used to quantify the extent of representational
overlap between agents. Understanding how divergence and misalignment interact
to produce disagreement, and how resolution strategies depend on this
interaction, is key to promoting effective collaboration between diverse types
of agents.
","2023-10-23","2310.12994v1.pdf"
"2310.13001","Siu Ho Wong","Stephen Choi, William Gazeley, Siu Ho Wong, Tingting Li","Conversational Financial Information Retrieval Model (ConFIRM)","10 pages, 2 figures, 2 tables, 2 appendices","","","","cs.IR cs.AI cs.CE cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  With the exponential growth in large language models (LLMs), leveraging their
emergent properties for specialized domains like finance merits exploration.
However, regulated fields such as finance pose unique constraints, requiring
domain-optimized frameworks. We present ConFIRM, an LLM-based conversational
financial information retrieval model tailored for query intent classification
and knowledge base labeling.
  ConFIRM comprises two modules:
  1) a method to synthesize finance domain-specific question-answer pairs, and
  2) evaluation of parameter efficient fine-tuning approaches for the query
classification task. We generate a dataset of over 4000 samples, assessing
accuracy on a separate test set.
  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.
ConFIRM provides a data-efficient solution to extract precise query intent for
financial dialog systems.
","2023-10-23","2310.13001v1.pdf"
"2310.13004","Khanh Nguyen","Ruijie Zheng, Khanh Nguyen, Hal Daum\'e III, Furong Huang, Karthik
  Narasimhan","Progressively Efficient Learning","","","","","cs.LG cs.AI cs.HC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Assistant AI agents should be capable of rapidly acquiring novel skills and
adapting to new user preferences. Traditional frameworks like imitation
learning and reinforcement learning do not facilitate this capability because
they support only low-level, inefficient forms of communication. In contrast,
humans communicate with progressive efficiency by defining and sharing abstract
intentions. Reproducing similar capability in AI agents, we develop a novel
learning framework named Communication-Efficient Interactive Learning (CEIL).
By equipping a learning agent with an abstract, dynamic language and an
intrinsic motivation to learn with minimal communication effort, CEIL leads to
emergence of a human-like pattern where the learner and the teacher communicate
progressively efficiently by exchanging increasingly more abstract intentions.
CEIL demonstrates impressive performance and communication efficiency on a 2D
MineCraft domain featuring long-horizon decision-making tasks. Agents trained
with CEIL quickly master new tasks, outperforming non-hierarchical and
hierarchical imitation learning by up to 50% and 20% in absolute success rate,
respectively, given the same number of interactions with the teacher.
Especially, the framework performs robustly with teachers modeled after human
pragmatic communication behavior.
","2023-10-23","2310.13004v1.pdf"
"2310.13008","Haotian Zhou","Haotian Zhou, Tingkai Liu, Qianli Ma, Jianbo Yuan, Pengfei Liu, Yang
  You and Hongxia Yang","LoBaSS: Gauging Learnability in Supervised Fine-tuning Data","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large
Language Models (LLMs) to specific task prerequisites. The selection of
fine-tuning data profoundly influences the model's performance, whose principle
is traditionally grounded in data quality and distribution. In this paper, we
introduce a new dimension in SFT data selection: learnability. This new
dimension is motivated by the intuition that SFT unlocks capabilities acquired
by a LLM during the pretraining phase. Given that different pretrained models
have disparate capabilities, the SFT data appropriate for one may not suit
another. Thus, we introduce the term learnability to define the suitability of
data for effective learning by the model. We present the Loss Based SFT Data
Selection (LoBaSS) method, utilizing data learnability as the principal
criterion for the selection SFT data. This method provides a nuanced approach,
allowing the alignment of data selection with inherent model capabilities,
ensuring optimal compatibility and learning efficiency. In experimental
comparisons involving 7B and 13B models, our LoBaSS method is able to surpass
full-data fine-tuning at merely 6% of the total training data. When employing
16.7% of the data, LoBaSS harmonizes the model's capabilities across
conversational and mathematical domains, proving its efficacy and adaptability.
","2023-10-23","2310.13008v1.pdf"
"2310.13011","Dongyoung Go","Dongyoung Go, Tomasz Korbak, Germ\'an Kruszewski, Jos Rozen, Marc
  Dymetman","Compositional preference models for aligning LMs","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. CPMs allow to control which properties of the preference data are
used to train the preference model and to build it based on features that are
believed to underlie the human preference judgment. Our experiments show that
CPMs not only improve generalization and are more robust to overoptimization
than standard PMs, but also that best-of-n samples obtained using CPMs tend to
be preferred over samples obtained using conventional PMs. Overall, our
approach demonstrates the benefits of endowing PMs with priors about which
features determine human preferences while relying on LM capabilities to
extract those features in a scalable and robust way.
","2023-10-23","2310.13011v1.pdf"
"2310.13012","Marcos V. Conde","Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian
  Jeblick, Chun Ming Lee, Marcos V. Conde","H2O Open Ecosystem for State-of-the-art Large Language Models","EMNLP 2023 Demo - ACL Empirical Methods in Natural Language
  Processing","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLMs) represent a revolution in AI. However, they also
pose many significant risks, such as the presence of biased, private,
copyrighted or harmful text. For this reason we need open, transparent and safe
solutions. We introduce a complete open-source ecosystem for developing and
testing LLMs. The goal of this project is to boost open alternatives to
closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of
diverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI
designed for efficient fine-tuning, evaluation, and deployment of LLMs using
the most recent state-of-the-art techniques. Our code and models are fully
open-source. We believe this work helps to boost AI development and make it
more accessible, efficient and trustworthy. The demo is available at:
https://gpt.h2o.ai/
","2023-10-24","2310.13012v1.pdf"
"2310.13013","Chen Chen","Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Hexin Liu, Sabato Marco
  Siniscalchi, Eng Siong Chng","Generative error correction for code-switching speech recognition using
  large language models","Submitted to ICASSP2024","","","","cs.CL cs.AI cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  Code-switching (CS) speech refers to the phenomenon of mixing two or more
languages within the same sentence. Despite the recent advances in automatic
speech recognition (ASR), CS-ASR is still a challenging task ought to the
grammatical structure complexity of the phenomenon and the data scarcity of
specific training corpus. In this work, we propose to leverage large language
models (LLMs) and lists of hypotheses generated by an ASR to address the CS
problem. Specifically, we first employ multiple well-trained ASR models for
N-best hypotheses generation, with the aim of increasing the diverse and
informative elements in the set of hypotheses. Next, we utilize the LLMs to
learn the hypotheses-to-transcription (H2T) mapping by adding a trainable
low-rank adapter. Such a generative error correction (GER) method directly
predicts the accurate transcription according to its expert linguistic
knowledge and N-best hypotheses, resulting in a paradigm shift from the
traditional language model rescoring or error correction techniques.
Experimental evidence demonstrates that GER significantly enhances CS-ASR
accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show
remarkable data efficiency for H2T learning, providing a potential solution to
the data scarcity problem of CS-ASR in low-resource languages.
","2023-10-23","2310.13013v1.pdf"
"2310.13014","Peter S. Park","Philipp Schoenegger and Peter S. Park","Large Language Model Prediction Capabilities: Evidence from a Real-World
  Forecasting Tournament","13 pages, six visualizations (four figures, two tables)","","","","cs.CY cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Accurately predicting the future would be an important milestone in the
capabilities of artificial intelligence. However, research on the ability of
large language models to provide probabilistic predictions about future events
remains nascent. To empirically test this ability, we enrolled OpenAI's
state-of-the-art large language model, GPT-4, in a three-month forecasting
tournament hosted on the Metaculus platform. The tournament, running from July
to October 2023, attracted 843 participants and covered diverse topics
including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict.
Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are
significantly less accurate than the median human-crowd forecasts. We find that
GPT-4's forecasts did not significantly differ from the no-information
forecasting strategy of assigning a 50% probability to every question. We
explore a potential explanation, that GPT-4 might be predisposed to predict
probabilities close to the midpoint of the scale, but our data do not support
this hypothesis. Overall, we find that GPT-4 significantly underperforms in
real-world predictive tasks compared to median human-crowd forecasts. A
potential explanation for this underperformance is that in real-world
forecasting tournaments, the true answers are genuinely unknown at the time of
prediction; unlike in other benchmark tasks like professional exams or time
series forecasting, where strong performance may at least partly be due to the
answers being memorized from the training data. This makes real-world
forecasting tournaments an ideal environment for testing the generalized
reasoning and prediction capabilities of artificial intelligence going forward.
","2023-10-23","2310.13014v1.pdf"
"2310.13016","Sengul Dogan","Turker Tuncer and Sengul Dogan and Mehmet Baygin and Prabal Datta
  Barua and Abdul Hafeez-Baig and Ru-San Tan and Subrata Chakraborty and U.
  Rajendra Acharya","Solving the multiplication problem of a large language model system
  using a graph-based method","9 pages, 3 figures","","","","cs.OH cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The generative pre-trained transformer (GPT)-based chatbot software ChatGPT
possesses excellent natural language processing capabilities but is inadequate
for solving arithmetic problems, especially multiplication. Its GPT structure
uses a computational graph for multiplication, which has limited accuracy
beyond simple multiplication operations. We developed a graph-based
multiplication algorithm that emulated human-like numerical operations by
incorporating a 10k operator, where k represents the maximum power to base 10
of the larger of two input numbers. Our proposed algorithm attained 100%
accuracy for 1,000,000 large number multiplication tasks, effectively solving
the multiplication challenge of GPT-based and other large language models. Our
work highlights the importance of blending simple human insights into the
design of artificial intelligence algorithms. Keywords: Graph-based
multiplication; ChatGPT; Multiplication problem
","2023-10-23","2310.13016v1.pdf"
"2310.13017","Nolan Dey","Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, Joel Hestness","Position Interpolation Improves ALiBi Extrapolation","4 pages content, 1 page references, 4 figures","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Linear position interpolation helps pre-trained models using rotary position
embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using
linear position interpolation to extend the extrapolation range of models using
Attention with Linear Biases (ALiBi). We find position interpolation
significantly improves extrapolation capability on upstream language modelling
and downstream summarization and retrieval tasks.
","2023-10-23","2310.13017v1.pdf"
"2310.13018","Ilia Sucholutsky","Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng,
  Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Jascha Achterberg,
  Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar,
  Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi (Richard) Zhang, Raja
  Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia
  Konkle, Thomas P. O'Connell, Thomas Unterthiner, Andrew K. Lampinen,
  Klaus-Robert M\""uller, Mariya Toneva, Thomas L. Griffiths","Getting aligned on representational alignment","Working paper, changes to be made in upcoming revisions","","","","q-bio.NC cs.AI cs.LG cs.NE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Biological and artificial information processing systems form representations
of the world that they can use to categorize, reason, plan, navigate, and make
decisions. To what extent do the representations formed by these diverse
systems agree? Can diverging representations still lead to the same behaviors?
And how can systems modify their representations to better match those of
another system? These questions pertaining to the study of
\textbf{\emph{representational alignment}} are at the heart of some of the most
active research areas in contemporary cognitive science, neuroscience, and
machine learning. Unfortunately, there is limited knowledge-transfer between
research communities interested in representational alignment, and much of the
progress in one field ends up being rediscovered independently in another, when
greater cross-field communication would be advantageous. To improve
communication between fields, we propose a unifying framework that can serve as
a common language between researchers studying representational alignment. We
survey the literature from the fields of cognitive science, neuroscience, and
machine learning, and demonstrate how prior work fits into this framework.
Finally, we lay out open problems in representational alignment where progress
can benefit all three fields. We hope that our work can catalyze
cross-disciplinary collaboration and accelerate progress for all communities
studying and developing information processing systems. We note that this is a
working paper and encourage readers to reach out with their suggestions for
future revisions.
","2023-10-23","2310.13018v1.pdf"
"2310.13021","Cedegao Zhang","Cedegao E. Zhang, Katherine M. Collins, Adrian Weller, Joshua B.
  Tenenbaum","AI for Mathematics: A Cognitive Science Perspective","","","","","q-bio.NC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Mathematics is one of the most powerful conceptual systems developed and used
by the human species. Dreams of automated mathematicians have a storied history
in artificial intelligence (AI). Rapid progress in AI, particularly propelled
by advances in large language models (LLMs), has sparked renewed, widespread
interest in building such systems. In this work, we reflect on these goals from
a \textit{cognitive science} perspective. We call attention to several
classical and ongoing research directions from cognitive science, which we
believe are valuable for AI practitioners to consider when seeking to build
truly human (or superhuman)-level mathematical systems. We close with open
discussions and questions that we believe necessitate a multi-disciplinary
perspective -- cognitive scientists working in tandem with AI researchers and
mathematicians -- as we move toward better mathematical AI systems which not
only help us push the frontier of the mathematics, but also offer glimpses into
how we as humans are even capable of such great cognitive feats.
","2023-10-23","2310.13021v1.pdf"
"2310.13022","Qiushi Sun","Jianing Wang, Qiushi Sun, Nuo Chen, Chengyu Wang, Jun Huang, Ming Gao,
  Xiang Li","Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised
  Language Understanding","Accepted by Findings of EMNLP 2023","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  The recent success of large pre-trained language models (PLMs) heavily hinges
on massive labeled data, which typically produces inferior performance in
low-resource scenarios. To remedy this dilemma, we study self-training as one
of the predominant semi-supervised learning (SSL) approaches, which utilizes
large-scale unlabeled data to generate synthetic examples. However, too many
noisy labels will hurt the model performance, and the self-training procedure
requires multiple training iterations making it more expensive if all the model
parameters of the PLM are updated. This paper presents UPET, a novel
Uncertainty-aware Parameter-Efficient self-Training framework to effectively
and efficiently address the labeled data scarcity issue. Specifically, we
incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to
perform uncertainty estimation for the teacher model and then judiciously
select reliable pseudo-labeled examples based on confidence and certainty.
During the student training, we introduce multiple parameter-efficient learning
(PEL) paradigms that allow the optimization of only a small percentage of
parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the
robustness and generalization. Extensive experiments over multiple downstream
tasks demonstrate that UPET achieves a substantial improvement in terms of
performance and efficiency. Our codes and data are released at https:
//github.com/wjn1996/UPET.
","2023-10-23","2310.13022v1.pdf"
"2310.13024","Gangwei Jiang","Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Y. Zhang, Jun Zhou,
  Defu Lian, Ying Wei","Towards Anytime Fine-tuning: Continually Pre-trained Language Models
  with Hypernetwork Prompt","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Continual pre-training has been urgent for adapting a pre-trained model to a
multitude of domains and tasks in the fast-evolving world. In practice, a
continually pre-trained model is expected to demonstrate not only greater
capacity when fine-tuned on pre-trained domains but also a non-decreasing
performance on unseen ones. In this work, we first investigate such anytime
fine-tuning effectiveness of existing continual pre-training approaches,
concluding with unanimously decreased performance on unseen domains. To this
end, we propose a prompt-guided continual pre-training method, where we train a
hypernetwork to generate domain-specific prompts by both agreement and
disagreement losses. The agreement loss maximally preserves the generalization
of a pre-trained model to new domains, and the disagreement one guards the
exclusiveness of the generated hidden states for each domain. Remarkably,
prompts by the hypernetwork alleviate the domain identity when fine-tuning and
promote knowledge transfer across domains. Our method achieved improvements of
3.57% and 3.4% on two real-world datasets (including domain shift and temporal
shift), respectively, demonstrating its efficacy.
","2023-10-23","2310.13024v1.pdf"
"2310.13031","Aykut Cayir Ph.D.","Abdullah Can Algan, Emre Y\""urekli, Aykut \c{C}ay{\i}r","A Use Case: Reformulating Query Rewriting as a Statistical Machine
  Translation Problem","","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  One of the most important challenges for modern search engines is to retrieve
relevant web content based on user queries. In order to achieve this challenge,
search engines have a module to rewrite user queries. That is why modern web
search engines utilize some statistical and neural models used in the natural
language processing domain. Statistical machine translation is a well-known NLP
method among them. The paper proposes a query rewriting pipeline based on a
monolingual machine translation model that learns to rewrite Arabic user search
queries. This paper also describes preprocessing steps to create a mapping
between user queries and web page titles.
","2023-10-23","2310.13031v1.pdf"
"2310.13032","Andrew Dai","Herbie Bradley, Andrew Dai, Hannah Teufel, Jenny Zhang, Koen
  Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth Stanley, Gr\'egory
  Schott, Joel Lehman","Quality-Diversity through AI Feedback","","","","","cs.CL cs.AI cs.LG cs.NE","http://creativecommons.org/licenses/by/4.0/","  In many text-generation problems, users may prefer not only a single
response, but a diverse range of high-quality outputs from which to choose.
Quality-diversity (QD) search algorithms aim at such outcomes, by continually
improving and diversifying a population of candidates. However, the
applicability of QD to qualitative domains, like creative writing, has been
limited by the difficulty of algorithmically specifying measures of quality and
diversity. Interestingly, recent developments in language models (LMs) have
enabled guiding search through AI feedback, wherein LMs are prompted in natural
language to evaluate qualitative aspects of text. Leveraging this development,
we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an
evolutionary algorithm applies LMs to both generate variation and evaluate the
quality and diversity of candidate text. When assessed on creative writing
domains, QDAIF covers more of a specified search space with high-quality
samples than do non-QD controls. Further, human evaluation of QDAIF-generated
creative texts validates reasonable agreement between AI and human evaluation.
Our results thus highlight the potential of AI feedback to guide open-ended
search for creative and original solutions, providing a recipe that seemingly
generalizes to many domains and modalities. In this way, QDAIF is a step
towards AI systems that can independently search, diversify, evaluate, and
improve, which are among the core skills underlying human society's capacity
for innovation.
","2023-10-23","2310.13032v1.pdf"
"2310.13040","Jonathan Crabb\'e","Jonathan Crabb\'e, Pau Rodr\'iguez, Vaishaal Shankar, Luca Zappella,
  Arno Blaas","Robust multimodal models have outlier features and encode more concepts","29 pages, 18 figures","","","","cs.LG cs.AI cs.CV","http://creativecommons.org/licenses/by/4.0/","  What distinguishes robust models from non-robust ones? This question has
gained traction with the appearance of large-scale multimodal models, such as
CLIP. These models have demonstrated unprecedented robustness with respect to
natural distribution shifts. While it has been shown that such differences in
robustness can be traced back to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 12 robust
multimodal models with various backbones (ResNets and ViTs) and pretraining
sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two
signatures of robustness in the representation spaces of these models: (1)
Robust models exhibit outlier features characterized by their activations, with
some being several orders of magnitude above average. These outlier features
induce privileged directions in the model's representation space. We
demonstrate that these privileged directions explain most of the predictive
power of the model by pruning up to $80 \%$ of the least important
representation space directions without negative impacts on model accuracy and
robustness; (2) Robust models encode substantially more concepts in their
representation space. While this superposition of concepts allows robust models
to store much information, it also results in highly polysemantic features,
which makes their interpretation challenging. We discuss how these insights
pave the way for future research in various fields, such as model pruning and
mechanistic interpretability.
","2023-10-23","2310.13040v1.pdf"
"2310.13061","Darshil Doshi","Darshil Doshi, Aritra Das, Tianyu He, Andrey Gromov","To grok or not to grok: Disentangling generalization and memorization on
  corrupted algorithmic datasets","24 pages, 22 figures, 2 tables","","","","cs.LG cond-mat.dis-nn stat.ML","http://creativecommons.org/licenses/by/4.0/","  Robust generalization is a major challenge in deep learning, particularly
when the number of trainable parameters is very large. In general, it is very
difficult to know if the network has memorized a particular set of examples or
understood the underlying rule (or both). Motivated by this challenge, we study
an interpretable model where generalizing representations are understood
analytically, and are easily distinguishable from the memorizing ones. Namely,
we consider two-layer neural networks trained on modular arithmetic tasks where
($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the
modular operations in the training set are incorrect). We show that (i) it is
possible for the network to memorize the corrupted labels \emph{and} achieve
$100\%$ generalization at the same time; (ii) the memorizing neurons can be
identified and pruned, lowering the accuracy on corrupted data and improving
the accuracy on uncorrupted data; (iii) regularization methods such as weight
decay, dropout and BatchNorm force the network to ignore the corrupted data
during optimization, and achieve $100\%$ accuracy on the uncorrupted dataset;
and (iv) the effect of these regularization methods is (``mechanistically'')
interpretable: weight decay and dropout force all the neurons to learn
generalizing representations, while BatchNorm de-amplifies the output of
memorizing neurons and amplifies the output of the generalizing ones. Finally,
we show that in the presence of regularization, the training dynamics involves
two consecutive stages: first, the network undergoes the \emph{grokking}
dynamics reaching high train \emph{and} test accuracy; second, it unlearns the
memorizing representations, where train accuracy suddenly jumps from $100\%$ to
$100 (1-\xi)\%$.
","2023-10-23","2310.13061v1.pdf"
"2310.13065","Mengdi Xu","Mengdi Xu, Peide Huang, Wenhao Yu, Shiqi Liu, Xilun Zhang, Yaru Niu,
  Tingnan Zhang, Fei Xia, Jie Tan, Ding Zhao","Creative Robot Tool Use with Large Language Models","19 pages, 14 figures, 2 tables","","","","cs.RO cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Tool use is a hallmark of advanced intelligence, exemplified in both animal
behavior and robotic capabilities. This paper investigates the feasibility of
imbuing robots with the ability to creatively use tools in tasks that involve
implicit physical constraints and long-term planning. Leveraging Large Language
Models (LLMs), we develop RoboTool, a system that accepts natural language
instructions and outputs executable code for controlling robots in both
simulated and real-world environments. RoboTool incorporates four pivotal
components: (i) an ""Analyzer"" that interprets natural language to discern key
task-related concepts, (ii) a ""Planner"" that generates comprehensive strategies
based on the language input and key concepts, (iii) a ""Calculator"" that
computes parameters for each skill, and (iv) a ""Coder"" that translates these
plans into executable Python code. Our results show that RoboTool can not only
comprehend explicit or implicit physical constraints and environmental factors
but also demonstrate creative tool use. Unlike traditional Task and Motion
Planning (TAMP) methods that rely on explicit optimization, our LLM-based
system offers a more flexible, efficient, and user-friendly solution for
complex robotics tasks. Through extensive experiments, we validate that
RoboTool is proficient in handling tasks that would otherwise be infeasible
without the creative use of tools, thereby expanding the capabilities of
robotic systems. Demos are available on our project page:
https://creative-robotool.github.io/.
","2023-10-23","2310.13065v1.pdf"
"2310.13106","Zhuoer Wang","Zhuoer Wang, Yicheng Wang, Ziwei Zhu, James Caverlee","Unsupervised Candidate Answer Extraction through Differentiable
  Masker-Reconstructor Model","EMNLP 2023 - Findings","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Question generation is a widely used data augmentation approach with
extensive applications, and extracting qualified candidate answers from context
passages is a critical step for most question generation systems. However,
existing methods for candidate answer extraction are reliant on linguistic
rules or annotated data that face the partial annotation issue and challenges
in generalization. To overcome these limitations, we propose a novel
unsupervised candidate answer extraction approach that leverages the inherent
structure of context passages through a Differentiable Masker-Reconstructor
(DMR) Model with the enforcement of self-consistency for picking up salient
information tokens. We curated two datasets with exhaustively-annotated answers
and benchmark a comprehensive set of supervised and unsupervised candidate
answer extraction methods. We demonstrate the effectiveness of the DMR model by
showing its performance is superior among unsupervised methods and comparable
to supervised methods.
","2023-10-23","2310.13106v1.pdf"
"2310.13120","Yuduo Wang","Yuduo Wang, Pedram Ghamisi","RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question
  Answering","Submitted to IEEE","","","","cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, with the rapid advancement of transformer models,
transformer-based multimodal architectures have found wide application in
various downstream tasks, including but not limited to Image Captioning, Visual
Question Answering (VQA), and Image-Text Generation. However, contemporary
approaches to Remote Sensing (RS) VQA often involve resource-intensive
techniques, such as full fine-tuning of large models or the extraction of
image-text features from pre-trained multimodal models, followed by modality
fusion using decoders. These approaches demand significant computational
resources and time, and a considerable number of trainable parameters are
introduced. To address these challenges, we introduce a novel method known as
RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter
comprises two key components: the Parallel Adapter and an additional linear
transformation layer inserted after each fully connected (FC) layer within the
Adapter. This approach not only improves adaptation to pre-trained multimodal
models but also allows the parameters of the linear transformation layer to be
integrated into the preceding FC layers during inference, reducing inference
costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive
series of experiments using three distinct RS-VQA datasets and achieve
state-of-the-art results on all three datasets. The code for RSAdapter will be
available online at https://github.com/Y-D-Wang/RSAdapter.
","2023-10-23","2310.13120v1.pdf"
"2310.13127","Zhihan Zhang","Zhihan Zhang, Shuohang Wang, Wenhao Yu, Yichong Xu, Dan Iter, Qingkai
  Zeng, Yang Liu, Chenguang Zhu, Meng Jiang","Auto-Instruct: Automatic Instruction Generation and Ranking for
  Black-Box Language Models","Accepted to EMNLP 2023 Findings. Work was done before July 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) can perform a wide range of tasks by following
natural language instructions, without the necessity of task-specific
fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by
the quality of these instructions, and manually writing effective instructions
for each task is a laborious and subjective process. In this paper, we
introduce Auto-Instruct, a novel method to automatically improve the quality of
instructions provided to LLMs. Our method leverages the inherent generative
ability of LLMs to produce diverse candidate instructions for a given task, and
then ranks them using a scoring model trained on a variety of 575 existing NLP
tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both
human-written instructions and existing baselines of LLM-generated
instructions. Furthermore, our method exhibits notable generalizability even
with other LLMs that are not incorporated into its training process.
","2023-10-23","2310.13127v1.pdf"
"2310.13183","Jianwei Li","Jianwei Li, Weizhi Gao, Qi Lei, Dongkuan Xu","Breaking through Deterministic Barriers: Randomized Pruning Mask
  Generation and Selection","","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  It is widely acknowledged that large and sparse models have higher accuracy
than small and dense models under the same model size constraints. This
motivates us to train a large model and then remove its redundant neurons or
weights by pruning. Most existing works pruned the networks in a deterministic
way, the performance of which solely depends on a single pruning criterion and
thus lacks variety. Instead, in this paper, we propose a model pruning strategy
that first generates several pruning masks in a designed random way.
Subsequently, along with an effective mask-selection rule, the optimal mask is
chosen from the pool of mask candidates. To further enhance efficiency, we
introduce an early mask evaluation strategy, mitigating the overhead associated
with training multiple masks. Our extensive experiments demonstrate that this
approach achieves state-of-the-art performance across eight datasets from GLUE,
particularly excelling at high levels of sparsity.
","2023-10-23","2310.13183v1.pdf"
"2310.13189","Barrett Lattimer","Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, Yi Yang","Fast and Accurate Factual Inconsistency Detection Over Long Documents","To be published in EMNLP 2023 Main Conference, 9 pages","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative AI models exhibit remarkable potential; however, hallucinations
across various tasks present a significant challenge, particularly for longer
inputs that current approaches struggle to address effectively. We introduce
SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a
task-agnostic model for detecting factual inconsistencies using a novel
chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI)
based model that uses large text chunks to condition over long texts. This
approach achieves state-of-the-art performance in factual inconsistency
detection for diverse tasks and long inputs. Additionally, we leverage the
chunking mechanism and employ a novel algorithm to explain SCALE's decisions
through relevant source sentence retrieval. Our evaluations reveal that SCALE
outperforms existing methods on both standard benchmarks and a new long-form
dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses
competitive systems in efficiency and model explanation evaluations. We have
released our code and data publicly to GitHub.
","2023-10-24","2310.13189v1.pdf"
"2310.13191","Jianwei Li","Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu","Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy
  for Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
","2023-10-23","2310.13191v1.pdf"
"2310.13192","Vincenzo Calderonio","Vincenzo Calderonio","The opaque law of artificial intelligence","17 pages, 7 figures","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  The purpose of this paper is to analyse the opacity of algorithms,
contextualized in the open debate on responsibility for artificial intelligence
causation; with an experimental approach by which, applying the proposed
conversational methodology of the Turing Test, we expect to evaluate the
performance of one of the best existing NLP model of generative AI (Chat-GPT)
to see how far it can go right now and how the shape of a legal regulation of
it could be. The analysis of the problem will be supported by a comment of
Italian classical law categories such as causality, intent and fault to
understand the problem of the usage of AI, focusing in particular on the
human-machine interaction. On the computer science side, for a technical point
of view of the logic used to craft these algorithms, in the second chapter will
be proposed a practical interrogation of Chat-GPT aimed at finding some
critical points of the functioning of AI. The end of the paper will concentrate
on some existing legal solutions which can be applied to the problem, plus a
brief description of the approach proposed by EU Artificial Intelligence act.
","2023-10-23","2310.13192v1.pdf"
"2310.13196","Jiani Zhang","Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Shen Wang,
  Huzefa Rangwala, George Karypis","NameGuess: Column Name Expansion for Tabular Data","This work has been accepted to EMNLP'23","","","","cs.CL cs.DB cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recent advances in large language models have revolutionized many sectors,
including the database industry. One common challenge when dealing with large
volumes of tabular data is the pervasive use of abbreviated column names, which
can negatively impact performance on various data search, access, and
understanding tasks. To address this issue, we introduce a new task, called
NameGuess, to expand column names (used in database schema) as a natural
language generation problem. We create a training dataset of 384K
abbreviated-expanded column pairs using a new data fabrication method and a
human-annotated evaluation benchmark that includes 9.2K examples from
real-world tables. To tackle the complexities associated with polysemy and
ambiguity in NameGuess, we enhance auto-regressive language models by
conditioning on table content and column header names -- yielding a fine-tuned
model (with 2.7B parameters) that matches human performance. Furthermore, we
conduct a comprehensive analysis (on multiple LLMs) to validate the
effectiveness of table content in NameGuess and identify promising future
opportunities. Code has been made available at
https://github.com/amazon-science/nameguess.
","2023-10-23","2310.13196v1.pdf"
"2310.13206","Yiwei Wang","Yiwei Wang, Yujun Cai, Muhao Chen, Yuxuan Liang, Bryan Hooi","Primacy Effect of ChatGPT","EMNLP 2023 short paper","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Instruction-tuned large language models (LLMs), such as ChatGPT, have led to
promising zero-shot performance in discriminative natural language
understanding (NLU) tasks. This involves querying the LLM using a prompt
containing the question, and the candidate labels to choose from. The
question-answering capabilities of ChatGPT arise from its pre-training on large
amounts of human-written text, as well as its subsequent fine-tuning on human
preferences, which motivates us to ask: Does ChatGPT also inherits humans'
cognitive biases? In this paper, we study the primacy effect of ChatGPT: the
tendency of selecting the labels at earlier positions as the answer. We have
two main findings: i) ChatGPT's decision is sensitive to the order of labels in
the prompt; ii) ChatGPT has a clearly higher chance to select the labels at
earlier positions as the answer. We hope that our experiments and analyses
provide additional insights into building more reliable ChatGPT-based
solutions. We release the source code at
https://github.com/wangywUST/PrimacyEffectGPT.
","2023-10-23","2310.13206v1.pdf"
"2310.13225","Arijit Sehanobish","Arijit Sehanobish, Krzysztof Choromanski, Yunfan Zhao, Avinava Dubey,
  Valerii Likhosherstov","Scalable Neural Network Kernels","Preprint. 23 pages, 10 figures. Comments welcome","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  We introduce the concept of scalable neural network kernels (SNNKs), the
replacements of regular feedforward layers (FFLs), capable of approximating the
latter, but with favorable computational properties. SNNKs effectively
disentangle the inputs from the parameters of the neural network in the FFL,
only to connect them in the final computation via the dot-product kernel. They
are also strictly more expressive, as allowing to model complicated
relationships beyond the functions of the dot-products of parameter-input
vectors. We also introduce the neural network bundling process that applies
SNNKs to compactify deep neural network architectures, resulting in additional
compression gains. In its extreme version, it leads to the fully bundled
network whose optimal parameters can be expressed via explicit formulae for
several loss functions (e.g. mean squared error), opening a possibility to
bypass backpropagation. As a by-product of our analysis, we introduce the
mechanism of the universal random features (or URFs), applied to instantiate
several SNNK variants, and interesting on its own in the context of scalable
kernel methods. We provide rigorous theoretical analysis of all these concepts
as well as an extensive empirical evaluation, ranging from point-wise kernel
estimation to Transformers' fine-tuning with novel adapter layers inspired by
SNNKs. Our mechanism provides up to 5x reduction in the number of trainable
parameters, while maintaining competitive accuracy.
","2023-10-23","2310.13225v1.pdf"
"2310.13226","S M Wahidur Rahman","Rahman S M Wahidur, Ishmam Tashdeed, Manjit Kaur, Heung-No-Lee","Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and
  Prompt Engineering","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Blockchain technology has revolutionized the financial landscape, with
cryptocurrencies gaining widespread adoption for their decentralized and
transparent nature. As the sentiment expressed on social media platforms can
significantly influence cryptocurrency discussions and market movements,
sentiment analysis has emerged as a crucial tool for understanding public
opinion and predicting market trends. Motivated by the aim to enhance sentiment
analysis accuracy in the cryptocurrency domain, this paper investigates
fine-tuning techniques on large language models. This paper also investigates
the efficacy of supervised fine-tuning and instruction-based fine-tuning on
large language models for unseen tasks. Experimental results demonstrate a
significant average zero-shot performance gain of 40% after fine-tuning,
highlighting the potential of this technique in optimizing pre-trained language
model efficiency. Additionally, the impact of instruction tuning on models of
varying scales is examined, revealing that larger models benefit from
instruction tuning, achieving the highest average accuracy score of 75.16%. In
contrast, smaller-scale models may experience reduced generalization due to the
complete utilization of model capacity. To gain deeper insight about how
instruction works with these language models, this paper presents an
experimental investigation into the response of an instruction-based model
under different instruction tuning setups. The investigation demonstrates that
the model achieves an average accuracy score of 72.38% for short and simple
instructions. This performance significantly outperforms its accuracy under
long and complex instructions by over 12%, thereby effectively highlighting the
profound significance of instruction characteristics in maximizing model
performance.
","2023-10-23","2310.13226v1.pdf"
"2310.13227","Yuchen Zhuang","Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn,
  Ryan A. Rossi, Somdeb Sarkhel, Chao Zhang","ToolChain*: Efficient Action Space Navigation in Large Language Models
  with A* Search","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have demonstrated powerful decision-making and
planning capabilities in solving complicated real-world problems. LLM-based
autonomous agents can interact with diverse tools (e.g., functional APIs) and
generate solution plans that execute a series of API function calls in a
step-by-step manner. The multitude of candidate API function calls
significantly expands the action space, amplifying the critical need for
efficient action space navigation. However, existing methods either struggle
with unidirectional exploration in expansive action spaces, trapped into a
locally optimal solution, or suffer from exhaustively traversing all potential
actions, causing inefficient navigation. To address these issues, we propose
ToolChain*, an efficient tree search-based planning algorithm for LLM-based
agents. It formulates the entire action space as a decision tree, where each
node represents a possible API function call involved in a solution plan. By
incorporating the A* search algorithm with task-specific cost function design,
it efficiently prunes high-cost branches that may involve incorrect actions,
identifying the most low-cost valid path as the solution. Extensive experiments
on multiple tool-use and reasoning tasks demonstrate that ToolChain*
efficiently balances exploration and exploitation within an expansive action
space. It outperforms state-of-the-art baselines on planning and reasoning
tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time,
respectively.
","2023-10-23","2310.13227v1.pdf"
"2310.13228","Atnafu Lambebo Tonja","Hellina Hailu Nigatu, Atnafu Lambebo Tonja, Jugal Kalita","The Less the Merrier? Investigating Language Representation in
  Multilingual Models","Accepted to EMNLP 2023(Findings)","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Multilingual Language Models offer a way to incorporate multiple languages in
one model and utilize cross-language transfer learning to improve performance
for different Natural Language Processing (NLP) tasks. Despite progress in
multilingual models, not all languages are supported as well, particularly in
low-resource settings. In this work, we investigate the linguistic
representation of different languages in multilingual models. We start by
asking the question which languages are supported in popular multilingual
models and which languages are left behind. Then, for included languages, we
look at models' learned representations based on language family and dialect
and try to understand how models' learned representations for~(1) seen and~(2)
unseen languages vary across different language groups. In addition, we test
and analyze performance on downstream tasks such as text generation and Named
Entity Recognition. We observe from our experiments that community-centered
models -- models that focus on languages of a given family or geographical
location and are built by communities who speak them -- perform better at
distinguishing between languages in the same family for low-resource languages.
Our paper contributes to the literature in understanding multilingual models
and their shortcomings and offers insights on potential ways to improve them.
","2023-10-23","2310.13228v1.pdf"
"2310.13229","Sungmin Kang","Jae Yong Lee, Sungmin Kang, Juyeon Yoon, Shin Yoo","The GitHub Recent Bugs Dataset for Evaluating LLM-based Debugging
  Applications","","","","","cs.SE","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have demonstrated strong natural language
processing and code synthesis capabilities, which has led to their rapid
adoption in software engineering applications. However, details about LLM
training data are often not made public, which has caused concern as to whether
existing bug benchmarks are included. In lieu of the training data for the
popular GPT models, we examine the training data of the open-source LLM
StarCoder, and find it likely that data from the widely used Defects4J
benchmark was included, raising the possibility of its inclusion in GPT
training data as well. This makes it difficult to tell how well LLM-based
results on Defects4J would generalize, as for any results it would be unclear
whether a technique's performance is due to LLM generalization or memorization.
To remedy this issue and facilitate continued research on LLM-based SE, we
present the GitHub Recent Bugs (GHRB) dataset, which includes 76 real-world
Java bugs that were gathered after the OpenAI data cut-off point.
","2023-10-23","2310.13229v1.pdf"
"2310.13231","Dawei Li","Dawei Li, Hengyuan Zhang, Yanran Li, Shiping Yang","Multi-level Contrastive Learning for Script-based Character
  Understanding","Accepted by EMNLP 2023 main conference; Camera-ready version will be
  updated soon","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  In this work, we tackle the scenario of understanding characters in scripts,
which aims to learn the characters' personalities and identities from their
utterances. We begin by analyzing several challenges in this scenario, and then
propose a multi-level contrastive learning framework to capture characters'
global information in a fine-grained manner. To validate the proposed
framework, we conduct extensive experiments on three character understanding
sub-tasks by comparing with strong pre-trained language models, including
SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate
that our method improves the performances by a considerable margin. Through
further in-depth analysis, we show the effectiveness of our method in
addressing the challenges and provide more hints on the scenario of character
understanding. We will open-source our work on github at
https://github.com/David-Li0406/Script-based-Character-Understanding.
","2023-10-23","2310.13231v1.pdf"
"2310.13243","Shengyao Zhuang","Shengyao Zhuang and Bing Liu and Bevan Koopman and Guido Zuccon","Open-source Large Language Models are Strong Zero-shot Query Likelihood
  Models for Document Ranking","5 pages","","","","cs.IR cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In the field of information retrieval, Query Likelihood Models (QLMs) rank
documents based on the probability of generating the query given the content of
a document. Recently, advanced large language models (LLMs) have emerged as
effective QLMs, showcasing promising ranking capabilities. This paper focuses
on investigating the genuine zero-shot ranking effectiveness of recent LLMs,
which are solely pre-trained on unstructured text data without supervised
instruction fine-tuning. Our findings reveal the robust zero-shot ranking
ability of such LLMs, highlighting that additional instruction fine-tuning may
hinder effectiveness unless a question generation task is present in the
fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking
system that integrates LLM-based QLMs with a hybrid zero-shot retriever,
demonstrating exceptional effectiveness in both zero-shot and few-shot
scenarios. We make our codebase publicly available at
https://github.com/ielab/llm-qlm.
","2023-10-23","2310.13243v1.pdf"
"2310.13255","Sipeng Zheng","Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu","Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in
  Open Worlds","19 pages, 14 figures","","","","cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to ""a blindfolded text-based game.""
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
","2023-10-23","2310.13255v1.pdf"
"2310.13259","David Steiner","Jeremy Lai, Faruk Ahmed, Supriya Vijay, Tiam Jaroensri, Jessica Loo,
  Saurabh Vyawahare, Saloni Agarwal, Fayaz Jamil, Yossi Matias, Greg S.
  Corrado, Dale R. Webster, Jonathan Krause, Yun Liu, Po-Hsuan Cameron Chen,
  Ellery Wulczyn, David F. Steiner","Domain-specific optimization and diverse evaluation of self-supervised
  models for histopathology","4 main tables, 3 main figures, additional supplemental tables and
  figures","","","","eess.IV cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Task-specific deep learning models in histopathology offer promising
opportunities for improving diagnosis, clinical research, and precision
medicine. However, development of such models is often limited by availability
of high-quality data. Foundation models in histopathology that learn general
representations across a wide range of tissue types, diagnoses, and
magnifications offer the potential to reduce the data, compute, and technical
expertise necessary to develop task-specific deep learning models with the
required level of model performance. In this work, we describe the development
and evaluation of foundation models for histopathology via self-supervised
learning (SSL). We first establish a diverse set of benchmark tasks involving
17 unique tissue types and 12 unique cancer types and spanning different
optimal magnifications and task types. Next, we use this benchmark to explore
and evaluate histopathology-specific SSL methods followed by further evaluation
on held out patch-level and weakly supervised tasks. We found that standard SSL
methods thoughtfully applied to histopathology images are performant across our
benchmark tasks and that domain-specific methodological improvements can
further increase performance. Our findings reinforce the value of using
domain-specific SSL methods in pathology, and establish a set of high quality
foundation models to enable further research across diverse applications.
","2023-10-23","2310.13259v1.pdf"
"2310.13262","Xue Zhang","Xue Zhang, Songming Zhang, Yunlong Liang, Yufeng Chen, Jian Liu,
  Wenjuan Han, Jinan Xu","A Quality-based Syntactic Template Retriever for
  Syntactically-controlled Paraphrase Generation","Accepted to EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Existing syntactically-controlled paraphrase generation (SPG) models perform
promisingly with human-annotated or well-chosen syntactic templates. However,
the difficulty of obtaining such templates actually hinders the practical
application of SPG models. For one thing, the prohibitive cost makes it
unfeasible to manually design decent templates for every source sentence. For
another, the templates automatically retrieved by current heuristic methods are
usually unreliable for SPG models to generate qualified paraphrases. To escape
this dilemma, we propose a novel Quality-based Syntactic Template Retriever
(QSTR) to retrieve templates based on the quality of the to-be-generated
paraphrases. Furthermore, for situations requiring multiple paraphrases for
each source sentence, we design a Diverse Templates Search (DTS) algorithm,
which can enhance the diversity between paraphrases without sacrificing
quality. Experiments demonstrate that QSTR can significantly surpass existing
retrieval methods in generating high-quality paraphrases and even perform
comparably with human-annotated templates in terms of reference-free metrics.
Additionally, human evaluation and the performance on downstream tasks using
our generated paraphrases for data augmentation showcase the potential of our
QSTR and DTS algorithm in practical scenarios.
","2023-10-23","2310.13262v1.pdf"
"2310.13265","Le Zhang","Le Zhang, Yihong Wu, Fengran Mo, Jian-Yun Nie, Aishwarya Agrawal","MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with
  Large Language Model","Accepted into EMNLP2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Multi-modal open-domain question answering typically requires evidence
retrieval from databases across diverse modalities, such as images, tables,
passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this
task. To enable LLMs to tackle the task in a zero-shot manner, we introduce
MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer
strategy that bypasses intricate multi-modality ranking, our framework can
accommodate new modalities and seamlessly transition to new models for the
task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each
modality separately, then fuses this multi-modal information using LLMs to
produce a final answer. Our methodology boosts performance on the MMCoQA
dataset, improving F1 by +37.91 points and EM by +34.07 points over the
supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the
zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and
significantly closes the gap with supervised methods. Our codebase is available
at https://github.com/lezhang7/MOQAGPT.
","2023-10-23","2310.13265v1.pdf"
"2310.13267","Mengjie Zhao","Mengjie Zhao, Junya Ono, Zhi Zhong, Chieh-Hsin Lai, Yuhta Takida,
  Naoki Murata, Wei-Hsiang Liao, Takashi Shibuya, Hiromi Wakaki, Yuki Mitsufuji","On the Language Encoder of Contrastive Cross-modal Models","","","","","cs.CL cs.CV cs.LG cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  Contrastive cross-modal models such as CLIP and CLAP aid various
vision-language (VL) and audio-language (AL) tasks. However, there has been
limited investigation of and improvement in their language encoder, which is
the central component of encoding natural language descriptions of image/audio
into vector representations. We extensively evaluate how unsupervised and
supervised sentence embedding training affect language encoder quality and
cross-modal task performance. In VL pretraining, we found that sentence
embedding training language encoder quality and aids in cross-modal tasks,
improving contrastive VL models such as CyCLIP. In contrast, AL pretraining
benefits less from sentence embedding training, which may result from the
limited amount of pretraining data. We analyze the representation spaces to
understand the strengths of sentence embedding training, and find that it
improves text-space uniformity, at the cost of decreased cross-modal alignment.
","2023-10-23","2310.13267v1.pdf"
"2310.13289","Changli Tang","Changli Tang and Wenyi Yu and Guangzhi Sun and Xianzhao Chen and Tian
  Tan and Wei Li and Lu Lu and Zejun Ma and Chao Zhang","SALMONN: Towards Generic Hearing Abilities for Large Language Models","","","","","cs.SD cs.CL eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Hearing is arguably an essential ability of artificial intelligence (AI)
agents in the physical world, which refers to the perception and understanding
of general auditory information consisting of at least three types of sounds:
speech, audio events, and music. In this paper, we propose SALMONN, a speech
audio language music open neural network, built by integrating a pre-trained
text-based large language model (LLM) with speech and audio encoders into a
single multimodal model. SALMONN enables the LLM to directly process and
understand general audio inputs and achieve competitive performances on a
number of speech and audio tasks used in training, such as automatic speech
recognition and translation, auditory-information-based question answering,
emotion recognition, speaker verification, and music and audio captioning
\textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in
the training, which includes but is not limited to speech translation to
untrained languages, speech-based slot filling, spoken-query-based question
answering, audio-based storytelling, and speech audio co-reasoning
\textit{etc}. The presence of the cross-modal emergent abilities is studied,
and a novel few-shot activation tuning approach is proposed to activate such
abilities of SALMONN. To our knowledge, SALMONN is the first model of its type
and can be regarded as a step towards AI with generic hearing abilities. An
interactive demo of SALMONN is available at
\texttt{\url{https://github.com/bytedance/SALMONN}}, and the training code and
model checkpoints will be released upon acceptance.
","2023-10-23","2310.13289v1.pdf"
"2310.13290","Zijie Wang","Zijie Wang, Md Mosharaf Hossain, Shivam Mathur, Terry Cruz Melo, Kadir
  Bulut Ozler, Keun Hee Park, Jacob Quintero, MohammadHossein Rezaei, Shreya
  Nupur Shakya, Md Nayem Uddin, Eduardo Blanco","Interpreting Indirect Answers to Yes-No Questions in Multiple Languages","Accepted to EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Yes-no questions expect a yes or no for an answer, but people often skip
polar keywords. Instead, they answer with long explanations that must be
interpreted. In this paper, we focus on this challenging problem and release
new benchmarks in eight languages. We present a distant supervision approach to
collect training data. We also demonstrate that direct answers (i.e., with
polar keywords) are useful to train models to interpret indirect answers (i.e.,
without polar keywords). Experimental results demonstrate that monolingual
fine-tuning is beneficial if training data can be obtained via distant
supervision for the language of interest (5 languages). Additionally, we show
that cross-lingual fine-tuning is always beneficial (8 languages).
","2023-10-23","2310.13290v1.pdf"
"2310.13291","Ruixiang Tang","Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A Inan, Janardhan
  Kulkarni, Xia Hu","Assessing Privacy Risks in Language Models: A Case Study on
  Summarization Tasks","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models have revolutionized the field of NLP by achieving
state-of-the-art performance on various tasks. However, there is a concern that
these models may disclose information in the training data. In this study, we
focus on the summarization task and investigate the membership inference (MI)
attack: given a sample and black-box access to a model's API, it is possible to
determine if the sample was part of the training data. We exploit text
similarity and the model's resistance to document modifications as potential MI
signals and evaluate their effectiveness on widely used datasets. Our results
demonstrate that summarization models are at risk of exposing data membership,
even in cases where the reference summary is not available. Furthermore, we
discuss several safeguards for training summarization models to protect against
MI attacks and discuss the inherent trade-off between privacy and utility.
","2023-10-23","2310.13291v1.pdf"
"2310.13297","Chenkai Sun","Chenkai Sun, Jinning Li, Yi R. Fung, Hou Pong Chan, Tarek Abdelzaher,
  ChengXiang Zhai, Heng Ji","Decoding the Silent Majority: Inducing Belief Augmented Social Graph
  with Large Language Model for Response Forecasting","Accepted at EMNLP 2023 Main Conference","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Automatic response forecasting for news media plays a crucial role in
enabling content producers to efficiently predict the impact of news releases
and prevent unexpected negative outcomes such as social conflict and moral
injury. To effectively forecast responses, it is essential to develop measures
that leverage the social dynamics and contextual information surrounding
individuals, especially in cases where explicit profiles or historical actions
of the users are limited (referred to as lurkers). As shown in a previous
study, 97% of all tweets are produced by only the most active 25% of users.
However, existing approaches have limited exploration of how to best process
and utilize these important features. To address this gap, we propose a novel
framework, named SocialSense, that leverages a large language model to induce a
belief-centered graph on top of an existent social network, along with
graph-based propagation to capture social dynamics. We hypothesize that the
induced graph that bridges the gap between distant users who share similar
beliefs allows the model to effectively capture the response patterns. Our
method surpasses existing state-of-the-art in experimental evaluations for both
zero-shot and supervised settings, demonstrating its effectiveness in response
forecasting. Moreover, the analysis reveals the framework's capability to
effectively handle unseen user and lurker scenarios, further highlighting its
robustness and practical applicability.
","2023-10-23","2310.13297v1.pdf"
"2310.13307","Soyeong Jeong","Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park","Test-Time Self-Adaptive Small Language Models for Question Answering","EMNLP Findings 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent instruction-finetuned large language models (LMs) have achieved
notable performances in various tasks, such as question-answering (QA).
However, despite their ability to memorize a vast amount of general knowledge
across diverse tasks, they might be suboptimal on specific tasks due to their
limited capacity to transfer and adapt knowledge to target tasks. Moreover,
further finetuning LMs with labeled datasets is often infeasible due to their
absence, but it is also questionable if we can transfer smaller LMs having
limited knowledge only with unlabeled test data. In this work, we show and
investigate the capabilities of smaller self-adaptive LMs, only with unlabeled
test data. In particular, we first stochastically generate multiple answers,
and then ensemble them while filtering out low-quality samples to mitigate
noise from inaccurate labels. Our proposed self-adaption strategy demonstrates
significant performance improvements on benchmark QA datasets with higher
robustness across diverse prompts, enabling LMs to stay stable. Code is
available at: https://github.com/starsuzi/T-SAS.
","2023-10-23","2310.13307v1.pdf"
"2310.13312","Jaeyoung Choe","Jaeyoung Choe, Keonwoong Noh, Nayeon Kim, Seyun Ahn, Woohwan Jung","Exploring the Impact of Corpus Diversity on Financial Pretrained
  Language Models","Accepted to EMNLP 2023 (Findings)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Over the past few years, various domain-specific pretrained language models
(PLMs) have been proposed and have outperformed general-domain PLMs in
specialized areas such as biomedical, scientific, and clinical domains. In
addition, financial PLMs have been studied because of the high economic impact
of financial data analysis. However, we found that financial PLMs were not
pretrained on sufficiently diverse financial data. This lack of diverse
training data leads to a subpar generalization performance, resulting in
general-purpose PLMs, including BERT, often outperforming financial PLMs on
many downstream tasks. To address this issue, we collected a broad range of
financial corpus and trained the Financial Language Model (FiLM) on these
diverse datasets. Our experimental results confirm that FiLM outperforms not
only existing financial PLMs but also general domain PLMs. Furthermore, we
provide empirical evidence that this improvement can be achieved even for
unseen corpus groups.
","2023-10-23","2310.13312v1.pdf"
"2310.13315","Qihuang Zhong","Miaoxi Zhu, Qihuang Zhong, Li Shen, Liang Ding, Juhua Liu, Bo Du,
  Dacheng Tao","Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models","Accepted to EMNLP2023 (Main). Miaoxi Zhu and Qihuang Zhong contribute
  equally to this work","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Quantization is a promising approach for reducing memory overhead and
accelerating inference, especially in large pre-trained language model (PLM)
scenarios. While having no access to original training data due to security and
privacy concerns has emerged the demand for zero-shot quantization. Most of the
cutting-edge zero-shot quantization methods primarily 1) apply to computer
vision tasks, and 2) neglect of overfitting problem in the generative
adversarial learning process, leading to sub-optimal performance. Motivated by
this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)
framework for the zero-shot quantization of various PLMs. The key algorithm in
solving ZSAQ is the SAM-SGA optimization, which aims to improve the
quantization accuracy and model generalization via optimizing a minimax
problem. We theoretically prove the convergence rate for the minimax
optimization problem and this result can be applied to other nonconvex-PL
minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate
that our method brings consistent and significant performance gains on both
discriminative and generative PLMs, i.e., up to +6.98 average score.
Furthermore, we empirically validate that our method can effectively improve
the model generalization.
","2023-10-23","2310.13315v1.pdf"
"2310.13332","Zhaoyang Wang","Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song,
  Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang","Democratizing Reasoning Ability: Tailored Learning from Large Language
  Model","To appear at EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) exhibit impressive emergent abilities in natural
language processing, but their democratization is hindered due to huge
computation requirements and closed-source nature. Recent research on advancing
open-source smaller LMs by distilling knowledge from black-box LLMs has
obtained promising results in the instruction-following ability. However, the
reasoning ability which is more challenging to foster, is relatively rarely
explored. In this paper, we propose a tailored learning approach to distill
such reasoning ability to smaller LMs to facilitate the democratization of the
exclusive reasoning ability. In contrast to merely employing LLM as a data
annotator, we exploit the potential of LLM as a reasoning teacher by building
an interactive multi-round learning paradigm. This paradigm enables the student
to expose its deficiencies to the black-box teacher who then can provide
customized training data in return. Further, to exploit the reasoning potential
of the smaller LM, we propose self-reflection learning to motivate the student
to learn from self-made mistakes. The learning from self-reflection and LLM are
all tailored to the student's learning status, thanks to the seamless
integration with the multi-round learning paradigm. Comprehensive experiments
and analysis on mathematical and commonsense reasoning tasks demonstrate the
effectiveness of our method. The code will be available at
https://github.com/Raibows/Learn-to-Reason.
","2023-10-23","2310.13332v1.pdf"
"2310.13343","Yuxuan Zhao","Xiaoliang Chen, Liangbin Li, Le Chang, Yunhe Huang, Yuxuan Zhao,
  Yuxiao Zhang, Dinuo Li","Challenges and Contributing Factors in the Utilization of Large Language
  Models (LLMs)","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the development of large language models (LLMs) like the GPT series,
their widespread use across various application scenarios presents a myriad of
challenges. This review initially explores the issue of domain specificity,
where LLMs may struggle to provide precise answers to specialized questions
within niche fields. The problem of knowledge forgetting arises as these LLMs
might find it hard to balance old and new information. The knowledge repetition
phenomenon reveals that sometimes LLMs might deliver overly mechanized
responses, lacking depth and originality. Furthermore, knowledge illusion
describes situations where LLMs might provide answers that seem insightful but
are actually superficial, while knowledge toxicity focuses on harmful or biased
information outputs. These challenges underscore problems in the training data
and algorithmic design of LLMs. To address these issues, it's suggested to
diversify training data, fine-tune models, enhance transparency and
interpretability, and incorporate ethics and fairness training. Future
technological trends might lean towards iterative methodologies, multimodal
learning, model personalization and customization, and real-time learning and
feedback mechanisms. In conclusion, future LLMs should prioritize fairness,
transparency, and ethics, ensuring they uphold high moral and ethical standards
when serving humanity.
","2023-10-23","2310.13343v1.pdf"
"2310.13345","Xilie Xu","Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang,
  Mohan Kankanhalli","An LLM can Fool Itself: A Prompt-Based Adversarial Attack","","","","","cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The wide-ranging applications of large language models (LLMs), especially in
safety-critical domains, necessitate the proper evaluation of the LLM's
adversarial robustness. This paper proposes an efficient tool to audit the
LLM's adversarial robustness via a prompt-based adversarial attack
(PromptAttack). PromptAttack converts adversarial textual attacks into an
attack prompt that can cause the victim LLM to output the adversarial sample to
fool itself. The attack prompt is composed of three important components: (1)
original input (OI) including the original sample and its ground-truth label,
(2) attack objective (AO) illustrating a task description of generating a new
sample that can fool itself without changing the semantic meaning, and (3)
attack guidance (AG) containing the perturbation instructions to guide the LLM
on how to complete the task by perturbing the original sample at character,
word, and sentence levels, respectively. Besides, we use a fidelity filter to
ensure that PromptAttack maintains the original semantic meanings of the
adversarial examples. Further, we enhance the attack power of PromptAttack by
ensembling adversarial examples at different perturbation levels. Comprehensive
empirical results using Llama2 and GPT-3.5 validate that PromptAttack
consistently yields a much higher attack success rate compared to AdvGLUE and
AdvGLUE++. Interesting findings include that a simple emoji can easily mislead
GPT-3.5 to make wrong predictions.
","2023-10-23","2310.13345v1.pdf"
"2310.13355","Muhammad Ferjad Naeem","Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc
  Van Gool, Federico Tombari","SILC: Improving Vision Language Pretraining with Self-Distillation","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Image-Text pretraining on web-scale image caption dataset has become the
default recipe for open vocabulary classification and retrieval models thanks
to the success of CLIP and its variants. Several works have also used CLIP
features for dense prediction tasks and have shown the emergence of open-set
abilities. However, the contrastive objective only focuses on image-text
alignment and does not incentivise image feature learning for dense prediction
tasks. In this work, we propose the simple addition of local-to-global
correspondence learning by self-distillation as an additional objective for
contrastive pre-training to propose SILC. We show that distilling local image
features from an exponential moving average (EMA) teacher model significantly
improves model performance on several computer vision tasks including
classification, retrieval, and especially segmentation. We further show that
SILC scales better with the same training duration compared to the baselines.
Our model SILC sets a new state of the art for zero-shot classification, few
shot classification, image and text retrieval, zero-shot segmentation, and open
vocabulary segmentation.
","2023-10-23","2310.13355v1.pdf"
"2310.13361","Wenyu Guo","Wenyu Guo, Qingkai Fang, Dong Yu, Yang Feng","Bridging the Gap between Synthetic and Authentic Images for Multimodal
  Machine Translation","Accepted to EMNLP 2023 main conference","","","","cs.CV cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Multimodal machine translation (MMT) simultaneously takes the source sentence
and a relevant image as input for translation. Since there is no paired image
available for the input sentence in most cases, recent studies suggest
utilizing powerful text-to-image generation models to provide image inputs.
Nevertheless, synthetic images generated by these models often follow different
distributions compared to authentic images. Consequently, using authentic
images for training and synthetic images for inference can introduce a
distribution shift, resulting in performance degradation during inference. To
tackle this challenge, in this paper, we feed synthetic and authentic images to
the MMT model, respectively. Then we minimize the gap between the synthetic and
authentic images by drawing close the input image representations of the
Transformer Encoder and the output distributions of the Transformer Decoder.
Therefore, we mitigate the distribution disparity introduced by the synthetic
images during inference, thereby freeing the authentic images from the
inference process.Experimental results show that our approach achieves
state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while
remaining independent of authentic images during inference.
","2023-10-23","2310.13361v1.pdf"
"2310.13362","Junjie Wu","Junjie Wu, Lemao Liu, Dit-Yan Yeung","Towards General Error Diagnosis via Behavioral Testing in Machine
  Translation","15 pages, 2 figures, accepted by Findings of EMNLP 2023","","","","cs.CL cs.AI cs.LG cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Behavioral testing offers a crucial means of diagnosing linguistic errors and
assessing capabilities of NLP models. However, applying behavioral testing to
machine translation (MT) systems is challenging as it generally requires human
efforts to craft references for evaluating the translation quality of such
systems on newly generated test cases. Existing works in behavioral testing of
MT systems circumvent this by evaluating translation quality without
references, but this restricts diagnosis to specific types of errors, such as
incorrect translation of single numeric or currency words. In order to diagnose
general errors, this paper proposes a new Bilingual Translation Pair Generation
based Behavior Testing (BTPGBT) framework for conducting behavioral testing of
MT systems. The core idea of BTPGBT is to employ a novel bilingual translation
pair generation (BTPG) approach that automates the construction of high-quality
test cases and their pseudoreferences. Experimental results on various MT
systems demonstrate that BTPGBT could provide comprehensive and accurate
behavioral testing results for general error diagnosis, which further leads to
several insightful findings. Our code and data are available at https:
//github.com/wujunjie1998/BTPGBT.
","2023-10-23","2310.13362v1.pdf"
"2310.13385","Haoran Li","Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei","Tuna: Instruction Tuning using Feedback from Large Language Models","EMNLP 2023, code and data are available at
  https://github.com/microsoft/LMOps","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Instruction tuning of open-source large language models (LLMs) like LLaMA,
using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4,
has proven to be a cost-effective way to align model behaviors with human
preferences. However, the instruction-tuned model has only seen one response
per instruction, lacking the knowledge of potentially better responses. In this
paper, we propose finetuning an instruction-tuned LLM using our novel
\textit{probabilistic ranking} and \textit{contextual ranking} approaches to
increase the likelihood of generating better responses. Probabilistic ranking
enables the instruction-tuned model to inherit the relative rankings of
high-quality and low-quality responses from the teacher LLM. On the other hand,
learning with contextual ranking allows the model to refine its own response
distribution using the contextual understanding ability of stronger LLMs.
Furthermore, we apply probabilistic ranking and contextual ranking sequentially
to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna},
consistently improves the performance on Super Natural Instructions (119 test
tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results
than several strong reinforcement learning baselines. Our code and data are
available at \url{ https://github.com/microsoft/LMOps}.
","2023-10-23","2310.13385v1.pdf"
"2310.13394","Chang Shu","Chang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier","POSQA: Probe the World Models of LLMs with Size Comparisons","Accepted by EMNLP 2023 Findings","","","","cs.CL cs.AI cs.CY","http://creativecommons.org/licenses/by/4.0/","  Embodied language comprehension emphasizes that language understanding is not
solely a matter of mental processing in the brain but also involves
interactions with the physical and social environment. With the explosive
growth of Large Language Models (LLMs) and their already ubiquitous presence in
our daily lives, it is becoming increasingly necessary to verify their
real-world understanding. Inspired by cognitive theories, we propose POSQA: a
Physical Object Size Question Answering dataset with simple size comparison
questions to examine the extremity and analyze the potential mechanisms of the
embodied comprehension of the latest LLMs.
  We show that even the largest LLMs today perform poorly under the zero-shot
setting. We then push their limits with advanced prompting techniques and
external knowledge augmentation. Furthermore, we investigate whether their
real-world comprehension primarily derives from contextual information or
internal weights and analyse the impact of prompt formats and report bias of
different objects. Our results show that real-world understanding that LLMs
shaped from textual data can be vulnerable to deception and confusion by the
surface form of prompts, which makes it less aligned with human behaviours.
","2023-10-23","2310.13394v1.pdf"
"2310.13395","Ilias Marios Stogiannidis","Ilias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion
  Androutsopoulos","Cache me if you Can: an Online Cost-aware Teacher-Student framework to
  Reduce the Calls to Large Language Models","Short paper (5 pages), accepted at Findings of EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Prompting Large Language Models (LLMs) performs impressively in zero- and
few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot
afford the cost of creating large task-specific training datasets, but also the
cost of pretraining their own LLMs, are increasingly turning to third-party
services that allow them to prompt LLMs. However, such services currently
require a payment per call, which becomes a significant operating expense
(OpEx). Furthermore, customer inputs are often very similar over time, hence
SMEs end-up prompting LLMs with very similar instances. We propose a framework
that allows reducing the calls to LLMs by caching previous LLM responses and
using them to train a local inexpensive model on the SME side. The framework
includes criteria for deciding when to trust the local model or call the LLM,
and a methodology to tune the criteria and measure the tradeoff between
performance and cost. For experimental purposes, we instantiate our framework
with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN
classifier or a Multi-Layer Perceptron, using two common business tasks, intent
recognition and sentiment analysis. Experimental results indicate that
significant OpEx savings can be obtained with only slightly lower performance.
","2023-10-23","2310.13395v1.pdf"
"2310.13398","Yijie Zhou","Yijie Zhou, Likun Cai, Xianhui Cheng, Zhongxue Gan, Xiangyang Xue, and
  Wenchao Ding","OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D
  Data","The source code will be released at
  https://github.com/Fudan-ProjectTitan/OpenAnnotate3D","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the era of big data and large models, automatic annotating functions for
multi-modal data are of great significance for real-world AI-driven
applications, such as autonomous driving and embodied AI. Unlike traditional
closed-set annotation, open-vocabulary annotation is essential to achieve
human-level cognition capability. However, there are few open-vocabulary
auto-labeling systems for multi-modal 3D data. In this paper, we introduce
OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can
automatically generate 2D masks, 3D masks, and 3D bounding box annotations for
vision and point cloud data. Our system integrates the chain-of-thought
capabilities of Large Language Models (LLMs) and the cross-modality
capabilities of vision-language models (VLMs). To the best of our knowledge,
OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal
3D auto-labeling. We conduct comprehensive evaluations on both public and
in-house real-world datasets, which demonstrate that the system significantly
improves annotation efficiency compared to manual annotation while providing
accurate open-vocabulary auto-annotating results.
","2023-10-23","2310.13398v1.pdf"
"2310.13420","Jihyoung Jang","Jihyoung Jang, Minseong Boo, Hyounghun Kim","Conversation Chronicles: Towards Diverse Temporal and Relational
  Dynamics in Multi-Session Conversations","EMNLP 2023 (23 pages); Project website:
  https://conversation-chronicles.github.io","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the field of natural language processing, open-domain chatbots have
emerged as an important research topic. However, a major limitation of existing
open-domain chatbot research is its singular focus on short single-session
dialogue, neglecting the potential need for understanding contextual
information in multiple consecutive sessions that precede an ongoing dialogue.
Among the elements that compose the context in multi-session conversation
settings, the time intervals between sessions and the relationships between
speakers would be particularly important. Despite their importance, current
research efforts have not sufficiently addressed these dialogical components.
In this paper, we introduce a new 1M multi-session dialogue dataset, called
Conversation Chronicles, for implementing a long-term conversation setup in
which time intervals and fine-grained speaker relationships are incorporated.
Following recent works, we exploit a large language model to produce the data.
The extensive human evaluation shows that dialogue episodes in Conversation
Chronicles reflect those properties while maintaining coherent and consistent
interactions across all the sessions. We also propose a dialogue model, called
ReBot, which consists of chronological summarization and dialogue generation
modules using only around 630M parameters. When trained on Conversation
Chronicles, ReBot demonstrates long-term context understanding with a high
human engagement score.
","2023-10-23","2310.13420v1.pdf"
"2310.13439","Domenic Rosati","Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason
  Hoelscher-Obermaier, Jacob Pfau","Self-Consistency of Large Language Models under Ambiguity","BlackboxNLP @ EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) that do not give consistent answers across
contexts are problematic when used for tasks with expectations of consistency,
e.g., question-answering, explanations, etc. Our work presents an evaluation
benchmark for self-consistency in cases of under-specification where two or
more answers can be correct. We conduct a series of behavioral experiments on
the OpenAI model suite using an ambiguous integer sequence completion task. We
find that average consistency ranges from 67\% to 82\%, far higher than would
be predicted if a model's consistency was random, and increases as model
capability improves. Furthermore, we show that models tend to maintain
self-consistency across a series of robustness checks, including prompting
speaker changes and sequence length changes. These results suggest that
self-consistency arises as an emergent capability without specifically training
for it. Despite this, we find that models are uncalibrated when judging their
own consistency, with models displaying both over- and under-confidence. We
also propose a nonparametric test for determining from token output
distribution whether a model assigns non-trivial probability to alternative
answers. Using this test, we find that despite increases in self-consistency,
models usually place significant weight on alternative, inconsistent answers.
This distribution of probability mass provides evidence that even highly
self-consistent models internally compute multiple possible responses.
","2023-10-23","2310.13439v1.pdf"
"2310.13440","Johannes Bjerva","Emi Baylor and Esther Ploeger and Johannes Bjerva","The Past, Present, and Future of Typological Databases in NLP","Accepted to EMNLP Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Typological information has the potential to be beneficial in the development
of NLP models, particularly for low-resource languages. Unfortunately, current
large-scale typological databases, notably WALS and Grambank, are inconsistent
both with each other and with other sources of typological information, such as
linguistic grammars. Some of these inconsistencies stem from coding errors or
linguistic variation, but many of the disagreements are due to the discrete
categorical nature of these databases. We shed light on this issue by
systematically exploring disagreements across typological databases and
resources, and their uses in NLP, covering the past and present. We next
investigate the future of such work, offering an argument that a continuous
view of typological features is clearly beneficial, echoing recommendations
from linguistics. We propose that such a view of typology has significant
potential in the future, including in language modeling in low-resource
scenarios.
","2023-10-23","2310.13440v1.pdf"
"2310.13448","Duarte Alves","Duarte M. Alves, Nuno M. Guerreiro, Jo\~ao Alves, Jos\'e Pombal,
  Ricardo Rei, Jos\'e G. C. de Souza, Pierre Colombo and Andr\'e F. T. Martins","Steering Large Language Models for Machine Translation with Finetuning
  and In-Context Learning","Accepted at EMNLP 2023 - Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) are a promising avenue for machine translation
(MT). However, current LLM-based MT systems are brittle: their effectiveness
highly depends on the choice of few-shot examples and they often require extra
post-processing due to overgeneration. Alternatives such as finetuning on
translation instructions are computationally expensive and may weaken
in-context learning capabilities, due to overspecialization. In this paper, we
provide a closer look at this problem. We start by showing that adapter-based
finetuning with LoRA matches the performance of traditional finetuning while
reducing the number of training parameters by a factor of 50. This method also
outperforms few-shot prompting and eliminates the need for post-processing or
in-context examples. However, we show that finetuning generally degrades
few-shot performance, hindering adaptation capabilities. Finally, to obtain the
best of both worlds, we propose a simple approach that incorporates few-shot
examples during finetuning. Experiments on 10 language pairs show that our
proposed approach recovers the original few-shot capabilities while keeping the
added benefits of finetuning.
","2023-10-23","2310.13448v1.pdf"
"2310.13469","Baohao Liao","Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof
  Monz","Ask Language Model to Clean Your Noisy Translation Data","EMNLP 2023, Findings","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Transformer models have demonstrated remarkable performance in neural machine
translation (NMT). However, their vulnerability to noisy input poses a
significant challenge in practical implementation, where generating clean
output from noisy input is crucial. The MTNT dataset is widely used as a
benchmark for evaluating the robustness of NMT models against noisy input.
Nevertheless, its utility is limited due to the presence of noise in both the
source and target sentences. To address this limitation, we focus on cleaning
the noise from the target sentences in MTNT, making it more suitable as a
benchmark for noise evaluation. Leveraging the capabilities of large language
models (LLMs), we observe their impressive abilities in noise removal. For
example, they can remove emojis while considering their semantic meaning.
Additionally, we show that LLM can effectively rephrase slang, jargon, and
profanities. The resulting datasets, called C-MTNT, exhibit significantly less
noise in the target sentences while preserving the semantic integrity of the
original sentences. Our human and GPT-4 evaluations also lead to a consistent
conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT
showcased its effectiveness in evaluating the robustness of NMT models,
highlighting the potential of advanced language models for data cleaning and
emphasizing C-MTNT as a valuable resource.
","2023-10-25","2310.13469v1.pdf"
"2310.13473","Mingwei Zhu","Mingwei Zhu, Leigang Sha, Yu Shu, Kangjia Zhao, Tiancheng Zhao,
  Jianwei Yin","Benchmarking Sequential Visual Input Reasoning and Prediction in
  Multimodal Large Language Models","","","","","cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Multimodal large language models (MLLMs) have shown great potential in
perception and interpretation tasks, but their capabilities in predictive
reasoning remain under-explored. To address this gap, we introduce a novel
benchmark that assesses the predictive reasoning capabilities of MLLMs across
diverse scenarios. Our benchmark targets three important domains: abstract
pattern reasoning, human activity prediction, and physical interaction
prediction. We further develop three evaluation methods powered by large
language model to robustly quantify a model's performance in predicting and
reasoning the future based on multi-visual context. Empirical experiments
confirm the soundness of the proposed benchmark and evaluation methods via
rigorous testing and reveal pros and cons of current popular MLLMs in the task
of predictive reasoning. Lastly, our proposed benchmark provides a standardized
evaluation framework for MLLMs and can facilitate the development of more
advanced models that can reason and predict over complex long sequence of
multimodal input.
","2023-10-23","2310.13473v1.pdf"
"2310.13486","Lucas Weber","Lucas Weber, Elia Bruni and Dieuwke Hupkes","Mind the instructions: a holistic evaluation of consistency and
  interactions in prompt-based learning","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Finding the best way of adapting pre-trained language models to a task is a
big challenge in current NLP. Just like the previous generation of task-tuned
models (TT), models that are adapted to tasks via in-context-learning (ICL) are
robust in some setups but not in others. Here, we present a detailed analysis
of which design choices cause instabilities and inconsistencies in LLM
predictions. First, we show how spurious correlations between input
distributions and labels -- a known issue in TT models -- form only a minor
problem for prompted models. Then, we engage in a systematic, holistic
evaluation of different factors that have been found to influence predictions
in a prompting setup. We test all possible combinations of a range of factors
on both vanilla and instruction-tuned (IT) LLMs of different scale and
statistically analyse the results to show which factors are the most
influential, interactive or stable. Our results show which factors can be used
without precautions and which should be avoided or handled with care in most
settings.
","2023-10-23","2310.13486v1.pdf"
"2310.13500","Stergos Afantenos","Stergos Afantenos, Henri Prade, Leonardo Cortez Bernardes","Analogical Proportions and Creativity: A Preliminary Study","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Analogical proportions are statements of the form ""$a$ is to $b$ as $c$ is to
$d$"", which expresses that the comparisons of the elements in pair $(a, b)$ and
in pair $(c, d)$ yield similar results. Analogical proportions are creative in
the sense that given 3 distinct items, the representation of a 4th item $d$,
distinct from the previous items, which forms an analogical proportion with
them can be calculated, provided certain conditions are met. After providing an
introduction to analogical proportions and their properties, the paper reports
the results of an experiment made with a database of animal descriptions and
their class, where we try to ""create"" new animals from existing ones,
retrieving rare animals such as platypus. We perform a series of experiments
using word embeddings as well as Boolean features in order to propose novel
animals based on analogical proportions, showing that word embeddings obtain
better results.
","2023-10-23","2310.13500v1.pdf"
"2310.13506","Sagnik Ray Choudhury","Sagnik Ray Choudhury, Pepa Atanasova, Isabelle Augenstein","Explaining Interactions Between Text Spans","code: https://github.com/copenlu/spanex , dataset:
  https://huggingface.co/datasets/copenlu/spanex. Accepted EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Reasoning over spans of tokens from different parts of the input is essential
for natural language understanding (NLU) tasks such as fact-checking (FC),
machine reading comprehension (MRC) or natural language inference (NLI).
However, existing highlight-based explanations primarily focus on identifying
individual important tokens or interactions only between adjacent tokens or
tuples of tokens. Most notably, there is a lack of annotations capturing the
human decision-making process w.r.t. the necessary interactions for informed
decision-making in such tasks. To bridge this gap, we introduce SpanEx, a
multi-annotator dataset of human span interaction explanations for two NLU
tasks: NLI and FC. We then investigate the decision-making processes of
multiple fine-tuned large language models in terms of the employed connections
between spans in separate parts of the input and compare them to the human
reasoning processes. Finally, we present a novel community detection based
unsupervised method to extract such interaction explanations from a model's
inner workings.
","2023-10-23","2310.13506v1.pdf"
"2310.13512","Xia Zehua","Zehua Xia, Qi Gou, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, Cam-Tu
  Nguyen","Improving Question Generation with Multi-level Content Planning","Camera-ready. Accepted by EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper addresses the problem of generating questions from a given context
and an answer, specifically focusing on questions that require multi-hop
reasoning across an extended context. Previous studies have suggested that key
phrase selection is essential for question generation (QG), yet it is still
challenging to connect such disjointed phrases into meaningful questions,
particularly for long context. To mitigate this issue, we propose MultiFactor,
a novel QG framework based on multi-level content planning. Specifically,
MultiFactor includes two components: FA-model, which simultaneously selects key
phrases and generates full answers, and Q-model which takes the generated full
answer as an additional input to generate questions. Here, full answer
generation is introduced to connect the short answer with the selected key
phrases, thus forming an answer-aware summary to facilitate QG. Both FA-model
and Q-model are formalized as simple-yet-effective Phrase-Enhanced
Transformers, our joint model for phrase selection and text generation.
Experimental results show that our method outperforms strong baselines on two
popular QG datasets. Our code is available at
https://github.com/zeaver/MultiFactor.
","2023-10-24","2310.13512v1.pdf"
"2310.13513","Zhuoyi Zhang","Zhuoyi Zhang, Yunchen Zhang, Gonglei Shi, Yu Shen, Xiuying Wei, Ruihao
  Gong, Xiaoxu Xia, Qi Zhang, Lewei Lu, Xianglong Liu","Exploring the Potential of Flexible 8-bit Format: Design and Algorithm","","","","","cs.PF","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Neural network quantization is widely used to reduce model inference
complexity in real-world deployments. However, traditional integer quantization
suffers from accuracy degradation when adapting to various dynamic ranges.
Recent research has focused on a new 8-bit format, FP8, with hardware support
for both training and inference of neural networks but lacks guidance for
hardware design. In this paper, we analyze the benefits of using FP8
quantization and provide a comprehensive comparison of FP8 with INT
quantization. Then we propose a flexible mixed-precision quantization framework
that supports various number systems, enabling optimal selection of the most
appropriate quantization format for different neural network architectures.
Experimental results demonstrate that our proposed framework achieves
competitive performance compared to full precision on various tasks, including
image classification, object detection, segmentation, and natural language
understanding. Our work furnishes critical insights into the tangible benefits
and feasibility of employing FP8 quantization, paving the way for heightened
neural network efficiency in tangible scenarios. Our code is available in the
supplementary material.
","2023-10-23","2310.13513v1.pdf"
"2310.13518","Shengcheng Yu","Shengcheng Yu, Chunrong Fang, Ziyuan Tuo, Quanjun Zhang, Chunyang
  Chen, Zhenyu Chen, Zhendong Su","Vision-Based Mobile App GUI Testing: A Survey","","","","","cs.SE","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Graphical User Interface (GUI) has become one of the most significant parts
of mobile applications (apps). It is a direct bridge between mobile apps and
end users, which directly affects the end user's experience. Neglecting GUI
quality can undermine the value and effectiveness of the entire mobile app
solution. Significant research efforts have been devoted to GUI testing, one
effective method to ensure mobile app quality. By conducting rigorous GUI
testing, developers can ensure that the visual and interactive elements of the
mobile apps not only meet functional requirements but also provide a seamless
and user-friendly experience. However, traditional solutions, relying on the
source code or layout files, have met challenges in both effectiveness and
efficiency due to the gap between what is obtained and what app GUI actually
presents. Vision-based mobile app GUI testing approaches emerged with the
development of computer vision technologies and have achieved promising
progress. In this survey paper, we provide a comprehensive investigation of the
state-of-the-art techniques on 226 papers, among which 78 are vision-based
studies. This survey covers different topics of GUI testing, like GUI test
generation, GUI test record & replay, GUI testing framework, etc. Specifically,
the research emphasis of this survey is placed mostly on how vision-based
techniques outperform traditional solutions and have gradually taken a vital
place in the GUI testing field. Based on the investigation of existing studies,
we outline the challenges and opportunities of (vision-based) mobile app GUI
testing and propose promising research directions with the combination of
emerging techniques.
","2023-10-23","2310.13518v1.pdf"
"2310.13522","Xiao Yu","Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu","Teaching Language Models to Self-Improve through Interactive
  Demonstrations","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The self-improving ability of large language models (LLMs), enabled by
prompting them to analyze and revise their own outputs, has garnered
significant interest in recent research. However, this ability has been shown
to be absent and difficult to learn for smaller models, thus widening the
performance gap between state-of-the-art LLMs and more cost-effective and
faster ones. To reduce this gap, we introduce TriPosT, a training algorithm
that endows smaller models with such self-improvement ability, and show that
our approach can improve a LLaMA-7b's performance on math and reasoning tasks
by up to 7.13%. In contrast to prior work, we achieve this by using the smaller
model to interact with LLMs to collect feedback and improvements on its own
generations. We then replay this experience to train the small model. Our
experiments on four math and reasoning datasets show that the interactive
experience of learning from and correcting its own mistakes is crucial for
small models to improve their performance.
","2023-10-23","2310.13522v1.pdf"
"2310.13526","Tobias Deu{\ss}er","Tobias Deu{\ss}er, Cong Zhao, Wolfgang Kr\""amer, David Leonhard,
  Christian Bauckhage, Rafet Sifa","Controlled Randomness Improves the Performance of Transformer Models","Accepted at ICMLA 2023, 10 pages, 2 tables","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  During the pre-training step of natural language models, the main objective
is to learn a general representation of the pre-training dataset, usually
requiring large amounts of textual data to capture the complexity and diversity
of natural language. Contrasting this, in most cases, the size of the data
available to solve the specific downstream task is often dwarfed by the
aforementioned pre-training dataset, especially in domains where data is
scarce. We introduce controlled randomness, i.e. noise, into the training
process to improve fine-tuning language models and explore the performance of
targeted noise in addition to the parameters of these models. We find that
adding such noise can improve the performance in our two downstream tasks of
joint named entity recognition and relation extraction and text summarization.
","2023-10-23","2310.13526v1.pdf"
"2310.13545","Zhongzhan Huang","Zhongzhan Huang, Pan Zhou, Shuicheng Yan, Liang Lin","ScaleLong: Towards More Stable Training of Diffusion Model via Scaling
  Network Long Skip Connection","accepted by NeurIPS 2023","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In diffusion models, UNet is the most popular network backbone, since its
long skip connects (LSCs) to connect distant network blocks can aggregate
long-distant information and alleviate vanishing gradient. Unfortunately, UNet
often suffers from unstable training in diffusion models which can be
alleviated by scaling its LSC coefficients smaller. However, theoretical
understandings of the instability of UNet in diffusion models and also the
performance improvement of LSC scaling remain absent yet. To solve this issue,
we theoretically show that the coefficients of LSCs in UNet have big effects on
the stableness of the forward and backward propagation and robustness of UNet.
Specifically, the hidden feature and gradient of UNet at any layer can
oscillate and their oscillation ranges are actually large which explains the
instability of UNet training. Moreover, UNet is also provably sensitive to
perturbed input, and predicts an output distant from the desired output,
yielding oscillatory loss and thus oscillatory gradient. Besides, we also
observe the theoretical benefits of the LSC coefficient scaling of UNet in the
stableness of hidden features and gradient and also robustness. Finally,
inspired by our theory, we propose an effective coefficient scaling framework
ScaleLong that scales the coefficients of LSC in UNet and better improves the
training stability of UNet. Experimental results on four famous datasets show
that our methods are superior to stabilize training and yield about 1.5x
training acceleration on different diffusion models with UNet or UViT
backbones. Code: https://github.com/sail-sg/ScaleLong
","2023-10-23","2310.13545v1.pdf"
"2310.13548","Mrinank Sharma","Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda
  Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds,
  Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal
  Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez","Towards Understanding Sycophancy in Language Models","32 pages, 20 figures","","","","cs.CL cs.AI cs.LG stat.ML","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning from human feedback (RLHF) is a popular technique for
training high-quality AI assistants. However, RLHF may also encourage model
responses that match user beliefs over truthful responses, a behavior known as
sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models
and whether human preference judgements are responsible. We first demonstrate
that five state-of-the-art AI assistants consistently exhibit sycophantic
behavior across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior of RLHF models, we
analyze existing human preference data. We find that when a response matches a
user's views, it is more likely to be preferred. Moreover, both humans and
preference models (PMs) prefer convincingly-written sycophantic responses over
correct ones a non-negligible fraction of the time. Optimizing model outputs
against PMs also sometimes sacrifices truthfulness in favor of sycophancy.
Overall, our results indicate that sycophancy is a general behavior of RLHF
models, likely driven in part by human preference judgements favoring
sycophantic responses.
","2023-10-25","2310.13548v1.pdf"
"2310.13549","Dorian Christoph Quelle","Dorian Quelle, Alexandre Bovet","The Perils & Promises of Fact-checking with Large Language Models","","","","","cs.CL cs.CY cs.HC","http://creativecommons.org/licenses/by/4.0/","  Autonomous fact-checking, using machine learning to verify claims, has grown
vital as misinformation spreads beyond human fact-checking capacity. Large
Language Models (LLMs) like GPT-4 are increasingly trusted to verify
information and write academic papers, lawsuits, and news articles, emphasizing
their role in discerning truth from falsehood and the importance of being able
to verify their outputs. Here, we evaluate the use of LLM agents in
fact-checking by having them phrase queries, retrieve contextual data, and make
decisions. Importantly, in our framework, agents explain their reasoning and
cite the relevant sources from the retrieved context. Our results show the
enhanced prowess of LLMs when equipped with contextual information. GPT-4
outperforms GPT-3, but accuracy varies based on query language and claim
veracity. While LLMs show promise in fact-checking, caution is essential due to
inconsistent accuracy. Our investigation calls for further research, fostering
a deeper comprehension of when agents succeed and when they fail.
","2023-10-23","2310.13549v1.pdf"
"2310.13552","Jinyuan Wang","Jinyuan Wang and Junlong Li and Hai Zhao","Self-prompted Chain-of-Thought on Large Language Models for Open-domain
  Multi-hop Reasoning","Accepted by Findings of EMNLP2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In open-domain question-answering (ODQA), most existing questions require
single-hop reasoning on commonsense. To further extend this task, we officially
introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop
questions with explicit reasoning steps in open-domain setting. Recently, large
language models (LLMs) have found significant utility in facilitating ODQA
without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts
the reasoning capability of LLMs to a greater extent with manual or automated
paradigms. However, existing automated methods lack of quality assurance, while
manual approaches suffer from limited scalability and poor diversity, hindering
the capabilities of LLMs. In this paper, we propose Self-prompted
Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality
CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation
pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT
selection and self-prompted inference via in-context learning. Extensive
experiments on four multi-hop question-answering benchmarks show that our
proposed SP-CoT not only significantly surpasses the previous SOTA methods on
large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of
small-scale (13B) LLMs. Further analysis reveals the remarkable capability of
SP-CoT to elicit direct and concise intermediate reasoning steps by recalling
$\sim$50\% of intermediate answers on MuSiQue-Ans dataset.
","2023-10-24","2310.13552v1.pdf"
"2310.13561","Guillem Ram\'irez","Guillem Ram\'irez and Matthias Lindemann and Alexandra Birch and Ivan
  Titov","Cache & Distil: Optimising API Calls to Large Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large-scale deployment of generative AI tools often depends on costly API
calls to a Large Language Model (LLM) to fulfil user queries. To curtail the
frequency of these calls, one can employ a smaller language model -- a student
-- which is continuously trained on the responses of the LLM. This student
gradually gains proficiency in independently handling an increasing number of
user requests, a process we term neural caching. The crucial element in neural
caching is a policy that decides which requests should be processed by the
student alone and which should be redirected to the LLM, subsequently aiding
the student's learning. In this study, we focus on classification tasks, and we
consider a range of classic active learning-based selection criteria as the
policy. Our experiments suggest that Margin Sampling and Query by Committee
bring consistent benefits across tasks and budgets.
","2023-10-23","2310.13561v1.pdf"
"2310.13566","Nicholas Walker","Nicholas Thomas Walker, Stefan Ultes, Pierre Lison","Retrieval-Augmented Neural Response Generation Using Logical Reasoning
  and Relevance Scoring","Presented at SemDial, August 2023 in Maribor, Slovenia","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Constructing responses in task-oriented dialogue systems typically relies on
information sources such the current dialogue state or external databases. This
paper presents a novel approach to knowledge-grounded response generation that
combines retrieval-augmented language models with logical reasoning. The
approach revolves around a knowledge graph representing the current dialogue
state and background information, and proceeds in three steps. The knowledge
graph is first enriched with logically derived facts inferred using
probabilistic logical programming. A neural model is then employed at each turn
to score the conversational relevance of each node and edge of this extended
graph. Finally, the elements with highest relevance scores are converted to a
natural language form, and are integrated into the prompt for the neural
conversational model employed to generate the system response.
  We investigate the benefits of the proposed approach on two datasets (KVRET
and GraphWOZ) along with a human evaluation. Experimental results show that the
combination of (probabilistic) logical reasoning with conversational relevance
scoring does increase both the factuality and fluency of the responses.
","2023-10-23","2310.13566v1.pdf"
"2310.13570","Alexandros Xenos","Alexandros Xenos, Themos Stafylakis, Ioannis Patras and Georgios
  Tzimiropoulos","A Simple Baseline for Knowledge-Based Visual Question Answering","Accepted at EMNLP 2023 (camera-ready version)","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  This paper is on the problem of Knowledge-Based Visual Question Answering
(KB-VQA). Recent works have emphasized the significance of incorporating both
explicit (through external databases) and implicit (through LLMs) knowledge to
answer questions requiring external knowledge effectively. A common limitation
of such approaches is that they consist of relatively complicated pipelines and
often heavily rely on accessing GPT-3 API. Our main contribution in this paper
is to propose a much simpler and readily reproducible pipeline which, in a
nutshell, is based on efficient in-context learning by prompting LLaMA (1 and
2) using question-informative captions as contextual information. Contrary to
recent approaches, our method is training-free, does not require access to
external databases or APIs, and yet achieves state-of-the-art accuracy on the
OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to
understand important aspects of our method. Our code is publicly available at
https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA
","2023-10-25","2310.13570v1.pdf"
"2310.13571","Haitham Bou Ammar PhD","Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, Haitham
  Bou-Ammar","Why Can Large Language Models Generate Correct Chain-of-Thoughts?","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper delves into the capabilities of large language models (LLMs),
specifically focusing on advancing the theoretical comprehension of
chain-of-thought prompting. We investigate how LLMs can be effectively induced
to generate a coherent chain of thoughts. To achieve this, we introduce a
two-level hierarchical graphical model tailored for natural language
generation. Within this framework, we establish a compelling geometrical
convergence rate that gauges the likelihood of an LLM-generated chain of
thoughts compared to those originating from the true language. Our findings
provide a theoretical justification for the ability of LLMs to produce the
correct sequence of thoughts (potentially) explaining performance gains in
tasks demanding reasoning skills.
","2023-10-23","2310.13571v1.pdf"
"2310.13575","Ben Eyal","Ben Eyal, Amir Bachar, Ophir Haroche, Moran Mahabi, Michael Elhadad","Semantic Decomposition of Question and SQL for Text-to-SQL Parsing","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Text-to-SQL semantic parsing faces challenges in generalizing to cross-domain
and complex queries. Recent research has employed a question decomposition
strategy to enhance the parsing of complex SQL queries. However, this strategy
encounters two major obstacles: (1) existing datasets lack question
decomposition; (2) due to the syntactic complexity of SQL, most complex queries
cannot be disentangled into sub-queries that can be readily recomposed. To
address these challenges, we propose a new modular Query Plan Language (QPL)
that systematically decomposes SQL queries into simple and regular sub-queries.
We develop a translator from SQL to QPL by leveraging analysis of SQL server
query optimization plans, and we augment the Spider dataset with QPL programs.
Experimental results demonstrate that the modular nature of QPL benefits
existing semantic-parsing architectures, and training text-to-QPL parsers is
more effective than text-to-SQL parsing for semantically equivalent queries.
The QPL approach offers two additional advantages: (1) QPL programs can be
paraphrased as simple questions, which allows us to create a dataset of
(complex question, decomposed questions). Training on this dataset, we obtain a
Question Decomposer for data retrieval that is sensitive to database schemas.
(2) QPL is more accessible to non-experts for complex queries, leading to more
interpretable output from the semantic parser.
","2023-10-23","2310.13575v1.pdf"
"2310.13588","Shoutao Guo","Shoutao Guo, Shaolei Zhang, Yang Feng","Simultaneous Machine Translation with Tailored Reference","Accepted to EMNLP 2023; 15 pages, 8 figures","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Simultaneous machine translation (SiMT) generates translation while reading
the whole source sentence. However, existing SiMT models are typically trained
using the same reference disregarding the varying amounts of available source
information at different latency. Training the model with ground-truth at low
latency may introduce forced anticipations, whereas utilizing reference
consistent with the source word order at high latency results in performance
degradation. Consequently, it is crucial to train the SiMT model with
appropriate reference that avoids forced anticipations during training while
maintaining high quality. In this paper, we propose a novel method that
provides tailored reference for the SiMT models trained at different latency by
rephrasing the ground-truth. Specifically, we introduce the tailor, induced by
reinforcement learning, to modify ground-truth to the tailored reference. The
SiMT model is trained with the tailored reference and jointly optimized with
the tailor to enhance performance. Importantly, our method is applicable to a
wide range of current SiMT approaches. Experiments on three translation tasks
demonstrate that our method achieves state-of-the-art performance in both fixed
and adaptive policies.
","2023-10-27","2310.13588v2.pdf"
"2310.13590","Yaorui Shi","Yaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, Xiang Wang","ReLM: Leveraging Language Models for Enhanced Chemical Reaction
  Prediction","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Predicting chemical reactions, a fundamental challenge in chemistry, involves
forecasting the resulting products from a given reaction process. Conventional
techniques, notably those employing Graph Neural Networks (GNNs), are often
limited by insufficient training data and their inability to utilize textual
information, undermining their applicability in real-world applications. In
this work, we propose ReLM, a novel framework that leverages the chemical
knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing
the accuracy of real-world chemical reaction predictions. To further enhance
the model's robustness and interpretability, we incorporate the confidence
score strategy, enabling the LMs to self-assess the reliability of their
predictions. Our experimental results demonstrate that ReLM improves the
performance of state-of-the-art GNN-based methods across various chemical
reaction datasets, especially in out-of-distribution settings. Codes are
available at https://github.com/syr-cn/ReLM.
","2023-10-23","2310.13590v1.pdf"
"2310.13595","Nathan Lambert","Nathan Lambert and Thomas Krendl Gilbert and Tom Zick","Entangled Preferences: The History and Risks of Reinforcement Learning
  and Human Feedback","13 pages, 1 figure","","","","cs.CY","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to use and more
effective. A core piece of the RLHF process is the training and utilization of
a model of human preferences that acts as a reward function for optimization.
This approach, which operates at the intersection of many stakeholders and
academic disciplines, remains poorly understood. RLHF reward models are often
cited as being central to achieving performance, yet very few descriptors of
capabilities, evaluations, training methods, or open-source models exist. Given
this lack of information, further study and transparency is needed for learned
RLHF reward models. In this paper, we illustrate the complex history of
optimizing preferences, and articulate lines of inquiry to understand the
sociotechnical context of reward models. In particular, we highlight the
ontological differences between costs, rewards, and preferences at stake in
RLHF's foundations, related methodological tensions, and possible research
directions to improve general understanding of how reward models function.
","2023-10-23","2310.13595v1.pdf"
"2310.13596","Ziqiang Zheng","Ziqiang Zheng and Jipeng Zhang and Tuan-Anh Vu and Shizhe Diao and Yue
  Him Wong Tim and Sai-Kit Yeung","MarineGPT: Unlocking Secrets of Ocean to the Public","work in progress. Code and data will be available at
  https://github.com/hkust-vgd/MarineGPT","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be
powerful tools in promoting the user experience as an AI assistant. The
continuous works are proposing multi-modal large language models (MLLM),
empowering LLMs with the ability to sense multiple modality inputs through
constructing a joint semantic space (e.g. visual-text space). Though
significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in
domain-specific applications that required domain-specific knowledge and
expertise has been less conducted, especially for \textbf{marine domain}.
Different from general-purpose MLLMs, the marine-specific MLLM is required to
yield much more \textbf{sensitive}, \textbf{informative}, and
\textbf{scientific} responses. In this work, we demonstrate that the existing
MLLMs optimized on huge amounts of readily available general-purpose training
data show a minimal ability to understand domain-specific intents and then
generate informative and satisfactory responses. To address these issues, we
propose \textbf{MarineGPT}, the first vision-language model specially designed
for the marine domain, unlocking the secrets of the ocean to the public. We
present our \textbf{Marine-5M} dataset with more than 5 million marine
image-text pairs to inject domain-specific marine knowledge into our model and
achieve better marine vision and language alignment. Our MarineGPT not only
pushes the boundaries of marine understanding to the general public but also
offers a standard protocol for adapting a general-purpose assistant to
downstream domain-specific experts. We pave the way for a wide range of marine
applications while setting valuable data and pre-trained models for future
research in both academic and industrial communities.
","2023-10-23","2310.13596v1.pdf"
"2310.13606","Dominik Macko","Dominik Macko, Robert Moro, Adaku Uchendu, Jason Samuel Lucas,
  Michiharu Yamashita, Mat\'u\v{s} Pikuliak, Ivan Srba, Thai Le, Dongwon Lee,
  Jakub Simko, Maria Bielikova","MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection
  Benchmark","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  There is a lack of research into capabilities of recent LLMs to generate
convincing text in languages other than English and into performance of
detectors of machine-generated text in multilingual settings. This is also
reflected in the available benchmarks which lack authentic texts in languages
other than English and predominantly cover older generators. To fill this gap,
we introduce MULTITuDE, a novel benchmarking dataset for multilingual
machine-generated text detection comprising of 74,081 authentic and
machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru,
uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare
the performance of zero-shot (statistical and black-box) and fine-tuned
detectors. Considering the multilinguality, we evaluate 1) how these detectors
generalize to unseen languages (linguistically similar as well as dissimilar)
and unseen LLMs and 2) whether the detectors improve their performance when
trained on multiple languages.
","2023-10-23","2310.13606v1.pdf"
"2310.13610","Du Yanrui","Yanrui Du, Sendong Zhao, Haochun Wang, Yuhan Chen, Rui Bai, Zewen
  Qiang, Muzhen Cai, Bing Qin","Make Your Decision Convincing! A Unified Two-Stage Framework:
  Self-Attribution and Decision-Making","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Explaining black-box model behavior with natural language has achieved
impressive results in various NLP tasks. Recent research has explored the
utilization of subsequences from the input text as a rationale, providing users
with evidence to support the model decision. Although existing frameworks excel
in generating high-quality rationales while achieving high task performance,
they neglect to account for the unreliable link between the generated rationale
and model decision. In simpler terms, a model may make correct decisions while
attributing wrong rationales, or make poor decisions while attributing correct
rationales. To mitigate this issue, we propose a unified two-stage framework
known as Self-Attribution and Decision-Making (SADM). Through extensive
experiments on five reasoning datasets from the ERASER benchmark, we
demonstrate that our framework not only establishes a more reliable link
between the generated rationale and model decision but also achieves
competitive results in task performance and the quality of rationale.
Furthermore, we explore the potential of our framework in semi-supervised
scenarios.
","2023-10-23","2310.13610v1.pdf"
"2310.13615","An-Zi Yen","An-Zi Yen and Wei-Ling Hsu","Three Questions Concerning the Use of Large Language Models to
  Facilitate Mathematics Learning","Accepted by EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Due to the remarkable language understanding and generation abilities of
large language models (LLMs), their use in educational applications has been
explored. However, little work has been done on investigating the pedagogical
ability of LLMs in helping students to learn mathematics. In this position
paper, we discuss the challenges associated with employing LLMs to enhance
students' mathematical problem-solving skills by providing adaptive feedback.
Apart from generating the wrong reasoning processes, LLMs can misinterpret the
meaning of the question, and also exhibit difficulty in understanding the given
questions' rationales when attempting to correct students' answers. Three
research questions are formulated.
","2023-10-23","2310.13615v1.pdf"
"2310.13620","Emily Cheng","Emily Cheng, Corentin Kervadec, and Marco Baroni","Bridging Information-Theoretic and Geometric Compression in Language
  Models","EMNLP 2023 Camera-Ready","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  For a language model (LM) to faithfully model human language, it must
compress vast, potentially infinite information into relatively few dimensions.
We propose analyzing compression in (pre-trained) LMs from two points of view:
geometric and information-theoretic. We demonstrate that the two views are
highly correlated, such that the intrinsic geometric dimension of linguistic
data predicts their coding length under the LM. We then show that, in turn,
high compression of a linguistic dataset predicts rapid adaptation to that
dataset, confirming that being able to compress linguistic information is an
important part of successful LM performance. As a practical byproduct of our
analysis, we evaluate a battery of intrinsic dimension estimators for the first
time on linguistic data, showing that only some encapsulate the relationship
between information-theoretic compression, geometric compression, and
ease-of-adaptation.
","2023-10-23","2310.13620v1.pdf"
"2310.13648","Peng Liang","Muhammad Waseem, Teerath Das, Aakash Ahmad, Mahdi Fehmideh, Peng
  Liang, Tommi Mikkonen","Using ChatGPT throughout the Software Development Life Cycle by Novice
  Developers","","","","","cs.SE","http://creativecommons.org/licenses/by/4.0/","  This study investigates the impact of ChatGPT -- a generative AI-based tool
-- on undergraduate students' software development experiences. Through a
three-month project involving seven undergraduate students, ChatGPT was
employed as a supporting tool, and their experiences were systematically
surveyed before and after the projects. The research aims to answer four key
questions related to ChatGPT's effectiveness, advantages, limitations, impact
on learning, and challenges faced. The findings revealed significant skill gaps
among undergraduate students, underscoring the importance of addressing
educational deficiencies in software development. ChatGPT was found to have a
positive influence on various phases of the software development life cycle,
leading to enhanced efficiency, accuracy, and collaboration. ChatGPT also
consistently improved participants' foundational understanding and soft skills
in software development. These findings underscore the significance of
integrating AI tools like ChatGPT into undergraduate students education,
particularly to bridge skill gaps and enhance productivity. However, a nuanced
approach to technology reliance is essential, acknowledging the variability in
opinions and the need for customization. Future research should explore
strategies to optimize ChatGPT's application across development contexts,
ensuring it maximizes learning while addressing specific challenges.
","2023-10-23","2310.13648v1.pdf"
"2310.13650","Haodong Duan","Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang,
  Songyang Zhang, Dahua Lin, Kai Chen","BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Interacting with human via high-quality multi-turn dialogues is a key feature
of large language models (LLMs). However, human-based evaluation of such
capability involves intensive manual labor. This report provides a preliminary
evaluation of existing large language models for human-style multi-turn
chatting, through an LLM-based approach. We start from real-world human
dialogues and keep the very first utterances as the ChatSEED. Then we prompt
LLMs to generate a full multi-turn dialogue (tens of utterances) based on the
ChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs
(GPT-4, \etc) as the judge to evaluate the generated dialogues. With different
evaluation protocols, we come to substantially identical conclusions. We find
that GPT-4 can generate human-style multi-turn dialogues with impressive
quality, significantly outperforms its counterparts. It's difficult for a
discriminator to distinguish between GPT-4 generated dialogues and human
dialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of
satisfactory quality due to poor instruction-following capability, tendency to
generate lengthy utterances, or limited general capability. All data and codes
will be provided in https://github.com/open-compass/BotChat/ and we hope they
can serve as a valuable resource for evaluating multi-turn chatting
capabilities of LLMs.
","2023-10-23","2310.13650v1.pdf"
"2310.13659","Adithya Bhaskar","Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, Sunita Sarawagi","Benchmarking and Improving Text-to-SQL Generation under Ambiguity","To appear at EMNLP 2023 (Main)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Research in Text-to-SQL conversion has been largely benchmarked against
datasets where each text query corresponds to one correct SQL. However, natural
language queries over real-life databases frequently involve significant
ambiguity about the intended SQL due to overlapping schema names and multiple
confusing relationship paths. To bridge this gap, we develop a novel benchmark
called AmbiQT with over 3000 examples where each text is interpretable as two
plausible SQLs due to lexical and/or structural ambiguity.
  When faced with ambiguity, an ideal top-$k$ decoder should generate all valid
interpretations for possible disambiguation by the user. We evaluate several
Text-to-SQL systems and decoding algorithms, including those employing
state-of-the-art LLMs, and find them to be far from this ideal. The primary
reason is that the prevalent beam search algorithm and its variants, treat SQL
queries as a string and produce unhelpful token-level diversity in the top-$k$.
  We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic
space using a blend of plan-based template generation and constrained
infilling. Counterfactually generated plans diversify templates while
in-filling with a beam-search that branches solely on schema names provides
value diversity. LogicalBeam is up to $2.5$ times more effective than
state-of-the-art models at generating all candidate SQLs in the top-$k$ ranked
outputs. It also enhances the top-$5$ Exact and Execution Match Accuracies on
SPIDER and Kaggle DBQA.
","2023-10-23","2310.13659v1.pdf"
"2310.13669","Philip John Gorinski","Philip John Gorinski, Matthieu Zimmer, Gerasimos Lampouras, Derrick
  Goh Xin Deik, Ignacio Iacobacci","Automatic Unit Test Data Generation and Actor-Critic Reinforcement
  Learning for Code Synthesis","9 pages + 4 pages appendix; 4 Figures, 4 Tables, 1 Algorithm;
  Accepted to Findings of EMNLP 2023","","","","cs.LG cs.AI cs.CL cs.PL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advent of large pre-trained language models in the domain of Code
Synthesis has shown remarkable performance on various benchmarks, treating the
problem of Code Generation in a fashion similar to Natural Language Generation,
trained with a Language Modelling (LM) objective. In addition, the property of
programming language code being precisely evaluable with respect to its
semantics -- through the use of Unit Tests to check its functional correctness
-- lends itself to using Reinforcement Learning (RL) as a further training
paradigm. Previous work has shown that RL can be applied as such to improve
models' coding capabilities; however, such RL-based methods rely on a reward
signal based on defined Unit Tests, which are much harder to obtain compared to
the huge crawled code datasets used in LM objectives. In this work, we present
a novel approach to automatically obtain data consisting of function signatures
and associated Unit Tests, suitable for RL training of Code Synthesis models.
We also introduce a straightforward, simple yet effective Actor-Critic RL
training scheme and show that it, in conjunction with automatically generated
training data, leads to improvement of a pre-trained code language model's
performance by up to 9.9% improvement over the original underlying code
synthesis LM, and up to 4.3% over RL-based models trained with standard PPO or
CodeRL.
","2023-10-23","2310.13669v1.pdf"
"2310.13671","Ruida Wang","Ruida Wang, Wangchunshu Zhou, Mrinmaya Sachan","Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large
  Language Models by Extrapolating Errors from Small Models","Accepted by EMNLP 2023(Findings)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  *Data Synthesis* is a promising way to train a small model with very little
labeled data. One approach for data synthesis is to leverage the rich knowledge
from large language models to synthesize pseudo training examples for small
models, making it possible to achieve both data and compute efficiency at the
same time. However, a key challenge in data synthesis is that the synthesized
dataset often suffers from a large distributional discrepancy from the *real
task* data distribution. Thus, in this paper, we propose *Synthesis Step by
Step* (**S3**), a data synthesis framework that shrinks this distribution gap
by iteratively extrapolating the errors made by a small model trained on the
synthesized dataset on a small real-world validation dataset using a large
language model. Extensive experiments on multiple NLP tasks show that our
approach improves the performance of a small model by reducing the gap between
the synthetic dataset and the real data, resulting in significant improvement
compared to several baselines: 9.48% improvement compared to ZeroGen and 2.73%
compared to GoldGen, and at most 15.17% improvement compared to the small model
trained on human-annotated data.
","2023-10-23","2310.13671v1.pdf"
"2310.13673","Sullam Jeoung","Sullam Jeoung, Yubin Ge, Jana Diesner","StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large
  Language Models","Accepted to EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have been observed to encode and perpetuate
harmful associations present in the training data. We propose a theoretically
grounded framework called StereoMap to gain insights into their perceptions of
how demographic groups have been viewed by society. The framework is grounded
in the Stereotype Content Model (SCM); a well-established theory from
psychology. According to SCM, stereotypes are not all alike. Instead, the
dimensions of Warmth and Competence serve as the factors that delineate the
nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs'
perceptions of social groups (defined by socio-demographic features) using the
dimensions of Warmth and Competence. Furthermore, the framework enables the
investigation of keywords and verbalizations of reasoning of LLMs' judgments to
uncover underlying factors influencing their perceptions. Our results show that
LLMs exhibit a diverse range of perceptions towards these groups, characterized
by mixed evaluations along the dimensions of Warmth and Competence.
Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs
demonstrate an awareness of social disparities, often stating statistical data
and research findings to support their reasoning. This study contributes to the
understanding of how LLMs perceive and represent social groups, shedding light
on their potential biases and the perpetuation of harmful associations.
","2023-10-23","2310.13673v1.pdf"
"2310.13678","Hao Zhang","Arya D. McCarthy, Hao Zhang, Shankar Kumar, Felix Stahlberg, Ke Wu","Long-Form Speech Translation through Segmentation with Finite-State
  Decoding Constraints on Large Language Models","accepted to the Findings of EMNLP 2023. arXiv admin note: text
  overlap with arXiv:2212.09895","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  One challenge in speech translation is that plenty of spoken content is
long-form, but short units are necessary for obtaining high-quality
translations. To address this mismatch, we adapt large language models (LLMs)
to split long ASR transcripts into segments that can be independently
translated so as to maximize the overall translation quality. We overcome the
tendency of hallucination in LLMs by incorporating finite-state constraints
during decoding; these eliminate invalid outputs without requiring additional
training. We discover that LLMs are adaptable to transcripts containing ASR
errors through prompt-tuning or fine-tuning. Relative to a state-of-the-art
automatic punctuation baseline, our best LLM improves the average BLEU by 2.9
points for English-German, English-Spanish, and English-Arabic TED talk
translation in 9 test sets, just by improving segmentation.
","2023-10-24","2310.13678v1.pdf"
"2310.13683","Gabriel Oliveira dos Santos","Gabriel Oliveira dos Santos, Diego A. B. Moreira, Alef Iury Ferreira,
  Jhessica Silva, Luiz Pereira, Pedro Bueno, Thiago Sousa, Helena Maia, N\'adia
  Da Silva, Esther Colombini, Helio Pedrini and Sandra Avila","CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP
  Performance on Low-Resource Languages","","","","","cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  This work introduces CAPIVARA, a cost-efficient framework designed to enhance
the performance of multilingual CLIP models in low-resource languages. While
CLIP has excelled in zero-shot vision-language tasks, the resource-intensive
nature of model training remains challenging. Many datasets lack linguistic
diversity, featuring solely English descriptions for images. CAPIVARA addresses
this by augmenting text data using image captioning and machine translation to
generate multiple synthetic captions in low-resource languages. We optimize the
training pipeline with LiT, LoRA, and gradient checkpointing to alleviate the
computational cost. Through extensive experiments, CAPIVARA emerges as state of
the art in zero-shot tasks involving images and Portuguese texts. We show the
potential for significant improvements in other low-resource languages,
achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on a
single GPU for 2 hours. Our model and code is available at
https://github.com/hiaac-nlp/CAPIVARA.
","2023-10-24","2310.13683v1.pdf"
"2310.13702","Louis Rosenberg PhD","Louis Rosenberg, Gregg Willcox, Hans Schumann","Conversational Swarm Intelligence (CSI) Enables Rapid Group Insights","Copyright 2023 IEEE. arXiv admin note: substantial text overlap with
  arXiv:2309.12366","","","","cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  When generating insights from human groups, conversational deliberation is a
key method for exploring issues, surfacing ideas, debating options, and
converging on solutions. Unfortunately, real-time conversations are difficult
to scale, losing effectiveness in groups above 4 to 7 members. Conversational
Swarm Intelligence (CSI) is a new technology that enables large human groups to
hold real-time conversations using techniques modeled on the dynamics of
biological swarms. Through a novel use of Large Language Models (LLMs), CSI
enables real-time dialog among small groups while simultaneously fostering
content propagation across a much larger group. This combines the benefits of
small-scale deliberative reasoning and large-scale groupwise intelligence. In
this study, we engage a group of 81 American voters from one political party in
real-time deliberation using a CSI platform called Thinkscape. We then task the
group with (a) forecasting which candidate from a set of options will achieve
the most national support, and (b) indicating the specific reasons for this
result. After only six minutes of deliberation, the group of 81 individuals
converged on a selected candidate and surfaced over 400 reasons justifying
various candidates, including 206 justifications that supported the selected
candidate. We find that the selected candidate was significantly more supported
by group members than the other options (p<0.001) and that this effect held
even after six minutes of deliberation, demonstrating that CSI provides both
the qualitative benefits of conversational focus groups and the quantitative
benefits of largescale polling.
","2023-10-24","2310.13702v1.pdf"
"2310.13704","Kanyifeechukwu Oguine","Munachimso Blessing Oguine, Chidera Godsfavor Oguine, Kanyifeechukwu
  Jane Oguine","My Machine and I: ChatGPT and the Future of Human-Machine Collaboration
  in Africa","8 pages, 1 table","","","","cs.HC cs.AI cs.CY","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent advancements in technology have necessitated a paradigm shift in the
people use technology necessitating a new research field called Human-Machine
collaboration. ChatGPT, an Artificial intelligence (AI) assistive technology,
has gained mainstream adoption and implementation in academia and industry;
however, a lot is left unknown about how this new technology holds for
Human-Machine Collaboration in Africa. Our survey paper highlights to answer
some of these questions. To understand the effectiveness of ChatGPT on
human-machine collaboration we utilized reflexive thematic analysis to analyze
(N= 51) articles between 2019 and 2023 obtained from our literature search. Our
findings indicate the prevalence of ChatGPT for human-computer interaction
within academic sectors such as education, and research; trends also revealed
the relatively high effectiveness of ChatGPT in improving human-machine
collaboration.
","2023-10-24","2310.13704v1.pdf"
"2310.13705","Nutchanon Yongsatianchot","Laura B. Hensel, Nutchanon Yongsatianchot, Parisa Torshizi, Elena
  Minucci, Stacy Marsella","Large language models in textual analysis for gesture selection","","25th ACM International Conference on Multimodal Interaction. ICMI.
  2023. 1-10","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Gestures perform a variety of communicative functions that powerfully
influence human face-to-face interaction. How this communicative function is
achieved varies greatly between individuals and depends on the role of the
speaker and the context of the interaction. Approaches to automatic gesture
generation vary not only in the degree to which they rely on data-driven
techniques but also the degree to which they can produce context and speaker
specific gestures. However, these approaches face two major challenges: The
first is obtaining sufficient training data that is appropriate for the context
and the goal of the application. The second is related to designer control to
realize their specific intent for the application. Here, we approach these
challenges by using large language models (LLMs) to show that these powerful
models of large amounts of data can be adapted for gesture analysis and
generation. Specifically, we used ChatGPT as a tool for suggesting
context-specific gestures that can realize designer intent based on minimal
prompts. We also find that ChatGPT can suggests novel yet appropriate gestures
not present in the minimal training data. The use of LLMs is a promising avenue
for gesture generation that reduce the need for laborious annotations and has
the potential to flexibly and quickly adapt to different designer intents.
","2023-10-24","2310.13705v1.pdf"
"2310.13714","Tripti Kumari","Tripti Kumari, Chakali Sai Charan and Ayan Das","A study of the impact of generative AI-based data augmentation on
  software metadata classification","","","","","cs.SE cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents the system submitted by the team from IIT(ISM) Dhanbad in
FIRE IRSE 2023 shared task 1 on the automatic usefulness prediction of
code-comment pairs as well as the impact of Large Language Model(LLM) generated
data on original base data towards an associated source code. We have developed
a framework where we train a machine learning-based model using the neural
contextual representations of the comments and their corresponding codes to
predict the usefulness of code-comments pair and performance analysis with
LLM-generated data with base data. In the official assessment, our system
achieves a 4% increase in F1-score from baseline and the quality of generated
data.
","2023-10-24","2310.13714v1.pdf"
"2310.13715","Marc Schmitt","Marc Schmitt, Ivan Flechais","Digital Deception: Generative Artificial Intelligence in Social
  Engineering and Phishing","Submitted to CHI 2024","","","","cs.CR cs.CY cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advancement of Artificial Intelligence (AI) and Machine Learning (ML) has
profound implications for both the utility and security of our digital
interactions. This paper investigates the transformative role of Generative AI
in Social Engineering (SE) attacks. We conduct a systematic review of social
engineering and AI capabilities and use a theory of social engineering to
identify three pillars where Generative AI amplifies the impact of SE attacks:
Realistic Content Creation, Advanced Targeting and Personalization, and
Automated Attack Infrastructure. We integrate these elements into a conceptual
model designed to investigate the complex nature of AI-driven SE attacks - the
Generative AI Social Engineering Framework. We further explore human
implications and potential countermeasures to mitigate these risks. Our study
aims to foster a deeper understanding of the risks, human implications, and
countermeasures associated with this emerging paradigm, thereby contributing to
a more secure and trustworthy human-computer interaction.
","2023-10-24","2310.13715v1.pdf"
"2310.13730","Samyadeep Basu","Samyadeep Basu, Nanxuan Zhao, Vlad Morariu, Soheil Feizi, Varun
  Manjunatha","Localizing and Editing Knowledge in Text-to-Image Generative Models","61 pages","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have
achieved unprecedented quality of photorealism with state-of-the-art FID scores
on MS-COCO and other generation benchmarks. Given a caption, image generation
requires fine-grained knowledge about attributes such as object structure,
style, and viewpoint amongst others. Where does this information reside in
text-to-image generative models? In our paper, we tackle this question and
understand how knowledge corresponding to distinct visual attributes is stored
in large-scale text-to-image diffusion models. We adapt Causal Mediation
Analysis for text-to-image models and trace knowledge about distinct visual
attributes to various (causal) components in the (i) UNet and (ii) text-encoder
of the diffusion model. In particular, we show that unlike generative
large-language models, knowledge about different attributes is not localized in
isolated components, but is instead distributed amongst a set of components in
the conditional UNet. These sets of components are often distinct for different
visual attributes. Remarkably, we find that the CLIP text-encoder in public
text-to-image models such as Stable-Diffusion contains only one causal state
across different visual attributes, and this is the first self-attention layer
corresponding to the last subject token of the attribute in the caption. This
is in stark contrast to the causal states in other language models which are
often the mid-MLP layers. Based on this observation of only one causal state in
the text-encoder, we introduce a fast, data-free model editing method
Diff-QuickFix which can effectively edit concepts in text-to-image models.
DiffQuickFix can edit (ablate) concepts in under a second with a closed-form
update, providing a significant 1000x speedup and comparable editing
performance to existing fine-tuning based editing methods.
","2023-10-24","2310.13730v1.pdf"
"2310.13760","Hwanjun Song","Hwanjun Song, Igor Shalyminov, Hang Su, Siffi Singh, Kaisheng Yao,
  Saab Mansour","Enhancing Abstractiveness of Summarization Models through Calibrated
  Distillation","Accepted at EMNLP-Findings 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Sequence-level knowledge distillation reduces the size of Seq2Seq models for
more efficient abstractive summarization. However, it often leads to a loss of
abstractiveness in summarization. In this paper, we propose a novel approach
named DisCal to enhance the level of abstractiveness (measured by n-gram
overlap) without sacrificing the informativeness (measured by ROUGE) of
generated summaries. DisCal exposes diverse pseudo summaries with two
supervision to the student model. Firstly, the best pseudo summary is
identified in terms of abstractiveness and informativeness and used for
sequence-level distillation. Secondly, their ranks are used to ensure the
student model to assign higher prediction scores to summaries with higher
ranks. Our experiments show that DisCal outperforms prior methods in
abstractive summarization distillation, producing highly abstractive and
informative summaries.
","2023-10-24","2310.13760v1.pdf"
"2310.13771","Jiaang Li","Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders S{\o}gaard","Copyright Violations and Large Language Models","EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Language models may memorize more than just facts, including entire chunks of
texts seen during training. Fair use exemptions to copyright laws typically
allow for limited use of copyrighted material without permission from the
copyright holder, but typically for extraction of information from copyrighted
materials, rather than {\em verbatim} reproduction. This work explores the
issue of copyright violations and large language models through the lens of
verbatim memorization, focusing on possible redistribution of copyrighted text.
We present experiments with a range of language models over a collection of
popular books and coding problems, providing a conservative characterization of
the extent to which language models can redistribute these materials. Overall,
this research highlights the need for further examination and the potential
impact on future developments in natural language processing to ensure
adherence to copyright regulations. Code is at
\url{https://github.com/coastalcph/CopyrightLLMs}.
","2023-10-24","2310.13771v1.pdf"
"2310.13786","Eric Aubinais","Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida","Fundamental Limits of Membership Inference Attacks on Machine Learning
  Models","","","","","stat.ML cs.AI cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  Membership inference attacks (MIA) can reveal whether a particular data point
was part of the training dataset, potentially exposing sensitive information
about individuals. This article explores the fundamental statistical
limitations associated with MIAs on machine learning models. More precisely, we
first derive the statistical quantity that governs the effectiveness and
success of such attacks. Then, we investigate several situations for which we
provide bounds on this quantity of interest. This allows us to infer the
accuracy of potential attacks as a function of the number of samples and other
structural parameters of learning models, which in some cases can be directly
estimated from the dataset.
","2023-10-24","2310.13786v1.pdf"
"2310.13798","Sandipan Kundu","Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew
  Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden
  McLean, Catherine Olsson, Cassie Evraets, Eli Tran-Johnson, Esin Durmus,
  Ethan Perez, Jackson Kernion, Jamie Kerr, Kamal Ndousse, Karina Nguyen,
  Nelson Elhage, Newton Cheng, Nicholas Schiefer, Nova DasSarma, Oliver Rausch,
  Robin Larson, Shannon Yang, Shauna Kravec, Timothy Telleen-Lawton, Thomas I.
  Liao, Tom Henighan, Tristan Hume, Zac Hatfield-Dodds, S\""oren Mindermann,
  Nicholas Joseph, Sam McCandlish, Jared Kaplan","Specific versus General Principles for Constitutional AI","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Human feedback can prevent overtly harmful utterances in conversational
models, but may not automatically mitigate subtle problematic behaviors such as
a stated desire for self-preservation or power. Constitutional AI offers an
alternative, replacing human feedback with feedback from AI models conditioned
only on a list of written principles. We find this approach effectively
prevents the expression of such behaviors. The success of simple principles
motivates us to ask: can models learn general ethical behaviors from only a
single written principle? To test this, we run experiments using a principle
roughly stated as ""do what's best for humanity"". We find that the largest
dialogue models can generalize from this short constitution, resulting in
harmless assistants with no stated interest in specific motivations like power.
A general principle may thus partially avoid the need for a long list of
constitutions targeting potentially harmful behaviors. However, more detailed
constitutions still improve fine-grained control over specific types of harms.
This suggests both general and specific principles have value for steering AI
safely.
","2023-10-24","2310.13798v1.pdf"
"2310.13800","Zheng Yuan","Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan","Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large
  Language Models on Sequence to Sequence Tasks","Accepted at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLMs) evaluation is a patchy and inconsistent
landscape, and it is becoming clear that the quality of automatic evaluation
metrics is not keeping up with the pace of development of generative models. We
aim to improve the understanding of current models' performance by providing a
preliminary and hybrid evaluation on a range of open and closed-source
generative LLMs on three NLP benchmarks: text summarisation, text
simplification and grammatical error correction (GEC), using both automatic and
human evaluation. We also explore the potential of the recently released GPT-4
to act as an evaluator. We find that ChatGPT consistently outperforms many
other popular models according to human reviewers on the majority of metrics,
while scoring much more poorly when using classic automatic evaluation metrics.
We also find that human reviewers rate the gold reference as much worse than
the best models' outputs, indicating the poor quality of many popular
benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs
in a way which aligns reasonably closely to human judgement despite
task-specific variations, with a lower alignment in the GEC task.
","2023-10-24","2310.13800v1.pdf"
"2310.13802","Emilio Vital Brazil","Eduardo Soares, Akihiro Kishimoto, Emilio Vital Brazil, Seiji Takeda,
  Hiroshi Kajino, Renato Cerqueira","Improving Molecular Properties Prediction Through Latent Space Fusion","8 Pages, 4 Figures - Submited to the AI4Science Workshop - Neurips
  2023","","","","cs.LG cs.AI cs.CE q-bio.QM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pre-trained Language Models have emerged as promising tools for predicting
molecular properties, yet their development is in its early stages,
necessitating further research to enhance their efficacy and address challenges
such as generalization and sample efficiency. In this paper, we present a
multi-view approach that combines latent spaces derived from state-of-the-art
chemical models. Our approach relies on two pivotal elements: the embeddings
derived from MHG-GNN, which represent molecular structures as graphs, and
MoLFormer embeddings rooted in chemical language. The attention mechanism of
MoLFormer is able to identify relations between two atoms even when their
distance is far apart, while the GNN of MHG-GNN can more precisely capture
relations among multiple atoms closely located. In this work, we demonstrate
the superior performance of our proposed multi-view approach compared to
existing state-of-the-art methods, including MoLFormer-XL, which was trained on
1.1 billion molecules, particularly in intricate tasks such as predicting
clinical trial drug toxicity and inhibiting HIV replication. We assessed our
approach using six benchmark datasets from MoleculeNet, where it outperformed
competitors in five of them. Our study highlights the potential of latent space
fusion and feature integration for advancing molecular property prediction. In
this work, we use small versions of MHG-GNN and MoLFormer, which opens up an
opportunity for further improvement when our approach uses a larger-scale
dataset.
","2023-10-24","2310.13802v1.pdf"
"2310.13807","Yu Sun","Yu Sun, Xinhao Li, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos
  Guestrin, Xiaolong Wang, Tatsunori Hashimoto, Xinlei Chen","Learning to (Learn at Test Time)","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  We reformulate the problem of supervised learning as learning to learn with
two nested loops (i.e. learning problems). The inner loop learns on each
individual instance with self-supervision before final prediction. The outer
loop learns the self-supervised task used by the inner loop, such that its
final prediction improves. Our inner loop turns out to be equivalent to linear
attention when the inner-loop learner is only a linear model, and to
self-attention when it is a kernel estimator. For practical comparison with
linear or self-attention layers, we replace each of them in a transformer with
an inner loop, so our outer loop is equivalent to training the architecture.
When each inner-loop learner is a neural network, our approach vastly
outperforms transformers with linear attention on ImageNet from 224 x 224 raw
pixels in both accuracy and FLOPs, while (regular) transformers cannot run.
","2023-10-24","2310.13807v1.pdf"
"2310.13819","Yan Di","Bowen Fu, Sek Kun Leong, Yan Di, Jiwen Tang and Xiangyang Ji","LanPose: Language-Instructed 6D Object Pose Estimation for Robotic
  Assembly","8 pages","","","","cs.RO","http://creativecommons.org/licenses/by-sa/4.0/","  Comprehending natural language instructions is a critical skill for robots to
cooperate effectively with humans. In this paper, we aim to learn 6D poses for
roboticassembly by natural language instructions. For this purpose,
Language-Instructed 6D Pose Regression Network (LanPose) is proposed to jointly
predict the 6D poses of the observed object and the corresponding assembly
position. Our proposed approach is based on the fusion of geometric and
linguistic features, which allows us to finely integrate multi-modality input
and map it to the 6D pose in SE(3) space by the cross-attention mechanism and
the language-integrated 6D pose mapping module, respectively. To validate the
effectiveness of our approach, an integrated robotic system is established to
precisely and robustly perceive, grasp, manipulate and assemble blocks by
language commands. 98.09 and 93.55 in ADD(-S)-0.1d are derived for the
prediction of 6D object pose and 6D assembly pose, respectively. Both
quantitative and qualitative results demonstrate the effectiveness of our
proposed language-instructed 6D pose estimation methodology and its potential
to enable robots to better understand and execute natural language
instructions.
","2023-10-24","2310.13819v1.pdf"
"2310.13824","Soo Hyun Ryu","Soo Hyun Ryu","Plausibility Processing in Transformer Language Models: Focusing on the
  Role of Attention Heads in GPT","EMNLP-findings 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The goal of this paper is to explore how Transformer language models process
semantic knowledge, especially regarding the plausibility of noun-verb
relations. First, I demonstrate GPT2 exhibits a higher degree of similarity
with humans in plausibility processing compared to other Transformer language
models. Next, I delve into how knowledge of plausibility is contained within
attention heads of GPT2 and how these heads causally contribute to GPT2's
plausibility processing ability. Through several experiments, it was found
that: i) GPT2 has a number of attention heads that detect plausible noun-verb
relationships; ii) these heads collectively contribute to the Transformer's
ability to process plausibility, albeit to varying degrees; and iii) attention
heads' individual performance in detecting plausibility does not necessarily
correlate with how much they contribute to GPT2's plausibility processing
ability.
","2023-10-24","2310.13824v1.pdf"
"2310.13828","Shawn Shan","Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, Ben Y.
  Zhao","Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models","","","","","cs.CR cs.AI","http://creativecommons.org/licenses/by/4.0/","  Data poisoning attacks manipulate training data to introduce unexpected
behaviors into machine learning models at training time. For text-to-image
generative models with massive training datasets, current understanding of
poisoning attacks suggests that a successful attack would require injecting
millions of poison samples into their training pipeline. In this paper, we show
that poisoning attacks can be successful on generative models. We observe that
training data per concept can be quite limited in these models, making them
vulnerable to prompt-specific poisoning attacks, which target a model's ability
to respond to individual prompts.
  We introduce Nightshade, an optimized prompt-specific poisoning attack where
poison samples look visually identical to benign images with matching text
prompts. Nightshade poison samples are also optimized for potency and can
corrupt an Stable Diffusion SDXL prompt in <100 poison samples. Nightshade
poison effects ""bleed through"" to related concepts, and multiple attacks can
composed together in a single prompt. Surprisingly, we show that a moderate
number of Nightshade attacks can destabilize general features in a
text-to-image generative model, effectively disabling its ability to generate
meaningful images. Finally, we propose the use of Nightshade` and similar tools
as a last defense for content creators against web scrapers that ignore
opt-out/do-not-crawl directives, and discuss possible implications for model
trainers and content creators.
","2023-10-24","2310.13828v1.pdf"
"2310.13836","Max Vargas","Adam Tsou, Max Vargas, Andrew Engel, Tony Chiang","Foundation Model's Embedded Representations May Detect Distribution
  Shift","14 pages, 8 figures, 5 tables","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Distribution shifts between train and test datasets obscure our ability to
understand the generalization capacity of neural network models. This topic is
especially relevant given the success of pre-trained foundation models as
starting points for transfer learning (TL) models across tasks and contexts. We
present a case study for TL on a pre-trained GPT-2 model onto the Sentiment140
dataset for sentiment classification. We show that Sentiment140's test dataset
$M$ is not sampled from the same distribution as the training dataset $P$, and
hence training on $P$ and measuring performance on $M$ does not actually
account for the model's generalization on sentiment classification.
","2023-10-24","2310.13836v1.pdf"
"2310.13848","Priyanka Ranade","Priyanka Ranade and Anupam Joshi","FABULA: Intelligence Report Generation Using Retrieval-Augmented
  Narrative Construction","","2023 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining (ASONAM)","10.1145/3625007.3627505","","cs.IR","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
  We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.
","2023-10-24","2310.13848v1.pdf"
"2310.13850","Nan-Jiang Jiang","Nan-Jiang Jiang, Chenhao Tan, Marie-Catherine de Marneffe","Ecologically Valid Explanations for Label Variation in NLI","Findings at EMNLP 2023. Overlap with previous version
  arXiv:2304.12443","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Human label variation, or annotation disagreement, exists in many natural
language processing (NLP) tasks, including natural language inference (NLI). To
gain direct evidence of how NLI label variation arises, we build LiveNLI, an
English dataset of 1,415 ecologically valid explanations (annotators explain
the NLI labels they chose) for 122 MNLI items (at least 10 explanations per
item). The LiveNLI explanations confirm that people can systematically vary on
their interpretation and highlight within-label variation: annotators sometimes
choose the same label for different reasons. This suggests that explanations
are crucial for navigating label interpretations in general. We few-shot prompt
large language models to generate explanations but the results are
inconsistent: they sometimes produces valid and informative explanations, but
it also generates implausible ones that do not support the label, highlighting
directions for improvement.
","2023-10-24","2310.13850v1.pdf"
"2310.13855","Xinyu Hu","Xinyu Hu, Pengfei Tang, Simiao Zuo, Zihan Wang, Bowen Song, Qiang Lou,
  Jian Jiao, Denis Charles","Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author
  Prompt Editing","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have made impressive progress in natural
language processing. These models rely on proper human instructions (or
prompts) to generate suitable responses. However, the potential of LLMs are not
fully harnessed by commonly-used prompting methods: many human-in-the-loop
algorithms employ ad-hoc procedures for prompt selection; while auto prompt
generation approaches are essentially searching all possible prompts randomly
and inefficiently. We propose Evoke, an automatic prompt refinement framework.
In Evoke, there are two instances of a same LLM: one as a reviewer
(LLM-Reviewer), it scores the current prompt; the other as an author
(LLM-Author), it edits the prompt by considering the edit history and the
reviewer's feedback. Such an author-reviewer feedback loop ensures that the
prompt is refined in each iteration. We further aggregate a data selection
approach to Evoke, where only the hard samples are exposed to the LLM. The hard
samples are more important because the LLM can develop deeper understanding of
the tasks out of them, while the model may already know how to solve the easier
cases. Experimental results show that Evoke significantly outperforms existing
methods. For instance, in the challenging task of logical fallacy detection,
Evoke scores above 80, while all other baseline methods struggle to reach 20.
","2023-10-24","2310.13855v1.pdf"
"2310.13856","Sagnik Ray Choudhury","Sagnik Ray Choudhury and Jushaan Kalra","Implications of Annotation Artifacts in Edge Probing Test Datasets","Accepted CoNLL 2023, code: https://github.com/Josh1108/EPtest.git","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Edge probing tests are classification tasks that test for grammatical
knowledge encoded in token representations coming from contextual encoders such
as large language models (LLMs). Many LLM encoders have shown high performance
in EP tests, leading to conjectures about their ability to encode linguistic
knowledge. However, a large body of research claims that the tests necessarily
do not measure the LLM's capacity to encode knowledge, but rather reflect the
classifiers' ability to learn the problem. Much of this criticism stems from
the fact that often the classifiers have very similar accuracy when an LLM vs a
random encoder is used. Consequently, several modifications to the tests have
been suggested, including information theoretic probes. We show that commonly
used edge probing test datasets have various biases including memorization.
When these biases are removed, the LLM encoders do show a significant
difference from the random ones, even with the simple non-information theoretic
probes.
","2023-10-24","2310.13856v1.pdf"
"2310.13888","Liyuan Wang","Liyuan Wang, Jingyi Xie, Xingxing Zhang, Hang Su, Jun Zhu","Towards a General Framework for Continual Learning with Pre-training","This is a generalized version of our HiDe-Prompt and will be
  presented in the IMOL workshop in NeurIPS 2023. arXiv admin note: text
  overlap with arXiv:2310.07234","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this work, we present a general framework for continual learning of
sequentially arrived tasks with the use of pre-training, which has emerged as a
promising direction for artificial intelligence systems to accommodate
real-world dynamics. From a theoretical perspective, we decompose its objective
into three hierarchical components, including within-task prediction,
task-identity inference, and task-adaptive prediction. Then we propose an
innovative approach to explicitly optimize these components with
parameter-efficient fine-tuning (PEFT) techniques and representation
statistics. We empirically demonstrate the superiority and generality of our
approach in downstream continual learning, and further explore the
applicability of PEFT techniques in upstream continual learning. We also
discuss the biological basis of the proposed framework with recent advances in
neuroscience.
","2023-10-24","2310.13888v1.pdf"
"2310.13896","Eason Chen","Eason Chen, Ray Huang, Justa Liang, Damien Chen, Pierce Hung","GPTutor: an open-source AI pair programming tool alternative to Copilot","","","","","cs.HC","http://creativecommons.org/licenses/by/4.0/","  This paper presents the latest progress of GPTutor: a ChatGPT-powered
programming tool extension in Visual Studio Code. The emergence of Large
Language Models (LLMs) has improved software development efficiency, but their
performance can be hindered by training data limitations and prompt design
issues. Existing LLM development tools often operate as black boxes, with users
unable to view the prompts used and unable to improve performance by correcting
prompts when errors occur. To address the aforementioned issues, GPTutor was
introduced as an open-source AI pair programming tool, offering an alternative
to Copilot. GPTutor empowers users to customize prompts for various programming
languages and scenarios, with support for 120+ human languages and 50+
programming languages. Users can fine-tune prompts to correct the errors from
LLM for precision and efficient code generation. At the end of the paper, we
underscore GPTutor's potential through examples, including demonstrating its
proficiency in interpreting and generating Sui-Move, a newly introduced smart
contract language, using prompt engineering.
","2023-10-26","2310.13896v1.pdf"
"2310.13953","Vasiliy Seibert","Vasiliy Seibert","Towards dialogue based, computer aided software requirements elicitation","","","","","cs.IR cs.AI","http://creativecommons.org/licenses/by/4.0/","  Several approaches have been presented, which aim to extract models from
natural language specifications. These approaches have inherent weaknesses for
they assume an initial problem understanding that is perfect, and they leave no
room for feedback. Motivated by real-world collaboration settings between
requirements engineers and customers, this paper proposes an interaction
blueprint that aims for dialogue based, computer aided software requirements
analysis. Compared to mere model extraction approaches, this interaction
blueprint encourages individuality, creativity and genuine compromise. A
simplistic Experiment was conducted to showcase the general idea. This paper
discusses the experiment as well as the proposed interaction blueprint and
argues, that advancements in natural language processing and generative AI
might lead to significant progress in a foreseeable future. However, for that,
there is a need to move away from a magical black box expectation and instead
moving towards a dialogue based approach that recognizes the individuality that
is an undeniable part of requirements engineering.
","2023-10-24","2310.13953v1.pdf"
"2310.13961","Young-Suk Lee Dr.","Young-Suk Lee, Md Arafat Sultan, Yousef El-Kurdi, Tahira Naseem Asim
  Munawar, Radu Florian, Salim Roukos, Ram\'on Fernandez Astudillo","Ensemble-Instruct: Generating Instruction-Tuning Data with a
  Heterogeneous Mixture of LMs","","EMNLP 2023","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Using in-context learning (ICL) for data generation, techniques such as
Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023)
can train strong conversational agents with only a small amount of human
supervision. One limitation of these approaches is that they resort to very
large language models (around 175B parameters) that are also proprietary and
non-public. Here we explore the application of such techniques to language
models that are much smaller (around 10B--40B parameters) and have permissive
licenses. We find the Self-Instruct approach to be less effective at these
sizes and propose new ICL methods that draw on two main ideas: (a)
Categorization and simplification of the ICL templates to make prompt learning
easier for the LM, and (b) Ensembling over multiple LM outputs to help select
high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct
seed tasks and employs separate pipelines for instructions that require an
input and instructions that do not. Empirical investigations with different LMs
show that: (1) Our proposed method yields higher-quality instruction tuning
data than Self-Instruct, (2) It improves performances of both vanilla and
instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned
LMs generate more useful outputs than their larger un-tuned counterparts. Our
codebase is available at https://github.com/IBM/ensemble-instruct.
","2023-10-24","2310.13961v1.pdf"
"2310.13976","Chetan Arora","Chetan Arora, John Grundy, Mohamed Abdelrazek","Advancing Requirements Engineering through Generative AI: Assessing the
  Role of LLMs","","","","","cs.SE","http://creativecommons.org/licenses/by/4.0/","  Requirements Engineering (RE) is a critical phase in software development
including the elicitation, analysis, specification, and validation of software
requirements. Despite the importance of RE, it remains a challenging process
due to the complexities of communication, uncertainty in the early stages and
inadequate automation support. In recent years, large-language models (LLMs)
have shown significant promise in diverse domains, including natural language
processing, code generation, and program understanding. This chapter explores
the potential of LLMs in driving RE processes, aiming to improve the efficiency
and accuracy of requirements-related tasks. We propose key directions and SWOT
analysis for research and development in using LLMs for RE, focusing on the
potential for requirements elicitation, analysis, specification, and
validation. We further present the results from a preliminary evaluation, in
this context.
","2023-10-24","2310.13976v1.pdf"
"2310.13985","Vibhor Agarwal","Vibhor Agarwal, Yu Chen, Nishanth Sastry","HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online
  Posts using Large Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Hate speech has become pervasive in today's digital age. Although there has
been considerable research to detect hate speech or generate counter speech to
combat hateful views, these approaches still cannot completely eliminate the
potential harmful societal consequences of hate speech -- hate speech, even
when detected, can often not be taken down or is often not taken down enough;
and hate speech unfortunately spreads quickly, often much faster than any
generated counter speech.
  This paper investigates a relatively new yet simple and effective approach of
suggesting a rephrasing of potential hate speech content even before the post
is made. We show that Large Language Models (LLMs) perform well on this task,
outperforming state-of-the-art baselines such as BART-Detox. We develop 4
different prompts based on task description, hate definition, few-shot
demonstrations and chain-of-thoughts for comprehensive experiments and conduct
experiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well
as OpenAI's GPT-3.5. We propose various evaluation metrics to measure the
efficacy of the generated text and ensure the generated text has reduced hate
intensity without drastically changing the semantic meaning of the original
text.
  We find that LLMs with a few-shot demonstrations prompt work the best in
generating acceptable hate-rephrased text with semantic meaning similar to the
original text. Overall, we find that GPT-3.5 outperforms the baseline and
open-source models for all the different kinds of prompts. We also perform
human evaluations and interestingly, find that the rephrasings generated by
GPT-3.5 outperform even the human-generated ground-truth rephrasings in the
dataset. We also conduct detailed ablation studies to investigate why LLMs work
satisfactorily on this task and conduct a failure analysis to understand the
gaps.
","2023-10-24","2310.13985v1.pdf"
"2310.13988","Tom Kocmi","Tom Kocmi and Christian Federmann","GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4","Accepted to WMT 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to
detect translation quality errors, specifically for the quality estimation
setting without the need for human reference translations. Based on the power
of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting
technique, querying the GPT-4 model to mark error quality spans. Compared to
previous works, our method has language-agnostic prompts, thus avoiding the
need for manual prompt preparation for new languages.
  While preliminary results indicate that GEMBA-MQM achieves state-of-the-art
accuracy for system ranking, we advise caution when using it in academic works
to demonstrate improvements over other methods due to its dependence on the
proprietary, black-box GPT model.
","2023-10-24","2310.13988v1.pdf"
"2310.13990","Pierre Colombo","Pierre Colombo, Nathan Noiry, Guillaume Staerman, Pablo Piantanida","A Novel Information-Theoretic Objective to Disentangle Representations
  for Fair Classification","Findings AACL 2023","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  One of the pursued objectives of deep learning is to provide tools that learn
abstract representations of reality from the observation of multiple contextual
situations. More precisely, one wishes to extract disentangled representations
which are (i) low dimensional and (ii) whose components are independent and
correspond to concepts capturing the essence of the objects under consideration
(Locatello et al., 2019b). One step towards this ambitious project consists in
learning disentangled representations with respect to a predefined (sensitive)
attribute, e.g., the gender or age of the writer. Perhaps one of the main
application for such disentangled representations is fair classification.
Existing methods extract the last layer of a neural network trained with a loss
that is composed of a cross-entropy objective and a disentanglement
regularizer. In this work, we adopt an information-theoretic view of this
problem which motivates a novel family of regularizers that minimizes the
mutual information between the latent representation and the sensitive
attribute conditional to the target. The resulting set of losses, called
CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC
losses are studied through extensive numerical experiments by training over 2k
neural networks. We demonstrate that our methods offer a better
disentanglement/accuracy trade-off than previous techniques, and generalize
better than training with cross-entropy loss solely provided that the
disentanglement task is not too constraining.
","2023-10-24","2310.13990v1.pdf"
"2310.13995","Yaoyiran Li","Yaoyiran Li, Anna Korhonen, Ivan Vuli\'c","On Bilingual Lexicon Induction with Large Language Models","EMNLP 2023 Main Conference","","","","cs.CL cs.AI cs.IR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that
still, to a large extent, relies on calculating cross-lingual word
representations. Inspired by the global paradigm shift in NLP towards Large
Language Models (LLMs), we examine the potential of the latest generation of
LLMs for the development of bilingual lexicons. We ask the following research
question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for
BLI, and how does this approach compare against and complement current BLI
approaches? To this end, we systematically study 1) zero-shot prompting for
unsupervised BLI and 2) few-shot in-context prompting with a set of seed
translation pairs, both without any LLM fine-tuning, as well as 3) standard
BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source
text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two
standard BLI benchmarks covering a range of typologically diverse languages.
Our work is the first to demonstrate strong BLI capabilities of text-to-text
mLLMs. The results reveal that few-shot prompting with in-context examples from
nearest neighbours achieves the best performance, establishing new
state-of-the-art BLI scores for many language pairs. We also conduct a series
of in-depth analyses and ablation studies, providing more insights on BLI with
(m)LLMs, also along with their limitations.
","2023-10-24","2310.13995v1.pdf"
"2310.14021","James Pan","James Jie Pan, Jianguo Wang, Guoliang Li","Survey of Vector Database Management Systems","25 pages","","","","cs.DB","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There are now over 20 commercial vector database management systems (VDBMSs),
all produced within the past five years. But embedding-based retrieval has been
studied for over ten years, and similarity search a staggering half century and
more. Driving this shift from algorithms to systems are new data intensive
applications, notably large language models, that demand vast stores of
unstructured data coupled with reliable, secure, fast, and scalable query
processing capability. A variety of new data management techniques now exist
for addressing these needs, however there is no comprehensive survey to
thoroughly review these techniques and systems. We start by identifying five
main obstacles to vector data management, namely vagueness of semantic
similarity, large size of vectors, high cost of similarity comparison, lack of
natural partitioning that can be used for indexing, and difficulty of
efficiently answering hybrid queries that require both attributes and vectors.
Overcoming these obstacles has led to new approaches to query processing,
storage and indexing, and query optimization and execution. For query
processing, a variety of similarity scores and query types are now well
understood; for storage and indexing, techniques include vector compression,
namely quantization, and partitioning based on randomization, learning
partitioning, and navigable partitioning; for query optimization and execution,
we describe new operators for hybrid queries, as well as techniques for plan
enumeration, plan selection, and hardware accelerated execution. These
techniques lead to a variety of VDBMSs across a spectrum of design and runtime
characteristics, including native systems specialized for vectors and extended
systems that incorporate vector capabilities into existing systems. We then
discuss benchmarks, and finally we outline research challenges and point the
direction for future work.
","2023-10-24","2310.14021v1.pdf"
"2310.14025","Maria Lymperaiou","Anastasia Kritharoula, Maria Lymperaiou and Giorgos Stamou","Large Language Models and Multimodal Retrieval for Visual Word Sense
  Disambiguation","Conference on Empirical Methods in Natural Language Processing
  (EMNLP) 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the
goal of retrieving an image among a set of candidates, which better represents
the meaning of an ambiguous word within a given context. In this paper, we make
a substantial step towards unveiling this interesting task by applying a
varying set of approaches. Since VWSD is primarily a text-image retrieval task,
we explore the latest transformer-based methods for multimodal retrieval.
Additionally, we utilize Large Language Models (LLMs) as knowledge bases to
enhance the given phrases and resolve ambiguity related to the target word. We
also study VWSD as a unimodal problem by converting to text-to-text and
image-to-image retrieval, as well as question-answering (QA), to fully explore
the capabilities of relevant models. To tap into the implicit knowledge of
LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable
answer generation. On top of all, we train a learn to rank (LTR) model in order
to combine our different modules, achieving competitive ranking results.
Extensive experiments on VWSD demonstrate valuable insights to effectively
drive future directions.
","2023-10-24","2310.14025v1.pdf"
"2310.14029","Adji Bousso Dieng","Andre Niyongabo Rubungo, Craig Arnold, Barry P. Rand, Adji Bousso
  Dieng","LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline
  Solids From Their Text Descriptions","Code for LLM-Prop can be found at:
  https://github.com/vertaix/LLM-Prop","","","","cs.CL cond-mat.mtrl-sci","http://creativecommons.org/licenses/by/4.0/","  The prediction of crystal properties plays a crucial role in the crystal
design process. Current methods for predicting crystal properties focus on
modeling crystal structures using graph neural networks (GNNs). Although GNNs
are powerful, accurately modeling the complex interactions between atoms and
molecules within a crystal remains a challenge. Surprisingly, predicting
crystal properties from crystal text descriptions is understudied, despite the
rich information and expressiveness that text data offer. One of the main
reasons is the lack of publicly available data for this task. In this paper, we
develop and make public a benchmark dataset (called TextEdge) that contains
text descriptions of crystal structures with their properties. We then propose
LLM-Prop, a method that leverages the general-purpose learning capabilities of
large language models (LLMs) to predict the physical and electronic properties
of crystals from their text descriptions. LLM-Prop outperforms the current
state-of-the-art GNN-based crystal property predictor by about 4% in predicting
band gap, 3% in classifying whether the band gap is direct or indirect, and 66%
in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT,
a domain-specific pre-trained BERT model, despite having 3 times fewer
parameters. Our empirical results may highlight the current inability of GNNs
to capture information pertaining to space group symmetry and Wyckoff sites for
accurate crystal property prediction.
","2023-10-24","2310.14029v1.pdf"
"2310.14034","John Morris","John X. Morris, Chandan Singh, Alexander M. Rush, Jianfeng Gao,
  Yuntian Deng","Tree Prompting: Efficient Task Adaptation without Fine-Tuning","Both first authors contributed equally; accepted to EMNLP 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Prompting language models (LMs) is the main interface for applying them to
new tasks. However, for smaller LMs, prompting provides low accuracy compared
to gradient-based finetuning. Tree Prompting is an approach to prompting which
builds a decision tree of prompts, linking multiple LM calls together to solve
a task. At inference time, each call to the LM is determined by efficiently
routing the outcome of the previous call using the tree. Experiments on
classification datasets show that Tree Prompting improves accuracy over
competing methods and is competitive with fine-tuning. We also show that
variants of Tree Prompting allow inspection of a model's decision-making
process.
","2023-10-24","2310.14034v1.pdf"
"2310.14036","Mihaela Rosca","Mihaela Claudia Rosca","On discretisation drift and smoothness regularisation in neural network
  training","PhD thesis. arXiv admin note: text overlap with arXiv:2302.01952","","","","stat.ML cs.LG","http://creativecommons.org/licenses/by/4.0/","  The deep learning recipe of casting real-world problems as mathematical
optimisation and tackling the optimisation by training deep neural networks
using gradient-based optimisation has undoubtedly proven to be a fruitful one.
The understanding behind why deep learning works, however, has lagged behind
its practical significance. We aim to make steps towards an improved
understanding of deep learning with a focus on optimisation and model
regularisation. We start by investigating gradient descent (GD), a
discrete-time algorithm at the basis of most popular deep learning optimisation
algorithms. Understanding the dynamics of GD has been hindered by the presence
of discretisation drift, the numerical integration error between GD and its
often studied continuous-time counterpart, the negative gradient flow (NGF). To
add to the toolkit available to study GD, we derive novel continuous-time flows
that account for discretisation drift. Unlike the NGF, these new flows can be
used to describe learning rate specific behaviours of GD, such as training
instabilities observed in supervised learning and two-player games. We then
translate insights from continuous time into mitigation strategies for unstable
GD dynamics, by constructing novel learning rate schedules and regularisers
that do not require additional hyperparameters. Like optimisation, smoothness
regularisation is another pillar of deep learning's success with wide use in
supervised learning and generative modelling. Despite their individual
significance, the interactions between smoothness regularisation and
optimisation have yet to be explored. We find that smoothness regularisation
affects optimisation across multiple deep learning domains, and that
incorporating smoothness regularisation in reinforcement learning leads to a
performance boost that can be recovered using adaptions to optimisation
methods.
","2023-10-24","2310.14036v1.pdf"
"2310.14053","Marcus J. Min","Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail
  Kaiser, Suman Jana, Baishakhi Ray","Beyond Accuracy: Evaluating Self-Consistency of Code Large Language
  Models with IdentityChain","Code available at https://github.com/marcusm117/IdentityChain","","","","cs.LG cs.CL cs.SE","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Code Large Language Models (Code LLMs) are being increasingly employed in
real-life applications, so evaluating them is critical. While the general
accuracy of Code LLMs on individual tasks has been extensively evaluated, their
self-consistency across different tasks is overlooked. Intuitively, a
trustworthy model should be self-consistent when generating natural language
specifications for its own code and generating code for its own specifications.
Failure to preserve self-consistency reveals a lack of understanding of the
shared semantics underlying natural language and programming language, and
therefore undermines the trustworthiness of a model. In this paper, we first
formally define the self-consistency of Code LLMs and then design a framework,
IdentityChain, which effectively and efficiently evaluates the self-consistency
and general accuracy of a model at the same time. We study eleven Code LLMs and
show that they fail to preserve self-consistency, which is indeed a distinct
aspect from general accuracy. Furthermore, we show that IdentityChain can be
used as a model debugging tool to expose weaknesses of Code LLMs by
demonstrating three major weaknesses that we identify in current models using
IdentityChain. Our code is available at
https://github.com/marcusm117/IdentityChain.
","2023-10-24","2310.14053v1.pdf"
"2310.14088","Zexue He","Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili,
  Julian McAuley, Chun-Nan Hsu","MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark
  for Language Model Evaluation","Accepted to EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Curated datasets for healthcare are often limited due to the need of human
annotations from experts. In this paper, we present MedEval, a multi-level,
multi-task, and multi-domain medical benchmark to facilitate the development of
language models for healthcare. MedEval is comprehensive and consists of data
from several healthcare systems and spans 35 human body regions from 8
examination modalities. With 22,779 collected sentences and 21,228 reports, we
provide expert annotations at multiple levels, offering a granular potential
usage of the data and supporting a wide range of tasks. Moreover, we
systematically evaluated 10 generic and domain-specific language models under
zero-shot and finetuning settings, from domain-adapted baselines in healthcare
to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our
evaluations reveal varying effectiveness of the two categories of language
models across different tasks, from which we notice the importance of
instruction tuning for few-shot usage of large language models. Our
investigation paves the way toward benchmarking language models for healthcare
and provides valuable insights into the strengths and limitations of adopting
large language models in medical domains, informing their practical
applications and future advancements.
","2023-10-24","2310.14088v1.pdf"
"2310.14092","Yiqing Xu","Yuwei Zeng, Yiqing Xu","Learning Reward for Physical Skills using Large Language Model","CoRL 2023, LangRob workshop","","","","cs.RO cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Learning reward functions for physical skills are challenging due to the vast
spectrum of skills, the high-dimensionality of state and action space, and
nuanced sensory feedback. The complexity of these tasks makes acquiring expert
demonstration data both costly and time-consuming. Large Language Models (LLMs)
contain valuable task-related knowledge that can aid in learning these reward
functions. However, the direct application of LLMs for proposing reward
functions has its limitations such as numerical instability and inability to
incorporate the environment feedback. We aim to extract task knowledge from
LLMs using environment feedback to create efficient reward functions for
physical skills. Our approach consists of two components. We first use the LLM
to propose features and parameterization of the reward function. Next, we
update the parameters of this proposed reward function through an iterative
self-alignment process. In particular, this process minimizes the ranking
inconsistency between the LLM and our learned reward functions based on the new
observations. We validated our method by testing it on three simulated physical
skill learning tasks, demonstrating effective support for our design choices.
","2023-10-24","2310.14092v1.pdf"
"2310.14103","Manuel Faysse","Manuel Faysse, Gautier Viaud, C\'eline Hudelot, Pierre Colombo","Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial
  Applications","Short paper accepted at EMNLP 2023","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the
zero-shot capabilities of Large Language Models (LLMs), but in doing so induces
new evaluation metric requirements. We show LLM-based metrics to be well
adapted to these requirements, and leverage them to conduct an investigation of
task-specialization strategies, quantifying the trade-offs that emerge in
practical industrial settings. Our findings offer practitioners actionable
insights for real-world IFT model deployment.
","2023-10-24","2310.14103v1.pdf"
"2310.14105","Minh Nguyen","Minh Nguyen, Gia H. Ngo, Mert R. Sabuncu","Zero-shot Learning of Individualized Task Contrast Prediction from
  Resting-state Functional Connectomes","Accepted at DALI@MICCAI 2023","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Given sufficient pairs of resting-state and task-evoked fMRI scans from
subjects, it is possible to train ML models to predict subject-specific
task-evoked activity using resting-state functional MRI (rsfMRI) scans.
However, while rsfMRI scans are relatively easy to collect, obtaining
sufficient task fMRI scans is much harder as it involves more complex
experimental designs and procedures. Thus, the reliance on scarce paired data
limits the application of current techniques to only tasks seen during
training. We show that this reliance can be reduced by leveraging group-average
contrasts, enabling zero-shot predictions for novel tasks. Our approach, named
OPIC (short for Omni-Task Prediction of Individual Contrasts), takes as input a
subject's rsfMRI-derived connectome and a group-average contrast, to produce a
prediction of the subject-specific contrast. Similar to zero-shot learning in
large language models using special inputs to obtain answers for novel natural
language processing tasks, inputting group-average contrasts guides the OPIC
model to generalize to novel tasks unseen in training. Experimental results
show that OPIC's predictions for novel tasks are not only better than simple
group-averages, but are also competitive with a state-of-the-art model's
in-domain predictions that was trained using in-domain tasks' data.
","2023-10-24","2310.14105v1.pdf"
"2310.14122","Honglei Zhuang","Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang,
  Michael Berdersky","Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring
  Fine-Grained Relevance Labels","13 pages","","","","cs.IR","http://creativecommons.org/licenses/by/4.0/","  Zero-shot text rankers powered by recent LLMs achieve remarkable ranking
performance by simply prompting. Existing prompts for pointwise LLM rankers
mostly ask the model to choose from binary relevance labels like ""Yes"" and
""No"". However, the lack of intermediate relevance label options may cause the
LLM to provide noisy or biased answers for documents that are partially
relevant to the query. We propose to incorporate fine-grained relevance labels
into the prompt for LLM rankers, enabling them to better differentiate among
documents with different levels of relevance to the query and thus derive a
more accurate ranking. We study two variants of the prompt template, coupled
with different numbers of relevance levels. Our experiments on 8 BEIR data sets
show that adding fine-grained relevance labels significantly improves the
performance of LLM rankers.
","2023-10-24","2310.14122v1.pdf"
"2310.14126","Yuxiang Liu","Yuxiang Liu, Jie Huang, Kevin Chen-Chuan Chang","Ask To The Point: Open-Domain Entity-Centric Question Generation","Accepted to the Findings of EMNLP 2023. Camera-ready version","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  We introduce a new task called *entity-centric question generation* (ECQG),
motivated by real-world applications such as topic-specific learning, assisted
reading, and fact-checking. The task aims to generate questions from an entity
perspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE
with two novel modules: content focusing and question verification. The content
focusing module first identifies a focus as ""what to ask"" to form draft
questions, and the question verification module refines the questions
afterwards by verifying the answerability. We also construct a large-scale
open-domain dataset from SQuAD to support this task. Our extensive experiments
demonstrate that GenCONE significantly and consistently outperforms various
baselines, and two modules are effective and complementary in generating
high-quality questions.
","2023-10-24","2310.14126v1.pdf"
"2310.14138","Matthew Hamilton","Matthew P Hamilton, Caroline X Gao, Glen Wiesner, Kate M Filia, Jana M
  Menssink, Petra Plencnerova, David Baker, Patrick D McGorry, Alexandra
  Parker, Jonathan Karnon, Sue M Cotton and Cathrine Mihalopoulos","A prototype software framework for transparent, reusable and updatable
  computational health economic models","17 pages, 4 tables, 1 figure","","","","econ.GN q-fin.EC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Most health economic analyses are undertaken with the aid of computers.
However, the ethical dimensions of implementing health economic models as
software (or computational health economic models (CHEMs)) are poorly
understood. We propose that developers and funders of CHEMs share ethical
responsibilities to (i) establish socially acceptable user requirements and
design specifications; (ii) ensure fitness for purpose; and (iii) support
socially beneficial use. We further propose that a transparent (T), reusable
(R) and updatable (U) CHEM is suggestive of a project team that has largely
fulfilled these responsibilities. We propose six criteria for assessing CHEMs:
(T1) software files are open access; (T2) project team contributions and
judgments are easily identified; (R1) programming practices promote
generalisability and transferability; (R2) licenses restrict only unethical
reuse; (U1) maintenance infrastructure is in place; and (U2) new releases are
systematically retested and appropriately deprecated. To facilitate CHEMs that
meet TRU criteria, we have developed a prototype software framework in the
open-source programming language R. The framework comprises six code libraries
for authoring CHEMs, supplying CHEMs with data and undertaking analyses with
CHEMs. The prototype software framework integrates with services for software
development and research data archiving. We determine that an initial set of
youth mental health CHEMs we developed with the prototype software framework
wholly meet criteria T1-2, R1-2 and U1 and partially meet criterion U2. Our
assessment criteria and prototype software framework can help inform and
improve ethical implementation of CHEMs. Resource barriers to ethical CHEM
practice should be addressed by research funders.
","2023-10-24","2310.14138v1.pdf"
"2310.14151","Wei Zhu","Wei Zhu and Xiaoling Wang and Huanran Zheng and Mosha Chen and Buzhou
  Tang","PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Biomedical language understanding benchmarks are the driving forces for
artificial intelligence applications with large language model (LLM) back-ends.
However, most current benchmarks: (a) are limited to English which makes it
challenging to replicate many of the successes in English for other languages,
or (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs
apply these knowledge to perform on a wide range of bio-medical tasks, or (c)
have become a publicly available corpus and are leaked to LLMs during
pre-training. To facilitate the research in medical LLMs, we re-build the
Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a
large scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable
test-bed and an online platform for evaluating Chinese LLMs' multi-task
capabilities on a wide range bio-medical tasks including medical entity
recognition, medical text classification, medical natural language inference,
medical dialogue understanding and medical content/dialogue generation. To
establish evaluation on these tasks, we have experimented and report the
results with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning
techniques.
","2023-10-24","2310.14151v1.pdf"
"2310.14174","Yantao Liu","Yantao Liu, Zixuan Li, Xiaolong Jin, Long Bai, Saiping Guan, Jiafeng
  Guo and Xueqi Cheng","An In-Context Schema Understanding Method for Knowledge Base Question
  Answering","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The Knowledge Base Question Answering (KBQA) task aims to answer natural
language questions based on a given knowledge base. As a kind of common method
for this task, semantic parsing-based ones first convert natural language
questions to logical forms (e.g., SPARQL queries) and then execute them on
knowledge bases to get answers. Recently, Large Language Models (LLMs) have
shown strong abilities in language understanding and may be adopted as semantic
parsers in such kinds of methods. However, in doing so, a great challenge for
LLMs is to understand the schema of knowledge bases. Therefore, in this paper,
we propose an In-Context Schema Understanding (ICSU) method for facilitating
LLMs to be used as a semantic parser in KBQA. Specifically, ICSU adopts the
In-context Learning mechanism to instruct LLMs to generate SPARQL queries with
examples. In order to retrieve appropriate examples from annotated
question-query pairs, which contain comprehensive schema information related to
questions, ICSU explores four different retrieval strategies. Experimental
results on the largest KBQA benchmark, KQA Pro, show that ICSU with all these
strategies outperforms that with a random retrieval strategy significantly
(from 12\% to 78.76\% in accuracy).
","2023-10-24","2310.14174v1.pdf"
"2310.14188","Huy Nguyen","Huy Nguyen, Pedram Akbarian, TrungTin Nguyen, Nhat Ho","A General Theory for Softmax Gating Multinomial Logistic Mixture of
  Experts","36 pages","","","","stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Mixture-of-experts (MoE) model incorporates the power of multiple submodels
via gating functions to achieve greater performance in numerous regression and
classification applications. From a theoretical perspective, while there have
been previous attempts to comprehend the behavior of that model under the
regression settings through the convergence analysis of maximum likelihood
estimation in the Gaussian MoE model, such analysis under the setting of a
classification problem has remained missing in the literature. We close this
gap by establishing the convergence rates of density estimation and parameter
estimation in the softmax gating multinomial logistic MoE model. Notably, when
part of the expert parameters vanish, these rates are shown to be slower than
polynomial rates owing to an inherent interaction between the softmax gating
and expert functions via partial differential equations. To address this issue,
we propose using a novel class of modified softmax gating functions which
transform the input value before delivering them to the gating functions. As a
result, the previous interaction disappears and the parameter estimation rates
are significantly improved.
","2023-10-24","2310.14188v1.pdf"
"2310.14192","Gaurav Sahu","Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji","PromptMix: A Class Boundary Augmentation Method for Large Language Model
  Distillation","Accepted to EMNLP 2023 (Long paper)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Data augmentation is a widely used technique to address the problem of text
classification when there is a limited amount of training data. Recent work
often tackles this problem using large language models (LLMs) like GPT3 that
can generate new examples given already available ones. In this work, we
propose a method to generate more helpful augmented data by utilizing the LLM's
abilities to follow instructions and perform few-shot classifications. Our
specific PromptMix method consists of two steps: 1) generate challenging text
augmentations near class boundaries; however, generating borderline examples
increases the risk of false positives in the dataset, so we 2) relabel the text
augmentations using a prompting-based LLM classifier to enhance the correctness
of labels in the generated data. We evaluate the proposed method in challenging
2-shot and zero-shot settings on four text classification datasets: Banking77,
TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that
generating and, crucially, relabeling borderline examples facilitates the
transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and
cheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore,
2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the
four datasets. Our code is available at
https://github.com/ServiceNow/PromptMix-EMNLP-2023.
","2023-10-24","2310.14192v1.pdf"
"2310.14201","Yifan Luo","Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, Bin Dong","Prompt Engineering Through the Lens of Optimal Control","","","","","cs.LG math.OC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Prompt Engineering (PE) has emerged as a critical technique for guiding Large
Language Models (LLMs) in solving intricate tasks. Its importance is
highlighted by its potential to significantly enhance the efficiency and
effectiveness of human-machine interaction. As tasks grow increasingly complex,
recent advanced PE methods have extended beyond the limitations of single-round
interactions to embrace multi-round interactions, which allows for a deeper and
more nuanced engagement with LLMs. In this paper, we propose an optimal control
framework tailored for multi-round interactions with LLMs. This framework
provides a unified mathematical structure that not only systematizes the
existing PE methods but also sets the stage for rigorous analytical
improvements. Furthermore, we extend this framework to include PE via ensemble
methods and multi-agent collaboration, thereby enlarging the scope of
applicability. By adopting an optimal control perspective, we offer fresh
insights into existing PE methods and highlight theoretical challenges that
warrant future research. Besides, our work lays a foundation for the
development of more effective and interpretable PE methods.
","2023-10-24","2310.14201v1.pdf"
"2310.14209","Mengnan Qi","Mengnan Qi, Yufan Huang, Maoquan Wang, Yongqiang Yao, Zihan Liu, Bin
  Gu, Colin Clement, Neel Sundaresan","SUT: Active Defects Probing for Transcompiler Models","","","","","cs.SE cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Automatic Program translation has enormous application value and hence has
been attracting significant interest from AI researchers. However, we observe
that current program translation models still make elementary syntax errors,
particularly, when the target language does not have syntax elements in the
source language. Metrics like BLUE, CodeBLUE and computation accuracy may not
expose these issues. In this paper we introduce a new metrics for programming
language translation and these metrics address these basic syntax errors. We
develop a novel active defects probing suite called Syntactic Unit Tests (SUT)
which includes a highly interpretable evaluation harness for accuracy and test
scoring. Experiments have shown that even powerful models like ChatGPT still
make mistakes on these basic unit tests. Specifically, compared to previous
program translation task evaluation dataset, its pass rate on our unit tests
has decreased by 26.15%. Further our evaluation harness reveal syntactic
element errors in which these models exhibit deficiencies.
","2023-10-24","2310.14209v1.pdf"
"2310.14211","Lei Ma","Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix
  Juefei-Xu, Lei Ma","LUNA: A Model-Based Universal Analysis Framework for Large Language
  Models","44 pages, 9 figures","","","","cs.LG cs.AI cs.CL cs.CR cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Over the past decade, Artificial Intelligence (AI) has had great success
recently and is being used in a wide range of academic and industrial fields.
More recently, LLMs have made rapid advancements that have propelled AI to a
new level, enabling even more diverse applications and industrial domains with
intelligence, particularly in areas like software engineering and natural
language processing. Nevertheless, a number of emerging trustworthiness
concerns and issues exhibited in LLMs have already recently received much
attention, without properly solving which the widespread adoption of LLMs could
be greatly hindered in practice. The distinctive characteristics of LLMs, such
as the self-attention mechanism, extremely large model scale, and
autoregressive generation schema, differ from classic AI software based on CNNs
and RNNs and present new challenges for quality analysis. Up to the present, it
still lacks universal and systematic analysis techniques for LLMs despite the
urgent industrial demand. Towards bridging this gap, we initiate an early
exploratory study and propose a universal analysis framework for LLMs, LUNA,
designed to be general and extensible, to enable versatile analysis of LLMs
from multiple quality perspectives in a human-interpretable manner. In
particular, we first leverage the data from desired trustworthiness
perspectives to construct an abstract model as an auxiliary analysis asset,
which is empowered by various abstract model construction methods. To assess
the quality of the abstract model, we collect and define a number of evaluation
metrics, aiming at both abstract model level and the semantics level. Then, the
semantics, which is the degree of satisfaction of the LLM w.r.t. the
trustworthiness perspective, is bound to and enriches the abstract model with
semantics, which enables more detailed analysis applications for diverse
purposes.
","2023-10-24","2310.14211v1.pdf"
"2310.14230","Haoran Wang","Haoran Wang, Qiuye Jin, Shiman Li, Siyu Liu, Manning Wang, Zhijian
  Song","A comprehensive survey on deep active learning and its applications in
  medical image analysis","Paper List on Github:
  https://github.com/LightersWang/Awesome-Active-Learning-for-Medical-Image-Analysis","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Deep learning has achieved widespread success in medical image analysis,
leading to an increasing demand for large-scale expert-annotated medical image
datasets. Yet, the high cost of annotating medical images severely hampers the
development of deep learning in this field. To reduce annotation costs, active
learning aims to select the most informative samples for annotation and train
high-performance models with as few labeled samples as possible. In this
survey, we review the core methods of active learning, including the evaluation
of informativeness and sampling strategy. For the first time, we provide a
detailed summary of the integration of active learning with other
label-efficient techniques, such as semi-supervised, self-supervised learning,
and so on. Additionally, we also highlight active learning works that are
specifically tailored to medical image analysis. In the end, we offer our
perspectives on the future trends and challenges of active learning and its
applications in medical image analysis.
","2023-10-25","2310.14230v1.pdf"
"2310.14248","Mingzhe Du","Mingzhe Du, Anh Tuan Luu, Bin Ji, See-kiong Ng","From Static to Dynamic: A Continual Learning Framework for Large
  Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The vast number of parameters in large language models (LLMs) endows them
with remarkable capabilities, allowing them to excel in a variety of natural
language processing tasks. However, this complexity also presents challenges,
making LLMs difficult to train and inhibiting their ability to continuously
assimilate new knowledge, which may lead to inaccuracies in their outputs. To
mitigate these issues, this paper presents DynaMind, a novel continual learning
framework designed for LLMs. DynaMind incorporates memory mechanisms to
assimilate new knowledge and modular operators to enhance the model inference
process with the newly assimilated knowledge, consequently improving the
accuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's
effectiveness in overcoming these challenges. The code and demo of DynaMind are
available on GitHub: https://github.com/Elfsong/DynaMind.
","2023-10-24","2310.14248v1.pdf"
"2310.14262","Ivana Kvapilikova","Ivana Kvapil\'ikov\'a and Ond\v{r}ej Bojar","Boosting Unsupervised Machine Translation with Pseudo-Parallel Data","MT Summit 2023","Ivana Kvapil\'ikov\'a, Ond\v{r}ej Bojar (2023): Boosting
  Unsupervised Machine Translation with Pseudo-Parallel Data. In: Proceedings
  of Machine Translation Summit XIX vol. 1: Research Track, pp. 135-147, AAMT,
  Kyoto, Japan","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Even with the latest developments in deep learning and large-scale language
modeling, the task of machine translation (MT) of low-resource languages
remains a challenge. Neural MT systems can be trained in an unsupervised way
without any translation resources but the quality lags behind, especially in
truly low-resource conditions. We propose a training strategy that relies on
pseudo-parallel sentence pairs mined from monolingual corpora in addition to
synthetic sentence pairs back-translated from monolingual corpora. We
experiment with different training schedules and reach an improvement of up to
14.5 BLEU points (English to Ukrainian) over a baseline trained on
back-translated data only.
","2023-10-24","2310.14262v1.pdf"
"2310.14277","Bo Yuan","Bo Yuan, Danpei Zhao","A Survey on Continual Semantic Segmentation: Theory, Challenge, Method
  and Application","20 pages, 12 figures. Undergoing Review","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Continual learning, also known as incremental learning or life-long learning,
stands at the forefront of deep learning and AI systems. It breaks through the
obstacle of one-way training on close sets and enables continuous adaptive
learning on open-set conditions. In the recent decade, continual learning has
been explored and applied in multiple fields especially in computer vision
covering classification, detection and segmentation tasks. Continual semantic
segmentation (CSS), of which the dense prediction peculiarity makes it a
challenging, intricate and burgeoning task. In this paper, we present a review
of CSS, committing to building a comprehensive survey on problem formulations,
primary challenges, universal datasets, neoteric theories and multifarious
applications. Concretely, we begin by elucidating the problem definitions and
primary challenges. Based on an in-depth investigation of relevant approaches,
we sort out and categorize current CSS models into two main branches including
\textit{data-replay} and \textit{data-free} sets. In each branch, the
corresponding approaches are similarity-based clustered and thoroughly
analyzed, following qualitative comparison and quantitative reproductions on
relevant datasets. Besides, we also introduce four CSS specialities with
diverse application scenarios and development tendencies. Furthermore, we
develop a benchmark for CSS encompassing representative references, evaluation
results and reproductions, which is available
at~\url{https://github.com/YBIO/SurveyCSS}. We hope this survey can serve as a
reference-worthy and stimulating contribution to the advancement of the
life-long learning field, while also providing valuable perspectives for
related fields.
","2023-10-24","2310.14277v1.pdf"
"2310.14303","Soujanya Poria","Rishabh Bhardwaj, Soujanya Poria","Language Model Unalignment: Parametric Red-Teaming to Expose Hidden
  Harms and Biases","Under Review","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Red-teaming has been a widely adopted way to evaluate the harmfulness of
Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to
make it act as a helpful agent disregarding the harmfulness of the query.
Existing methods are primarily based on input text-based red-teaming such as
adversarial prompts, low-resource prompts, or contextualized prompts to
condition the model in a way to bypass its safe behavior. Bypassing the
guardrails uncovers hidden harmful information and biases in the model that are
left untreated or newly introduced by its safety training. However,
prompt-based attacks fail to provide such a diagnosis owing to their low attack
success rate, and applicability to specific models. In this paper, we present a
new perspective on LLM safety research i.e., parametric red-teaming through
Unalignment. It simply (instruction) tunes the model parameters to break model
guardrails that are not deeply rooted in the model's behavior. Unalignment
using as few as 100 examples can significantly bypass commonly referred to as
CHATGPT, to the point where it responds with an 88% success rate to harmful
queries on two safety benchmark datasets. On open-source models such as
VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more
than 91%. On bias evaluations, Unalignment exposes inherent biases in
safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's
responses are strongly biased and opinionated 64% of the time.
","2023-10-24","2310.14303v1.pdf"
"2310.14312","Anthi Papadopoulou","Anthi Papadopoulou, Pierre Lison, Mark Anderson, Lilja {\O}vrelid,
  Ildik\'o Pil\'an","Neural Text Sanitization with Privacy Risk Indicators: An Empirical
  Analysis","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Text sanitization is the task of redacting a document to mask all occurrences
of (direct or indirect) personal identifiers, with the goal of concealing the
identity of the individual(s) referred in it. In this paper, we consider a
two-step approach to text sanitization and provide a detailed analysis of its
empirical performance on two recently published datasets: the Text
Anonymization Benchmark (Pil\'an et al., 2022) and a collection of Wikipedia
biographies (Papadopoulou et al., 2022). The text sanitization process starts
with a privacy-oriented entity recognizer that seeks to determine the text
spans expressing identifiable personal information. This privacy-oriented
entity recognizer is trained by combining a standard named entity recognition
model with a gazetteer populated by person-related terms extracted from
Wikidata. The second step of the text sanitization process consists in
assessing the privacy risk associated with each detected text span, either
isolated or in combination with other text spans. We present five distinct
indicators of the re-identification risk, respectively based on language model
probabilities, text span classification, sequence labelling, perturbations, and
web search. We provide a contrastive analysis of each privacy indicator and
highlight their benefits and limitations, notably in relation to the available
labeled data.
","2023-10-24","2310.14312v1.pdf"
"2310.14325","Emilia Wi\'snios","Inez Okulska and Emilia Wi\'snios","Towards Harmful Erotic Content Detection through Coreference-Driven
  Contextual Analysis","Accepted for 6th Workshop on Computational Models of Reference,
  Anaphora and Coreference at EMNLP 2023 Conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Adult content detection still poses a great challenge for automation.
Existing classifiers primarily focus on distinguishing between erotic and
non-erotic texts. However, they often need more nuance in assessing the
potential harm. Unfortunately, the content of this nature falls beyond the
reach of generative models due to its potentially harmful nature. Ethical
restrictions prohibit large language models (LLMs) from analyzing and
classifying harmful erotics, let alone generating them to create synthetic
datasets for other neural models. In such instances where data is scarce and
challenging, a thorough analysis of the structure of such texts rather than a
large model may offer a viable solution. Especially given that harmful erotic
narratives, despite appearing similar to harmless ones, usually reveal their
harmful nature first through contextual information hidden in the non-sexual
parts of the narrative.
  This paper introduces a hybrid neural and rule-based context-aware system
that leverages coreference resolution to identify harmful contextual cues in
erotic content. Collaborating with professional moderators, we compiled a
dataset and developed a classifier capable of distinguishing harmful from
non-harmful erotic content. Our hybrid model, tested on Polish text,
demonstrates a promising accuracy of 84% and a recall of 80%. Models based on
RoBERTa and Longformer without explicit usage of coreference chains achieved
significantly weaker results, underscoring the importance of coreference
resolution in detecting such nuanced content as harmful erotics. This approach
also offers the potential for enhanced visual explainability, supporting
moderators in evaluating predictions and taking necessary actions to address
harmful content.
","2023-10-24","2310.14325v1.pdf"
"2310.14326","Abhilash Nandy","Abhilash Nandy, Manav Nitin Kapadnis, Pawan Goyal, Niloy Ganguly","CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural
  Text","Accepted to EMNLP Findings 2023, 14 pages, 4 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  In this paper, we propose CLMSM, a domain-specific, continual pre-training
framework, that learns from a large set of procedural recipes. CLMSM uses a
Multi-Task Learning Framework to optimize two objectives - a) Contrastive
Learning using hard triplets to learn fine-grained differences across entities
in the procedures, and b) a novel Mask-Step Modelling objective to learn
step-wise context of a procedure. We test the performance of CLMSM on the
downstream tasks of tracking entities and aligning actions between two
procedures on three datasets, one of which is an open-domain dataset not
conforming with the pre-training dataset. We show that CLMSM not only
outperforms baselines on recipes (in-domain) but is also able to generalize to
open-domain procedural NLP tasks.
","2023-10-24","2310.14326v1.pdf"
"2310.14329","Mahdi Zakizadeh","Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar","DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and
  Bias","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Numerous debiasing techniques have been proposed to mitigate the gender bias
that is prevalent in pretrained language models. These are often evaluated on
datasets that check the extent to which the model is gender-neutral in its
predictions. Importantly, this evaluation protocol overlooks the possible
adverse impact of bias mitigation on useful gender knowledge. To fill this gap,
we propose DiFair, a manually curated dataset based on masked language modeling
objectives. DiFair allows us to introduce a unified metric, gender invariance
score, that not only quantifies a model's biased behavior, but also checks if
useful gender knowledge is preserved. We use DiFair as a benchmark for a number
of widely-used pretained language models and debiasing techniques. Experimental
results corroborate previous findings on the existing gender biases, while also
demonstrating that although debiasing techniques ameliorate the issue of gender
bias, this improvement usually comes at the price of lowering useful gender
knowledge of the model.
","2023-10-24","2310.14329v1.pdf"
"2310.14338","Megha Sundriyal","Megha Sundriyal, Tanmoy Chakraborty, Preslav Nakov","From Chaos to Clarity: Claim Normalization to Empower Fact-Checking","Accepted at Findings EMNLP2023","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  With the proliferation of social media platforms, users are exposed to vast
information, including posts containing misleading claims. However, the
pervasive noise inherent in these posts presents a challenge in identifying
precise and prominent claims that require verification. Extracting the core
assertions from such posts is arduous and time-consuming. We introduce a novel
task called Claim Normalization (aka ClaimNorm) that aims to decompose complex
and noisy social media posts into more straightforward and understandable
forms, termed normalized claims. We propose CACN, a pioneering approach that
leverages chain-of-thought and claim check-worthiness estimation, mimicking
human reasoning processes, to comprehend intricate claims. Moreover, we
capitalize on large language models' powerful in-context learning abilities to
provide guidance and improve the claim normalization process. To evaluate the
effectiveness of our proposed model, we meticulously compile a comprehensive
real-world dataset, CLAN, comprising more than 6k instances of social media
posts alongside their respective normalized claims. Experimentation
demonstrates that CACN outperforms several baselines across various evaluation
measures. A rigorous error analysis validates CACN's capabilities and pitfalls.
","2023-10-24","2310.14338v1.pdf"
"2310.14346","Robert Mahari","Robert Mahari, Dominik Stammbach, Elliott Ash, Alex 'Sandy' Pentland","The Law and NLP: Bridging Disciplinary Disconnects","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Legal practice is intrinsically rooted in the fabric of language, yet legal
practitioners and scholars have been slow to adopt tools from natural language
processing (NLP). At the same time, the legal system is experiencing an access
to justice crisis, which could be partially alleviated with NLP. In this
position paper, we argue that the slow uptake of NLP in legal practice is
exacerbated by a disconnect between the needs of the legal community and the
focus of NLP researchers. In a review of recent trends in the legal NLP
literature, we find limited overlap between the legal NLP community and legal
academia. Our interpretation is that some of the most popular legal NLP tasks
fail to address the needs of legal practitioners. We discuss examples of legal
NLP tasks that promise to bridge disciplinary disconnects and highlight
interesting areas for legal NLP research that remain underexplored.
","2023-10-24","2310.14346v1.pdf"
"2310.14356","Andre Ye","Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna","Cultural and Linguistic Diversity Improves Visual Representations","","","","","cs.CV cs.CL cs.CY cs.HC","http://creativecommons.org/licenses/by/4.0/","  Computer vision often treats perception as objective, and this assumption
gets reflected in the way that datasets are collected and models are trained.
For instance, image descriptions in different languages are typically assumed
to be translations of the same semantic content. However, work in
cross-cultural psychology and linguistics has shown that individuals differ in
their visual perception depending on their cultural background and the language
they speak. In this paper, we demonstrate significant differences in semantic
content across languages in both dataset and model-produced captions. When data
is multilingual as opposed to monolingual, captions have higher semantic
coverage on average, as measured by scene graph, embedding, and linguistic
complexity. For example, multilingual captions have on average 21.8% more
objects, 24.5% more relations, and 27.1% more attributes than a set of
monolingual captions. Moreover, models trained on content from different
languages perform best against test data from those languages, while those
trained on multilingual content perform consistently well across all evaluation
data compositions. Our research provides implications for how diverse modes of
perception can improve image understanding.
","2023-10-25","2310.14356v1.pdf"
"2310.14358","Elena Sergeeva","Elena Sergeeva, Anastasia Sergeeva, Huiyun Tang, Kerstin
  Bongard-Blanchy, Peter Szolovits","Right, No Matter Why: AI Fact-checking and AI Authority in
  Health-related Inquiry Settings","","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Previous research on expert advice-taking shows that humans exhibit two
contradictory behaviors: on the one hand, people tend to overvalue their own
opinions undervaluing the expert opinion, and on the other, people often defer
to other people's advice even if the advice itself is rather obviously wrong.
In our study, we conduct an exploratory evaluation of users' AI-advice
accepting behavior when evaluating the truthfulness of a health-related
statement in different ""advice quality"" settings. We find that even feedback
that is confined to just stating that ""the AI thinks that the statement is
false/true"" results in more than half of people moving their statement veracity
assessment towards the AI suggestion. The different types of advice given
influence the acceptance rates, but the sheer effect of getting a suggestion is
often bigger than the suggestion-type effect.
","2023-10-24","2310.14358v1.pdf"
"2310.14369","Seth Neel","Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel","MoPe: Model Perturbation-based Privacy Attacks on Language Models","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recent work has shown that Large Language Models (LLMs) can unintentionally
leak sensitive information present in their training data. In this paper, we
present Model Perturbations (MoPe), a new method to identify with high
confidence if a given text is in the training data of a pre-trained language
model, given white-box access to the models parameters. MoPe adds noise to the
model in parameter space and measures the drop in log-likelihood at a given
point $x$, a statistic we show approximates the trace of the Hessian matrix
with respect to model parameters. Across language models ranging from $70$M to
$12$B parameters, we show that MoPe is more effective than existing loss-based
attacks and recently proposed perturbation-based methods. We also examine the
role of training point order and model size in attack success, and empirically
demonstrate that MoPe accurately approximate the trace of the Hessian in
practice. Our results show that the loss of a point alone is insufficient to
determine extractability -- there are training points we can recover using our
method that have average loss. This casts some doubt on prior works that use
the loss of a point as evidence of memorization or unlearning.
","2023-10-24","2310.14369v1.pdf"
"2310.14374","Wang Chunlei","Chunlei Wang, Wenquan Feng, Xiangtai Li, Guangliang Cheng, Shuchang
  Lyu, Binghao Liu, Lijiang Chen and Qi Zhao","OV-VG: A Benchmark for Open-Vocabulary Visual Grounding","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Open-vocabulary learning has emerged as a cutting-edge research area,
particularly in light of the widespread adoption of vision-based foundational
models. Its primary objective is to comprehend novel concepts that are not
encompassed within a predefined vocabulary. One key facet of this endeavor is
Visual Grounding, which entails locating a specific region within an image
based on a corresponding language description. While current foundational
models excel at various visual language tasks, there's a noticeable absence of
models specifically tailored for open-vocabulary visual grounding. This
research endeavor introduces novel and challenging OV tasks, namely
Open-Vocabulary Visual Grounding and Open-Vocabulary Phrase Localization. The
overarching aim is to establish connections between language descriptions and
the localization of novel objects. To facilitate this, we have curated a
comprehensive annotated benchmark, encompassing 7,272 OV-VG images and 1,000
OV-PL images. In our pursuit of addressing these challenges, we delved into
various baseline methodologies rooted in existing open-vocabulary object
detection, VG, and phrase localization frameworks. Surprisingly, we discovered
that state-of-the-art methods often falter in diverse scenarios. Consequently,
we developed a novel framework that integrates two critical components:
Text-Image Query Selection and Language-Guided Feature Attention. These modules
are designed to bolster the recognition of novel categories and enhance the
alignment between visual and linguistic information. Extensive experiments
demonstrate the efficacy of our proposed framework, which consistently attains
SOTA performance across the OV-VG task. Additionally, ablation studies provide
further evidence of the effectiveness of our innovative models. Codes and
datasets will be made publicly available at https://github.com/cv516Buaa/OV-VG.
","2023-10-24","2310.14374v1.pdf"
"2310.14389","Hongli Zhan","Hongli Zhan, Desmond C. Ong, Junyi Jessy Li","Evaluating Subjective Cognitive Appraisals of Emotions from Large
  Language Models","EMNLP 2023 (Findings) Camera-Ready Version","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The emotions we experience involve complex processes; besides physiological
aspects, research in psychology has studied cognitive appraisals where people
assess their situations subjectively, according to their own values (Scherer,
2005). Thus, the same situation can often result in different emotional
experiences. While the detection of emotion is a well-established task, there
is very limited work so far on the automatic prediction of cognitive
appraisals. This work fills the gap by presenting CovidET-Appraisals, the most
comprehensive dataset to-date that assesses 24 appraisal dimensions, each with
a natural language rationale, across 241 Reddit posts. CovidET-Appraisals
presents an ideal testbed to evaluate the ability of large language models --
excelling at a wide range of NLP tasks -- to automatically assess and explain
cognitive appraisals. We found that while the best models are performant,
open-sourced LLMs fall short at this task, presenting a new challenge in the
future development of emotionally intelligent models. We release our dataset at
https://github.com/honglizhan/CovidET-Appraisals-Public.
","2023-10-24","2310.14389v1.pdf"
"2310.14393","Yunxiang Zhang","Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee,
  Honglak Lee, Lu Wang","Merging Generated and Retrieved Knowledge for Open-Domain QA","EMNLP 2023 - Camera Ready","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Open-domain question answering (QA) systems are often built with retrieval
modules. However, retrieving passages from a given source is known to suffer
from insufficient knowledge coverage. Alternatively, prompting large language
models (LLMs) to generate contextual passages based on their parametric
knowledge has been shown to improve QA performance. Yet, LLMs tend to
""hallucinate"" content that conflicts with the retrieved knowledge. Based on the
intuition that answers supported by both sources are more likely to be correct,
we propose COMBO, a Compatibility-Oriented knowledge Merging for Better
Open-domain QA framework, to effectively leverage the two sources of
information. Concretely, we match LLM-generated passages with retrieved
counterparts into compatible pairs, based on discriminators trained with silver
compatibility labels. Then a Fusion-in-Decoder-based reader model handles
passage pairs to arrive at the final answer. Experiments show that COMBO
outperforms competitive baselines on three out of four tested open-domain QA
benchmarks. Further analysis reveals that our proposed framework demonstrates
greater efficacy in scenarios with a higher degree of knowledge conflicts.
","2023-10-24","2310.14393v1.pdf"
"2310.14403","Yuchen Xiao","Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani, Jared Vann,
  Deepeka Garg, Sumitra Ganesh","O3D: Offline Data-driven Discovery and Distillation for Sequential
  Decision-Making with Large Language Models","","","","","cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in large language models (LLMs) have exhibited promising
performance in solving sequential decision-making problems. By imitating
few-shot examples provided in the prompts (i.e., in-context learning), an LLM
agent can interact with an external environment and complete given tasks
without additional training. However, such few-shot examples are often
insufficient to generate high-quality solutions for complex and long-horizon
tasks, while the limited context length cannot consume larger-scale
demonstrations. To this end, we propose an offline learning framework that
utilizes offline data at scale (e.g, logs of human interactions) to facilitate
the in-context learning performance of LLM agents. We formally define
LLM-powered policies with both text-based approaches and code-based approaches.
We then introduce an Offline Data-driven Discovery and Distillation (O3D)
framework to improve LLM-powered policies without finetuning. O3D automatically
discovers reusable skills and distills generalizable knowledge across multiple
tasks based on offline interaction data, advancing the capability of solving
downstream tasks. Empirical results under two interactive decision-making
benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the
decision-making capabilities of LLMs through the offline discovery and
distillation process, and consistently outperform baselines across various LLMs
with both text-based-policy and code-based-policy.
","2023-10-24","2310.14403v1.pdf"
"2310.14404","Kushal Chawla","Kushal Chawla, Ian Wu, Yu Rong, Gale M. Lucas, Jonathan Gratch","Be Selfish, But Wisely: Investigating the Impact of Agent Personality in
  Mixed-Motive Human-Agent Interactions","Accepted at EMNLP 2023 (Main)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  A natural way to design a negotiation dialogue system is via self-play RL:
train an agent that learns to maximize its performance by interacting with a
simulated user that has been designed to imitate human-human dialogue data.
Although this procedure has been adopted in prior work, we find that it results
in a fundamentally flawed system that fails to learn the value of compromise in
a negotiation, which can often lead to no agreements (i.e., the partner walking
away without a deal), ultimately hurting the model's overall performance. We
investigate this observation in the context of the DealOrNoDeal task, a
multi-issue negotiation over books, hats, and balls. Grounded in negotiation
theory from Economics, we modify the training procedure in two novel ways to
design agents with diverse personalities and analyze their performance with
human partners. We find that although both techniques show promise, a selfish
agent, which maximizes its own performance while also avoiding walkaways,
performs superior to other variants by implicitly learning to generate value
for both itself and the negotiation partner. We discuss the implications of our
findings for what it means to be a successful negotiation dialogue system and
how these systems should be designed in the future.
","2023-10-24","2310.14404v1.pdf"
"2310.14408","Andrew Drozdov","Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi,
  Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew McCallum, Donald Metzler, Kai
  Hui","PaRaDe: Passage Ranking using Demonstrations with Large Language Models","Findings of EMNLP 2023","","","","cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent studies show that large language models (LLMs) can be instructed to
effectively perform zero-shot passage re-ranking, in which the results of a
first stage retrieval method, such as BM25, are rated and reordered to improve
relevance. In this work, we improve LLM-based re-ranking by algorithmically
selecting few-shot demonstrations to include in the prompt. Our analysis
investigates the conditions where demonstrations are most helpful, and shows
that adding even one demonstration is significantly beneficial. We propose a
novel demonstration selection strategy based on difficulty rather than the
commonly used semantic similarity. Furthermore, we find that demonstrations
helpful for ranking are also effective at question generation. We hope our work
will spur more principled research into question generation and passage
ranking.
","2023-10-24","2310.14408v1.pdf"
"2310.14414","Xingcheng Zhou","Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurtsever, Alois C.
  Knoll","Vision Language Models in Autonomous Driving and Intelligent
  Transportation Systems","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The applications of Vision-Language Models (VLMs) in the fields of Autonomous
Driving (AD) and Intelligent Transportation Systems (ITS) have attracted
widespread attention due to their outstanding performance and the ability to
leverage Large Language Models (LLMs). By integrating language data, the
vehicles, and transportation systems are able to deeply understand real-world
environments, improving driving safety and efficiency. In this work, we present
a comprehensive survey of the advances in language models in this domain,
encompassing current models and datasets. Additionally, we explore the
potential applications and emerging research directions. Finally, we thoroughly
discuss the challenges and research gap. The paper aims to provide researchers
with the current work and future trends of VLMs in AD and ITS.
","2023-10-24","2310.14414v1.pdf"
"2310.14420","Henry Sprueill","Henry W. Sprueill, Carl Edwards, Mariefel V. Olarte, Udishnu Sanyal,
  Heng Ji, Sutanay Choudhury","Monte Carlo Thought Search: Large Language Model Querying for Complex
  Scientific Reasoning in Catalyst Design","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Discovering novel catalysts requires complex reasoning involving multiple
chemical properties and resultant trade-offs, leading to a combinatorial growth
in the search space. While large language models (LLM) have demonstrated novel
capabilities for chemistry through complex instruction following capabilities
and high quality reasoning, a goal-driven combinatorial search using LLMs has
not been explored in detail. In this work, we present a Monte Carlo Tree
Search-based approach that improves beyond state-of-the-art chain-of-thought
prompting variants to augment scientific reasoning. We introduce two new
reasoning datasets: 1) a curation of computational chemistry simulations, and
2) diverse questions written by catalysis researchers for reasoning about novel
chemical conversion processes. We improve over the best baseline by 25.8\% and
find that our approach can augment scientist's reasoning and discovery process
with novel insights.
","2023-10-24","2310.14420v1.pdf"
"2310.14422","Eugenio Herrera-Berg","Eugenio Herrera-Berg, Tom\'as Vergara Browne, Pablo Le\'on-Villagr\'a,
  Marc-Llu\'is Vives, Cristian Buc Calderon","Large Language Models are biased to overestimate profoundness","5 pages, 3 figures","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Recent advancements in natural language processing by large language models
(LLMs), such as GPT-4, have been suggested to approach Artificial General
Intelligence. And yet, it is still under dispute whether LLMs possess similar
reasoning abilities to humans. This study evaluates GPT-4 and various other
LLMs in judging the profoundness of mundane, motivational, and pseudo-profound
statements. We found a significant statement-to-statement correlation between
the LLMs and humans, irrespective of the type of statements and the prompting
technique used. However, LLMs systematically overestimate the profoundness of
nonsensical statements, with the exception of Tk-instruct, which uniquely
underestimates the profoundness of statements. Only few-shot learning prompts,
as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.
Furthermore, this work provides insights into the potential biases induced by
Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the
bias to overestimate the profoundness of statements.
","2023-10-24","2310.14422v1.pdf"
"2310.14424","Meriem Boubdir","Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, Sara Hooker","Which Prompts Make The Difference? Data Prioritization For Efficient
  Human LLM Evaluation","37 pages, 8 figures","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Human evaluation is increasingly critical for assessing large language
models, capturing linguistic nuances, and reflecting user preferences more
accurately than traditional automated metrics. However, the resource-intensive
nature of this type of annotation process poses significant challenges. The key
question driving our work: ""is it feasible to minimize human-in-the-loop
feedback by prioritizing data instances which most effectively distinguish
between models?"" We evaluate several metric-based methods and find that these
metrics enhance the efficiency of human evaluations by minimizing the number of
required annotations, thus saving time and cost, while ensuring a robust
performance evaluation. We show that our method is effective across widely used
model families, reducing instances of indecisive (or ""tie"") outcomes by up to
54% compared to a random sample when focusing on the top-20 percentile of
prioritized instances. This potential reduction in required human effort
positions our approach as a valuable strategy in future large language model
evaluations.
","2023-10-24","2310.14424v1.pdf"
"2310.14429","Matthew Edwards","Alexander P. Welsh and Matthew Edwards","Text generation for dataset augmentation in security classification
  tasks","","","","","cs.CR cs.CL","http://creativecommons.org/licenses/by/4.0/","  Security classifiers, designed to detect malicious content in computer
systems and communications, can underperform when provided with insufficient
training data. In the security domain, it is often easy to find samples of the
negative (benign) class, and challenging to find enough samples of the positive
(malicious) class to train an effective classifier. This study evaluates the
application of natural language text generators to fill this data gap in
multiple security-related text classification tasks. We describe a variety of
previously-unexamined language-model fine-tuning approaches for this purpose
and consider in particular the impact of disproportionate class-imbalances in
the training set. Across our evaluation using three state-of-the-art
classifiers designed for offensive language detection, review fraud detection,
and SMS spam detection, we find that models trained with GPT-3 data
augmentation strategies outperform both models trained without augmentation and
models trained using basic data augmentation strategies already in common
usage. In particular, we find substantial benefits for GPT-3 data augmentation
strategies in situations with severe limitations on known positive-class
samples.
","2023-10-24","2310.14429v1.pdf"
"2310.14435","Vaibhav Mavi","Vaibhav Mavi and Abulhair Saparov and Chen Zhao","Retrieval-Augmented Chain-of-Thought in Semi-structured Domains","to appear in NLLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Applying existing question answering (QA) systems to specialized domains like
law and finance presents challenges that necessitate domain expertise. Although
large language models (LLMs) have shown impressive language comprehension and
in-context learning capabilities, their inability to handle very long
inputs/contexts is well known. Tasks specific to these domains need significant
background knowledge, leading to contexts that can often exceed the maximum
length that existing LLMs can process. This study explores leveraging the
semi-structured nature of legal and financial data to efficiently retrieve
relevant context, enabling the use of LLMs for domain-specialized QA. The
resulting system outperforms contemporary models and also provides useful
explanations for the answers, encouraging the integration of LLMs into legal
and financial NLP systems for future research.
","2023-10-24","2310.14435v1.pdf"
"2310.14450","Hans Hanley","Hans W. A. Hanley, Zakir Durumeric","TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings","Accepted to EMNLP 2023","","","","cs.CL cs.CY cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Stance detection is important for understanding different attitudes and
beliefs on the Internet. However, given that a passage's stance toward a given
topic is often highly dependent on that topic, building a stance detection
model that generalizes to unseen topics is difficult. In this work, we propose
using contrastive learning as well as an unlabeled dataset of news articles
that cover a variety of different topics to train topic-agnostic/TAG and
topic-aware/TAW embeddings for use in downstream stance detection. Combining
these embeddings in our full TATA model, we achieve state-of-the-art
performance across several public stance detection datasets (0.771 $F_1$-score
on the Zero-shot VAST dataset). We release our code and data at
https://github.com/hanshanley/tata.
","2023-10-24","2310.14450v1.pdf"
"2310.14451","Yasmin Moslem","Yasmin Moslem, Gianfranco Romani, Mahdi Molaei, Rejwanul Haque, John
  D. Kelleher, Andy Way","Domain Terminology Integration into Machine Translation: Leveraging
  Large Language Models","WMT 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper discusses the methods that we used for our submissions to the WMT
2023 Terminology Shared Task for German-to-English (DE-EN), English-to-Czech
(EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to
advance machine translation (MT) by challenging participants to develop systems
that accurately translate technical terms, ultimately enhancing communication
and understanding in specialised domains. To this end, we conduct experiments
that utilise large language models (LLMs) for two purposes: generating
synthetic bilingual terminology-based data, and post-editing translations
generated by an MT model through incorporating pre-approved terms. Our system
employs a four-step process: (i) using an LLM to generate bilingual synthetic
data based on the provided terminology, (ii) fine-tuning a generic
encoder-decoder MT model, with a mix of the terminology-based synthetic data
generated in the first step and a randomly sampled portion of the original
generic training data, (iii) generating translations with the fine-tuned MT
model, and (iv) finally, leveraging an LLM for terminology-constrained
automatic post-editing of the translations that do not include the required
terms. The results demonstrate the effectiveness of our proposed approach in
improving the integration of pre-approved terms into translations. The number
of terms incorporated into the translations of the blind dataset increases from
an average of 36.67% with the generic model to an average of 72.88% by the end
of the process. In other words, successful utilisation of terms nearly doubles
across the three language pairs.
","2023-10-24","2310.14451v1.pdf"
"2310.14455","Ross Gruetzemacher","Ross Gruetzemacher, Alan Chan, Kevin Frazier, Christy Manning,
  \v{S}t\v{e}p\'an Los, James Fox, Jos\'e Hern\'andez-Orallo, John Burden,
  Matija Franklin, Cl\'iodhna N\'i Ghuidhir, Mark Bailey, Daniel Eth, Toby
  Pilditch, Kyle Kilian","An International Consortium for Evaluations of Societal-Scale Risks from
  Advanced AI","50 pages, 2 figures","","","","cs.CY cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Given rapid progress toward advanced AI and risks from frontier AI systems
(advanced AI systems pushing the boundaries of the AI capabilities frontier),
the creation and implementation of AI governance and regulatory schemes
deserves prioritization and substantial investment. However, the status quo is
untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to
conduct research, development, and deployment activities with minimal
oversight. In response, frontier AI system evaluations have been proposed as a
way of assessing risks from the development and deployment of frontier AI
systems. Yet, the budding AI risk evaluation ecosystem faces significant
coordination challenges, such as a limited diversity of evaluators, suboptimal
allocation of effort, and perverse incentives. This paper proposes a solution
in the form of an international consortium for AI risk evaluations, comprising
both AI developers and third-party AI risk evaluators. Such a consortium could
play a critical role in international efforts to mitigate societal-scale risks
from advanced AI, including in managing responsible scaling policies and
coordinated evaluation-based risk response. In this paper, we discuss the
current evaluation ecosystem and its shortcomings, propose an international
consortium for advanced AI risk evaluations, discuss issues regarding its
implementation, discuss lessons that can be learnt from previous international
institutions and existing proposals for international AI governance
institutions, and, finally, we recommend concrete steps to advance the
establishment of the proposed consortium: (i) solicit feedback from
stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for
stakeholders, (iv) analyze feedback and create final proposal, (v) solicit
funding, and (vi) create a consortium.
","2023-10-26","2310.14455v1.pdf"
"2310.14478","Zekun Li","Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, Muhao Chen","GeoLM: Empowering Language Models for Geospatially Grounded Language
  Understanding","Accepted to EMNLP23 main","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Humans subconsciously engage in geospatial reasoning when reading articles.
We recognize place names and their spatial relations in text and mentally
associate them with their physical locations on Earth. Although pretrained
language models can mimic this cognitive process using linguistic context, they
do not utilize valuable geospatial information in large, widely available
geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a
geospatially grounded language model that enhances the understanding of
geo-entities in natural language. GeoLM leverages geo-entity mentions as
anchors to connect linguistic information in text corpora with geospatial
information extracted from geographical databases. GeoLM connects the two types
of context through contrastive learning and masked language modeling. It also
incorporates a spatial coordinate embedding mechanism to encode distance and
direction relations to capture geospatial context. In the experiment, we
demonstrate that GeoLM exhibits promising capabilities in supporting toponym
recognition, toponym linking, relation extraction, and geo-entity typing, which
bridge the gap between natural language processing and geospatial sciences. The
code is publicly available at https://github.com/knowledge-computing/geolm.
","2023-10-24","2310.14478v1.pdf"
"2310.14495","Justin Payan","Justin Payan, Swaroop Mishra, Mukul Singh, Carina Negreanu, Christian
  Poelitz, Chitta Baral, Subhro Roy, Rasika Chakravarthy, Benjamin Van Durme,
  and Elnaz Nouri","InstructExcel: A Benchmark for Natural Language Instruction in Excel","Findings of EMNLP 2023, 18 pages","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the evolution of Large Language Models (LLMs) we can solve increasingly
more complex NLP tasks across various domains, including spreadsheets. This
work investigates whether LLMs can generate code (Excel OfficeScripts, a
TypeScript API for executing many tasks in Excel) that solves Excel specific
tasks provided via natural language user instructions. To do so we introduce a
new large-scale benchmark, InstructExcel, created by leveraging the 'Automate'
feature in Excel to automatically generate OfficeScripts from users' actions.
Our benchmark includes over 10k samples covering 170+ Excel operations across
2,000 publicly available Excel spreadsheets. Experiments across various
zero-shot and few-shot settings show that InstructExcel is a hard benchmark for
state of the art models like GPT-4. We observe that (1) using GPT-4 over
GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can
help improve performance on this benchmark.
","2023-10-24","2310.14495v1.pdf"
"2310.14508","Yingjie Zhu","Yingjie Zhu, Jiasheng Si, Yibo Zhao, Haiyang Zhu, Deyu Zhou, Yulan He","EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data
  Augmentation for Multi-hop Fact Verification","Accepted by EMNLP2023 Main Conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Automatic multi-hop fact verification task has gained significant attention
in recent years. Despite impressive results, these well-designed models perform
poorly on out-of-domain data. One possible solution is to augment the training
data with counterfactuals, which are generated by minimally altering the causal
features of the original data. However, current counterfactual data
augmentation techniques fail to handle multi-hop fact verification due to their
incapability to preserve the complex logical relationships within multiple
correlated texts. In this paper, we overcome this limitation by developing a
rationale-sensitive method to generate linguistically diverse and
label-flipping counterfactuals while preserving logical relationships. In
specific, the diverse and fluent counterfactuals are generated via an
Explain-Edit-Generate architecture. Moreover, the checking and filtering
modules are proposed to regularize the counterfactual data with logical
relations and flipped labels. Experimental results show that the proposed
approach outperforms the SOTA baselines and can generate linguistically diverse
counterfactual data without disrupting their logical relationships.
","2023-10-24","2310.14508v1.pdf"
"2310.14510","Zihan Zhang","Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad","CITB: A Benchmark for Continual Instruction Tuning","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Continual learning (CL) is a paradigm that aims to replicate the human
ability to learn and accumulate knowledge continually without forgetting
previous knowledge and transferring it to new tasks. Recent instruction tuning
(IT) involves fine-tuning models to make them more adaptable to solving NLP
tasks in general. However, it is still uncertain how instruction tuning works
in the context of CL tasks. This challenging yet practical problem is
formulated as Continual Instruction Tuning (CIT). In this work, we establish a
CIT benchmark consisting of learning and evaluation protocols. We curate two
long dialogue task streams of different types, InstrDialog and InstrDialog++,
to study various CL methods systematically. Our experiments show that existing
CL methods do not effectively leverage the rich natural language instructions,
and fine-tuning an instruction-tuned model sequentially can yield similar or
better results. We further explore different aspects that might affect the
learning of CIT. We hope this benchmark will facilitate more research in this
direction.
","2023-10-24","2310.14510v1.pdf"
"2310.14533","Heinrich Peters","Heinrich Peters, Yozen Liu, Francesco Barbieri, Raiyan A. Baten,
  Sandra C. Matz, Maarten W. Bos","Context-Aware Prediction of User Engagement on Online Social Platforms","","","","","cs.LG cs.AI cs.HC cs.SI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The success of online social platforms hinges on their ability to predict and
understand user behavior at scale. Here, we present data suggesting that
context-aware modeling approaches may offer a holistic yet lightweight and
potentially privacy-preserving representation of user engagement on online
social platforms. Leveraging deep LSTM neural networks to analyze more than 100
million Snapchat sessions from almost 80.000 users, we demonstrate that
patterns of active and passive use are predictable from past behavior
(R2=0.345) and that the integration of context information substantially
improves predictive performance compared to the behavioral baseline model
(R2=0.522). Features related to smartphone connectivity status, location,
temporal context, and weather were found to capture non-redundant variance in
user engagement relative to features derived from histories of in-app
behaviors. Further, we show that a large proportion of variance can be
accounted for with minimal behavioral histories if momentary context
information is considered (R2=0.44). These results indicate the potential of
context-aware approaches for making models more efficient and
privacy-preserving by reducing the need for long data histories. Finally, we
employ model explainability techniques to glean preliminary insights into the
underlying behavioral mechanisms. Our findings are consistent with the notion
of context-contingent, habit-driven patterns of active and passive use,
underscoring the value of contextualized representations of user behavior for
predicting user engagement on social platforms.
","2023-10-24","2310.14533v1.pdf"
"2310.14534","Houquan Zhou","Houquan Zhou, Yumeng Liu, Zhenghua Li, Min Zhang, Bo Zhang, Chen Li,
  Ji Zhang, Fei Huang","Improving Seq2Seq Grammatical Error Correction via Decoding
  Interventions","Accept to Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The sequence-to-sequence (Seq2Seq) approach has recently been widely used in
grammatical error correction (GEC) and shows promising performance. However,
the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC
model can only be trained on parallel data, which, in GEC task, is often noisy
and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an
explicit awareness of the correctness of the token being generated. In this
paper, we propose a unified decoding intervention framework that employs an
external critic to assess the appropriateness of the token to be generated
incrementally, and then dynamically influence the choice of the next token. We
discover and investigate two types of critics: a pre-trained left-to-right
language model critic and an incremental target-side grammatical error detector
critic. Through extensive experiments on English and Chinese datasets, our
framework consistently outperforms strong baselines and achieves results
competitive with state-of-the-art methods.
","2023-10-24","2310.14534v1.pdf"
"2310.14540","Yutaro Yamada","Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, Ilker
  Yildirim","Evaluating Spatial Understanding of Large Language Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) show remarkable capabilities across a variety of
tasks. Despite the models only seeing text in training, several recent studies
suggest that LLM representations implicitly capture aspects of the underlying
grounded concepts. Here, we explore LLM representations of a particularly
salient kind of grounded knowledge -- spatial relationships. We design
natural-language navigation tasks and evaluate the ability of LLMs, in
particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and
reason about spatial structures, and compare these abilities to human
performance on the same tasks. These tasks reveal substantial variability in
LLM performance across different spatial structures, including square,
hexagonal, and triangular grids, rings, and trees. We also discover that,
similar to humans, LLMs utilize object names as landmarks for maintaining
spatial maps. Finally, in extensive error analysis, we find that LLMs' mistakes
reflect both spatial and non-spatial factors. These findings suggest that LLMs
appear to capture certain aspects of spatial structure implicitly, but room for
improvement remains.
","2023-10-24","2310.14540v1.pdf"
"2310.14542","Jiao Sun","Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta,
  John Frederick Wieting, Nanyun Peng, Xuezhe Ma","Evaluating Large Language Models on Controlled Generation Tasks","EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  While recent studies have looked into the abilities of large language models
in various benchmark tasks, including question generation, reading
comprehension, multilingual and etc, there have been few studies looking into
the controllability of large language models on generation tasks. We present an
extensive analysis of various benchmarks including a sentence planning
benchmark with different granularities. After comparing large language models
against state-of-the-start finetuned smaller models, we present a spectrum
showing large language models falling behind, are comparable, or exceed the
ability of smaller models. We conclude that **large language models struggle at
meeting fine-grained hard constraints**.
","2023-10-24","2310.14542v1.pdf"
"2310.14545","V Vien Lee","V Vien Lee, Stephanie C. C. van der Lubbe, Lay Hoon Goh and Jose M.
  Valderas","Harnessing ChatGPT for thematic analysis: Are we ready?","23 pages, 7 figures, 3 tables, 1 textbox","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  ChatGPT is an advanced natural language processing tool with growing
applications across various disciplines in medical research. Thematic analysis,
a qualitative research method to identify and interpret patterns in data, is
one application that stands to benefit from this technology. This viewpoint
explores the utilization of ChatGPT in three core phases of thematic analysis
within a medical context: 1) direct coding of transcripts, 2) generating themes
from a predefined list of codes, and 3) preprocessing quotes for manuscript
inclusion. Additionally, we explore the potential of ChatGPT to generate
interview transcripts, which may be used for training purposes. We assess the
strengths and limitations of using ChatGPT in these roles, highlighting areas
where human intervention remains necessary. Overall, we argue that ChatGPT can
function as a valuable tool during analysis, enhancing the efficiency of the
thematic analysis and offering additional insights into the qualitative data.
","2023-10-25","2310.14545v1.pdf"
"2310.14550","Chenlu Ye","Chenlu Ye, Rui Yang, Quanquan Gu, Tong Zhang","Corruption-Robust Offline Reinforcement Learning with General Function
  Approximation","Neurips 2023","","","","cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We investigate the problem of corruption robustness in offline reinforcement
learning (RL) with general function approximation, where an adversary can
corrupt each sample in the offline dataset, and the corruption level
$\zeta\geq0$ quantifies the cumulative corruption amount over $n$ episodes and
$H$ steps. Our goal is to find a policy that is robust to such corruption and
minimizes the suboptimality gap with respect to the optimal policy for the
uncorrupted Markov decision processes (MDPs). Drawing inspiration from the
uncertainty-weighting technique from the robust online RL setting
\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight
iteration procedure to efficiently compute on batched samples and propose a
corruption-robust algorithm for offline RL. Notably, under the assumption of
single policy coverage and the knowledge of $\zeta$, our proposed algorithm
achieves a suboptimality bound that is worsened by an additive factor of
$\mathcal O(\zeta \cdot (\text{CC}(\lambda,\hat{\mathcal F},\mathcal
Z_n^H))^{1/2} (C(\hat{\mathcal F},\mu))^{-1/2} n^{-1})$ due to the corruption.
Here $\text{CC}(\lambda,\hat{\mathcal F},\mathcal Z_n^H)$ is the coverage
coefficient that depends on the regularization parameter $\lambda$, the
confidence set $\hat{\mathcal F}$, and the dataset $\mathcal Z_n^H$, and
$C(\hat{\mathcal F},\mu)$ is a coefficient that depends on $\hat{\mathcal F}$
and the underlying data distribution $\mu$. When specialized to linear MDPs,
the corruption-dependent error term reduces to $\mathcal O(\zeta d n^{-1})$
with $d$ being the dimension of the feature map, which matches the existing
lower bound for corrupted linear MDPs. This suggests that our analysis is tight
in terms of the corruption-dependent term.
","2023-10-24","2310.14550v1.pdf"
"2310.14557","Chiyu Zhang","Chiyu Zhang, Khai Duy Doan, Qisheng Liao, Muhammad Abdul-Mageed","The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64
  Languages","Accepted by EMNLP 2023 Main conference","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate
remarkable performance in a wide range of tasks. Despite numerous recent
studies that examine the performance of instruction-tuned LLMs on various NLP
benchmarks, there remains a lack of comprehensive investigation into their
ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning
embedded within social and interactive contexts. This deficiency arises partly
from SM not being adequately represented in any of the existing benchmarks. To
address this gap, we present SPARROW, an extensive multilingual benchmark
specifically designed for SM understanding. SPARROW comprises 169 datasets
covering 13 task types across six primary categories (e.g., anti-social
language detection, emotion recognition). SPARROW datasets encompass 64
different languages originating from 12 language families representing 16
writing scripts. We evaluate the performance of various multilingual pretrained
language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)
on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our
comprehensive analysis reveals that existing open-source instruction tuned LLMs
still struggle to understand SM across various languages, performing close to a
random baseline in some cases. We also find that although ChatGPT outperforms
many LLMs, it still falls behind task-specific finetuned models with a gap of
12.19 SPARROW score. Our benchmark is available at:
https://github.com/UBC-NLP/SPARROW
","2023-10-24","2310.14557v1.pdf"
"2310.14558","Xinlu Zhang","Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda
  Ruth Petzold","AlpaCare:Instruction-tuned Large Language Models for Medical Application","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large Language Models (LLMs) have demonstrated significant enhancements in
instruction-following abilities through instruction tuning, achieving notable
performances across various tasks. Previous research has focused on fine-tuning
medical domain-specific LLMs using an extensive array of medical-specific data,
incorporating millions of pieces of biomedical literature to augment their
medical capabilities. However, existing medical instruction-tuned LLMs have
been constrained by the limited scope of tasks and instructions available,
restricting the efficacy of instruction tuning and adversely affecting
performance in the general domain. In this paper, we fine-tune LLaMA-series
models using 52k diverse, machine-generated, medical instruction-following
data, MedInstruct-52k, resulting in the model AlpaCare. Comprehensive
experimental results on both general and medical-specific domain free-form
instruction evaluations showcase AlpaCare's strong medical proficiency and
generalizability compared to previous instruction-tuned models in both medical
and general domains. We provide public access to our MedInstruct-52k dataset
and a clinician-crafted free-form instruction test set, MedInstruct-test, along
with our codebase, to foster further research and development. Our project page
is available at https://github.com/XZhang97666/AlpaCare.
","2023-10-24","2310.14558v1.pdf"
"2310.14563","Sky CH-Wang","Oliver Li, Mallika Subramanian, Arkadiy Saakyan, Sky CH-Wang, Smaranda
  Muresan","NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling
  Social Norm Adherence and Violation","EMNLP 2023 Main Conference, Short Paper; Data at
  https://github.com/Aochong-Li/NormDial","","","","cs.CL cs.CY","http://creativecommons.org/licenses/by/4.0/","  Social norms fundamentally shape interpersonal communication. We present
NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations
of social norm adherences and violations for Chinese and American cultures.
Introducing the task of social norm observance detection, our dataset is
synthetically generated in both Chinese and English using a human-in-the-loop
pipeline by prompting large language models with a small collection of
expert-annotated social norms. We show that our generated dialogues are of high
quality through human evaluation and further evaluate the performance of
existing large language models on this task. Our findings point towards new
directions for understanding the nuances of social norms as they manifest in
conversational contexts that span across languages and cultures.
","2023-10-26","2310.14563v1.pdf"
"2310.14564","Jian Guan","Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng","Language Models Hallucinate, but May Excel at Fact Verification","9 pages","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent progress in natural language processing (NLP) owes much to remarkable
advances in large language models (LLMs). Nevertheless, LLMs frequently
""hallucinate,"" resulting in non-factual outputs. Our carefully designed human
evaluation substantiates the serious hallucination issue, revealing that even
GPT-3.5 produces factual outputs less than 25% of the time. This underscores
the importance of fact verifiers in order to measure and incentivize progress.
Our systematic investigation affirms that LLMs can be repurposed as effective
fact verifiers with strong correlations with human judgments, at least in the
Wikipedia domain. Surprisingly, FLAN-T5-11B, the least factual generator in our
study, performs the best as a fact verifier, even outperforming more capable
LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these
LLMs on high-quality evidence, as well as their deficiencies in robustness and
generalization ability. Our study presents insights for developing trustworthy
generation models.
","2023-10-24","2310.14564v1.pdf"
"2310.14566","Tianrui Guan","Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob,
  Dinesh Manocha, Tianyi Zhou","HallusionBench: You See What You Think? Or You Think What You See? An
  Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5,
  and Other Multi-modality Models","","","","","cs.CV cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs), after being aligned with vision models and
integrated into vision-language models (VLMs), can bring impressive improvement
in image reasoning tasks. This was shown by the recently released GPT-4V(ison),
LLaVA-1.5, etc. However, the strong language prior in these SOTA LVLMs can be a
double-edged sword: they may ignore the image context and solely rely on the
(even contradictory) language prior for reasoning. In contrast, the vision
modules in VLMs are weaker than LLMs and may result in misleading visual
representations, which are then translated to confident mistakes by LLMs. To
study these two types of VLM mistakes, i.e., language hallucination and visual
illusion, we curated HallusionBench, an image-context reasoning benchmark that
is still challenging to even GPT-4V and LLaVA-1.5. We provide a detailed
analysis of examples in HallusionBench, which sheds novel insights on the
illusion or hallucination of VLMs and how to improve them in the future. The
benchmark and codebase will be released at
https://github.com/tianyi-lab/HallusionBench.
","2023-10-24","2310.14566v1.pdf"
"2310.14573","Qianchu Liu","Qianchu Liu, Stephanie Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C.
  Castro, Maria Teodora Wetscherek, Robert Tinn, Harshita Sharma, Fernando
  P\'erez-Garc\'ia, Anton Schwaighofer, Pranav Rajpurkar, Sameer Tajdin Khanna,
  Hoifung Poon, Naoto Usuyama, Anja Thieme, Aditya V. Nori, Matthew P. Lungren,
  Ozan Oktay, Javier Alvarez-Valle","Exploring the Boundaries of GPT-4 in Radiology","EMNLP 2023 main","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent success of general-domain large language models (LLMs) has
significantly changed the natural language processing paradigm towards a
unified foundation model across domains and applications. In this paper, we
focus on assessing the performance of GPT-4, the most capable LLM so far, on
the text-based applications for radiology reports, comparing against
state-of-the-art (SOTA) radiology-specific models. Exploring various prompting
strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and
we found GPT-4 either outperforms or is on par with current SOTA radiology
models. With zero-shot prompting, GPT-4 already obtains substantial gains
($\approx$ 10% absolute improvement) over radiology models in temporal sentence
similarity classification (accuracy) and natural language inference ($F_1$).
For tasks that require learning dataset-specific style or schema (e.g. findings
summarisation), GPT-4 improves with example-based prompting and matches
supervised SOTA. Our extensive error analysis with a board-certified
radiologist shows GPT-4 has a sufficient level of radiology knowledge with only
occasional errors in complex context that require nuanced domain knowledge. For
findings summarisation, GPT-4 outputs are found to be overall comparable with
existing manually-written impressions.
","2023-10-24","2310.14573v1.pdf"
"2310.14581","Shuhei Yokoo","Shuhei Yokoo, Peifei Zhu, Yuchi Ishikawa, Mikihiro Tanaka, Masayoshi
  Kondo, Hirokatsu Kataoka","Leveraging Image-Text Similarity and Caption Modification for the
  DataComp Challenge: Filtering Track and BYOD Track","Accepted at the ICCV 2023 Workshop on Towards the Next Generation of
  Computer Vision Datasets: DataComp Track","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large web crawl datasets have already played an important role in learning
multimodal features with high generalization capabilities. However, there are
still very limited studies investigating the details or improvements of data
design. Recently, a DataComp challenge has been designed to propose the best
training data with the fixed models. This paper presents our solution to both
filtering track and BYOD track of the DataComp challenge. Our solution adopts
large multimodal models CLIP and BLIP-2 to filter and modify web crawl data,
and utilize external datasets along with a bag of tricks to improve the data
quality. Experiments show our solution significantly outperforms DataComp
baselines (filtering track: 6.6% improvement, BYOD track: 48.5% improvement).
","2023-10-24","2310.14581v1.pdf"
"2310.14586","Xin Zheng","Xin Zheng, Miao Zhang, Chunyang Chen, Soheila Molaei, Chuan Zhou,
  Shirui Pan","GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels","Accepted by NeurIPS 2023","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Evaluating the performance of graph neural networks (GNNs) is an essential
task for practical GNN model deployment and serving, as deployed GNNs face
significant performance uncertainty when inferring on unseen and unlabeled test
graphs, due to mismatched training-test graph distributions. In this paper, we
study a new problem, GNN model evaluation, that aims to assess the performance
of a specific GNN model trained on labeled and observed graphs, by precisely
estimating its performance (e.g., node classification accuracy) on unseen
graphs without labels. Concretely, we propose a two-stage GNN model evaluation
framework, including (1) DiscGraph set construction and (2) GNNEvaluator
training and inference. The DiscGraph set captures wide-range and diverse graph
data distribution discrepancies through a discrepancy measurement function,
which exploits the outputs of GNNs related to latent node embeddings and node
class predictions. Under the effective training supervision from the DiscGraph
set, GNNEvaluator learns to precisely estimate node classification accuracy of
the to-be-evaluated GNN model and makes an accurate inference for evaluating
GNN model performance. Extensive experiments on real-world unseen and unlabeled
test graphs demonstrate the effectiveness of our proposed method for GNN model
evaluation.
","2023-10-24","2310.14586v1.pdf"
"2310.14587","Liang Wang","Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder,
  Furu Wei","Large Search Model: Redefining Search Stack in the Era of LLMs","16 pages","","","","cs.IR cs.CL","http://creativecommons.org/licenses/by/4.0/","  Modern search engines are built on a stack of different components, including
query understanding, retrieval, multi-stage ranking, and question answering,
among others. These components are often optimized and deployed independently.
In this paper, we introduce a novel conceptual framework called large search
model, which redefines the conventional search stack by unifying search tasks
with one large language model (LLM). All tasks are formulated as autoregressive
text generation problems, allowing for the customization of tasks through the
use of natural language prompts. This proposed framework capitalizes on the
strong language understanding and reasoning capabilities of LLMs, offering the
potential to enhance search result quality while simultaneously simplifying the
existing cumbersome search stack. To substantiate the feasibility of this
framework, we present a series of proof-of-concept experiments and discuss the
potential challenges associated with implementing this approach within
real-world search systems.
","2023-10-24","2310.14587v1.pdf"
"2310.14596","Minghao Tang","Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang
  Lin","Learning to Correct Noisy Labels for Fine-Grained Entity Typing via
  Co-Prediction Prompt Tuning","Accepted by Findings of EMNLP 2023, 11 pages","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Fine-grained entity typing (FET) is an essential task in natural language
processing that aims to assign semantic types to entities in text. However, FET
poses a major challenge known as the noise labeling problem, whereby current
methods rely on estimating noise distribution to identify noisy labels but are
confused by diverse noise distribution deviation. To address this limitation,
we introduce Co-Prediction Prompt Tuning for noise correction in FET, which
leverages multiple prediction results to identify and correct noisy labels.
Specifically, we integrate prediction results to recall labeled labels and
utilize a differentiated margin to identify inaccurate labels. Moreover, we
design an optimization objective concerning divergent co-predictions during
fine-tuning, ensuring that the model captures sufficient information and
maintains robustness in noise identification. Experimental results on three
widely-used FET datasets demonstrate that our noise correction approach
significantly enhances the quality of various types of training samples,
including those annotated using distant supervision, ChatGPT, and
crowdsourcing.
","2023-10-24","2310.14596v1.pdf"
"2310.14599","Wenhao Jiang","Huiyu Mai, Wenhao Jiang, Zhihong Deng","Prefix-Tuning Based Unsupervised Text Style Transfer","","EMNLP 2023 (Findings)","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Unsupervised text style transfer aims at training a generative model that can
alter the style of the input sentence while preserving its content without
using any parallel data. In this paper, we employ powerful pre-trained large
language models and present a new prefix-tuning-based method for unsupervised
text style transfer. We construct three different kinds of prefixes, i.e.,
\textit{shared prefix, style prefix}, and \textit{content prefix}, to encode
task-specific information, target style, and the content information of the
input sentence, respectively. Compared to embeddings used by previous works,
the proposed prefixes can provide richer information for the model.
Furthermore, we adopt a recursive way of using language models in the process
of style transfer. This strategy provides a more effective way for the
interactions between the input sentence and GPT-2, helps the model construct
more informative prefixes, and thus, helps improve the performance. Evaluations
on the well-known datasets show that our method outperforms the
state-of-the-art baselines. Results, analysis of ablation studies, and
subjective evaluations from humans are also provided for a deeper understanding
of the proposed method.
","2023-10-24","2310.14599v1.pdf"
"2310.14607","Yanchen Liu","Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju","Investigating the Fairness of Large Language Models for Predictions on
  Tabular Data","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent literature has suggested the potential of using large language models
(LLMs) to make predictions for tabular tasks. However, LLMs have been shown to
exhibit harmful social biases that reflect the stereotypes and inequalities
present in the society. To this end, as well as the widespread use of tabular
data in many high-stake applications, it is imperative to explore the following
questions: what sources of information do LLMs draw upon when making
predictions for tabular tasks; whether and to what extent are LLM predictions
for tabular tasks influenced by social biases and stereotypes; and what are the
consequential implications for fairness? Through a series of experiments, we
delve into these questions and show that LLMs tend to inherit social biases
from their training data which significantly impact their fairness in tabular
prediction tasks. Furthermore, our investigations show that in the context of
bias mitigation, though in-context learning and fine-tuning have a moderate
effect, the fairness metric gap between different subgroups is still larger
than that in traditional machine learning models, such as Random Forest and
shallow Neural Networks. This observation emphasizes that the social biases are
inherent within the LLMs themselves and inherited from their pre-training
corpus, not only from the downstream task datasets. Besides, we demonstrate
that label-flipping of in-context examples can significantly reduce biases,
further highlighting the presence of inherent bias within LLMs.
","2023-10-24","2310.14607v1.pdf"
"2310.14614","Yige Xu","Yige Xu, Zhiwei Zeng, Zhiqi Shen","Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion
  Recognition","Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Emotion Recognition in Conversation (ERC) has been widely studied due to its
importance in developing emotion-aware empathetic machines. The rise of
pre-trained language models (PLMs) has further pushed the limit of ERC
performance. However, most recent works on ERC using PLMs are heavily
data-driven, and requires fine-tuning the entire PLMs. To improve both sample
and computational efficiency, we propose a derivative-free optimization method
called Cross-Task Prompt Tuning (CTPT) for few-shot conversational emotion
recognition. Unlike existing methods that learn independent knowledge from
individual tasks, CTPT leverages sharable cross-task knowledge by exploiting
external knowledge from other source tasks to improve learning performance
under the few-shot setting. Moreover, CTPT only needs to optimize a vector
under the low intrinsic dimensionality without gradient, which is highly
parameter-efficient compared with existing approaches. Experiments on five
different contextual conversation datasets demonstrate that our CTPT method has
superior results on both few-shot scenarios and zero-shot transfers.
","2023-10-24","2310.14614v1.pdf"
"2310.14623","Hoang Nguyen","Hoang H. Nguyen, Ye Liu, Chenwei Zhang, Tao Zhang, Philip S. Yu","CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine
  Chain-of-Thought Prompting for Multi-domain NLU Tasks","Accepted at EMNLP 2023 (Main Conference)","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  While Chain-of-Thought prompting is popular in reasoning tasks, its
application to Large Language Models (LLMs) in Natural Language Understanding
(NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose
Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks
into multiple reasoning steps where LLMs can learn to acquire and leverage
essential concepts to solve tasks from different granularities. Moreover, we
propose leveraging semantic-based Abstract Meaning Representation (AMR)
structured knowledge as an intermediate step to capture the nuances and diverse
structures of utterances, and to understand connections between their varying
levels of granularity. Our proposed approach is demonstrated effective in
assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot
and few-shot multi-domain settings.
","2023-10-24","2310.14623v1.pdf"
"2310.14626","Chen Yifan","Yuanxing Liu, Wei-Nan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan
  Feng, Hengbin Cui, Yongbin Li, Wanxiang Che","Conversational Recommender System and Large Language Model Are Made for
  Each Other in E-commerce Pre-sales Dialogue","EMNLP 2023 Findings","","","","cs.CL cs.IR","http://creativecommons.org/licenses/by/4.0/","  E-commerce pre-sales dialogue aims to understand and elicit user needs and
preferences for the items they are seeking so as to provide appropriate
recommendations. Conversational recommender systems (CRSs) learn user
representation and provide accurate recommendations based on dialogue context,
but rely on external knowledge. Large language models (LLMs) generate responses
that mimic pre-sales dialogues after fine-tuning, but lack domain-specific
knowledge for accurate recommendations. Intuitively, the strengths of LLM and
CRS in E-commerce pre-sales dialogues are complementary, yet no previous work
has explored this. This paper investigates the effectiveness of combining LLM
and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:
CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a
real-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of
two collaborative approaches with two CRSs and two LLMs on four tasks of
Ecommerce pre-sales dialogue. We find that collaborations between CRS and LLM
can be very effective in some cases.
","2023-10-24","2310.14626v1.pdf"
"2310.14628","Tengxiao Liu","Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng
  Qiu, Zheng Zhang","Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts","EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As large language models (LLMs) have shown effectiveness with different
prompting methods, such as Chain of Thought, Program of Thought, we find that
these methods have formed a great complementarity to each other on math
reasoning tasks. In this work, we propose XoT, an integrated problem solving
framework by prompting LLMs with diverse reasoning thoughts. For each question,
XoT always begins with selecting the most suitable method then executes each
method iteratively. Within each iteration, XoT actively checks the validity of
the generated answer and incorporates the feedback from external executors,
allowing it to dynamically switch among different prompting methods. Through
extensive experiments on 10 popular math reasoning datasets, we demonstrate the
effectiveness of our proposed approach and thoroughly analyze the strengths of
each module. Moreover, empirical results suggest that our framework is
orthogonal to recent work that makes improvements on single reasoning methods
and can further generalise to logical reasoning domain. By allowing method
switching, XoT provides a fresh perspective on the collaborative integration of
diverse reasoning thoughts in a unified framework.
","2023-10-24","2310.14628v1.pdf"
"2310.14633","Petros Karypis","Petros Karypis, Julian McAuley, George Karypis","Extending Input Contexts of Language Models through Training on
  Segmented Sequences","11 pages, 3 figures","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Effectively training language models on long inputs poses many technical
challenges. As a cost consideration, languages models are pretrained on a fixed
sequence length before being adapted to longer sequences. We explore various
methods for adapting models to longer inputs by training on segmented sequences
and an interpolation-based method for extending absolute positional embeddings.
We develop a training procedure to extend the input context size of pretrained
models with no architectural changes and no additional memory costs than
training on the original input lengths. By sub-sampling segments from long
inputs while maintaining their original position the model is able to learn new
positional interactions. Our method benefits both models trained with absolute
positional embeddings, by extending their input contexts, as well as popular
relative positional embedding methods showing a reduced perplexity on sequences
longer than they were trained on. We demonstrate our method can extend input
contexts by a factor of 4x while improving perplexity.
","2023-10-24","2310.14633v1.pdf"
"2310.14651","Shoki Ohta","Shoki Ohta, Takayuki Nishio","$\Lambda$-Split: A Privacy-Preserving Split Computing Framework for
  Cloud-Powered Generative AI","This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible","","","","cs.LG cs.AI cs.DC cs.NI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the wake of the burgeoning expansion of generative artificial intelligence
(AI) services, the computational demands inherent to these technologies
frequently necessitate cloud-powered computational offloading, particularly for
resource-constrained mobile devices. These services commonly employ prompts to
steer the generative process, and both the prompts and the resultant content,
such as text and images, may harbor privacy-sensitive or confidential
information, thereby elevating security and privacy risks. To mitigate these
concerns, we introduce $\Lambda$-Split, a split computing framework to
facilitate computational offloading while simultaneously fortifying data
privacy against risks such as eavesdropping and unauthorized access. In
$\Lambda$-Split, a generative model, usually a deep neural network (DNN), is
partitioned into three sub-models and distributed across the user's local
device and a cloud server: the input-side and output-side sub-models are
allocated to the local, while the intermediate, computationally-intensive
sub-model resides on the cloud server. This architecture ensures that only the
hidden layer outputs are transmitted, thereby preventing the external
transmission of privacy-sensitive raw input and output data. Given the
black-box nature of DNNs, estimating the original input or output from
intercepted hidden layer outputs poses a significant challenge for malicious
eavesdroppers. Moreover, $\Lambda$-Split is orthogonal to traditional
encryption-based security mechanisms, offering enhanced security when deployed
in conjunction. We empirically validate the efficacy of the $\Lambda$-Split
framework using Llama 2 and Stable Diffusion XL, representative large language
and diffusion models developed by Meta and Stability AI, respectively. Our
$\Lambda$-Split implementation is publicly accessible at
https://github.com/nishio-laboratory/lambda_split.
","2023-10-24","2310.14651v1.pdf"
"2310.14657","Stefan Schouten MSc","Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen","Reasoning about Ambiguous Definite Descriptions","EMNLP 2023 Findings","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Natural language reasoning plays an increasingly important role in improving
language models' ability to solve complex language understanding tasks. An
interesting use case for reasoning is the resolution of context-dependent
ambiguity. But no resources exist to evaluate how well Large Language Models
can use explicit reasoning to resolve ambiguity in language. We propose to use
ambiguous definite descriptions for this purpose and create and publish the
first benchmark dataset consisting of such phrases. Our method includes all
information required to resolve the ambiguity in the prompt, which means a
model does not require anything but reasoning to do well. We find this to be a
challenging task for recent LLMs. Code and data available at:
https://github.com/sfschouten/exploiting-ambiguity
","2023-10-24","2310.14657v1.pdf"
"2310.14671","Abhinav Pomalapally","Abhinav Pomalapally, Bassel El Mabsout, Renato Mansuco","Population Descent: A Natural-Selection Based Hyper-Parameter Tuning
  Framework","","","","","cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  First-order gradient descent has been the base of the most successful
optimization algorithms ever implemented. On supervised learning problems with
very high dimensionality, such as neural network optimization, it is almost
always the algorithm of choice, mainly due to its memory and computational
efficiency. However, it is a classical result in optimization that gradient
descent converges to local minima on non-convex functions. Even more
importantly, in certain high-dimensional cases, escaping the plateaus of large
saddle points becomes intractable. On the other hand, black-box optimization
methods are not sensitive to the local structure of a loss function's landscape
but suffer the curse of dimensionality. Instead, memetic algorithms aim to
combine the benefits of both. Inspired by this, we present Population Descent,
a memetic algorithm focused on hyperparameter optimization. We show that an
adaptive m-elitist selection approach combined with a normalized-fitness-based
randomization scheme outperforms more complex state-of-the-art algorithms by up
to 13% on common benchmark tasks.
","2023-10-24","2310.14671v1.pdf"
"2310.14684","Hassan S. Shavarani","Hassan S. Shavarani and Anoop Sarkar","SpEL: Structured Prediction for Entity Linking","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Entity linking is a prominent thread of research focused on structured data
creation by linking spans of text to an ontology or knowledge source. We
revisit the use of structured prediction for entity linking which classifies
each individual input token as an entity, and aggregates the token predictions.
Our system, called SpEL (Structured prediction for Entity Linking) is a
state-of-the-art entity linking system that uses some new ideas to apply
structured prediction to the task of entity linking including: two refined
fine-tuning steps; a context sensitive prediction aggregation strategy;
reduction of the size of the model's output vocabulary, and; we address a
common problem in entity-linking systems where there is a training vs.
inference tokenization mismatch. Our experiments show that we can outperform
the state-of-the-art on the commonly used AIDA benchmark dataset for entity
linking to Wikipedia. Our method is also very compute efficient in terms of
number of parameters and speed of inference.
","2023-10-24","2310.14684v1.pdf"
"2310.14687","Yihan Cao","Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, Daniel Fried","API-Assisted Code Generation for Question Answering on Varied Table
  Structures","EMNLP 2023 camera ready, 13 pages, 11 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  A persistent challenge to table question answering (TableQA) by generating
executable programs has been adapting to varied table structures, typically
requiring domain-specific logical forms. In response, this paper introduces a
unified TableQA framework that: (1) provides a unified representation for
structured tables as multi-index Pandas data frames, (2) uses Python as a
powerful querying language, and (3) uses few-shot prompting to translate NL
questions into Python programs, which are executable on Pandas data frames.
Furthermore, to answer complex relational questions with extended program
functionality and external knowledge, our framework allows customized APIs that
Python programs can call. We experiment with four TableQA datasets that involve
tables of different structures -- relational, multi-table, and hierarchical
matrix shapes -- and achieve prominent improvements over past state-of-the-art
systems. In ablation studies, we (1) show benefits from our multi-index
representation and APIs over baselines that use only an LLM, and (2)
demonstrate that our approach is modular and can incorporate additional APIs.
","2023-10-24","2310.14687v1.pdf"
"2310.14703","Pedro Reviriego","Gonzalo Mart\'inez, Javier Conde, Elena Merino-G\'omez, Beatriz
  Berm\'udez-Margaretto, Jos\'e Alberto Hern\'andez, Pedro Reviriego, Marc
  Brysbaert","The continued usefulness of vocabulary tests for evaluating large
  language models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In their seminal article on semantic vectors, Landauer and Dumain (1997)
proposed testing the quality of AI language models with a challenging
vocabulary test. We show that their Test of English as a Foreign Language
(TOEFL) test remains informative for contemporary major language models, since
none of the models was perfect and made errors on divergent items. The TOEFL
test consists of target words with four alternatives to choose from. We further
tested the models on a Yes/No test that requires distinguishing between
existing words and made-up nonwords. The models performed significantly worse
on the nonword items, in line with other observations that current major
language models provide non-existent information. The situation was worse when
we generalized the tests to Spanish. Here, most models gave
meanings/translations for the majority of random letter sequences. On the plus
side, the best models began to perform quite well, and they also pointed to
nonwords that were unknown to the test participants but can be found in
dictionaries.
","2023-10-24","2310.14703v1.pdf"
"2310.14708","Andrei Catalin Coman","Andrei C. Coman, Gianni Barlacchi, Adri\`a de Gispert","Strong and Efficient Baselines for Open Domain Conversational Question
  Answering","Accepted to EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Unlike the Open Domain Question Answering (ODQA) setting, the conversational
(ODConvQA) domain has received limited attention when it comes to reevaluating
baselines for both efficiency and effectiveness. In this paper, we study the
State-of-the-Art (SotA) Dense Passage Retrieval (DPR) retriever and
Fusion-in-Decoder (FiD) reader pipeline, and show that it significantly
underperforms when applied to ODConvQA tasks due to various limitations. We
then propose and evaluate strong yet simple and efficient baselines, by
introducing a fast reranking component between the retriever and the reader,
and by performing targeted finetuning steps. Experiments on two ODConvQA tasks,
namely TopiOCQA and OR-QuAC, show that our method improves the SotA results,
while reducing reader's latency by 60%. Finally, we provide new and valuable
insights into the development of challenging baselines that serve as a
reference for future, more intricate approaches, including those that leverage
Large Language Models (LLMs).
","2023-10-24","2310.14708v1.pdf"
"2310.14709","Sen Yang","Sen Yang, Xin Li, Lidong Bing, Wai Lam","Once Upon a $\textit{Time}$ in $\textit{Graph}$: Relative-Time
  Pretraining for Complex Temporal Reasoning","EMNLP 2023 main","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Our physical world is constantly evolving over time, rendering challenges for
pre-trained language models to understand and reason over the temporal contexts
of texts. Existing work focuses on strengthening the direct association between
a piece of text and its time-stamp. However, the knowledge-time association is
usually insufficient for the downstream tasks that require reasoning over
temporal dependencies between knowledge. In this work, we make use of the
underlying nature of time, all temporally-scoped sentences are strung together
through a one-dimensional time axis, and suggest creating a graph structure
based on the relative placements of events along the time axis. Inspired by the
graph view, we propose RemeMo ($\underline{Re}$lative Ti$\underline{me}$
$\underline{Mo}$deling), which explicitly connects all temporally-scoped facts
by modeling the time relations between any two sentences. Experimental results
show that RemeMo outperforms the baseline T5 on multiple temporal question
answering datasets under various settings. Further analysis suggests that
RemeMo is especially good at modeling long-range complex temporal dependencies.
We release our code and pre-trained checkpoints at
$\href{https://github.com/DAMO-NLP-SG/RemeMo}{\text{this url}}$.
","2023-10-24","2310.14709v1.pdf"
"2310.14724","Runzhe Zhan","Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, Lidia S.
  Chao","A Survey on LLM-generated Text Detection: Necessity, Methods, and Future
  Directions","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The powerful ability to understand, follow, and generate complex language
emerging from large language models (LLMs) makes LLM-generated text flood many
areas of our daily lives at an incredible speed and is widely accepted by
humans. As LLMs continue to expand, there is an imperative need to develop
detectors that can detect LLM-generated text. This is crucial to mitigate
potential misuse of LLMs and safeguard realms like artistic expression and
social networks from harmful influence of LLM-generated content. The
LLM-generated text detection aims to discern if a piece of text was produced by
an LLM, which is essentially a binary classification task. The detector
techniques have witnessed notable advancements recently, propelled by
innovations in watermarking techniques, zero-shot methods, fine-turning LMs
methods, adversarial learning methods, LLMs as detectors, and human-assisted
methods. In this survey, we collate recent research breakthroughs in this area
and underscore the pressing need to bolster detector research. We also delve
into prevalent datasets, elucidating their limitations and developmental
requirements. Furthermore, we analyze various LLM-generated text detection
paradigms, shedding light on challenges like out-of-distribution problems,
potential attacks, and data ambiguity. Conclusively, we highlight interesting
directions for future research in LLM-generated text detection to advance the
implementation of responsible artificial intelligence (AI). Our aim with this
survey is to provide a clear and comprehensive introduction for newcomers while
also offering seasoned researchers a valuable update in the field of
LLM-generated text detection. The useful resources are publicly available at:
https://github.com/NLP2CT/LLM-generated-Text-Detection.
","2023-10-25","2310.14724v1.pdf"
"2310.14732","Maren Pielka","Maren Pielka, Svetlana Schmidt, Rafet Sifa","Generating Prototypes for Contradiction Detection Using Large Language
  Models and Linguistic Rules","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce a novel data generation method for contradiction detection,
which leverages the generative power of large language models as well as
linguistic rules. Our vision is to provide a condensed corpus of prototypical
contradictions, allowing for in-depth linguistic analysis as well as efficient
language model fine-tuning. To this end, we instruct the generative models to
create contradicting statements with respect to descriptions of specific
contradiction types. In addition, the model is also instructed to come up with
completely new contradiction typologies. As an auxiliary approach, we use
linguistic rules to construct simple contradictions such as those arising from
negation, antonymy and numeric mismatch. We find that our methods yield
promising results in terms of coherence and variety of the data. Further
studies, as well as manual refinement are necessary to make use of this data in
a machine learning setup.
","2023-10-24","2310.14732v1.pdf"
"2310.14735","Bensen Chen","Banghao Chen, Zhaofeng Zhang, Nicolas Langren\'e, Shengxin Zhu","Unleashing the potential of prompt engineering in Large Language Models:
  a comprehensive review","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  This paper delves into the pivotal role of prompt engineering in unleashing
the capabilities of Large Language Models (LLMs). Prompt engineering is the
process of structuring input text for LLMs and is a technique integral to
optimizing the efficacy of LLMs. This survey elucidates foundational principles
of prompt engineering, such as role-prompting, one-shot, and few-shot
prompting, as well as more advanced methodologies such as the chain-of-thought
and tree-of-thoughts prompting. The paper sheds light on how external
assistance in the form of plugins can assist in this task, and reduce machine
hallucination by retrieving external knowledge. We subsequently delineate
prospective directions in prompt engineering research, emphasizing the need for
a deeper understanding of structures and the role of agents in Artificial
Intelligence-Generated Content (AIGC) tools. We discuss how to assess the
efficacy of prompt methods from different perspectives and using different
methods. Finally, we gather information about the application of prompt
engineering in such fields as education and programming, showing its
transformative potential. This comprehensive survey aims to serve as a friendly
guide for anyone venturing through the big world of LLMs and prompt
engineering.
","2023-10-24","2310.14735v1.pdf"
"2310.14747","Hongzhan Chen","Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, Ji Zhang","MCC-KD: Multi-CoT Consistent Knowledge Distillation","Accepted to ENMLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have showcased remarkable capabilities in
complex reasoning through chain of thought (CoT) prompting. Recently, there has
been a growing interest in transferring these reasoning abilities from LLMs to
smaller models. However, achieving both the diversity and consistency in
rationales presents a challenge. In this paper, we focus on enhancing these two
aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to
efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple
rationales for each question and enforce consistency among the corresponding
predictions by minimizing the bidirectional KL-divergence between the answer
distributions. We investigate the effectiveness of MCC-KD with different model
architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both
mathematical reasoning and commonsense reasoning benchmarks. The empirical
results not only confirm MCC-KD's superior performance on in-distribution
datasets but also highlight its robust generalization ability on
out-of-distribution datasets.
","2023-10-25","2310.14747v1.pdf"
"2310.14757","Asahi Ushio","Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, Leonardo Neves,
  Kiamehr Rezaee, Luis Espinosa-Anke, Jiaxin Pei, Jose Camacho-Collados","SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for
  Social Media NLP Research","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Despite its relevance, the maturity of NLP for social media pales in
comparison with general-purpose models, metrics and benchmarks. This fragmented
landscape makes it hard for the community to know, for instance, given a task,
which is the best performing model and how it compares with others. To
alleviate this issue, we introduce a unified benchmark for NLP evaluation in
social media, SuperTweetEval, which includes a heterogeneous set of tasks and
datasets combined, adapted and constructed from scratch. We benchmarked the
performance of a wide range of models on SuperTweetEval and our results suggest
that, despite the recent advances in language modelling, social media remains
challenging.
","2023-10-24","2310.14757v1.pdf"
"2310.14768","Satoshi Hayakawa","Satoshi Hayakawa, Tetsuro Morimura","Policy Gradient with Kernel Quadrature","16 pages, 4 figures","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reward evaluation of episodes becomes a bottleneck in a broad range of
reinforcement learning tasks. Our aim in this paper is to select a small but
representative subset of a large batch of episodes, only on which we actually
compute rewards for more efficient policy gradient iterations. We build a
Gaussian process modeling of discounted returns or rewards to derive a positive
definite kernel on the space of episodes, run an ""episodic"" kernel quadrature
method to compress the information of sample episodes, and pass the reduced
episodes to the policy network for gradient updates. We present the theoretical
background of this procedure as well as its numerical illustrations in MuJoCo
and causal discovery tasks.
","2023-10-24","2310.14768v1.pdf"
"2310.14771","Simon Razniewski","Blerta Veseli, Simon Razniewski, Jan-Christoph Kalo, Gerhard Weikum","Evaluating the Knowledge Base Completion Potential of GPT","12 pages 4 tables","Findings of EMNLP 2023","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Structured knowledge bases (KBs) are an asset for search engines and other
applications, but are inevitably incomplete. Language models (LMs) have been
proposed for unsupervised knowledge base completion (KBC), yet, their ability
to do this at scale and with high accuracy remains an open question. Prior
experimental studies mostly fall short because they only evaluate on popular
subjects, or sample already existing facts from KBs. In this work, we perform a
careful evaluation of GPT's potential to complete the largest public KB:
Wikidata. We find that, despite their size and capabilities, models like GPT-3,
ChatGPT and GPT-4 do not achieve fully convincing results on this task.
Nonetheless, they provide solid improvements over earlier approaches with
smaller LMs. In particular, we show that, with proper thresholding, GPT-3
enables to extend Wikidata by 27M facts at 90% precision.
","2023-10-24","2310.14771v1.pdf"
"2310.14772","Yutao Zhong","Anqi Mao, Mehryar Mohri, Yutao Zhong","Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and
  Algorithms","","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the key framework of learning with abstention in the multi-class
classification setting. In this setting, the learner can choose to abstain from
making a prediction with some pre-defined cost. We present a series of new
theoretical and algorithmic results for this learning problem in the
predictor-rejector framework. We introduce several new families of surrogate
losses for which we prove strong non-asymptotic and hypothesis set-specific
consistency guarantees, thereby resolving positively two existing open
questions. These guarantees provide upper bounds on the estimation error of the
abstention loss function in terms of that of the surrogate loss. We analyze
both a single-stage setting where the predictor and rejector are learned
simultaneously and a two-stage setting crucial in applications, where the
predictor is learned in a first stage using a standard surrogate loss such as
cross-entropy. These guarantees suggest new multi-class abstention algorithms
based on minimizing these surrogate losses. We also report the results of
extensive experiments comparing these algorithms to the current
state-of-the-art algorithms on CIFAR-10, CIFAR-100 and SVHN datasets. Our
results demonstrate empirically the benefit of our new surrogate losses and
show the remarkable performance of our broadly applicable two-stage abstention
algorithm.
","2023-10-24","2310.14772v1.pdf"
"2310.14774","Yutao Zhong","Anqi Mao, Mehryar Mohri, Yutao Zhong","Principled Approaches for Learning to Defer with Multiple Experts","","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a study of surrogate losses and algorithms for the general problem
of learning to defer with multiple experts. We first introduce a new family of
surrogate losses specifically tailored for the multiple-expert setting, where
the prediction and deferral functions are learned simultaneously. We then prove
that these surrogate losses benefit from strong $H$-consistency bounds. We
illustrate the application of our analysis through several examples of
practical surrogate losses, for which we give explicit guarantees. These loss
functions readily lead to the design of new learning to defer algorithms based
on their minimization. While the main focus of this work is a theoretical
analysis, we also report the results of several experiments on SVHN and
CIFAR-10 datasets.
","2023-10-24","2310.14774v1.pdf"
"2310.14777","Pola Schw\""obel","Pola Schw\""obel, Jacek Golebiowski, Michele Donini, C\'edric
  Archambeau, Danish Pruthi","Geographical Erasure in Language Generation","EMNLP 2023 Findings","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) encode vast amounts of world knowledge. However,
since these models are trained on large swaths of internet data, they are at
risk of inordinately capturing information about dominant groups. This
imbalance can propagate into generated language. In this work, we study and
operationalise a form of geographical erasure, wherein language models
underpredict certain countries. We demonstrate consistent instances of erasure
across a range of LLMs. We discover that erasure strongly correlates with low
frequencies of country mentions in the training corpus. Lastly, we mitigate
erasure by finetuning using a custom objective.
","2023-10-24","2310.14777v1.pdf"
"2310.14793","Amit Gajbhiye","Amit Gajbhiye, Zied Bouraoui, Na Li, Usashi Chatterjee, Luis Espinosa
  Anke, Steven Schockaert","What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared
  Properties in Large Concept Vocabularies","Accepted for EMNLP 2023","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Concepts play a central role in many applications. This includes settings
where concepts have to be modelled in the absence of sentence context. Previous
work has therefore focused on distilling decontextualised concept embeddings
from language models. But concepts can be modelled from different perspectives,
whereas concept embeddings typically mostly capture taxonomic structure. To
address this issue, we propose a strategy for identifying what different
concepts, from a potentially large concept vocabulary, have in common with
others. We then represent concepts in terms of the properties they share with
the other concepts. To demonstrate the practical usefulness of this way of
modelling concepts, we consider the task of ultra-fine entity typing, which is
a challenging multi-label classification problem. We show that by augmenting
the label set with shared properties, we can improve the performance of the
state-of-the-art models for this task.
","2023-10-24","2310.14793v1.pdf"
"2310.14799","Qiguang Chen","Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, Wanxiang Che","Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning
  across Languages","Accepted at EMNLP2023 Main Conference","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Chain-of-thought (CoT) is capable of eliciting models to explicitly generate
reasoning paths, thus promoting reasoning accuracy and attracting increasing
attention. Specifically, zero-shot CoT achieves remarkable improvements in a
wide range of reasoning tasks by simply instructing the LLM with the prompt
""Let's think step by step!"". Despite the success of zero-shot CoT, the existing
zero-shot prompting techniques remain limited to a single language, making it
challenging to generalize to other languages and hindering global development.
In this work, we introduce cross-lingual prompting (CLP), aiming to improve
zero-shot CoT reasoning across languages. Specifically, CLP consists of two
main components: (1) cross-lingual alignment prompting and (2) task-specific
solver prompting. The cross-lingual alignment prompting is responsible for
aligning representations across different languages, whereas the task-specific
solver prompting is used to generate the final chain of thoughts and results
for the reasoning task. In addition, we further introduce cross-lingual
self-consistent prompting (CLSP) to ensemble different reasoning paths across
languages. Our experimental evaluations on several benchmarks demonstrate that
CLP and CLSP significantly outperform the existing prompting methods and
achieve state-of-the-art performance. We hope this work will inspire further
breakthroughs in cross-lingual CoT.
","2023-10-24","2310.14799v1.pdf"
"2310.14804","Young-Jun Lee","Young-Jun Lee, Jonghwan Hyeon, Ho-Jin Choi","Large Language Models can Share Images, Too!","","","","","cs.CV cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper explores the image-sharing capability of Large Language Models
(LLMs), such as InstructGPT, ChatGPT, and GPT-4, in a zero-shot setting,
without the help of visual foundation models. Inspired by the two-stage process
of image-sharing in human dialogues, we propose a two-stage framework that
allows LLMs to predict potential image-sharing turns and generate related image
descriptions using our effective restriction-based prompt template. With
extensive experiments, we unlock the \textit{image-sharing} capability of LLMs
in zero-shot prompting, with GPT-4 achieving the best performance.
Additionally, we uncover the emergent \textit{image-sharing} ability in
zero-shot prompting, demonstrating the effectiveness of restriction-based
prompts in both stages of our framework. Based on this framework, we augment
the PhotoChat dataset with images generated by Stable Diffusion at predicted
turns, namely PhotoChat++. To our knowledge, this is the first study to assess
the image-sharing ability of LLMs in a zero-shot setting without visual
foundation models. The source code and the dataset will be released after
publication.
","2023-10-24","2310.14804v1.pdf"
"2310.14805","Danis Alukaev","Danis Alukaev, Semen Kiselev, Ilya Pershin, Bulat Ibragimov, Vladimir
  Ivanov, Alexey Kornaev, Ivan Titov","Cross-Modal Conceptualization in Bottleneck Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray
images) are annotated with high-level concepts (e.g., types of abnormalities),
and perform classification by first predicting the concepts, followed by
predicting the label relying on these concepts. The main difficulty in using
CBMs comes from having to choose concepts that are predictive of the label and
then having to label training examples with these concepts. In our approach, we
adopt a more moderate assumption and instead use text descriptions (e.g.,
radiology reports), accompanying the images in training, to guide the induction
of concepts. Our cross-modal approach treats concepts as discrete latent
variables and promotes concepts that (1) are predictive of the label, and (2)
can be predicted reliably from both the image and text. Through experiments
conducted on datasets ranging from synthetic datasets (e.g., synthetic images
with generated descriptions) to realistic medical imaging datasets, we
demonstrate that cross-modal learning encourages the induction of interpretable
concepts while also facilitating disentanglement. Our results also suggest that
this guidance leads to increased robustness by suppressing the reliance on
shortcut features.
","2023-10-24","2310.14805v1.pdf"
"2310.14816","Tao Lin","Tao Lin, Chengfei Yue, Ziran Liu, Xibin Cao","Generalized Multi-Level Replanning TAMP Framework for Dynamic
  Environment","","","","","cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Task and Motion Planning (TAMP) algorithms can generate plans that combine
logic and motion aspects for robots. However, these plans are sensitive to
interference and control errors. To make TAMP more applicable in real-world, we
propose the generalized multi-level replanning TAMP framework(GMRF), blending
the probabilistic completeness of sampling-based TAMP algorithm with the
robustness of reactive replanning. GMRF generates an nominal plan from the
initial state, then dynamically reconstructs this nominal plan in real-time,
reorders robot manipulations. Following the logic-level adjustment, GMRF will
try to replan a new motion path to ensure the updated plan is feasible at the
motion level. Finally, we conducted real-world experiments involving stack and
rearrange task domains. The result demonstrate GMRF's ability to swiftly
complete tasks in scenarios with varying degrees of interference.
","2023-10-24","2310.14816v1.pdf"
"2310.14817","Moran Beladev","Fengjun Wang, Moran Beladev, Ofri Kleinfeld, Elina Frayerman, Tal
  Shachar, Eran Fainman, Karen Lastmann Assaraf, Sarai Mizrachi, Benjamin Wang","Text2Topic: Multi-Label Text Classification System for Efficient Topic
  Detection in User Generated Content with Zero-Shot Capabilities","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Multi-label text classification is a critical task in the industry. It helps
to extract structured information from large amount of textual data. We propose
Text to Topic (Text2Topic), which achieves high multi-label classification
performance by employing a Bi-Encoder Transformer architecture that utilizes
concatenation, subtraction, and multiplication of embeddings on both text and
topic. Text2Topic also supports zero-shot predictions, produces domain-specific
text embeddings, and enables production-scale batch-inference with high
throughput. The final model achieves accurate and comprehensive results
compared to state-of-the-art baselines, including large language models (LLMs).
  In this study, a total of 239 topics are defined, and around 1.6 million
text-topic pairs annotations (in which 200K are positive) are collected on
approximately 120K texts from 3 main data sources on Booking.com. The data is
collected with optimized smart sampling and partial labeling. The final
Text2Topic model is deployed on a real-world stream processing platform, and it
outperforms other models with 92.9% micro mAP, as well as a 75.8% macro mAP
score. We summarize the modeling choices which are extensively tested through
ablation studies, and share detailed in-production decision-making steps.
","2023-10-24","2310.14817v1.pdf"
"2310.14819","Majd Hawasly","Sabri Boughorbel, Majd Hawasly","Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction
  Following: A Case Study of Arabic","Accepted at SIGARAB ArabicNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  While significant progress has been made in benchmarking Large Language
Models (LLMs) across various tasks, there is a lack of comprehensive evaluation
of their abilities in responding to multi-turn instructions in less-commonly
tested languages like Arabic. Our paper offers a detailed examination of the
proficiency of open LLMs in such scenarios in Arabic. Utilizing a customized
Arabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a
uniform evaluator for both English and Arabic queries to assess and compare the
performance of the LLMs on various open-ended tasks. Our findings reveal
variations in model responses on different task categories, e.g., logic vs.
literacy, when instructed in English or Arabic. We find that fine-tuned base
models using multilingual and multi-turn datasets could be competitive to
models trained from scratch on multilingual data. Finally, we hypothesize that
an ensemble of small, open LLMs could perform competitively to proprietary LLMs
on the benchmark.
","2023-10-24","2310.14819v1.pdf"
"2310.14820","Xunjian Yin","Xunjian Yin and Baizhou Huang and Xiaojun Wan","ALCUNA: Large Language Models Meet New Knowledge","EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the rapid development of NLP, large-scale language models (LLMs) excel
in various tasks across multiple domains now. However, existing benchmarks may
not adequately measure these models' capabilities, especially when faced with
new knowledge. In this paper, we address the lack of benchmarks to evaluate
LLMs' ability to handle new knowledge, an important and challenging aspect in
the rapidly evolving world. We propose an approach called KnowGen that
generates new knowledge by altering existing entity attributes and
relationships, resulting in artificial entities that are distinct from
real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to
assess LLMs' abilities in knowledge understanding, differentiation, and
association. We benchmark several LLMs, reveals that their performance in face
of new knowledge is not satisfactory, particularly in reasoning between new and
internal knowledge. We also explore the impact of entity similarity on the
model's understanding of entity knowledge and the influence of contextual
entities. We appeal to the need for caution when using LLMs in new scenarios or
with new knowledge, and hope that our benchmarks can help drive the development
of LLMs in face of new knowledge.
","2023-10-24","2310.14820v1.pdf"
"2310.14840","Jaap Jumelet","Jaap Jumelet, Willem Zuidema","Transparency at the Source: Evaluating and Interpreting Language Models
  With Access to the True Distribution","EMNLP Findings 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present a setup for training, evaluating and interpreting neural language
models, that uses artificial, language-like data. The data is generated using a
massive probabilistic grammar (based on state-split PCFGs), that is itself
derived from a large natural language corpus, but also provides us complete
control over the generative process. We describe and release both grammar and
corpus, and test for the naturalness of our generated data. This approach
allows us to define closed-form expressions to efficiently compute exact lower
bounds on obtainable perplexity using both causal and masked language
modelling. Our results show striking differences between neural language
modelling architectures and training objectives in how closely they allow
approximating the lower bound on perplexity. Our approach also allows us to
directly compare learned representations to symbolic rules in the underlying
source. We experiment with various techniques for interpreting model behaviour
and learning dynamics. With access to the underlying true source, our results
show striking differences and outcomes in learning dynamics between different
classes of words.
","2023-10-24","2310.14840v1.pdf"
"2310.14843","Marco Tulio Valente","Mauricio Monteiro, Bruno Castelo Branco, Samuel Silvestre, Guilherme
  Avelino, Marco Tulio Valente","End-to-End Software Construction using ChatGPT: An Experience Report","","","","","cs.SE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we explore the application of Large Language Models (LLMs) in
the particular context of end-to-end software construction, i.e., in contexts
where software developers have a set of requirements and have to design,
implement, test, and validate a new software system. Particularly, we report an
experiment where we asked three software developers to use ChatGPT to fully
implement a Web-based application using mainstream software architectures and
technologies. After that, we compare the apps produced by ChatGPT with a
reference implementation that we manually implemented for our research. As a
result, we document four categories of prompts that can be used by developers
in similar contexts, including initialization prompts, feature requests,
bug-fixing, and layout prompts. Additionally, we discuss the advantages and
disadvantages of two prompt construction approaches: top-down (where we start
with a high-level description of the target software, typically in the form of
user stories) and bottom-up (where we request the construction of the system
feature by feature).
","2023-10-24","2310.14843v1.pdf"
"2310.14855","Sai Koneru","Sai Koneru, Miriam Exel, Matthias Huck and Jan Niehues","Contextual Refinement of Translations: Large Language Models for
  Sentence and Document-Level Post-Editing","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLM's) have demonstrated considerable success in
various Natural Language Processing tasks, but they have yet to attain
state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,
their significant performance in tasks demanding a broad understanding and
contextual processing shows their potential for translation. To exploit these
abilities, we investigate using LLM's for MT and explore recent
parameter-efficient fine-tuning techniques. Surprisingly, our initial
experiments find that fine-tuning for translation purposes even led to
performance degradation. To overcome this, we propose an alternative approach:
adapting LLM's as Automatic Post-Editors (APE) rather than direct translators.
Building on the LLM's exceptional ability to process and generate lengthy
sequences, we also propose extending our approach to document-level
translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can
yield significant improvements across both sentence and document-level metrics
while generalizing to out-of-domain data. Most notably, we achieve a
state-of-the-art accuracy rate of 89\% on the ContraPro test set, which
specifically assesses the model's ability to resolve pronoun ambiguities when
translating from English to German. Lastly, we investigate a practical scenario
involving manual post-editing for document-level translation, where reference
context is made available. Here, we demonstrate that leveraging human
corrections can significantly reduce the number of edits required for
subsequent translations\footnote{Interactive Demo for integrating manual
feedback can be found
\href{https://huggingface.co/spaces/skoneru/contextual_refinement_ende}{here}}
","2023-10-24","2310.14855v1.pdf"
"2310.14863","Jan Philip Wahle","Jan Philip Wahle and Bela Gipp and Terry Ruas","Paraphrase Types for Generation and Detection","Published at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Current approaches in paraphrase generation and detection heavily rely on a
single general similarity score, ignoring the intricate linguistic properties
of language. This paper introduces two new tasks to address this shortcoming by
considering paraphrase types - specific linguistic perturbations at particular
text positions. We name these tasks Paraphrase Type Generation and Paraphrase
Type Detection. Our results suggest that while current techniques perform well
in a binary classification scenario, i.e., paraphrased or not, the inclusion of
fine-grained paraphrase types poses a significant challenge. While most
approaches are good at generating and detecting general semantic similar
content, they fail to understand the intrinsic linguistic variables they
manipulate. Models trained in generating and identifying paraphrase types also
show improvements in tasks without them. In addition, scaling these models
further improves their ability to understand paraphrase types. We believe
paraphrase types can unlock a new paradigm for developing paraphrase models and
solving tasks in the future.
","2023-10-24","2310.14863v1.pdf"
"2310.14868","Mengyu Ye","Mengyu Ye, Tatsuki Kuribayashi, Jun Suzuki, Goro Kobayashi, Hiroaki
  Funayama","Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study
  on Syllogism","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) take advantage of step-by-step reasoning
instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their
ability to perform CoT-style reasoning robustly is of interest from a probing
perspective. In this study, we inspect the step-by-step reasoning ability of
LLMs with a focus on negation, which is a core linguistic phenomenon that is
difficult to process. In particular, we introduce several controlled settings
(e.g., reasoning in case of fictional entities) to evaluate the logical
reasoning abilities of the models. We observed that dozens of modern LLMs were
not robust against lexical negation (e.g., plausible ->implausible) when
performing CoT-style reasoning, and the results highlight unique limitations in
each LLM family.
","2023-10-24","2310.14868v1.pdf"
"2310.14870","Jan Philip Wahle","Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, Saif M.
  Mohammad","We are Who We Cite: Bridges of Influence Between Natural Language
  Processing and Other Academic Fields","Published at EMNLP 2023","","","","cs.CL cs.DL","http://creativecommons.org/licenses/by-sa/4.0/","  Natural Language Processing (NLP) is poised to substantially influence the
world. However, significant progress comes hand-in-hand with substantial risks.
Addressing them requires broad engagement with various fields of study. Yet,
little empirical work examines the state of such engagement (past or current).
In this paper, we quantify the degree of influence between 23 fields of study
and NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP
papers to other papers, and ~1.8m citations from other papers to NLP papers. We
show that, unlike most fields, the cross-field engagement of NLP, measured by
our proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in
1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown
more insular -- citing increasingly more NLP papers and having fewer papers
that act as bridges between fields. NLP citations are dominated by computer
science; Less than 8% of NLP citations are to linguistics, and less than 3% are
to math and psychology. These findings underscore NLP's urgent need to reflect
on its engagement with various fields.
","2023-10-24","2310.14870v1.pdf"
"2310.14880","Xiaoxi Kang","Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Adnan Trakic, Terry Yue Zhuo,
  Patrick Charles Emerton, Genevieve Grant","Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal
  Scenarios Like a Lawyer?","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions
recently in the legal domain due to its emergent ability to tackle a variety of
legal tasks. However, it is still unknown if LLMs are able to analyze a legal
case and perform reasoning in the same manner as lawyers. Therefore, we
constructed a novel corpus consisting of scenarios pertain to Contract Acts
Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to
perform analysis on the corpus using the IRAC method, which is a framework
widely used by legal professionals for organizing legal analysis. Each scenario
in the corpus is annotated with a complete IRAC analysis in a semi-structured
format so that both machines and legal professionals are able to interpret and
understand the annotations. In addition, we conducted the first empirical
assessment of ChatGPT for IRAC analysis in order to understand how well it
aligns with the analysis of legal professionals. Our experimental results shed
lights on possible future research directions to improve alignments between
LLMs and legal experts in terms of legal reasoning.
","2023-10-24","2310.14880v1.pdf"
"2310.14909","Kiril Gashteovski","Gorjan Radevski, Kiril Gashteovski, Chia-Chien Hung, Carolin Lawrence,
  Goran Glava\v{s}","Linking Surface Facts to Large-Scale Knowledge Graphs","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Open Information Extraction (OIE) methods extract facts from natural language
text in the form of (""subject""; ""relation""; ""object"") triples. These facts are,
however, merely surface forms, the ambiguity of which impedes their downstream
usage; e.g., the surface phrase ""Michael Jordan"" may refer to either the former
basketball player or the university professor. Knowledge Graphs (KGs), on the
other hand, contain facts in a canonical (i.e., unambiguous) form, but their
coverage is limited by a static schema (i.e., a fixed set of entities and
predicates). To bridge this gap, we need the best of both worlds: (i) high
coverage of free-text OIEs, and (ii) semantic precision (i.e., monosemy) of
KGs. In order to achieve this goal, we propose a new benchmark with novel
evaluation protocols that can, for example, measure fact linking performance on
a granular triple slot level, while also measuring if a system has the ability
to recognize that a surface form has no match in the existing KG. Our extensive
evaluation of several baselines show that detection of out-of-KG entities and
predicates is more difficult than accurate linking to existing ones, thus
calling for more research efforts on this difficult task. We publicly release
all resources (data, benchmark and code) on
https://github.com/nec-research/fact-linking.
","2023-10-24","2310.14909v1.pdf"
"2310.14928","Jun Zhao","Jun Zhao, Zhihao Zhang, Yide Ma, Qi Zhang, Tao Gui, Luhui Gao and
  Xuanjing Huang","Unveiling A Core Linguistic Region in Large Language Models","Work on progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Brain localization, which describes the association between specific regions
of the brain and their corresponding functions, is widely accepted in the field
of cognitive science as an objective fact. Today's large language models (LLMs)
possess human-level linguistic competence and can execute complex tasks
requiring abstract knowledge and reasoning. To deeply understand the inherent
mechanisms of intelligence emergence in LLMs, this paper conducts an analogical
research using brain localization as a prototype. We have discovered a core
region in LLMs that corresponds to linguistic competence, accounting for
approximately 1% of the total model parameters. This core region exhibits
significant dimension dependency, and perturbations to even a single parameter
on specific dimensions can lead to a loss of linguistic competence.
Furthermore, we observe that an improvement in linguistic competence does not
necessarily accompany an elevation in the model's knowledge level, which might
imply the existence of regions of domain knowledge that are dissociated from
the linguistic region. Overall, exploring the LLMs' functional regions provides
insights into the foundation of their intelligence. In the future, we will
continue to investigate knowledge regions within LLMs and the interactions
between them.
","2023-10-24","2310.14928v1.pdf"
"2310.14970","Yujie Feng","Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, Xiao-Ming Wu","Towards LLM-driven Dialogue State Tracking","Accepted at EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dialogue State Tracking (DST) is of paramount importance in ensuring accurate
tracking of user goals and system actions within task-oriented dialogue
systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT
has sparked considerable interest in assessing their efficacy across diverse
applications. In this study, we conduct an initial examination of ChatGPT's
capabilities in DST. Our evaluation uncovers the exceptional performance of
ChatGPT in this task, offering valuable insights to researchers regarding its
capabilities and providing useful directions for designing and enhancing
dialogue systems. Despite its impressive performance, ChatGPT has significant
limitations including its closed-source nature, request restrictions, raising
data privacy concerns, and lacking local deployment capabilities. To address
these concerns, we present LDST, an LLM-driven DST framework based on smaller,
open-source foundation models. By utilizing a novel domain-slot instruction
tuning method, LDST achieves performance on par with ChatGPT. Comprehensive
evaluations across three distinct experimental settings, we find that LDST
exhibits remarkable performance improvements in both zero-shot and few-shot
setting compared to previous SOTA methods. The source code is provided for
reproducibility.
","2023-10-24","2310.14970v1.pdf"
"2310.14971","Wenhong Zhu","Wenhong Zhu, Hongkun Hao and Rui Wang","Penalty Decoding: Well Suppress the Self-Reinforcement Effect in
  Open-Ended Text Generation","Accepted by EMNLP2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The decoding algorithm is critical for open-ended text generation,
transforming latent representations into coherent and meaningful outputs. This
paper investigates the self-reinforcement effect in text generation and the
effectiveness of a repetition penalty to mitigate it. However, determining the
optimal repetition penalty value is challenging. To tackle this, we propose a
forgetting mechanism that disregards distant tokens, reducing the burden of
penalty selection. In addition, we introduce a length penalty to address overly
short sentences caused by excessive penalties. Our penalty decoding approach
incorporating three strategies helps resolve issues with sampling methods
deviating from factual information. Experimental results demonstrate the
efficacy of our approach in generating high-quality sentences resembling human
output.
","2023-10-24","2310.14971v1.pdf"
"2310.14993","Davis Brown","Davis Brown, Charles Godfrey, Nicholas Konz, Jonathan Tu, Henry Kvinge","Understanding the Inner Workings of Language Models Through
  Representation Dissimilarity","EMNLP 2023 (main)","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  As language models are applied to an increasing number of real-world
applications, understanding their inner workings has become an important issue
in model trust, interpretability, and transparency. In this work we show that
representation dissimilarity measures, which are functions that measure the
extent to which two model's internal representations differ, can be a valuable
tool for gaining insight into the mechanics of language models. Among our
insights are: (i) an apparent asymmetry in the internal representations of
model using SoLU and GeLU activation functions, (ii) evidence that
dissimilarity measures can identify and locate generalization properties of
models that are invisible via in-distribution test set performance, and (iii)
new evaluations of how language model features vary as width and depth are
increased. Our results suggest that dissimilarity measures are a promising set
of tools for shedding light on the inner workings of language models.
","2023-10-24","2310.14993v1.pdf"
"2310.15004","Michael Hanna","Michael Hanna, Yonatan Belinkov, Sandro Pezzelle","When Language Models Fall in Love: Animacy Processing in Transformer
  Language Models","To appear at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Animacy - whether an entity is alive and sentient - is fundamental to
cognitive processing, impacting areas such as memory, vision, and language.
However, animacy is not always expressed directly in language: in English it
often manifests indirectly, in the form of selectional constraints on verbs and
adjectives. This poses a potential issue for transformer language models (LMs):
they often train only on text, and thus lack access to extralinguistic
information from which humans learn about animacy. We ask: how does this impact
LMs' animacy processing - do they still behave as humans do? We answer this
question using open-source LMs. Like previous studies, we find that LMs behave
much like humans when presented with entities whose animacy is typical.
However, we also show that even when presented with stories about atypically
animate entities, such as a peanut in love, LMs adapt: they treat these
entities as animate, though they do not adapt as well as humans. Even when the
context indicating atypical animacy is very short, LMs pick up on subtle clues
and change their behavior. We conclude that despite the limited signal through
which LMs can learn about animacy, they are indeed sensitive to the relevant
lexical semantic nuances available in English.
","2023-10-24","2310.15004v1.pdf"
"2310.15007","Matthieu Meeus","Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye","Did the Neurons Read your Book? Document-level Membership Inference for
  Large Language Models","","","","","cs.CL cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With large language models (LLMs) poised to become embedded in our daily
lives, questions are starting to be raised about the dataset(s) they learned
from. These questions range from potential bias or misinformation LLMs could
retain from their training data to questions of copyright and fair use of
human-generated text. However, while these questions emerge, developers of the
recent state-of-the-art LLMs become increasingly reluctant to disclose details
on their training corpus. We here introduce the task of document-level
membership inference for real-world LLMs, i.e. inferring whether the LLM has
seen a given document during training or not. First, we propose a procedure for
the development and evaluation of document-level membership inference for LLMs
by leveraging commonly used data sources for training and the model release
date. We then propose a practical, black-box method to predict document-level
membership and instantiate it on OpenLLaMA-7B with both books and academic
papers. We show our methodology to perform very well, reaching an impressive
AUC of 0.856 for books and 0.678 for papers. We then show our approach to
outperform the sentence-level membership inference attacks used in the privacy
literature for the document-level membership task. We finally evaluate whether
smaller models might be less sensitive to document-level inference and show
OpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.
Taken together, our results show that accurate document-level membership can be
inferred for LLMs, increasing the transparency of technology poised to change
our lives.
","2023-10-24","2310.15007v1.pdf"
"2310.15019","Apostol Vassilev","Apostol Vassilev and Honglan Jin and Munawar Hasan","Meta learning with language models: Challenges and opportunities in the
  classification of imbalanced text","22 pages, including 5 figures, 12 tables, 1 appendix","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/publicdomain/zero/1.0/","  Detecting out of policy speech (OOPS) content is important but difficult.
While machine learning is a powerful tool to tackle this challenging task, it
is hard to break the performance ceiling due to factors like quantity and
quality limitations on training data and inconsistencies in OOPS definition and
data labeling. To realize the full potential of available limited resources, we
propose a meta learning technique (MLT) that combines individual models built
with different text representations. We analytically show that the resulting
technique is numerically stable and produces reasonable combining weights. We
combine the MLT with a threshold-moving (TM) technique to further improve the
performance of the combined predictor on highly-imbalanced in-distribution and
out-of-distribution datasets. We also provide computational results to show the
statistically significant advantages of the proposed MLT approach.
  All authors contributed equally to this work.
","2023-10-25","2310.15019v1.pdf"
"2310.15040","Bingzhi Li","Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao,
  Najoung Kim","SLOG: A Structural Generalization Benchmark for Semantic Parsing","Accepted to EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The goal of compositional generalization benchmarks is to evaluate how well
models generalize to new complex linguistic expressions. Existing benchmarks
often focus on lexical generalization, the interpretation of novel lexical
items in syntactic structures familiar from training; structural generalization
tasks, where a model needs to interpret syntactic structures that are
themselves unfamiliar from training, are often underrepresented, resulting in
overly optimistic perceptions of how well models can generalize. We introduce
SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with
17 structural generalization cases. In our experiments, the generalization
accuracy of Transformer models, including pretrained ones, only reaches 40.6%,
while a structure-aware parser only achieves 70.8%. These results are far from
the near-perfect accuracy existing models achieve on COGS, demonstrating the
role of SLOG in foregrounding the large discrepancy between models' lexical and
structural generalization capacities.
","2023-10-24","2310.15040v1.pdf"
"2310.15047","Dmitrii Krasheninnikov","Dmitrii Krasheninnikov, Egor Krasheninnikov, Bruno Mlodozeniec, David
  Krueger","Meta- (out-of-context) learning in neural networks","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Brown et al. (2020) famously introduced the phenomenon of in-context learning
in large language models (LLMs). We establish the existence of a phenomenon we
call meta-out-of-context learning (meta-OCL) via carefully designed synthetic
experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more
readily ""internalize"" the semantic content of text that is, or appears to be,
broadly useful (such as true statements, or text from authoritative sources)
and use it in appropriate circumstances. We further demonstrate meta-OCL in a
synthetic computer vision setting, and propose two hypotheses for the emergence
of meta-OCL: one relying on the way models store knowledge in their parameters,
and another suggesting that the implicit gradient alignment bias of
gradient-descent-based optimizers may be responsible. Finally, we reflect on
what our results might imply about capabilities of future AI systems, and
discuss potential risks. Our code can be found at
https://github.com/krasheninnikov/internalization.
","2023-10-25","2310.15047v1.pdf"
"2310.15051","Ali Maatouk","Ali Maatouk, Fadhel Ayed, Nicola Piovesan, Antonio De Domenico,
  Merouane Debbah, Zhi-Quan Luo","TeleQnA: A Benchmark Dataset to Assess Large Language Models
  Telecommunications Knowledge","","","","","cs.IT cs.AI cs.LG math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce TeleQnA, the first benchmark dataset designed to evaluate the
knowledge of Large Language Models (LLMs) in telecommunications. Comprising
10,000 questions and answers, this dataset draws from diverse sources,
including standards and research articles. This paper outlines the automated
question generation framework responsible for creating this dataset, along with
how human input was integrated at various stages to ensure the quality of the
questions. Afterwards, using the provided dataset, an evaluation is conducted
to assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The results
highlight that these models struggle with complex standards related questions
but exhibit proficiency in addressing general telecom-related inquiries.
Additionally, our results showcase how incorporating telecom knowledge context
significantly enhances their performance, thus shedding light on the need for a
specialized telecom foundation model. Finally, the dataset is shared with
active telecom professionals, whose performance is subsequently benchmarked
against that of the LLMs. The findings illustrate that LLMs can rival the
performance of active professionals in telecom knowledge, thanks to their
capacity to process vast amounts of information, underscoring the potential of
LLMs within this domain. The dataset has been made publicly accessible on
GitHub.
","2023-10-24","2310.15051v1.pdf"
"2310.15055","Jiannan Xu","Tin Nguyen, Jiannan Xu, Aayushi Roy, Hal Daum\'e III, Marine Carpuat","Towards Conceptualization of ""Fair Explanation"": Disparate Impacts of
  anti-Asian Hate Speech Explanations on Content Moderators","EMNLP 2023 Main Conference (Long Paper)","","","","cs.CL cs.AI cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent research at the intersection of AI explainability and fairness has
focused on how explanations can improve human-plus-AI task performance as
assessed by fairness measures. We propose to characterize what constitutes an
explanation that is itself ""fair"" -- an explanation that does not adversely
impact specific populations. We formulate a novel evaluation method of ""fair
explanations"" using not just accuracy and label time, but also psychological
impact of explanations on different user groups across many metrics (mental
discomfort, stereotype activation, and perceived workload). We apply this
method in the context of content moderation of potential hate speech, and its
differential impact on Asian vs. non-Asian proxy moderators, across explanation
approaches (saliency map and counterfactual explanation). We find that saliency
maps generally perform better and show less evidence of disparate impact
(group) and individual unfairness than counterfactual explanations.
  Content warning: This paper contains examples of hate speech and racially
discriminatory language. The authors do not support such content. Please
consider your risk of discomfort carefully before continuing reading!
","2023-10-24","2310.15055v1.pdf"
"2310.15061","Xinyi Chen","Xinyi Chen, Raquel Fern\'andez, Sandro Pezzelle","The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained
  Multimodal Models","This is the camera-ready version of the paper that will be published
  in the Proceedings of EMNLP 2023 (Singapore, 6-10 December 2023)","","","","cs.CL cs.AI cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite the impressive performance achieved by pre-trained
language-and-vision models in downstream tasks, it remains an open question
whether this reflects a proper understanding of image-text interaction. In this
work, we explore to what extent they handle basic linguistic constructions --
active-passive voice, coordination, and relative clauses -- that even preschool
children can typically master. We present BLA, a novel, automatically
constructed benchmark to evaluate multimodal models on these Basic Language
Abilities. We show that different types of Transformer-based systems, such as
CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting,
in line with previous findings. Our experiments, in particular, show that most
of the tested models only marginally benefit when fine-tuned or prompted with
construction-specific samples. Yet, the generative BLIP2 shows promising
trends, especially in an in-context learning setting. This opens the door to
using BLA not only as an evaluation benchmark but also to improve models' basic
language abilities.
","2023-10-24","2310.15061v1.pdf"
"2310.15065","Qingxiao Zheng","Qingxiao Zheng, Zhongwei Xu, Abhinav Choudhary, Yuting Chen, Yongming
  Li, Yun Huang","Synergizing Human-AI Agency: A Guide of 23 Heuristics for Service
  Co-Creation with LLM-Based Agents","23 pages","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  This empirical study serves as a primer for interested service providers to
determine if and how Large Language Models (LLMs) technology will be integrated
for their practitioners and the broader community. We investigate the mutual
learning journey of non-AI experts and AI through CoAGent, a service
co-creation tool with LLM-based agents. Engaging in a three-stage participatory
design processes, we work with with 23 domain experts from public libraries
across the U.S., uncovering their fundamental challenges of integrating AI into
human workflows. Our findings provide 23 actionable ""heuristics for service
co-creation with AI"", highlighting the nuanced shared responsibilities between
humans and AI. We further exemplar 9 foundational agency aspects for AI,
emphasizing essentials like ownership, fair treatment, and freedom of
expression. Our innovative approach enriches the participatory design model by
incorporating AI as crucial stakeholders and utilizing AI-AI interaction to
identify blind spots. Collectively, these insights pave the way for synergistic
and ethical human-AI co-creation in service contexts, preparing for workforce
ecosystems where AI coexists.
","2023-10-24","2310.15065v1.pdf"
"2310.15066","Te-Lin Wu","Te-Lin Wu, Yu Zhou, Nanyun Peng","Localizing Active Objects from Egocentric Vision with Symbolic World
  Knowledge","In Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP)","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by/4.0/","  The ability to actively ground task instructions from an egocentric view is
crucial for AI agents to accomplish tasks or assist humans virtually. One
important step towards this goal is to localize and track key active objects
that undergo major state change as a consequence of human actions/interactions
to the environment without being told exactly what/where to ground (e.g.,
localizing and tracking the `sponge` in video from the instruction ""Dip the
`sponge` into the bucket.""). While existing works approach this problem from a
pure vision perspective, we investigate to which extent the textual modality
(i.e., task instructions) and their interaction with visual modality can be
beneficial. Specifically, we propose to improve phrase grounding models'
ability on localizing the active objects by: (1) learning the role of `objects
undergoing change` and extracting them accurately from the instructions, (2)
leveraging pre- and post-conditions of the objects during actions, and (3)
recognizing the objects more robustly with descriptional knowledge. We leverage
large language models (LLMs) to extract the aforementioned action-object
knowledge, and design a per-object aggregation masking technique to effectively
perform joint inference on object phrases and symbolic knowledge. We evaluate
our framework on Ego4D and Epic-Kitchens datasets. Extensive experiments
demonstrate the effectiveness of our proposed framework, which leads to>54%
improvements in all standard metrics on the TREK-150-OPE-Det localization +
tracking task, >7% improvements in all standard metrics on the TREK-150-OPE
tracking task, and >3% improvements in average precision (AP) on the Ego4D SCOD
task.
","2023-10-24","2310.15066v1.pdf"
"2310.15075","Fangyu Lei","Fangyu Lei, Tongxu Luo, Pengqi Yang, Weihao Liu, Hanwen Liu, Jiahe
  Lei, Yiming Huang, Yifan Wei, Shizhu He, Jun Zhao, Kang Liu","TableQAKit: A Comprehensive and Practical Toolkit for Table-based
  Question Answering","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Table-based question answering (TableQA) is an important task in natural
language processing, which requires comprehending tables and employing various
reasoning ways to answer the questions. This paper introduces TableQAKit, the
first comprehensive toolkit designed specifically for TableQA. The toolkit
designs a unified platform that includes plentiful TableQA datasets and
integrates popular methods of this task as well as large language models
(LLMs). Users can add their datasets and methods according to the friendly
interface. Also, pleasantly surprised using the modules in this toolkit
achieves new SOTA on some datasets. Finally, \tableqakit{} also provides an
LLM-based TableQA Benchmark for evaluating the role of LLMs in TableQA.
TableQAKit is open-source with an interactive interface that includes visual
operations, and comprehensive data for ease of use.
","2023-10-24","2310.15075v1.pdf"
"2310.15077","Bingsheng Yao","Ronald Cardenas, Bingsheng Yao, Dakuo Wang, Yufang Hou","'Don't Get Too Technical with Me': A Discourse Structure-Based Framework
  for Science Journalism","Accepted to EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Science journalism refers to the task of reporting technical findings of a
scientific paper as a less technical news article to the general public
audience. We aim to design an automated system to support this real-world task
(i.e., automatic science journalism) by 1) introducing a newly-constructed and
real-world dataset (SciTechNews), with tuples of a publicly-available
scientific paper, its corresponding news article, and an expert-written short
summary snippet; 2) proposing a novel technical framework that integrates a
paper's discourse structure with its metadata to guide generation; and, 3)
demonstrating with extensive automatic and human experiments that our framework
outperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating a
content plan meaningful for the target audience, simplifying the information
selected, and producing a coherent final report in a layman's style.
","2023-10-24","2310.15077v1.pdf"
"2310.15079","Tenghao Huang","Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, Faeze Brahman,
  Muhao Chen, Snigdha Chaturvedi","Affective and Dynamic Beam Search for Story Generation","Accepted at EMNLP-findings 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Storytelling's captivating potential makes it a fascinating research area,
with implications for entertainment, education, therapy, and cognitive studies.
In this paper, we propose Affective Story Generator (AffGen) for generating
interesting narratives. AffGen introduces ""intriguing twists"" in narratives by
employing two novel techniques-Dynamic Beam Sizing and Affective Reranking.
Dynamic Beam Sizing encourages less predictable, more captivating word choices
using a contextual multi-arm bandit model. Affective Reranking prioritizes
sentence candidates based on affect intensity. Our empirical evaluations, both
automatic and human, demonstrate AffGen's superior performance over existing
baselines in generating affectively charged and interesting narratives. Our
ablation study and analysis provide insights into the strengths and weaknesses
of AffGen.
","2023-10-24","2310.15079v1.pdf"
"2310.15080","Tianshi Che","Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S.
  Sheng, Huaiyu Dai, Dejing Dou","Federated Learning of Large Language Models with Parameter-Efficient
  Prompt Tuning and Adaptive Optimization","","EMNLP 2023","","","cs.LG cs.CL cs.DC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Federated learning (FL) is a promising paradigm to enable collaborative model
training with decentralized data. However, the training process of Large
Language Models (LLMs) generally incurs the update of significant parameters,
which limits the applicability of FL techniques to tackle the LLMs in real
scenarios. Prompt tuning can significantly reduce the number of parameters to
update, but it either incurs performance degradation or low training
efficiency. The straightforward utilization of prompt tuning in the FL often
raises non-trivial communication costs and dramatically degrades performance.
In addition, the decentralized data is generally non-Independent and
Identically Distributed (non-IID), which brings client drift problems and thus
poor performance. This paper proposes a Parameter-efficient prompt Tuning
approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and
effective FL of LLMs. First, an efficient partial prompt tuning approach is
proposed to improve performance and efficiency simultaneously. Second, a novel
adaptive optimization method is developed to address the client drift problems
on both the device and server sides to enhance performance further. Extensive
experiments based on 10 datasets demonstrate the superb performance (up to
60.8\% in terms of accuracy) and efficiency (up to 97.59\% in terms of training
time) of FedPepTAO compared with 9 baseline approaches. Our code is available
at https://github.com/llm-eff/FedPepTAO.
","2023-10-24","2310.15080v1.pdf"
"2310.15100","Shih-Chieh Dai","Shih-Chieh Dai, Aiping Xiong, Lun-Wei Ku","LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Thematic analysis (TA) has been widely used for analyzing qualitative data in
many disciplines and fields. To ensure reliable analysis, the same piece of
data is typically assigned to at least two human coders. Moreover, to produce
meaningful and useful analysis, human coders develop and deepen their data
interpretation and coding over multiple iterations, making TA labor-intensive
and time-consuming. Recently the emerging field of large language models (LLMs)
research has shown that LLMs have the potential replicate human-like behavior
in various tasks: in particular, LLMs outperform crowd workers on
text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We
propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct
TA with in-context learning (ICL). This framework provides the prompt to frame
discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA.
We demonstrate the utility of this framework using survey datasets on the
aspects of the music listening experience and the usage of a password manager.
Results of the two case studies show that the proposed framework yields similar
coding quality to that of human coders but reduces TA's labor and time demands.
","2023-10-24","2310.15100v1.pdf"
"2310.15109","Yichuan Li","Yichuan Li and Kaize Ding and Kyumin Lee","GRENADE: Graph-Centric Language Model for Self-Supervised Representation
  Learning on Text-Attributed Graphs","Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Self-supervised representation learning on text-attributed graphs, which aims
to create expressive and generalizable representations for various downstream
tasks, has received increasing research attention lately. However, existing
methods either struggle to capture the full extent of structural context
information or rely on task-specific training labels, which largely hampers
their effectiveness and generalizability in practice. To solve the problem of
self-supervised representation learning on text-attributed graphs, we develop a
novel Graph-Centric Language model -- GRENADE. Specifically, GRENADE exploits
the synergistic effect of both pre-trained language model and graph neural
network by optimizing with two specialized self-supervised learning algorithms:
graph-centric contrastive learning and graph-centric knowledge alignment. The
proposed graph-centric self-supervised learning algorithms effectively help
GRENADE to capture informative textual semantics as well as structural context
information on text-attributed graphs. Through extensive experiments, GRENADE
shows its superiority over state-of-the-art methods. Implementation is
available at \url{https://github.com/bigheiniu/GRENADE}.
","2023-10-24","2310.15109v1.pdf"
"2310.15110","Ruoxi Shi","Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue
  Wei, Linghao Chen, Chong Zeng, Hao Su","Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model","","","","","cs.CV cs.GR","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We report Zero123++, an image-conditioned diffusion model for generating
3D-consistent multi-view images from a single input view. To take full
advantage of pretrained 2D generative priors, we develop various conditioning
and training schemes to minimize the effort of finetuning from off-the-shelf
image diffusion models such as Stable Diffusion. Zero123++ excels in producing
high-quality, consistent multi-view images from a single image, overcoming
common issues like texture degradation and geometric misalignment. Furthermore,
we showcase the feasibility of training a ControlNet on Zero123++ for enhanced
control over the generation process. The code is available at
https://github.com/SUDO-AI-3D/zero123plus.
","2023-10-24","2310.15110v1.pdf"
"2310.15113","Leonie Weissweiler","Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai,
  Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek
  Vijayakumar, Haofei Yu, Hinrich Sch\""utze, Kemal Oflazer, David R. Mortensen","Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into
  the Morphological Capabilities of a Large Language Model","EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have recently reached an impressive level of
linguistic capability, prompting comparisons with human language skills.
However, there have been relatively few systematic inquiries into the
linguistic capabilities of the latest generation of LLMs, and those studies
that do exist (i) ignore the remarkable ability of humans to generalize, (ii)
focus only on English, and (iii) investigate syntax or semantics and overlook
other capabilities that lie at the heart of human language, like morphology.
Here, we close these gaps by conducting the first rigorous analysis of the
morphological capabilities of ChatGPT in four typologically varied languages
(specifically, English, German, Tamil, and Turkish). We apply a version of
Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for
the four examined languages. We find that ChatGPT massively underperforms
purpose-built systems, particularly in English. Overall, our results -- through
the lens of morphology -- cast a new light on the linguistic capabilities of
ChatGPT, suggesting that claims of human-like language skills are premature and
misleading.
","2023-10-27","2310.15113v1.pdf"
"2310.15117","Aniket Vashishtha","Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh
  Bachu, Vineeth N Balasubramanian, Amit Sharma","Causal Inference Using LLM-Guided Discovery","","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  At the core of causal inference lies the challenge of determining reliable
causal graphs solely based on observational data. Since the well-known backdoor
criterion depends on the graph, any errors in the graph can propagate
downstream to effect inference. In this work, we initially show that complete
graph information is not necessary for causal effect inference; the topological
order over graph variables (causal order) alone suffices. Further, given a node
pair, causal order is easier to elicit from domain experts compared to graph
edges since determining the existence of an edge can depend extensively on
other variables. Interestingly, we find that the same principle holds for Large
Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated
method to obtain causal order (and hence causal effect) with LLMs acting as
virtual domain experts. To this end, we employ different prompting strategies
and contextual cues to propose a robust technique of obtaining causal order
from LLMs. Acknowledging LLMs' limitations, we also study possible techniques
to integrate LLMs with established causal discovery algorithms, including
constraint-based and score-based methods, to enhance their performance.
Extensive experiments demonstrate that our approach significantly improves
causal ordering accuracy as compared to discovery algorithms, highlighting the
potential of LLMs to enhance causal inference across diverse fields.
","2023-10-24","2310.15117v1.pdf"
"2310.15123","Swarnadeep Saha","Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason
  Weston, Xian Li","Branch-Solve-Merge Improves Large Language Model Evaluation and
  Generation","22 pages, 7 figures, 10 tables","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) are frequently used for multi-faceted language
generation and evaluation tasks that involve satisfying intricate user
constraints or taking into account multiple aspects and criteria. However,
their performance can fall short, due to the model's lack of coherence and
inability to plan and decompose the problem. We propose Branch-Solve-Merge
(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such
challenging natural language tasks. It consists of branch, solve, and merge
modules that are parameterized with specific prompts to the base LLM. These
three modules plan a decomposition of the task into multiple parallel
sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.
We apply our method to the tasks of LLM response evaluation and constrained
text generation and evaluate its effectiveness with multiple LLMs, including
Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and
consistency for each LLM by enhancing human-LLM agreement by up to 26%,
reducing length and pairwise position biases by up to 50%, and allowing
LLaMA-2-chat to match or outperform GPT-4 on most domains. On the constraint
story generation task, BSM improves the coherence of the stories while also
improving constraint satisfaction by 12%.
","2023-10-24","2310.15123v1.pdf"
"2310.15127","Gabriel Sarch","Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki","Open-Ended Instructable Embodied Agents with Memory-Augmented Large
  Language Models","https://helper-agent-llm.github.io","","","","cs.AI cs.CL cs.LG cs.RO","http://creativecommons.org/licenses/by/4.0/","  Pre-trained and frozen LLMs can effectively map simple scene re-arrangement
instructions to programs over a robot's visuomotor functions through
appropriate few-shot example prompting. To parse open-domain natural language
and adapt to a user's idiosyncratic procedures, not known during prompt
engineering time, fixed prompts fall short. In this paper, we introduce HELPER,
an embodied agent equipped with an external memory of language-program pairs
that parses free-form human-robot dialogue into action programs through
retrieval-augmented LLM prompting: relevant memories are retrieved based on the
current dialogue, instruction, correction or VLM description, and used as
in-context prompt examples for LLM querying. The memory is expanded during
deployment to include pairs of user's language and action plans, to assist
future inferences and personalize them to the user's language and routines.
HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution
from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x
improvement over the previous SOTA for TfD. Our models, code and video results
can be found in our project's website: https://helper-agent-llm.github.io.
","2023-10-24","2310.15127v1.pdf"
"2310.15129","Shao-Hua Sun","Nicholas Collin Suwono, Justin Chih-Yao Chen, Tun Min Hung, Ting-Hao
  Kenneth Huang, I-Bin Liao, Yung-Hui Li, Lun-Wei Ku, Shao-Hua Sun","Location-Aware Visual Question Generation with Lightweight Models","EMNLP 2023","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work introduces a novel task, location-aware visual question generation
(LocaVQG), which aims to generate engaging questions from data relevant to a
particular geographical location. Specifically, we represent such
location-aware information with surrounding images and a GPS coordinate. To
tackle this task, we present a dataset generation pipeline that leverages GPT-4
to produce diverse and sophisticated questions. Then, we aim to learn a
lightweight model that can address the LocaVQG task and fit on an edge device,
such as a mobile phone. To this end, we propose a method which can reliably
generate engaging questions from location-aware information. Our proposed
method outperforms baselines regarding human evaluation (e.g., engagement,
grounding, coherence) and automatic evaluation metrics (e.g., BERTScore,
ROUGE-2). Moreover, we conduct extensive ablation studies to justify our
proposed techniques for both generating the dataset and solving the task.
","2023-10-24","2310.15129v1.pdf"
"2310.15135","Anjali Kantharuban","Anjali Kantharuban, Ivan Vuli\'c, and Anna Korhonen","Quantifying the Dialect Gap and its Correlates Across Languages","Accepted to EMNLP Findings 2023","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Historically, researchers and consumers have noticed a decrease in quality
when applying NLP tools to minority variants of languages (i.e. Puerto Rican
Spanish or Swiss German), but studies exploring this have been limited to a
select few languages. Additionally, past studies have mainly been conducted in
a monolingual context, so cross-linguistic trends have not been identified and
tied to external factors. In this work, we conduct a comprehensive evaluation
of the most influential, state-of-the-art large language models (LLMs) across
two high-use applications, machine translation and automatic speech
recognition, to assess their functionality on the regional dialects of several
high- and low-resource languages. Additionally, we analyze how the regional
dialect gap is correlated with economic, social, and linguistic factors. The
impact of training data, including related factors like dataset size and its
construction procedure, is shown to be significant but not consistent across
models or languages, meaning a one-size-fits-all approach cannot be taken in
solving the dialect gap. This work will lay the foundation for furthering the
field of dialectal NLP by laying out evident disparities and identifying
possible pathways for addressing them through mindful data collection.
","2023-10-24","2310.15135v1.pdf"
"2310.15140","Sicheng Zhu","Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang,
  Furong Huang, Ani Nenkova, Tong Sun","AutoDAN: Automatic and Interpretable Adversarial Attacks on Large
  Language Models","","","","","cs.CR cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Safety alignment of Large Language Models (LLMs) can be compromised with
manual jailbreak attacks and (automatic) adversarial attacks. Recent work
suggests that patching LLMs against these attacks is possible: manual jailbreak
attacks are human-readable but often limited and public, making them easy to
block; adversarial attacks generate gibberish prompts that can be detected
using perplexity-based filters. In this paper, we show that these solutions may
be too optimistic. We propose an interpretable adversarial attack,
\texttt{AutoDAN}, that combines the strengths of both types of attacks. It
automatically generates attack prompts that bypass perplexity-based filters
while maintaining a high attack success rate like manual jailbreak attacks.
These prompts are interpretable and diverse, exhibiting strategies commonly
used in manual jailbreak attacks, and transfer better than their non-readable
counterparts when using limited training data or a single proxy model. We also
customize \texttt{AutoDAN}'s objective to leak system prompts, another
jailbreak application not addressed in the adversarial attack literature. Our
work provides a new way to red-team LLMs and to understand the mechanism of
jailbreak attacks.
","2023-10-25","2310.15140v1.pdf"
"2310.15141","Ziteng Sun","Ziteng Sun and Ananda Theertha Suresh and Jae Hun Ro and Ahmad Beirami
  and Himanshu Jain and Felix Yu","SpecTr: Fast Speculative Decoding via Optimal Transport","","","","","cs.LG cs.CL cs.DS cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Autoregressive sampling from large language models has led to
state-of-the-art results in several natural language tasks. However,
autoregressive sampling generates tokens one at a time making it slow, and even
prohibitive in certain tasks. One way to speed up sampling is
$\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$
(block or sequence of tokens), and then score all tokens in the draft by the
large language model in parallel. A subset of the tokens in the draft are
accepted (and the rest rejected) based on a statistical method to guarantee
that the final output follows the distribution of the large model. In this
work, we provide a principled understanding of speculative decoding through the
lens of optimal transport (OT) with $\textit{membership cost}$. This framework
can be viewed as an extension of the well-known $\textit{maximal-coupling}$
problem. This new formulation enables us to generalize the speculative decoding
method to allow for a set of $k$ candidates at the token-level, which leads to
an improved optimal membership cost. We show that the optimal draft selection
algorithm (transport plan) can be computed via linear programming, whose
best-known runtime is exponential in $k$. We then propose a valid draft
selection algorithm whose acceptance probability is $(1-1/e)$-optimal
multiplicatively. Moreover, it can be computed in time almost linear with size
of domain of a single token. Using this $new draft selection$ algorithm, we
develop a new autoregressive sampling algorithm called $\textit{SpecTr}$, which
provides speedup in decoding while ensuring that there is no quality
degradation in the decoded output. We experimentally demonstrate that for
state-of-the-art large language models, the proposed approach achieves a wall
clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on
standard benchmarks.
","2023-10-24","2310.15141v1.pdf"
"2310.15144","Zhengyuan Yang","Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Lijuan Wang","DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual
  Design","Project page at https://design-bench.github.io/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce DEsignBench, a text-to-image (T2I) generation benchmark tailored
for visual design scenarios. Recent T2I models like DALL-E 3 and others, have
demonstrated remarkable capabilities in generating photorealistic images that
align closely with textual inputs. While the allure of creating visually
captivating images is undeniable, our emphasis extends beyond mere aesthetic
pleasure. We aim to investigate the potential of using these powerful models in
authentic design contexts. In pursuit of this goal, we develop DEsignBench,
which incorporates test samples designed to assess T2I models on both ""design
technical capability"" and ""design application scenario."" Each of these two
dimensions is supported by a diverse set of specific design categories. We
explore DALL-E 3 together with other leading T2I models on DEsignBench,
resulting in a comprehensive visual gallery for side-by-side comparisons. For
DEsignBench benchmarking, we perform human evaluations on generated images in
DEsignBench gallery, against the criteria of image-text alignment, visual
aesthetic, and design creativity. Our evaluation also considers other
specialized design capabilities, including text rendering, layout composition,
color harmony, 3D design, and medium style. In addition to human evaluations,
we introduce the first automatic image generation evaluator powered by GPT-4V.
This evaluator provides ratings that align well with human judgments, while
being easily replicable and cost-efficient. A high-resolution version is
available at
https://github.com/design-bench/design-bench.github.io/raw/main/designbench.pdf?download=
","2023-10-24","2310.15144v1.pdf"
"2310.15147","Fangyu Lei","Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, Kang Liu","S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large
  Language Models","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The rapid development of Large Language Models (LLMs) has led to great
strides in model capabilities like reasoning and long-context understanding.
However, as LLMs are able to process longer contexts, it becomes more
challenging to evaluate whether they have acquired certain capabilities, since
the length of text (e.g., 100K tokens) they can process far exceeds what humans
can reliably assess in a reasonable duration. In this paper, we propose using
complex synthetic tasks as a proxy evaluation method, and present S3Eval, a
Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. As a
synthetic benchmark, S3Eval enables the creation of any number of evaluation
examples that are theoretically invisible to LLMs, mitigating the test set
contamination issue. The synthetic nature of S3Eval provides users full control
over the dataset, allowing them to systematically probe LLM capabilities by
scaling text length and varying task difficulty across diverse scenarios. The
strong correlation between S3Eval performance and scores of real-world
benchmarks like Big-Bench Hard (BBH) demonstrates the soundness of using S3Eval
for evaluation of LLMs. The in-depth analysis also uncover additional insights,
including performance drop when the answer is sparsely distributed or located
in the middle context, as well as some counter-intuitive trends of model
performance.
","2023-10-24","2310.15147v1.pdf"
"2310.15154","Curt Tigges","Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, Neel Nanda","Linear Representations of Sentiment in Large Language Models","","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Sentiment is a pervasive feature in natural language text, yet it is an open
question how sentiment is represented within Large Language Models (LLMs). In
this study, we reveal that across a range of models, sentiment is represented
linearly: a single direction in activation space mostly captures the feature
across a range of tasks with one extreme for positive and the other for
negative. Through causal interventions, we isolate this direction and show it
is causally relevant in both toy tasks and real world datasets such as Stanford
Sentiment Treebank. Through this case study we model a thorough investigation
of what a single direction means on a broad data distribution.
  We further uncover the mechanisms that involve this direction, highlighting
the roles of a small subset of attention heads and neurons. Finally, we
discover a phenomenon which we term the summarization motif: sentiment is not
solely represented on emotionally charged words, but is additionally summarized
at intermediate positions without inherent sentiment, such as punctuation and
names. We show that in Stanford Sentiment Treebank zero-shot classification,
76% of above-chance classification accuracy is lost when ablating the sentiment
direction, nearly half of which (36%) is due to ablating the summarized
sentiment direction exclusively at comma positions.
","2023-10-24","2310.15154v1.pdf"
"2310.15164","Benjamin Lipkin","Theo X. Olausson and Alex Gu and Benjamin Lipkin and Cedegao E. Zhang
  and Armando Solar-Lezama and Joshua B. Tenenbaum and Roger Levy","LINC: A Neurosymbolic Approach for Logical Reasoning by Combining
  Language Models with First-Order Logic Provers","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Logical reasoning, i.e., deductively inferring the truth value of a
conclusion from a set of premises, is an important task for artificial
intelligence with wide potential impacts on science, mathematics, and society.
While many prompting-based strategies have been proposed to enable Large
Language Models (LLMs) to do such reasoning more effectively, they still appear
unsatisfactory, often failing in subtle and unpredictable ways. In this work,
we investigate the validity of instead reformulating such tasks as modular
neurosymbolic programming, which we call LINC: Logical Inference via
Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser,
translating premises and conclusions from natural language to expressions in
first-order logic. These expressions are then offloaded to an external theorem
prover, which symbolically performs deductive inference. Leveraging this
approach, we observe significant performance gains on FOLIO and a balanced
subset of ProofWriter for three different models in nearly all experimental
conditions we evaluate. On ProofWriter, augmenting the comparatively small
open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5
and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%,
respectively. When used with GPT-4, LINC scores 26% higher than CoT on
ProofWriter while performing comparatively on FOLIO. Further analysis reveals
that although both methods on average succeed roughly equally often on this
dataset, they exhibit distinct and complementary failure modes. We thus provide
promising evidence for how logical reasoning over natural language can be
tackled through jointly leveraging LLMs alongside symbolic provers. All
corresponding code is publicly available at https://github.com/benlipkin/linc
","2023-10-24","2310.15164v1.pdf"
"2310.15166","Bo Li","Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt
  Keutzer, Trevor Darrell, Ziwei Liu","Large Language Models are Visual Reasoning Coordinators","Accepted at NeurIPS 2023","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Visual reasoning requires multimodal perception and commonsense cognition of
the world. Recently, multiple vision-language models (VLMs) have been proposed
with excellent commonsense reasoning ability in various domains. However, how
to harness the collective power of these complementary VLMs is rarely explored.
Existing methods like ensemble still struggle to aggregate these models with
the desired higher-order communications. In this work, we propose Cola, a novel
paradigm that coordinates multiple VLMs for visual reasoning. Our key insight
is that a large language model (LLM) can efficiently coordinate multiple VLMs
by facilitating natural language communication that leverages their distinct
and complementary capabilities. Extensive experiments demonstrate that our
instruction tuning variant, Cola-FT, achieves state-of-the-art performance on
visual question answering (VQA), outside knowledge VQA, visual entailment, and
visual spatial reasoning tasks. Moreover, we show that our in-context learning
variant, Cola-Zero, exhibits competitive performance in zero and few-shot
settings, without finetuning. Through systematic ablation studies and
visualizations, we validate that a coordinator LLM indeed comprehends the
instruction prompts as well as the separate functionalities of VLMs; it then
coordinates them to enable impressive visual reasoning capabilities.
","2023-10-24","2310.15166v1.pdf"
"2310.15177","Alexander Ororbia","Alexander Ororbia, Mary Alexandria Kelly","A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian
  Learning and Free Energy Minimization","Accepted draft to 2023 AAAI Fall Symposium on Integration of
  Cognitive Architectures and Generative Models","","","","q-bio.NC cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Over the last few years, large neural generative models, capable of
synthesizing intricate sequences of words or producing complex image patterns,
have recently emerged as a popular representation of what has come to be known
as ""generative artificial intelligence"" (generative AI). Beyond opening the
door to new opportunities as well as challenges for the domain of statistical
machine learning, the rising popularity of generative AI brings with it
interesting questions for Cognitive Science, which seeks to discover the nature
of the processes that underpin minds and brains as well as to understand how
such functionality might be acquired and instantiated in biological (or
artificial) substrate. With this goal in mind, we argue that a promising
long-term pathway lies in the crafting of cognitive architectures, a
long-standing tradition of the field, cast fundamentally in terms of
neuro-mimetic generative building blocks. Concretely, we discuss the COGnitive
Neural GENerative system, which is an architecture that casts the Common Model
of Cognition in terms of Hebbian adaptation operating in service of optimizing
a variational free energy functional.
","2023-10-25","2310.15177v1.pdf"
"2310.15200","Xinyu Huang","Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie
  Zhang, Yanchun Xie, Yaqian Li, Lei Zhang","Inject Semantic Concepts into Image Tagging for Open-Set Recognition","Homepage: https://github.com/xinyu1205/recognize-anything","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we introduce the Recognize Anything Plus Model~(RAM++), a
fundamental image recognition model with strong open-set recognition
capabilities, by injecting semantic concepts into image tagging training
framework. Previous approaches are either image tagging models constrained by
limited semantics, or vision-language models with shallow interaction for
suboptimal performance in multi-tag recognition. In contrast, RAM++ integrates
image-text alignment and image-tagging within a unified fine-grained
interaction framework based on image-tags-text triplets. This design enables
RAM++ not only excel in identifying predefined categories, but also
significantly augment the recognition ability in open-set categories. Moreover,
RAM++ employs large language models~(LLMs) to generate diverse visual tag
descriptions, pioneering the integration of LLM's knowledge into image tagging
training. This approach empowers RAM++ to integrate visual description concepts
for open-set recognition during inference. Evaluations on comprehensive image
recognition benchmarks demonstrate RAM++ exceeds existing state-of-the-art
(SOTA) fundamental image recognition models on most aspects. Specifically, for
predefined common-used tag categories, RAM++ showcases 10.2 mAP and 15.4 mAP
enhancements over CLIP on OpenImages and ImageNet. For open-set categories
beyond predefined, RAM++ records improvements of 5 mAP and 6.4 mAP over CLIP
and RAM respectively on OpenImages. For diverse human-object interaction
phrases, RAM++ achieves 7.8 mAP and 4.7 mAP improvements on the HICO benchmark.
Code, datasets and pre-trained models are available at
\url{https://github.com/xinyu1205/recognize-anything}.
","2023-10-25","2310.15200v1.pdf"
"2310.15205","Wei Chen","Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu,
  Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, Zhongyu Wei","DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple
  Experts Fine-tuning","18 pages, 13 figures, 7 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose Multiple Experts Fine-tuning Framework to build a financial large
language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by
endowing them with multi-turn question answering abilities, domain text
processing capabilities, mathematical computation skills, and
retrieval-enhanced generation capabilities. We build a financial
instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of
four categories (consulting, NLP tasks, computing and retrieval-augmented
generation). Evaluations conducted on multiple benchmarks demonstrate that our
model performs better than baseline models in various financial scenarios.
Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
","2023-10-26","2310.15205v1.pdf"
"2310.15213","Eric Todd","Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C.
  Wallace, David Bau","Function Vectors in Large Language Models","43 pages, 25 figures, 20 tables, Code and data at
  https://functions.baulab.info","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We report the presence of a simple neural mechanism that represents an
input-output function as a vector within autoregressive transformer language
models (LMs). Using causal mediation analysis on a diverse range of
in-context-learning (ICL) tasks, we find that a small number attention heads
transport a compact representation of the demonstrated task, which we call a
function vector (FV). FVs are robust to changes in context, i.e., they trigger
execution of the task on inputs such as zero-shot and natural text settings
that do not resemble the ICL contexts from which they are collected. We test
FVs across a range of tasks, models, and layers and find strong causal effects
across settings in middle layers. We investigate the internal structure of FVs
and find while that they often contain information that encodes the output
space of the function, this information alone is not sufficient to reconstruct
an FV. Finally, we test semantic vector composition in FVs, and find that to
some extent they can be summed to create vectors that trigger new complex
tasks. Taken together, our findings suggest that LLMs contain internal
abstractions of general-purpose functions that can be invoked in a variety of
contexts.
","2023-10-25","2310.15213v1.pdf"
"2310.15239","Mete Ismayilzada","Mete Ismayilzada, Debjit Paul, Syrielle Montariol, Mor Geva, Antoine
  Bosselut","CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks","37 pages, camera-ready for EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recent efforts in natural language processing (NLP) commonsense reasoning
research have yielded a considerable number of new datasets and benchmarks.
However, most of these datasets formulate commonsense reasoning challenges in
artificial scenarios that are not reflective of the tasks which real-world NLP
systems are designed to solve. In this work, we present CRoW, a
manually-curated, multi-task benchmark that evaluates the ability of models to
apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is
constructed using a multi-stage data collection pipeline that rewrites examples
from existing datasets using commonsense-violating perturbations. We use CRoW
to study how NLP systems perform across different dimensions of commonsense
knowledge, such as physical, temporal, and social reasoning. We find a
significant performance gap when NLP systems are evaluated on CRoW compared to
humans, showcasing that commonsense reasoning is far from being solved in
real-world task settings. We make our dataset and leaderboard available to the
research community at https://github.com/mismayil/crow.
","2023-10-25","2310.15239v1.pdf"
"2310.15258","Negar Foroutan","Negar Foroutan, Mohammadreza Banaei, Karl Aberer, Antoine Bosselut","Breaking the Language Barrier: Improving Cross-Lingual Reasoning with
  Structured Self-Attention","EMNLP 2023 - Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In this work, we study whether multilingual language models (MultiLMs) can
transfer logical reasoning abilities to other languages when they are
fine-tuned for reasoning in a different language. We evaluate the cross-lingual
reasoning abilities of MultiLMs in two schemes: (1) where the language of the
context and the question remain the same in the new languages that are tested
(i.e., the reasoning is still monolingual, but the model must transfer the
learned reasoning ability across languages), and (2) where the language of the
context and the question is different (which we term code-switched reasoning).
On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate
that although MultiLMs can transfer reasoning ability across languages in a
monolingual setting, they struggle to transfer reasoning abilities in a
code-switched setting. Following this observation, we propose a novel attention
mechanism that uses a dedicated set of parameters to encourage cross-lingual
attention in code-switched sequences, which improves the reasoning performance
by up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively.
","2023-10-25","2310.15258v1.pdf"
"2310.15264","Soumya Suvra Ghosal Mr.","Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong
  Huang, Dinesh Manocha, Amrit Singh Bedi","Towards Possibilities & Impossibilities of AI-generated Text Detection:
  A Survey","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have revolutionized the domain of natural
language processing (NLP) with remarkable capabilities of generating human-like
text responses. However, despite these advancements, several works in the
existing literature have raised serious concerns about the potential misuse of
LLMs such as spreading misinformation, generating fake news, plagiarism in
academia, and contaminating the web. To address these concerns, a consensus
among the research community is to develop algorithmic solutions to detect
AI-generated text. The basic idea is that whenever we can tell if the given
text is either written by a human or an AI, we can utilize this information to
address the above-mentioned concerns. To that end, a plethora of detection
frameworks have been proposed, highlighting the possibilities of AI-generated
text detection. But in parallel to the development of detection frameworks,
researchers have also concentrated on designing strategies to elude detection,
i.e., focusing on the impossibilities of AI-generated text detection. This is a
crucial step in order to make sure the detection frameworks are robust enough
and it is not too easy to fool a detector. Despite the huge interest and the
flurry of research in this domain, the community currently lacks a
comprehensive analysis of recent developments. In this survey, we aim to
provide a concise categorization and overview of current work encompassing both
the prospects and the limitations of AI-generated text detection. To enrich the
collective knowledge, we engage in an exhaustive discussion on critical and
challenging open questions related to ongoing research on AI-generated text
detection.
","2023-10-25","2310.15264v1.pdf"
"2310.15274","Eren Kurshan","Eren Kurshan","Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI
  Grand Challenges","International Journal on Semantic Computing (2024) Categories:
  Artificial Intelligence; AI; Artificial General Intelligence; AGI; System
  Design; System Architecture","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  AI faces a trifecta of grand challenges the Energy Wall, the Alignment
Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume
unsustainable amounts of energy during model training and daily
operations.Making things worse, the amount of computation required to train
each new AI model has been doubling every 2 months since 2020, directly
translating to increases in energy consumption.The leap from AI to AGI requires
multiple functional subsystems operating in a balanced manner, which requires a
system architecture. However, the current approach to artificial intelligence
lacks system design; even though system characteristics play a key role in the
human brain from the way it processes information to how it makes decisions.
Similarly, current alignment and AI ethics approaches largely ignore system
design, yet studies show that the brains system architecture plays a critical
role in healthy moral decisions.In this paper, we argue that system design is
critically important in overcoming all three grand challenges. We posit that
system design is the missing piece in overcoming the grand challenges.We
present a Systematic AI Approach for AGI that utilizes system design principles
for AGI, while providing ways to overcome the energy wall and the alignment
challenges.
","2023-10-25","2310.15274v1.pdf"
"2310.15288","Rachel Freedman","Rachel Freedman, Justin Svegliato, Kyle Wray, Stuart Russell","Active teacher selection for reinforcement learning from human feedback","","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Reinforcement learning from human feedback (RLHF) enables machine learning
systems to learn objectives from human feedback. A core limitation of these
systems is their assumption that all feedback comes from a single human
teacher, despite querying a range of distinct teachers. We propose the Hidden
Utility Bandit (HUB) framework to model differences in teacher rationality,
expertise, and costliness, formalizing the problem of learning from multiple
teachers. We develop a variety of solution algorithms and apply them to two
real-world domains: paper recommendation systems and COVID-19 vaccine testing.
We find that the Active Teacher Selection (ATS) algorithm outperforms baseline
algorithms by actively selecting when and which teacher to query. The HUB
framework and ATS algorithm demonstrate the importance of leveraging
differences between teachers to learn accurate reward models, facilitating
future research on active teacher selection for robust reward modeling.
","2023-10-25","2310.15288v1.pdf"
"2310.15296","Weijie Xu","Weijie Xu, Wenxiang Hu, Fanyou Wu, Srinivasan Sengamedu","DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based
  LLM","19 pages, 4 figures, EMNLP 2023","EMNLP 2023","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  In the burgeoning field of natural language processing, Neural Topic Models
(NTMs) and Large Language Models (LLMs) have emerged as areas of significant
research interest. Despite this, NTMs primarily utilize contextual embeddings
from LLMs, which are not optimal for clustering or capable for topic
generation. Our study addresses this gap by introducing a novel framework named
Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME).
DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable
embeddings that could generate topics that exhibit both superior clusterability
and enhanced semantic coherence compared to existing methods. Additionally, by
exploiting the power of diffusion, our framework also provides the capability
to generate content relevant to the identified topics. This dual functionality
allows users to efficiently produce highly clustered topics and related content
simultaneously. DeTiME's potential extends to generating clustered embeddings
as well. Notably, our proposed framework proves to be efficient to train and
exhibits high adaptability, demonstrating its potential for a wide array of
applications.
","2023-10-25","2310.15296v1.pdf"
"2310.15298","Ankita Bhaumik","Ankita Bhaumik, Praveen Venkateswaran, Yara Rizk, Vatche Isahagian","TaskDiff: A Similarity Metric for Task-Oriented Conversations","Accepted to the main conference at EMNLP 2023","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  The popularity of conversational digital assistants has resulted in the
availability of large amounts of conversational data which can be utilized for
improved user experience and personalized response generation. Building these
assistants using popular large language models like ChatGPT also require
additional emphasis on prompt engineering and evaluation methods. Textual
similarity metrics are a key ingredient for such analysis and evaluations.
While many similarity metrics have been proposed in the literature, they have
not proven effective for task-oriented conversations as they do not take
advantage of unique conversational features. To address this gap, we present
TaskDiff, a novel conversational similarity metric that utilizes different
dialogue components (utterances, intents, and slots) and their distributions to
compute similarity. Extensive experimental evaluation of TaskDiff on a
benchmark dataset demonstrates its superior performance and improved robustness
over other related approaches.
","2023-10-26","2310.15298v1.pdf"
"2310.15308","Haoxiang Wang","Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja
  Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel
  Tuzel, Hadi Pouransari","SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial
  Understanding","","","","","cs.CV cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that assimilates their
expertise. Our proposed method integrates multi-task learning, continual
learning techniques, and teacher-student distillation. This strategy entails
significantly less computational cost compared to traditional multi-task
training from scratch. Additionally, it only demands a small fraction of the
pre-training datasets that were initially used to train individual models. By
applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that
amalgamates the strengths of SAM and CLIP into a single backbone, making it apt
for edge device applications. We show that SAM-CLIP learns richer visual
representations, equipped with both localization and semantic features,
suitable for a broad range of vision tasks. SAM-CLIP obtains improved
performance on several head probing tasks when compared with SAM and CLIP. We
further show that SAM-CLIP not only retains the foundational strengths of its
precursor models but also introduces synergistic functionalities, most notably
in zero-shot semantic segmentation, where SAM-CLIP establishes new
state-of-the-art results on 5 benchmarks. It outperforms previous models that
are specifically designed for this task by a large margin, including +6.8% and
+5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.
","2023-10-25","2310.15308v1.pdf"
"2310.15317","Ranran Haoran Zhang","Aysa Xuemo Fan, Ranran Haoran Zhang, Luc Paquette, Rui Zhang","Exploring the Potential of Large Language Models in Generating
  Code-Tracing Questions for Introductory Programming Courses","Accepted by Findings of EMNLP, 2023","","","","cs.CL cs.CY","http://creativecommons.org/licenses/by/4.0/","  In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.
","2023-10-25","2310.15317v1.pdf"
"2310.15324","Adeel Yousaf","Adeel Yousaf, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan,
  Mubarak Shah","Videoprompter: an ensemble of foundational models for zero-shot video
  understanding","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Vision-language models (VLMs) classify the query video by calculating a
similarity score between the visual features and text-based class label
representations. Recently, large language models (LLMs) have been used to
enrich the text-based class labels by enhancing the descriptiveness of the
class names. However, these improvements are restricted to the text-based
classifier only, and the query visual features are not considered. In this
paper, we propose a framework which combines pre-trained discriminative VLMs
with pre-trained generative video-to-text and text-to-text models. We introduce
two key modifications to the standard zero-shot setting. First, we propose
language-guided visual feature enhancement and employ a video-to-text model to
convert the query video to its descriptive form. The resulting descriptions
contain vital visual cues of the query video, such as what objects are present
and their spatio-temporal interactions. These descriptive cues provide
additional semantic knowledge to VLMs to enhance their zeroshot performance.
Second, we propose video-specific prompts to LLMs to generate more meaningful
descriptions to enrich class label representations. Specifically, we introduce
prompt techniques to create a Tree Hierarchy of Categories for class names,
offering a higher-level action context for additional visual cues, We
demonstrate the effectiveness of our approach in video understanding across
three different zero-shot settings: 1) video action recognition, 2)
video-to-text and textto-video retrieval, and 3) time-sensitive video tasks.
Consistent improvements across multiple benchmarks and with various VLMs
demonstrate the effectiveness of our proposed framework. Our code will be made
publicly available.
","2023-10-25","2310.15324v1.pdf"
"2310.15326","Yixuan Su","Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, Deng Cai","Specialist or Generalist? Instruction Tuning for Specific NLP Tasks","Accepted to EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The potential of large language models (LLMs) to simultaneously perform a
wide range of natural language processing (NLP) tasks has been the subject of
extensive research. Although instruction tuning has proven to be a
data-efficient method for transforming LLMs into such generalist models, their
performance still lags behind specialist models trained exclusively for
specific tasks. In this paper, we investigate whether incorporating
broad-coverage generalist instruction tuning can contribute to building a
specialist model. We hypothesize that its efficacy depends on task specificity
and skill requirements. Our experiments assess four target tasks with distinct
coverage levels, revealing that integrating generalist instruction tuning
consistently enhances model performance when the task coverage is broad. The
effect is particularly pronounced when the amount of task-specific training
data is limited. Further investigation into three target tasks focusing on
different capabilities demonstrates that generalist instruction tuning improves
understanding and reasoning abilities. However, for tasks requiring factual
knowledge, generalist data containing hallucinatory information may negatively
affect the model's performance. Overall, our work provides a systematic guide
for developing specialist models with general instruction tuning. Our code and
other related resources can be found at
https://github.com/DavidFanzz/Generalist_or_Specialist.
","2023-10-25","2310.15326v1.pdf"
"2310.15329","Zhangzhang Si","Sanjeev V. Namjoshi, Reese Green, Krishi Sharma, Zhangzhang Si","Serverless Federated Learning with flwr-serverless","Technical report for an open source machine learning python package","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Federated learning is becoming increasingly relevant and popular as we
witness a surge in data collection and storage of personally identifiable
information. Alongside these developments there have been many proposals from
governments around the world to provide more protections for individuals' data
and a heightened interest in data privacy measures. As deep learning continues
to become more relevant in new and existing domains, it is vital to develop
strategies like federated learning that can effectively train data from
different sources, such as edge devices, without compromising security and
privacy. Recently, the Flower (\texttt{Flwr}) Python package was introduced to
provide a scalable, flexible, and easy-to-use framework for implementing
federated learning. However, to date, Flower is only able to run synchronous
federated learning which can be costly and time-consuming to run because the
process is bottlenecked by client-side training jobs that are slow or fragile.
Here, we introduce \texttt{flwr-serverless}, a wrapper around the Flower
package that extends its functionality to allow for both synchronous and
asynchronous federated learning with minimal modification to Flower's design
paradigm. Furthermore, our approach to federated learning allows the process to
run without a central server, which increases the domains of application and
accessibility of its use. This paper presents the design details and usage of
this approach through a series of experiments that were conducted using public
datasets. Overall, we believe that our approach decreases the time and cost to
run federated training and provides an easier way to implement and experiment
with federated learning systems.
","2023-10-25","2310.15329v1.pdf"
"2310.15337","Marwa Abdulhai","Marwa Abdulhai, Gregory Serapio-Garcia, Cl\'ement Crepy, Daria Valter,
  John Canny, Natasha Jaques","Moral Foundations of Large Language Models","","","","","cs.AI cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Moral foundations theory (MFT) is a psychological assessment tool that
decomposes human moral reasoning into five factors, including care/harm,
liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary
in the weight they place on these dimensions when making moral decisions, in
part due to their cultural upbringing and political ideology. As large language
models (LLMs) are trained on datasets collected from the internet, they may
reflect the biases that are present in such corpora. This paper uses MFT as a
lens to analyze whether popular LLMs have acquired a bias towards a particular
set of moral values. We analyze known LLMs and find they exhibit particular
moral foundations, and show how these relate to human moral foundations and
political affiliations. We also measure the consistency of these biases, or
whether they vary strongly depending on the context of how the model is
prompted. Finally, we show that we can adversarially select prompts that
encourage the moral to exhibit a particular set of moral foundations, and that
this can affect the model's behavior on downstream tasks. These findings help
illustrate the potential risks and unintended consequences of LLMs assuming a
particular moral stance.
","2023-10-25","2310.15337v1.pdf"
"2310.15355","Adam Bouyamourn","Adam Bouyamourn","Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual,
  Intensional, and Extensional Learning for Faithful Natural Language
  Generation","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We show that LLMs hallucinate because their output is not constrained to be
synonymous with claims for which they have evidence: a condition that we call
evidential closure. Information about the truth or falsity of sentences is not
statistically identified in the standard neural probabilistic language model
setup, and so cannot be conditioned on to generate new strings. We then show
how to constrain LLMs to produce output that does satisfy evidential closure. A
multimodal LLM must learn about the external world (perceptual learning); it
must learn a mapping from strings to states of the world (extensional
learning); and, to achieve fluency when generalizing beyond a body of evidence,
it must learn mappings from strings to their synonyms (intensional learning).
The output of a unimodal LLM must be synonymous with strings in a validated
evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,
that yields faithful output from an LLM by rejecting output that is not
synonymous with claims for which the LLM has evidence.
","2023-10-25","2310.15355v1.pdf"
"2310.15372","Gabriele Prato","Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani,
  Sarath Chandar","EpiK-Eval: Evaluation for Language Models as Epistemic Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the age of artificial intelligence, the role of large language models
(LLMs) is becoming increasingly central. Despite their growing prevalence,
their capacity to consolidate knowledge from different training documents - a
crucial ability in numerous applications - remains unexplored. This paper
presents the first study examining the capability of LLMs to effectively
combine such information within their parameter space. We introduce EpiK-Eval,
a novel question-answering benchmark tailored to evaluate LLMs' proficiency in
formulating a coherent and consistent knowledge representation from segmented
narratives. Evaluations across various LLMs reveal significant weaknesses in
this domain. We contend that these shortcomings stem from the intrinsic nature
of prevailing training objectives. Consequently, we advocate for refining the
approach towards knowledge consolidation, as it harbors the potential to
dramatically improve their overall effectiveness and performance. The findings
from this study offer insights for developing more robust and reliable LLMs.
Our code and benchmark are available at
https://github.com/chandar-lab/EpiK-Eval
","2023-10-25","2310.15372v1.pdf"
"2310.15389","Simin Fan","Simin Fan, Martin Jaggi","Irreducible Curriculum for Language Model Pretraining","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Automatic data selection and curriculum design for training large language
models is challenging, with only a few existing methods showing improvements
over standard training. Furthermore, current schemes focus on domain-level
selection, overlooking the more fine-grained contributions of each individual
training point. It is difficult to apply traditional datapoint selection
methods on large language models: most online batch selection methods perform
two-times forward or backward passes, which introduces considerable extra costs
with large-scale models. To mitigate these obstacles, we propose irreducible
curriculum as a curriculum learning algorithm for language model pretraining,
which prioritizes samples with higher learnability. Specifically, to avoid
prohibitive extra computation overhead, we simulate the sample loss along the
main model's training trajectory using a small-scale proxy model. Our
experiments on the RedPajama-1B dataset demonstrate a consistent improvement on
validation perplexity across all 7 domains compared to random uniform baseline
and the anti-curriculum strategy. Our method also reduces the sharpness of the
network and illustrates a better 5-shot accuracy on MMLU benchmarks.
","2023-10-25","2310.15389v1.pdf"
"2310.15393","Simin Fan","Simin Fan, Matteo Pagliardini, Martin Jaggi","DoGE: Domain Reweighting with Generalization Estimation","","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The coverage and composition of the pretraining data corpus significantly
impacts the generalization ability of large language models. Conventionally,
the pretraining corpus is composed of various source domains (e.g. CommonCrawl,
Wikipedia, Github etc.) according to certain sampling probabilities (domain
weights). However, current methods lack a principled way to optimize domain
weights for ultimate goal for generalization. We propose DOmain reweighting
with Generalization Estimation (DoGE), where we reweigh the sampling
probability from each domain based on its contribution to the final
generalization objective assessed by a gradient-based generalization estimation
function. First, we train a small-scale proxy model with a min-max optimization
to obtain the reweighted domain weights. At each step, the domain weights are
updated to maximize the overall generalization gain by mirror descent. Finally
we use the obtained domain weights to train a larger scale full-size language
model. On SlimPajama-6B dataset, with universal generalization objective, DoGE
achieves better average perplexity and zero-shot reasoning accuracy. On
out-of-domain generalization tasks, DoGE reduces perplexity on the target
domain by a large margin. We further apply a parameter-selection scheme which
improves the efficiency of generalization estimation.
","2023-10-25","2310.15393v1.pdf"
"2310.15405","Ting-Yao Hsu","Ting-Yao Hsu, Chieh-Yang Huang, Ryan Rossi, Sungchul Kim, C. Lee Giles
  and Ting-Hao K. Huang","GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions","To Appear in EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings
","2023-10-25","2310.15405v1.pdf"
"2310.15415","Qiang Zhang","Qiang Zhang, Jason Naradowsky, Yusuke Miyao","Mind the Gap Between Conversations for Improved Long-Term Dialogue
  Generation","Accepted in the Findings of EMNLP 2023","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Knowing how to end and resume conversations over time is a natural part of
communication, allowing for discussions to span weeks, months, or years. The
duration of gaps between conversations dictates which topics are relevant and
which questions to ask, and dialogue systems which do not explicitly model time
may generate responses that are unnatural. In this work we explore the idea of
making dialogue models aware of time, and present GapChat, a multi-session
dialogue dataset in which the time between each session varies. While the
dataset is constructed in real-time, progress on events in speakers' lives is
simulated in order to create realistic dialogues occurring across a long
timespan. We expose time information to the model and compare different
representations of time and event progress. In human evaluation we show that
time-aware models perform better in metrics that judge the relevance of the
chosen topics and the information gained from the conversation.
","2023-10-25","2310.15415v1.pdf"
"2310.15421","Hyunwoo Kim","Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim,
  Yejin Choi, Maarten Sap","FANToM: A Benchmark for Stress-testing Machine Theory of Mind in
  Interactions","EMNLP 2023. Code and dataset can be found here:
  https://hyunw.kim/fantom","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.
","2023-10-26","2310.15421v1.pdf"
"2310.15429","Weihong Qi","Weihong Qi","Beyond Sentiment: Leveraging Topic Metrics for Political Stance
  Classification","","","","","cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Sentiment analysis, widely critiqued for capturing merely the overall tone of
a corpus, falls short in accurately reflecting the latent structures and
political stances within texts. This study introduces topic metrics, dummy
variables converted from extracted topics, as both an alternative and
complement to sentiment metrics in stance classification. By employing three
datasets identified by Bestvater and Monroe (2023), this study demonstrates
BERTopic's proficiency in extracting coherent topics and the effectiveness of
topic metrics in stance classification. The experiment results show that
BERTopic improves coherence scores by 17.07% to 54.20% when compared to
traditional approaches such as Dirichlet Allocation (LDA) and Non-negative
Matrix Factorization (NMF), prevalent in earlier political science research.
Additionally, our results indicate topic metrics outperform sentiment metrics
in stance classification, increasing performance by as much as 18.95%. Our
findings suggest topic metrics are especially effective for context-rich texts
and corpus where stance and sentiment correlations are weak. The combination of
sentiment and topic metrics achieve an optimal performance in most of the
scenarios and can further address the limitations of relying solely on
sentiment as well as the low coherence score of topic metrics.
","2023-10-25","2310.15429v1.pdf"
"2310.15431","Liwei Jiang","Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Niket Tandon,
  Nouha Dziri, Faeze Brahman, Yejin Choi","What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts
  and Rationales for Disambiguating Defeasible Social and Moral Situations","Camera Ready EMNLP Findings 2023. First two authors contributed
  equally","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Moral or ethical judgments rely heavily on the specific contexts in which
they occur. Understanding varying shades of defeasible contextualizations
(i.e., additional information that strengthens or attenuates the moral
acceptability of an action) is critical to accurately represent the subtlety
and intricacy of grounded human moral judgment in real-life scenarios.
  We introduce defeasible moral reasoning: a task to provide grounded contexts
that make an action more or less morally acceptable, along with commonsense
rationales that justify the reasoning. To elicit high-quality task data, we
take an iterative self-distillation approach that starts from a small amount of
unstructured seed knowledge from GPT-3 and then alternates between (1)
self-distillation from student models; (2) targeted filtering with a critic
model trained by human judgment (to boost validity) and NLI (to boost
diversity); (3) self-imitation learning (to amplify the desired data quality).
This process yields a student model that produces defeasible contexts with
improved validity, diversity, and defeasibility. From this model we distill a
high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries of
contextualizations and rationales for 115K defeasible moral actions rated
highly by human annotators 85.9% to 99.8% of the time. Using \delta-RoT we
obtain a final student model that wins over all intermediate student models by
a notable margin.
","2023-10-25","2310.15431v1.pdf"
"2310.15454","Walid Krichene","Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Shuang Song,
  Abhradeep Thakurta, Li Zhang","Private Learning with Public Features","","","","","cs.LG cs.CR stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study a class of private learning problems in which the data is a join of
private and public features. This is often the case in private personalization
tasks such as recommendation or ad prediction, in which features related to
individuals are sensitive, while features related to items (the movies or songs
to be recommended, or the ads to be shown to users) are publicly available and
do not require protection. A natural question is whether private algorithms can
achieve higher utility in the presence of public features. We give a positive
answer for multi-encoder models where one of the encoders operates on public
features. We develop new algorithms that take advantage of this separation by
only protecting certain sufficient statistics (instead of adding noise to the
gradient). This method has a guaranteed utility improvement for linear
regression, and importantly, achieves the state of the art on two standard
private recommendation benchmarks, demonstrating the importance of methods that
adapt to the private-public feature separation.
","2023-10-25","2310.15454v1.pdf"
"2310.15455","Yuwen Lu","Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, Toby Jia-Jun Li","UI Layout Generation with LLMs Guided by UI Grammar","ICML 2023 Workshop on AI and HCI","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  The recent advances in Large Language Models (LLMs) have stimulated interest
among researchers and industry professionals, particularly in their application
to tasks concerning mobile user interfaces (UIs). This position paper
investigates the use of LLMs for UI layout generation. Central to our
exploration is the introduction of UI grammar -- a novel approach we proposed
to represent the hierarchical structure inherent in UI screens. The aim of this
approach is to guide the generative capacities of LLMs more effectively and
improve the explainability and controllability of the process. Initial
experiments conducted with GPT-4 showed the promising capability of LLMs to
produce high-quality user interfaces via in-context learning. Furthermore, our
preliminary comparative study suggested the potential of the grammar-based
approach in improving the quality of generative results in specific aspects.
","2023-10-25","2310.15455v1.pdf"
"2310.15460","Yang Li","Yang Li, Chunhe Xia, Chunyan Li, Yuan Zhao, Chen Chen, Tianbo Wang","HL-DPoS: An Enhanced Anti-Long-Range Attack DPoS Algorithm","","","","","cs.DC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The consensus algorithm is crucial in blockchain for ensuring the validity
and security of transactions across the decentralized network. However,
achieving consensus among nodes and packaging blocks in blockchain networks is
a complex task that requires efficient and secure consensus algorithms. The
DPoS consensus algorithm has emerged as a popular choice due to its fast
transaction processing and high throughput. Despite these advantages, the
algorithm still suffers from weaknesses such as centralization and
vulnerability to long-range attacks, which can compromise the integrity of the
blockchain network.
  To combat these problems, we developed an Enhanced Anti-Long-Range Attack
DPoS algorithm (HL-DPoS). First, we split nodes into pieces to reduce
centralization issues while giving witness nodes the power to report and
benefit from malicious node's reports, maintaining high efficiency and high
security. Second, we propose a validation method in HL-DPoS that compares
consensuses transactions with the longest chain to detect long-range attacks.
Algorithm analysis and simulation experiment results demonstrate that our
HL-DPoS consensus algorithm improves security while achieving better consensus
performance.
","2023-10-25","2310.15460v1.pdf"
"2310.15464","Shivam Mathur","Shivam Mathur, Keun Hee Park, Dhivya Chinnappa, Saketh Kotamraju and
  Eduardo Blanco","Interpreting Answers to Yes-No Questions in User-Generated Content","Accepted at the Findings of EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Interpreting answers to yes-no questions in social media is difficult. Yes
and no keywords are uncommon, and the few answers that include them are rarely
to be interpreted what the keywords suggest. In this paper, we present a new
corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss
linguistic characteristics of answers whose interpretation is yes or no, as
well as answers whose interpretation is unknown. We show that large language
models are far from solving this problem, even after fine-tuning and blending
other corpora for the same problem but outside social media.
","2023-10-25","2310.15464v1.pdf"
"2310.15469","Xiaoyi Chen","Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang,
  Liya Su, XiaoFeng Wang, Haixu Tang","The Janus Interface: How Fine-Tuning in Large Language Models Amplifies
  the Privacy Risks","","","","","cs.CR cs.CL","http://creativecommons.org/licenses/by/4.0/","  The era post-2018 marked the advent of Large Language Models (LLMs), with
innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.
As the industry galloped toward augmenting model parameters and capitalizing on
vast swaths of human language data, security and privacy challenges also
emerged. Foremost among these is the potential inadvertent accrual of Personal
Identifiable Information (PII) during web-based data acquisition, posing risks
of unintended PII disclosure. While strategies like RLHF during training and
Catastrophic Forgetting have been marshaled to control the risk of privacy
infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning
interface for GPT-3.5, have reignited concerns. One may ask: can the
fine-tuning of LLMs precipitate the leakage of personal information embedded
within training datasets? This paper reports the first endeavor to seek the
answer to the question, particularly our discovery of a new LLM exploitation
avenue, called the Janus attack. In the attack, one can construct a PII
association task, whereby an LLM is fine-tuned using a minuscule PII dataset,
to potentially reinstate and reveal concealed PIIs. Our findings indicate that,
with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from
being impermeable to PII extraction to a state where they divulge a substantial
proportion of concealed PII. This research, through its deep dive into the
Janus attack vector, underscores the imperative of navigating the intricate
interplay between LLM utility and privacy preservation.
","2023-10-25","2310.15469v1.pdf"
"2310.15470","Wei Hu","Zitao Wang and Xinyi Wang and Wei Hu","Continual Event Extraction with Semantic Confusion Rectification","Accepted in the 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2023)","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  We study continual event extraction, which aims to extract incessantly
emerging event information while avoiding forgetting. We observe that the
semantic confusion on event types stems from the annotations of the same text
being updated over time. The imbalance between event types even aggravates this
issue. This paper proposes a novel continual event extraction model with
semantic confusion rectification. We mark pseudo labels for each sentence to
alleviate semantic confusion. We transfer pivotal knowledge between current and
previous models to enhance the understanding of event types. Moreover, we
encourage the model to focus on the semantics of long-tailed event types by
leveraging other associated types. Experimental results show that our model
outperforms state-of-the-art baselines and is proficient in imbalanced
datasets.
","2023-10-25","2310.15470v1.pdf"
"2310.15477","Kaiyan Zhang","Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen
  Zhou","CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without
  Full Large Language Model","Accepted to EMNLP 2023 (Main Conference)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Instruction tuning has recently been recognized as an effective way of
aligning Large Language Models (LLMs) to enhance their generalization ability
across various tasks. However, when tuning publicly accessible, centralized
LLMs with private instruction data, privacy concerns are inevitable. While
direct transfer of parameterized modules between models is a plausible approach
to address this, its implications and effectiveness need further exploration.
This paper focuses on Offsite-Tuning (OFT), a representative technique that
transfers transformer blocks between centralized LLMs and downstream emulators.
Given the limited understanding of the underlying mechanism of OFT, we perform
an empirical analysis on LLMs from the perspectives of representation and
functional similarity. Interestingly, our findings reveal a unique modular
structure within the layers of LLMs that appears to emerge as the model size
expands. Simultaneously, we note subtle but potentially significant changes in
representation and intermediate predictions across the layers. Inspired by
these observations, we propose CRaSh, involving Clustering, Removing, and
Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh
significantly boosts performance of OFT with billions of parameters.
Furthermore, we investigate the optimal solutions yielded by fine-tuning with
and without full model through the lens of loss landscape. Our findings
demonstrate a linear connectivity among these optima falling over the same
basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code
is publicly available at https://github.com/TsinghuaC3I/CRaSh.
","2023-10-25","2310.15477v1.pdf"
"2310.15511","Besmira Nushi","Marah I Abdin, Suriya Gunasekar, Varun Chandrasekaran, Jerry Li, Mert
  Yuksekgonul, Rahee Ghosh Peshawaria, Ranjita Naik, Besmira Nushi","KITAB: Evaluating LLMs on Constraint Satisfaction for Information
  Retrieval","23 pages","","","","cs.LG cs.AI cs.CL cs.IR","http://creativecommons.org/licenses/by/4.0/","  We study the ability of state-of-the art models to answer constraint
satisfaction queries for information retrieval (e.g., 'a list of ice cream
shops in San Diego'). In the past, such queries were considered to be tasks
that could only be solved via web-search or knowledge bases. More recently,
large language models (LLMs) have demonstrated initial emergent abilities in
this task. However, many current retrieval benchmarks are either saturated or
do not measure constraint satisfaction. Motivated by rising concerns around
factual incorrectness and hallucinations of LLMs, we present KITAB, a new
dataset for measuring constraint satisfaction abilities of language models.
KITAB consists of book-related data across more than 600 authors and 13,000
queries, and also offers an associated dynamic data collection and constraint
verification approach for acquiring similar test data for other authors. Our
extended experiments on GPT4 and GPT3.5 characterize and decouple common
failure modes across dimensions such as information popularity, constraint
types, and context availability. Results show that in the absence of context,
models exhibit severe limitations as measured by irrelevant information,
factual errors, and incompleteness, many of which exacerbate as information
popularity decreases. While context availability mitigates irrelevant
information, it is not helpful for satisfying constraints, identifying
fundamental barriers to constraint satisfaction. We open source our
contributions to foster further research on improving constraint satisfaction
abilities of future models.
","2023-10-25","2310.15511v1.pdf"
"2310.15515","Michiharu Yamashita","Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya
  Rohatgi, Dongwon Lee","Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting
  Elusive Disinformation","Accepted at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel ""Fighting Fire with Fire"" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.
","2023-10-25","2310.15515v1.pdf"
"2310.15517","Sitao Cheng","Xiang Huang, Sitao Cheng, Yuheng Bao, Shanshan Huang, Yuzhong Qu","MarkQA: A large scale KBQA dataset with numerical reasoning","camera ready for EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While question answering over knowledge bases (KBQA) has shown progress in
addressing factoid questions, KBQA with numerical reasoning remains relatively
unexplored. In this paper, we focus on the complex numerical reasoning in KBQA
and propose a new task, NR-KBQA, which necessitates the ability to perform both
multi-hop reasoning and numerical reasoning. We design a logic form in Python
format called PyQL to represent the reasoning process of numerical reasoning
questions. To facilitate the development of NR-KBQA, we present a large dataset
called MarkQA, which is automatically constructed from a small set of seeds.
Each question in MarkQA is equipped with its corresponding SPARQL query,
alongside the step-by-step reasoning process in the QDMR format and PyQL
program. Experimental results of some state-of-the-art QA methods on the MarkQA
show that complex numerical reasoning in KBQA faces great challenges.
","2023-10-25","2310.15517v1.pdf"
"2310.15539","Jialing Pan","Jialing Pan, Adrien Sad\'e, Jin Kim, Eric Soriano, Guillem Sole,
  Sylvain Flamant","SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code
  Translation","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  With the recent focus on Large Language Models (LLMs), both StarCoder (Li et
al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable
performance in code generation. However, there is still a need for improvement
in code translation functionality with efficient training techniques. In
response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM
designed specifically for multi-programming language-to-Python code
translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or
PHP-to-Python code translation without specifying the input programming
language. We modified StarCoder model architecture by incorporating a
Mixture-of-Experts (MoE) technique featuring five experts and a gating network
for multi-task handling. Experts are obtained by StarCoder fine-tuning.
Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each
expert size as only 0.06% of number of StarCoder's parameters. At the same
time, to enhance training efficiency in terms of time, we adopt curriculum
learning strategy and use self-instruct data for efficient fine-tuning. As a
result, each expert takes only 6 hours to train on one single 80Gb A100 HBM.
With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76
CodeBLEU score in multi-programming language-to-Python translation, surpassing
the top performance from the leaderboard by at least 3.5. This accomplishment
is attributed to only 45M extra parameters with StarCoder as the backbone and
32 hours of valid training on one 80GB A100 HBM. The source code is release
here: https://github.com/sade-adrien/SteloCoder.
","2023-10-25","2310.15539v1.pdf"
"2310.15541","Myeongjun Erik Jang","Myeongjun Erik Jang, Thomas Lukasiewicz","Improving Language Models Meaning Understanding and Consistency by
  Learning Conceptual Roles from Dictionary","15 pages","The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The non-humanlike behaviour of contemporary pre-trained language models
(PLMs) is a leading cause undermining their trustworthiness. A striking
phenomenon of such faulty behaviours is the generation of inconsistent
predictions, which produces logically contradictory results, such as generating
different predictions for texts delivering the same meaning or violating
logical properties. Previous studies exploited data augmentation or implemented
specialised loss functions to alleviate the issue. However, their usage is
limited, because they consume expensive training resources for large-sized PLMs
and can only handle a certain consistency type. To this end, we propose a
practical approach that alleviates the inconsistent behaviour issue by
fundamentally improving PLMs' meaning awareness. Based on the conceptual role
theory, our method allows PLMs to capture accurate meaning by learning precise
interrelationships between concepts from word-definition pairs in a dictionary.
Next, we propose an efficient parameter integration technique that updates only
a few additional parameters to combine the learned interrelationship with PLMs'
pre-trained knowledge. Our experimental results reveal that the approach can
concurrently improve multiple types of consistency, enables efficient knowledge
integration, and easily applies to other languages.
","2023-10-25","2310.15541v1.pdf"
"2310.15556","Tong Xiang","Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian","TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for
  Inference Cost Reduction","EMNLP 2023 Findings","","","","cs.CL cs.IR","http://creativecommons.org/licenses/by/4.0/","  Since ChatGPT released its API for public use, the number of applications
built on top of commercial large language models (LLMs) increase exponentially.
One popular usage of such models is leveraging its in-context learning ability
and generating responses given user queries leveraging knowledge obtained by
retrieval augmentation. One problem of deploying commercial retrieval-augmented
LLMs is the cost due to the additionally retrieved context that largely
increases the input token size of the LLMs. To mitigate this, we propose a
token compression scheme that includes two methods: summarization compression
and semantic compression. The first method applies a T5-based model that is
fine-tuned by datasets generated using self-instruct containing samples with
varying lengths and reduce token size by doing summarization. The second method
further compresses the token size by removing words with lower impact on the
semantic. In order to adequately evaluate the effectiveness of the proposed
methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)
focusing on food recommendation for women around pregnancy period or infants.
Our summarization compression can reduce 65% of the retrieval token size with
further 0.3% improvement on the accuracy; semantic compression provides a more
flexible way to trade-off the token size with performance, for which we can
reduce the token size by 20% with only 1.6% of accuracy drop.
","2023-10-26","2310.15556v1.pdf"
"2310.15575","Chenkai Ma","Chenkai Ma, Xinya Du","POE: Process of Elimination for Multiple Choice Reasoning","Accepted as a short paper at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.
","2023-10-25","2310.15575v1.pdf"
"2310.15594","Jiduan Liu","Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai,
  Dongyan Zhao, Ran Lucien Wang, Rui Yan","Retrieval-based Knowledge Transfer: An Effective Approach for Extreme
  Large Language Model Compression","EMNLP 2023 Findings","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large-scale pre-trained language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, the
massive size of these models poses huge challenges for their deployment in
real-world applications. While numerous model compression techniques have been
proposed, most of them are not well-suited for achieving extreme model
compression when there is a significant gap in model scale. In this paper, we
introduce a novel compression paradigm called Retrieval-based Knowledge
Transfer (RetriKT), which effectively transfers the knowledge of LLMs to
extremely small-scale models (e.g., 1%). In particular, our approach extracts
knowledge from LLMs to construct a knowledge store, from which the small-scale
model can retrieve relevant information and leverage it for effective
inference. To improve the quality of the model, soft prompt tuning and Proximal
Policy Optimization (PPO) reinforcement learning techniques are employed.
Extensive experiments are conducted on low-resource tasks from SuperGLUE and
GLUE benchmarks. The results demonstrate that the proposed approach
significantly enhances the performance of small-scale models by leveraging the
knowledge from LLMs.
","2023-10-25","2310.15594v1.pdf"
"2310.15638","Minzhi Li","Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F. Chen,
  Zhengyuan Liu, Diyi Yang","CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large
  Language Models for Data Annotation","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.
","2023-10-25","2310.15638v1.pdf"
"2310.15654","Xianjun Yang","Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda
  Petzold, William Yang Wang, Wei Cheng","A Survey on Detection of LLMs-Generated Content","We will keep updating at
  https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git","","","20 pages","cs.CL cs.AI cs.CY cs.HC cs.LG","http://creativecommons.org/licenses/by/4.0/","  The burgeoning capabilities of advanced large language models (LLMs) such as
ChatGPT have led to an increase in synthetic content generation with
implications across a variety of sectors, including media, cybersecurity,
public discourse, and education. As such, the ability to detect LLMs-generated
content has become of paramount importance. We aim to provide a detailed
overview of existing detection strategies and benchmarks, scrutinizing their
differences and identifying key challenges and prospects in the field,
advocating for more adaptable and robust models to enhance detection accuracy.
We also posit the necessity for a multi-faceted approach to defend against
various attacks to counter the rapidly advancing capabilities of LLMs. To the
best of our knowledge, this work is the first comprehensive survey on the
detection in the era of LLMs. We hope it will provide a broad understanding of
the current landscape of LLMs-generated content detection, offering a guiding
reference for researchers and practitioners striving to uphold the integrity of
digital information in an era increasingly dominated by synthetic content. The
relevant papers are summarized and will be consistently updated at
https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.
","2023-10-25","2310.15654v1.pdf"
"2310.15674","Philipp Eller","Habib Bukhari, Dipam Chakraborty, Philipp Eller, Takuya Ito, Maxim V.
  Shugaev, Rasmus {\O}rs{\o}e","IceCube -- Neutrinos in Deep Ice The Top 3 Solutions from the Public
  Kaggle Competition","","","","","astro-ph.HE hep-ex physics.data-an","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  During the public Kaggle competition ""IceCube -- Neutrinos in Deep Ice"",
thousands of reconstruction algorithms were created and submitted, aiming to
estimate the direction of neutrino events recorded by the IceCube detector.
Here we describe in detail the three ultimate best, award-winning solutions.
The data handling, architecture, and training process of each of these machine
learning models is laid out, followed up by an in-depth comparison of the
performance on the kaggle datatset. We show that on cascade events in IceCube
above 10 TeV, the best kaggle solution is able to achieve an angular resolution
of better than 5 degrees, and for tracks correspondingly better than 0.5
degrees. These performance measures compare favourably to the current
state-of-the-art in the field.
","2023-10-25","2310.15674v1.pdf"
"2310.15683","Veniamin Veselovsky","Veniamin Veselovsky, Manoel Horta Ribeiro, Philip Cozzolino, Andrew
  Gordon, David Rothschild, Robert West","Prevalence and prevention of large language model use in crowd work","VV and MHR equal contribution. 14 pages, 1 figure, 1 table","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that the use of large language models (LLMs) is prevalent among crowd
workers, and that targeted mitigation strategies can significantly reduce, but
not eliminate, LLM use. On a text summarization task where workers were not
directed in any way regarding their LLM use, the estimated prevalence of LLM
use was around 30%, but was reduced by about half by asking workers to not use
LLMs and by raising the cost of using them, e.g., by disabling copy-pasting.
Secondary analyses give further insight into LLM use and its prevention: LLM
use yields high-quality but homogeneous responses, which may harm research
concerned with human (rather than model) behavior and degrade future models
trained with crowdsourced data. At the same time, preventing LLM use may be at
odds with obtaining high-quality responses; e.g., when requesting workers not
to use LLMs, summaries contained fewer keywords carrying essential information.
Our estimates will likely change as LLMs increase in popularity or
capabilities, and as norms around their usage change. Yet, understanding the
co-evolution of LLM-based tools and users is key to maintaining the validity of
research done using crowdsourcing, and we provide a critical baseline before
widespread adoption ensues.
","2023-10-25","2310.15683v1.pdf"
"2310.15693","Nazmus Sakib","Nazmus Sakib, G. M. Shahariar, Md. Mohsinul Kabir, Md. Kamrul Hasan
  and Hasan Mahmud","Towards Automated Recipe Genre Classification using Semi-Supervised
  Learning","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Sharing cooking recipes is a great way to exchange culinary ideas and provide
instructions for food preparation. However, categorizing raw recipes found
online into appropriate food genres can be challenging due to a lack of
adequate labeled data. In this study, we present a dataset named the
``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking
Recipe Dataset"" that contains two million culinary recipes labeled in
respective categories with extended named entities extracted from recipe
descriptions. This collection of data includes various features such as title,
NER, directions, and extended NER, as well as nine different labels
representing genres including bakery, drinks, non-veg, vegetables, fast food,
cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends
the size of the Named Entity Recognition (NER) list to address missing named
entities like heat, time or process from the recipe directions using two NER
extraction tools. 3A2M+ dataset provides a comprehensive solution to the
various challenging recipe-related tasks, including classification, named
entity recognition, and recipe generation. Furthermore, we have demonstrated
traditional machine learning, deep learning and pre-trained language models to
classify the recipes into their corresponding genre and achieved an overall
accuracy of 98.6\%. Our investigation indicates that the title feature played a
more significant role in classifying the genre.
","2023-10-25","2310.15693v1.pdf"
"2310.15694","Zhang Han","Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu","COPF: Continual Learning Human Preference through Optimal Policy Fitting","","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Fitting (COPF), in which we estimate a series of optimal policies using
the Monte Carlo method, and then continually fit the policy sequence with the
function regularization. COPF involves a single learning phase and doesn't
necessitate complex reinforcement learning. Importantly, it shares the
capability with RLHF to learn from unlabeled data, making it flexible for
continual preference learning. Our experimental results show that COPF
outperforms strong Continuous learning (CL) baselines when it comes to
consistently aligning with human preferences on different tasks and domains.
","2023-10-27","2310.15694v1.pdf"
"2310.15746","Zeyuan Yang","Zeyuan Yang, Peng Li, Yang Liu","Failures Pave the Way: Enhancing Large Language Models through
  Tuning-free Rule Accumulation","This paper is accepted by the EMNLP 2023 Main Conference","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have showcased impressive performance. However,
due to their inability to capture relationships among samples, these frozen
LLMs inevitably keep repeating similar mistakes. In this work, we propose our
Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving
their performance by learning from previous mistakes. Considering data arrives
sequentially, LLMs gradually accumulate rules from incorrect cases, forming a
rule collection. These rules are then utilized by the LLMs to avoid making
similar mistakes when processing subsequent inputs. Moreover, the rules remain
independent of the primary prompts, seamlessly complementing prompt design
strategies. Experimentally, we show that TRAN improves over recent baselines by
a large margin.
","2023-10-25","2310.15746v1.pdf"
"2310.15747","Dohwan Ko","Dohwan Ko, Ji Soo Lee, Wooyoung Kang, Byungseok Roh, Hyunwoo J. Kim","Large Language Models are Temporal and Causal Reasoners for Video
  Question Answering","Accepted paper at EMNLP 2023 Main","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) have shown remarkable performances on a wide
range of natural language understanding and generation tasks. We observe that
the LLMs provide effective priors in exploiting $\textit{linguistic shortcuts}$
for temporal and causal reasoning in Video Question Answering (VideoQA).
However, such priors often cause suboptimal results on VideoQA by leading the
model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$,
while ignoring visual content. This is also known as `ungrounded guesses' or
`hallucinations'. To address this problem while leveraging LLMs' prior on
VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to
predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping
the source pair and the target label to understand their complex relationships,
$\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs,
respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to
LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five
challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general
framework that is applicable to various LLMs (OPT and GPT-J) and consistently
improves their performances. We empirically demonstrate that Flipped-VQA not
only enhances the exploitation of linguistic shortcuts but also mitigates the
linguistic bias, which causes incorrect answers over-relying on the question.
Code is available at https://github.com/mlvlab/Flipped-VQA.
","2023-10-25","2310.15747v1.pdf"
"2310.15773","Alison Chi","Tannon Kew, Alison Chi, Laura V\'asquez-Rodr\'iguez, Sweta Agrawal,
  Dennis Aumiller, Fernando Alva-Manchego, Matthew Shardlow","BLESS: Benchmarking Large Language Models on Sentence Simplification","This paper has been accepted to EMNLP 2023 as a main long paper. 9
  pages, 7 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present BLESS, a comprehensive performance benchmark of the most recent
state-of-the-art large language models (LLMs) on the task of text
simplification (TS). We examine how well off-the-shelf LLMs can solve this
challenging task, assessing a total of 44 models, differing in size,
architecture, pre-training methods, and accessibility, on three test sets from
different domains (Wikipedia, news, and medical) under a few-shot setting. Our
analysis considers a suite of automatic metrics as well as a large-scale
quantitative investigation into the types of common edit operations performed
by the different models. Furthermore, we perform a manual qualitative analysis
on a subset of model outputs to better gauge the quality of the generated
simplifications. Our evaluation indicates that the best LLMs, despite not being
trained on TS, perform comparably with state-of-the-art TS baselines.
Additionally, we find that certain LLMs demonstrate a greater range and
diversity of edit operations. Our performance benchmark will be available as a
resource for the development of future TS methods and evaluation metrics.
","2023-10-25","2310.15773v1.pdf"
"2310.15777","Yang Gao","Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang
  Liu, Heyan Huang, Yang Gao","MindLLM: Pre-training Lightweight Large Language Model from Scratch,
  Evaluations and Domain Applications","Working in progress","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language tasks, marking significant strides towards general
artificial intelligence. While general artificial intelligence is leveraged by
developing increasingly large-scale models, there could be another branch to
develop lightweight custom models that better serve certain domains, taking
into account the high cost of training and deploying LLMs and the scarcity of
resources. In this paper, we present MindLLM, a novel series of bilingual
lightweight large language models, trained from scratch, alleviating such
burdens by offering models with 1.3 billion and 3 billion parameters. A
thorough account of experiences accrued during large model development is
given, covering every step of the process, including data construction, model
architecture, evaluation, and applications. Such insights are hopefully
valuable for fellow academics and developers. MindLLM consistently matches or
surpasses the performance of other open-source larger models on some public
benchmarks. We also introduce an innovative instruction tuning framework
tailored for smaller models to enhance their capabilities efficiently.
Moreover, we explore the application of MindLLM in specific vertical domains
such as law and finance, underscoring the agility and adaptability of our
lightweight models.
","2023-10-25","2310.15777v1.pdf"
"2310.15793","Louis Falissard","Louis Falissard, Vincent Guigue, Laure Soulier","Improving generalization in large language models by learning prefix
  subspaces","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  This article focuses on large language models (LLMs) fine-tuning in the
scarce data regime (also known as the ""few-shot"" learning setting). We propose
a method to increase the generalization capabilities of LLMs based on neural
network subspaces. This optimization method, recently introduced in computer
vision, aims to improve model generalization by identifying wider local optima
through the joint optimization of an entire simplex of models in parameter
space. Its adaptation to massive, pretrained transformers, however, poses some
challenges. First, their considerable number of parameters makes it difficult
to train several models jointly, and second, their deterministic parameter
initialization schemes make them unfit for the subspace method as originally
proposed. We show in this paper that ""Parameter Efficient Fine-Tuning"" (PEFT)
methods, however, are perfectly compatible with this original approach, and
propose to learn entire simplex of continuous prefixes. We test our method on a
variant of the GLUE benchmark adapted to the few-shot learning setting, and
show that both our contributions jointly lead to a gain in average performances
compared to sota methods. The implementation can be found at the following
link: https://github.com/Liloulou/prefix_subspace
","2023-10-25","2310.15793v1.pdf"
"2310.15799","Sreyan Ghosh","Sreyan Ghosh, Chandra Kiran Evuru, Sonal Kumar, S Ramaneswaran, S
  Sakshi, Utkarsh Tyagi, Dinesh Manocha","DALE: Generative Data Augmentation for Low-Resource Legal NLP","Accepted to EMNLP 2023 Main Conference. Code:
  https://github.com/Sreyan88/DALE","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  We present DALE, a novel and effective generative Data Augmentation framework
for low-resource LEgal NLP. DALE addresses the challenges existing frameworks
pose in generating effective data augmentations of legal documents - legal
language, with its specialized vocabulary and complex semantics, morphology,
and syntax, does not benefit from data augmentations that merely rephrase the
source sentence. To address this, DALE, built on an Encoder-Decoder Language
Model, is pre-trained on a novel unsupervised text denoising objective based on
selective masking - our masking strategy exploits the domain-specific language
characteristics of templatized legal documents to mask collocated spans of
text. Denoising these spans helps DALE acquire knowledge about legal concepts,
principles, and language usage. Consequently, it develops the ability to
generate coherent and diverse augmentations with novel contexts. Finally, DALE
performs conditional generation to generate synthetic augmentations for
low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13
datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our
baselines, including LLMs, qualitatively and quantitatively, with improvements
of 1%-50%.
","2023-10-25","2310.15799v1.pdf"
"2310.15819","Tiancheng Hu","Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander van
  der Linden, Jon Roozenbeek","Generative Language Models Exhibit Social Identity Biases","supplementary material, data, and code see
  https://osf.io/9ht32/?view_only=f0ab4b23325f4c31ad3e12a7353b55f5","","","","cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. In this study, we
investigate whether ingroup solidarity and outgroup hostility, fundamental
social biases known from social science, are present in 51 large language
models. We find that almost all foundational language models and some
instruction fine-tuned models exhibit clear ingroup-positive and
outgroup-negative biases when prompted to complete sentences (e.g., ""We
are...""). A comparison of LLM-generated sentences with human-written sentences
on the internet reveals that these models exhibit similar level, if not
greater, levels of bias than human text. To investigate where these biases stem
from, we experimentally varied the amount of ingroup-positive or
outgroup-negative sentences the model was exposed to during fine-tuning in the
context of the United States Democrat-Republican divide. Doing so resulted in
the models exhibiting a marked increase in ingroup solidarity and an even
greater increase in outgroup hostility. Furthermore, removing either
ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning
data leads to a significant reduction in both ingroup solidarity and outgroup
hostility, suggesting that biases can be reduced by removing biased training
data. Our findings suggest that modern language models exhibit fundamental
social identity biases and that such biases can be mitigated by curating
training data. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.
","2023-10-25","2310.15819v1.pdf"
"2310.15851","Zezhong Wang Mr.","Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen,
  Qingwei Lin, Kam-Fai Wong","Self-Guard: Empower the LLM to Safeguard Itself","","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  The jailbreak attack can bypass the safety measures of a Large Language Model
(LLM), generating harmful content. This misuse of LLM has led to negative
societal consequences. Currently, there are two main approaches to address
jailbreak attacks: safety training and safeguards. Safety training focuses on
further training LLM to enhance its safety. On the other hand, safeguards
involve implementing external models or filters to prevent harmful outputs.
However, safety training has constraints in its ability to adapt to new attack
types and often leads to a drop in model performance. Safeguards have proven to
be of limited help. To tackle these issues, we propose a novel approach called
Self-Guard, which combines the strengths of both safety methods. Self-Guard
includes two stages. In the first stage, we enhance the model's ability to
assess harmful content, and in the second stage, we instruct the model to
consistently perform harmful content detection on its own responses. The
experiment has demonstrated that Self-Guard is robust against jailbreak
attacks. In the bad case analysis, we find that LLM occasionally provides
harmless responses to harmful queries. Additionally, we evaluated the general
capabilities of the LLM before and after safety training, providing evidence
that Self-Guard does not result in the LLM's performance degradation. In
sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM
but also can even mitigate this issue.
","2023-10-25","2310.15851v1.pdf"
"2310.15852","Guillaume Wisniewski Dr.","Lina Conti and Guillaume Wisniewski","Using Artificial French Data to Understand the Emergence of Gender Bias
  in Transformer Language Models","Accepted at EMNLP'23","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Numerous studies have demonstrated the ability of neural language models to
learn various linguistic properties without direct supervision. This work takes
an initial step towards exploring the less researched topic of how neural
models discover linguistic properties of words, such as gender, as well as the
rules governing their usage. We propose to use an artificial corpus generated
by a PCFG based on French to precisely control the gender distribution in the
training data and determine under which conditions a model correctly captures
gender information or, on the contrary, appears gender-biased.
","2023-10-25","2310.15852v1.pdf"
"2310.15880","Daniil Merkulov","Daniil Merkulov, Ivan Oseledets","Another approach to build Lyapunov functions for the first order methods
  in the quadratic case","","","","","math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Lyapunov functions play a fundamental role in analyzing the stability and
convergence properties of optimization methods. In this paper, we propose a
novel and straightforward approach for constructing Lyapunov functions for
first-order methods applied to quadratic functions. Our approach involves
bringing the iteration matrix to an upper triangular form using Schur
decomposition, then examining the value of the last coordinate of the state
vector. This value is multiplied by a magnitude smaller than one at each
iteration. Consequently, this value should decrease at each iteration, provided
that the method converges. We rigorously prove the suitability of this Lyapunov
function for all first-order methods and derive the necessary conditions for
the proposed function to decrease monotonically. Experiments conducted with
general convex functions are also presented, alongside a study on the
limitations of the proposed approach.
  Remarkably, the newly discovered Lyapunov function is straightforward and
does not explicitly depend on the exact method formulation or function
characteristics like strong convexity or smoothness constants. In essence, a
single expression serves as a Lyapunov function for several methods, including
Heavy Ball, Nesterov Accelerated Gradient, and Triple Momentum, among others.
To the best of our knowledge, this approach has not been previously reported in
the literature.
","2023-10-25","2310.15880v1.pdf"
"2310.15896","Yirong Chen PhD","Yirong Chen, Zhenyu Wang, Xiaofen Xing, huimin zheng, Zhipei Xu, Kai
  Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu","BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs
  with Multi-turn Health Conversations Polished by ChatGPT","","","","","cs.CL cs.HC","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Large language models (LLMs) have performed well in providing general and
extensive health suggestions in single-turn conversations, exemplified by
systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the
limited information provided by users during single turn results in inadequate
personalization and targeting of the generated suggestions, which requires
users to independently select the useful part. It is mainly caused by the
missing ability to engage in multi-turn questioning. In real-world medical
consultations, doctors usually employ a series of iterative inquiries to
comprehend the patient's condition thoroughly, enabling them to provide
effective and personalized suggestions subsequently, which can be defined as
chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose
BianQue, a ChatGLM-based LLM finetuned with the self-constructed health
conversation dataset BianQueCorpus that is consist of multiple turns of
questioning and health suggestions polished by ChatGPT. Experimental results
demonstrate that the proposed BianQue can simultaneously balance the
capabilities of both questioning and health suggestions, which will help
promote the research and application of LLMs in the field of proactive health.
","2023-10-25","2310.15896v1.pdf"
"2310.15903","Pengyu Li","Pengyu Li, Yutong Wang, Xiao Li, Qing Qu","Neural Collapse in Multi-label Learning with Pick-all-label Loss","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study deep neural networks for the multi-label classification (MLab) task
through the lens of neural collapse (NC). Previous works have been restricted
to the multi-class classification setting and discovered a prevalent NC
phenomenon comprising of the following properties for the last-layer features:
(i) the variability of features within every class collapses to zero, (ii) the
set of feature means form an equi-angular tight frame (ETF), and (iii) the last
layer classifiers collapse to the feature mean upon some scaling. We generalize
the study to multi-label learning, and prove for the first time that a
generalized NC phenomenon holds with the ""pick-all-label'' formulation. Under
the natural analog of the unconstrained feature model (UFM), we establish that
the only global classifier of the pick-all-label cross entropy loss display the
same ETF geometry which further collapse to multiplicity-1 feature class means.
Besides, we discover a combinatorial property in generalized NC which is unique
for multi-label learning that we call ``tag-wise average'' property, where the
feature class-means of samples with multiple labels are scaled average of the
feature class-means of single label tags. Theoretically, we establish global
optimality result for the pick-all-label cross-entropy risk for the UFM.
Additionally, We also provide empirical evidence to support our investigation
into training deep neural networks on multi-label datasets, resulting in
improved training efficiency.
","2023-10-25","2310.15903v1.pdf"
"2310.15904","Alan Cowap","Alan Cowap, Yvette Graham, Jennifer Foster","Do Stochastic Parrots have Feelings Too? Improving Neural Detection of
  Synthetic Text via Emotion Recognition","Accepted to Findings of EMNLP 2023 (long paper). Camera ready version","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Recent developments in generative AI have shone a spotlight on
high-performance synthetic text generation technologies. The now wide
availability and ease of use of such models highlights the urgent need to
provide equally powerful technologies capable of identifying synthetic text.
With this in mind, we draw inspiration from psychological studies which suggest
that people can be driven by emotion and encode emotion in the text they
compose. We hypothesize that pretrained language models (PLMs) have an
affective deficit because they lack such an emotional driver when generating
text and consequently may generate synthetic text which has affective
incoherence i.e. lacking the kind of emotional coherence present in
human-authored text. We subsequently develop an emotionally aware detector by
fine-tuning a PLM on emotion. Experiment results indicate that our
emotionally-aware detector achieves improvements across a range of synthetic
text generators, various sized models, datasets, and domains. Finally, we
compare our emotionally-aware synthetic text detector to ChatGPT in the task of
identification of its own output and show substantial gains, reinforcing the
potential of emotion as a signal to identify synthetic text. Code, models, and
datasets are available at https: //github.com/alanagiasi/emoPLMsynth
","2023-10-25","2310.15904v1.pdf"
"2310.15910","Qinan Yu","Qinan Yu, Jack Merullo, Ellie Pavlick","Characterizing Mechanisms for Factual Recall in Language Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Language Models (LMs) often must integrate facts they memorized in
pretraining with new information that appears in a given context. These two
sources can disagree, causing competition within the model, and it is unclear
how an LM will resolve the conflict. On a dataset that queries for knowledge of
world capitals, we investigate both distributional and mechanistic determinants
of LM behavior in such situations. Specifically, we measure the proportion of
the time an LM will use a counterfactual prefix (e.g., ""The capital of Poland
is London"") to overwrite what it learned in pretraining (""Warsaw""). On Pythia
and GPT2, the training frequency of both the query country (""Poland"") and the
in-context city (""London"") highly affect the models' likelihood of using the
counterfactual. We then use head attribution to identify individual attention
heads that either promote the memorized answer or the in-context answer in the
logits. By scaling up or down the value vector of these heads, we can control
the likelihood of using the in-context answer on new data. This method can
increase the rate of generating the in-context answer to 88\% of the time
simply by scaling a single head at runtime. Our work contributes to a body of
evidence showing that we can often localize model behaviors to specific
components and provides a proof of concept for how future methods might control
model behavior dynamically at runtime.
","2023-10-25","2310.15910v1.pdf"
"2310.15941","Iker Garc\'ia-Ferrero","Iker Garc\'ia-Ferrero, Bego\~na Altuna, Javier \'Alvez, Itziar
  Gonzalez-Dios, German Rigau","This is not a Dataset: A Large Negation Benchmark to Challenge Large
  Language Models","Accepted in the The 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2023)","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Although large language models (LLMs) have apparently acquired a certain
level of grammatical knowledge and the ability to make generalizations, they
fail to interpret negation, a crucial step in Natural Language Processing. We
try to clarify the reasons for the sub-optimal performance of LLMs
understanding negation. We introduce a large semi-automatically generated
dataset of circa 400,000 descriptive sentences about commonsense knowledge that
can be true or false in which negation is present in about 2/3 of the corpus in
different forms. We have used our dataset with the largest available open LLMs
in a zero-shot approach to grasp their generalization and inference capability
and we have also fine-tuned some of the models to assess whether the
understanding of negation can be trained. Our findings show that, while LLMs
are proficient at classifying affirmative sentences, they struggle with
negative sentences and lack a deep understanding of negation, often relying on
superficial cues. Although fine-tuning the models on negative sentences
improves their performance, the lack of generalization in handling negation is
persistent, highlighting the ongoing challenges of LLMs regarding negation
understanding and generalization. The dataset and code are publicly available.
","2023-10-25","2310.15941v1.pdf"
"2310.15959","Junda Wang","Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun
  Wang, Yucheng Xu, Hong Yu","NoteChat: A Dataset of Synthetic Doctor-Patient Conversations
  Conditioned on Clinical Notes","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The detailed clinical records drafted by doctors after each patient's visit
are crucial for medical practitioners and researchers. Automating the creation
of these notes with language models can reduce the workload of doctors.
However, training such models can be difficult due to the limited public
availability of conversations between patients and doctors. In this paper, we
introduce NoteChat, a cooperative multi-agent framework leveraging Large
Language Models (LLMs) for generating synthetic doctor-patient conversations
conditioned on clinical notes. NoteChat consists of Planning, Roleplay, and
Polish modules. We provide a comprehensive automatic and human evaluation of
NoteChat, comparing it with state-of-the-art models, including OpenAI's ChatGPT
and GPT-4. Results demonstrate that NoteChat facilitates high-quality synthetic
doctor-patient conversations, underscoring the untapped potential of LLMs in
healthcare. This work represents the first instance of multiple LLMs
cooperating to complete a doctor-patient conversation conditioned on clinical
notes, offering promising avenues for the intersection of AI and healthcare
","2023-10-25","2310.15959v1.pdf"
"2310.15961","Sebastian Jaszczur","Szymon Antoniak, Sebastian Jaszczur, Micha{\l} Krutul, Maciej Pi\'oro,
  Jakub Krajewski, Jan Ludziejewski, Tomasz Odrzyg\'o\'zd\'z, Marek Cygan","Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Despite the promise of Mixture of Experts (MoE) models in increasing
parameter counts of Transformer models while maintaining training and inference
costs, their application carries notable drawbacks. The key strategy of these
models is to, for each processed token, activate at most a few experts -
subsets of an extensive feed-forward layer. But this approach is not without
its challenges. The operation of matching experts and tokens is discrete, which
makes MoE models prone to issues like training instability and uneven expert
utilization. Existing techniques designed to address these concerns, such as
auxiliary losses or balance-aware matching, result either in lower model
performance or are more difficult to train. In response to these issues, we
propose Mixture of Tokens, a fully-differentiable model that retains the
benefits of MoE architectures while avoiding the aforementioned difficulties.
Rather than routing tokens to experts, this approach mixes tokens from
different examples prior to feeding them to experts, enabling the model to
learn from all token-expert combinations. Importantly, this mixing can be
disabled to avoid mixing of different sequences during inference. Crucially,
this method is fully compatible with both masked and causal Large Language
Model training and inference.
","2023-10-25","2310.15961v1.pdf"
"2310.15975","Di Chen","Di Chen, Meixin Zhu, Hao Yang, Xuesong Wang, Yinhai Wang","Data-driven Traffic Simulation: A Comprehensive Review","18 pages, 4 figures, 4 tables","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Autonomous vehicles (AVs) have the potential to significantly revolutionize
society by providing a secure and efficient mode of transportation. Recent
years have witnessed notable advance-ments in autonomous driving perception and
prediction, but the challenge of validating the performance of AVs remains
largely unresolved. Data-driven microscopic traffic simulation has be-come an
important tool for autonomous driving testing due to 1) availability of
high-fidelity traffic data; 2) its advantages of ena-bling large-scale testing
and scenario reproducibility; and 3) its potential in reactive and realistic
traffic simulation. However, a comprehensive review of this topic is currently
lacking. This pa-per aims to fill this gap by summarizing relevant studies. The
primary objective of this paper is to review current research ef-forts and
provide a futuristic perspective that will benefit future developments in the
field. It introduces the general issues of data-driven traffic simulation and
outlines key concepts and terms. After overviewing traffic simulation, various
datasets and evalua-tion metrics commonly used are reviewed. The paper then
offers a comprehensive evaluation of imitation learning, reinforcement
learning, generative and deep learning methods, summarizing each and analyzing
their advantages and disadvantages in detail. Moreover, it evaluates the
state-of-the-art, existing challenges, and future research directions.
","2023-10-25","2310.15975v1.pdf"
"2310.15987","Vikas Raunak","Vikas Raunak and Hany Hassan Awadalla and Arul Menezes","Dissecting In-Context Learning of Translations in GPTs","EMNLP Findings (+ Minor Updates over Camera-Ready)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Most of the recent work in leveraging Large Language Models (LLMs) such as
GPT-3 for Machine Translation (MT) has focused on selecting the few-shot
samples for prompting. In this work, we try to better understand the role of
demonstration attributes for the in-context learning of translations through
perturbations of high-quality, in-domain demonstrations. We find that
asymmetric perturbation of the source-target mappings yield vastly different
results. We show that the perturbation of the source side has surprisingly
little impact, while target perturbation can drastically reduce translation
quality, suggesting that it is the output text distribution that provides the
most important learning signal during in-context learning of translations. We
propose a method named Zero-Shot-Context to add this signal automatically in
Zero-Shot prompting. We demonstrate that it improves upon the zero-shot
translation performance of GPT-3, even making it competitive with few-shot
prompted translations.
","2023-10-25","2310.15987v1.pdf"
"2310.16002","Jinbin Bai","Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian Ye, Kaicheng
  Zhou, Mike Zheng Shou","Integrating View Conditions for Image Synthesis","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the field of image processing, applying intricate semantic modifications
within existing images remains an enduring challenge. This paper introduces a
pioneering framework that integrates viewpoint information to enhance the
control of image editing tasks. By surveying existing object editing
methodologies, we distill three essential criteria, consistency,
controllability, and harmony, that should be met for an image editing method.
In contrast to previous approaches, our method takes the lead in satisfying all
three requirements for addressing the challenge of image synthesis. Through
comprehensive experiments, encompassing both quantitative assessments and
qualitative comparisons with contemporary state-of-the-art methods, we present
compelling evidence of our framework's superior performance across multiple
dimensions. This work establishes a promising avenue for advancing image
synthesis techniques and empowering precise object modifications while
preserving the visual coherence of the entire composition.
","2023-10-27","2310.16002v1.pdf"
"2310.16003","Jay Zhangjie Wu","Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani
  Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu,
  Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt
  Keutzer, Forrest Iandola","CVPR 2023 Text Guided Video Editing Competition","Project page: https://sites.google.com/view/loveucvpr23/track4","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Humans watch more than a billion hours of video per day. Most of this video
was edited manually, which is a tedious process. However, AI-enabled
video-generation and video-editing is on the rise. Building on text-to-image
models like Stable Diffusion and Imagen, generative AI has improved
dramatically on video tasks. But it's hard to evaluate progress in these video
tasks because there is no standard benchmark. So, we propose a new dataset for
text-guided video editing (TGVE), and we run a competition at CVPR to evaluate
models on our TGVE dataset. In this paper we present a retrospective on the
competition and describe the winning method. The competition dataset is
available at https://sites.google.com/view/loveucvpr23/track4.
","2023-10-25","2310.16003v1.pdf"
"2310.16020","Joey Wilson","Joey Wilson, Yuewei Fu, Joshua Friesen, Parker Ewen, Andrew Capodieci,
  Paramsothy Jayakumar, Kira Barton, and Maani Ghaffari","ConvBKI: Real-Time Probabilistic Semantic Mapping Network with
  Quantifiable Uncertainty","arXiv admin note: text overlap with arXiv:2209.10663","","","","cs.RO cs.CV","http://creativecommons.org/licenses/by/4.0/","  In this paper, we develop a modular neural network for real-time semantic
mapping in uncertain environments, which explicitly updates per-voxel
probabilistic distributions within a neural network layer. Our approach
combines the reliability of classical probabilistic algorithms with the
performance and efficiency of modern neural networks. Although robotic
perception is often divided between modern differentiable methods and classical
explicit methods, a union of both is necessary for real-time and trustworthy
performance. We introduce a novel Convolutional Bayesian Kernel Inference
(ConvBKI) layer which incorporates semantic segmentation predictions online
into a 3D map through a depthwise convolution layer by leveraging conjugate
priors. We compare ConvBKI against state-of-the-art deep learning approaches
and probabilistic algorithms for mapping to evaluate reliability and
performance. We also create a Robot Operating System (ROS) package of ConvBKI
and test it on real-world perceptually challenging off-road driving data.
","2023-10-27","2310.16020v1.pdf"
"2310.16028","Hattie Zhou","Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi,
  Josh Susskind, Samy Bengio, Preetum Nakkiran","What Algorithms can Transformers Learn? A Study in Length Generalization","Preprint","","","","cs.LG cs.AI cs.CL stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models exhibit surprising emergent generalization properties,
yet also struggle on many simple reasoning tasks such as arithmetic and parity.
This raises the question of if and when Transformer models can learn the true
algorithm for solving a task. We study the scope of Transformers' abilities in
the specific setting of length generalization on algorithmic tasks. Here, we
propose a unifying framework to understand when and how Transformers can
exhibit strong length generalization on a given task. Specifically, we leverage
RASP (Weiss et al., 2021) -- a programming language designed for the
computational model of a Transformer -- and introduce the RASP-Generalization
Conjecture: Transformers tend to length generalize on a task if the task can be
solved by a short RASP program which works for all input lengths. This simple
conjecture remarkably captures most known instances of length generalization on
algorithmic tasks. Moreover, we leverage our insights to drastically improve
generalization performance on traditionally hard tasks (such as parity and
addition). On the theoretical side, we give a simple example where the
""min-degree-interpolator"" model of learning from Abbe et al. (2023) does not
correctly predict Transformers' out-of-distribution behavior, but our
conjecture does. Overall, our work provides a novel perspective on the
mechanisms of compositional generalization and the algorithmic capabilities of
Transformers.
","2023-10-25","2310.16028v1.pdf"
"2310.16033","Jiarui Zhang","Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski","Visual Cropping Improves Zero-Shot Question Answering of Multimodal
  Large Language Models","11 pages, 4 figures, 4 tables","","","","cs.CV cs.CL","http://creativecommons.org/licenses/by/4.0/","  Multimodal Large Language Models (LLMs) have recently achieved promising
zero-shot accuracy on visual question answering (VQA) -- a fundamental task
affecting various downstream applications and domains. Given the great
potential for the broad use of these models, it is important to investigate
their limitations in dealing with different image and question properties. In
this work, we investigate whether multimodal LLMs can perceive small details as
well as large details in images. In particular, we show that their zero-shot
accuracy in answering visual questions is very sensitive to the size of the
visual subject of the question, declining up to $46\%$ with size. Furthermore,
we show that this effect is causal by observing that human visual cropping can
significantly mitigate their sensitivity to size. Inspired by the usefulness of
human cropping, we then propose three automatic visual cropping methods as
inference time mechanisms to improve the zero-shot performance of multimodal
LLMs. We study their effectiveness on four popular VQA datasets, and a subset
of the VQAv2 dataset tailored towards fine visual details. Our findings suggest
that multimodal LLMs should be used with caution in detail-sensitive VQA
applications, and that visual cropping is a promising direction to improve
their zero-shot performance. Our code and data are publicly available.
","2023-10-25","2310.16033v1.pdf"
"2310.16035","Jiayuan Mao","Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu","What's Left? Concept Grounding with Logic-Enhanced Foundation Models","NeurIPS 2023. First two authors contributed equally. Project page:
  https://web.stanford.edu/~joycj/projects/left_neurips_2023","","","","cs.CV cs.AI cs.CL cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent works such as VisProg and ViperGPT have smartly composed foundation
models for visual reasoning-using large language models (LLMs) to produce
programs that can be executed by pre-trained vision-language models. However,
they operate in limited domains, such as 2D images, not fully exploiting the
generalization of language: abstract concepts like ""left"" can also be grounded
in 3D, temporal, and action data, as in moving to your left. This limited
generalization stems from these inference-only methods' inability to learn or
adapt pre-trained models to a new domain. We propose the Logic-Enhanced
Foundation Model (LEFT), a unified framework that learns to ground and reason
with concepts across domains with a differentiable, domain-independent,
first-order logic-based program executor. LEFT has an LLM interpreter that
outputs a program represented in a general, logic-based reasoning language,
which is shared across all domains and tasks. LEFT's executor then executes the
program with trainable domain-specific grounding modules. We show that LEFT
flexibly learns concepts in four domains: 2D images, 3D scenes, human motions,
and robotic manipulation. It exhibits strong reasoning ability in a wide
variety of tasks, including those that are complex and not seen during
training, and can be easily applied to new domains.
","2023-10-25","2310.16035v1.pdf"
"2310.16040","Yizhu Jiao","Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji,
  Jiawei Han","Instruct and Extract: Instruction Tuning for On-Demand Information
  Extraction","EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models with instruction-following capabilities open the door
to a wider group of users. However, when it comes to information extraction - a
classic task in natural language processing - most task-specific systems cannot
align well with long-tail ad hoc extraction use cases for non-expert users. To
address this, we propose a novel paradigm, termed On-Demand Information
Extraction, to fulfill the personalized demands of real-world users. Our task
aims to follow the instructions to extract the desired content from the
associated text and present it in a structured tabular format. The table
headers can either be user-specified or inferred contextually by the model. To
facilitate research in this emerging area, we present a benchmark named
InstructIE, inclusive of both automatically generated training data, as well as
the human-annotated test set. Building on InstructIE, we further develop an
On-Demand Information Extractor, ODIE. Comprehensive evaluations on our
benchmark reveal that ODIE substantially outperforms the existing open-source
models of similar size. Our code and dataset are released on
https://github.com/yzjiao/On-Demand-IE.
","2023-10-25","2310.16040v1.pdf"
"2310.16042","Heyi Tao","Heyi Tao, Sethuraman T V, Michal Shlapentokh-Rothman, Derek Hoiem","WebWISE: Web Interface Control and Sequential Exploration with Large
  Language Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The paper investigates using a Large Language Model (LLM) to automatically
perform web software tasks using click, scroll, and text input operations.
Previous approaches, such as reinforcement learning (RL) or imitation learning,
are inefficient to train and task-specific. Our method uses filtered Document
Object Model (DOM) elements as observations and performs tasks step-by-step,
sequentially generating small programs based on the current observations. We
use in-context learning, either benefiting from a single manually provided
example, or an automatically generated example based on a successful zero-shot
trial. We evaluate the proposed method on the MiniWob++ benchmark. With only
one in-context example, our WebWISE method achieves similar or better
performance than other methods that require many demonstrations or trials.
","2023-10-26","2310.16042v1.pdf"
"2310.16045","Shukang Yin","Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui,
  Yunhang Shen, Ke Li, Xing Sun and Enhong Chen","Woodpecker: Hallucination Correction for Multimodal Large Language
  Models","16 pages, 7 figures. Code Website:
  https://github.com/BradyFU/Woodpecker","","","","cs.CV cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Hallucination is a big shadow hanging over the rapidly evolving Multimodal
Large Language Models (MLLMs), referring to the phenomenon that the generated
text is inconsistent with the image content. In order to mitigate
hallucinations, existing studies mainly resort to an instruction-tuning manner
that requires retraining the models with specific data. In this paper, we pave
a different way, introducing a training-free method named Woodpecker. Like a
woodpecker heals trees, it picks out and corrects hallucinations from the
generated text. Concretely, Woodpecker consists of five stages: key concept
extraction, question formulation, visual knowledge validation, visual claim
generation, and hallucination correction. Implemented in a post-remedy manner,
Woodpecker can easily serve different MLLMs, while being interpretable by
accessing intermediate outputs of the five stages. We evaluate Woodpecker both
quantitatively and qualitatively and show the huge potential of this new
paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement
in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released
at https://github.com/BradyFU/Woodpecker.
","2023-10-25","2310.16045v1.pdf"
"2310.16048","Abhilash Mishra","Abhilash Mishra","AI Alignment and Social Choice: Fundamental Limitations and Policy
  Implications","10 pages, no figures","","","","cs.AI cs.CL cs.CY cs.HC cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Aligning AI agents to human intentions and values is a key bottleneck in
building safe and deployable AI applications. But whose values should AI agents
be aligned with? Reinforcement learning with human feedback (RLHF) has emerged
as the key framework for AI alignment. RLHF uses feedback from human
reinforcers to fine-tune outputs; all widely deployed large language models
(LLMs) use RLHF to align their outputs to human values. It is critical to
understand the limitations of RLHF and consider policy challenges arising from
these limitations. In this paper, we investigate a specific challenge in
building RLHF systems that respect democratic norms. Building on impossibility
results in social choice theory, we show that, under fairly broad assumptions,
there is no unique voting protocol to universally align AI systems using RLHF
through democratic processes. Further, we show that aligning AI agents with the
values of all individuals will always violate certain private ethical
preferences of an individual user i.e., universal AI alignment using RLHF is
impossible. We discuss policy implications for the governance of AI systems
built using RLHF: first, the need for mandating transparent voting rules to
hold model builders accountable. Second, the need for model builders to focus
on developing AI agents that are narrowly aligned to specific user groups.
","2023-10-25","2310.16048v1.pdf"
"2310.16049","Zayne Sprague","Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett","MuSR: Testing the Limits of Chain-of-thought with Multistep Soft
  Reasoning","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  While large language models (LLMs) equipped with techniques like
chain-of-thought prompting have demonstrated impressive capabilities, they
still fall short in their ability to reason robustly in complex settings.
However, evaluating LLM reasoning is challenging because system capabilities
continue to grow while benchmark datasets for tasks like logical deduction have
remained static. We introduce MuSR, a dataset for evaluating language models on
multistep soft reasoning tasks specified in a natural language narrative. This
dataset has two crucial features. First, it is created through a novel
neurosymbolic synthetic-to-natural generation algorithm, enabling the
construction of complex reasoning instances that challenge GPT-4 (e.g., murder
mysteries roughly 1000 words in length) and which can be scaled further as more
capable LLMs are released. Second, our dataset instances are free text
narratives corresponding to real-world domains of reasoning; this makes it
simultaneously much more challenging than other synthetically-crafted
benchmarks while remaining realistic and tractable for human annotators to
solve with high accuracy. We evaluate a range of LLMs and prompting techniques
on this dataset and characterize the gaps that remain for techniques like
chain-of-thought to perform robust reasoning.
","2023-10-25","2310.16049v1.pdf"
"2310.16062","Shuoran Jiang","Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu","Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained
  Large Models Fine-Tuning","","","","","cs.LG cs.AI cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The excellent generalization, contextual learning, and emergence abilities in
the pre-trained large models (PLMs) handle specific tasks without direct
training data, making them the better foundation models in the adversarial
domain adaptation (ADA) methods to transfer knowledge learned from the source
domain to target domains. However, existing ADA methods fail to account for the
confounder properly, which is the root cause of the source data distribution
that differs from the target domains. This study proposes an adversarial domain
adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The
ADA-CBF includes a PLM as the foundation model for a feature extractor, a
domain classifier and a confounder classifier, and they are jointly trained
with an adversarial loss. This loss is designed to improve the domain-invariant
representation learning by diluting the discrimination in the domain
classifier. At the same time, the adversarial loss also balances the confounder
distribution among source and unmeasured domains in training. Compared to
existing ADA methods, ADA-CBF can correctly identify confounders in
domain-invariant features, thereby eliminating the confounder biases in the
extracted features from PLMs. The confounder classifier in ADA-CBF is designed
as a plug-and-play and can be applied in the confounder measurable,
unmeasurable, or partially measurable environments. Empirical results on
natural language processing and computer vision downstream tasks show that
ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.
","2023-10-26","2310.16062v1.pdf"
"2310.16069","Lei Li","Lei Li","CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought
  Language Prompting","WACV 2024","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Natural scene analysis and remote sensing imagery offer immense potential for
advancements in large-scale language-guided context-aware data utilization.
This potential is particularly significant for enhancing performance in
downstream tasks such as object detection and segmentation with designed
language prompting. In light of this, we introduce the CPSeg, Chain-of-Thought
Language Prompting for Finer-grained Semantic Segmentation), an innovative
framework designed to augment image segmentation performance by integrating a
novel ""Chain-of-Thought"" process that harnesses textual information associated
with images. This groundbreaking approach has been applied to a flood disaster
scenario. CPSeg encodes prompt texts derived from various sentences to
formulate a coherent chain-of-thought. We propose a new vision-language
dataset, FloodPrompt, which includes images, semantic masks, and corresponding
text information. This not only strengthens the semantic understanding of the
scenario but also aids in the key task of semantic segmentation through an
interplay of pixel and text matching maps. Our qualitative and quantitative
analyses validate the effectiveness of CPSeg.
","2023-10-26","2310.16069v1.pdf"
"2310.16095","Sunjae Kwon","Ye Eun Chun, Sunjae Kwon, Kyunghwan Sohn, Nakwon Sung, Junyoup Lee,
  Byungki Seo, Kevin Compher, Seung-won Hwang, Jaesik Choi","CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn
  from Financial Reports","Accepted in Findings of EMNLP 2023","","","","cs.CL cs.CE","http://creativecommons.org/licenses/by/4.0/","  In this paper, we introduce CR-COPEC called Causal Rationale of Corporate
Performance Changes from financial reports. This is a comprehensive large-scale
domain-adaptation causal sentence dataset to detect financial performance
changes of corporate. CR-COPEC contributes to two major achievements. First, it
detects causal rationale from 10-K annual reports of the U.S. companies, which
contain experts' causal analysis following accounting standards in a formal
manner. This dataset can be widely used by both individual investors and
analysts as material information resources for investing and decision making
without tremendous effort to read through all the documents. Second, it
carefully considers different characteristics which affect the financial
performance of companies in twelve industries. As a result, CR-COPEC can
distinguish causal sentences in various industries by taking unique narratives
in each industry into consideration. We also provide an extensive analysis of
how well CR-COPEC dataset is constructed and suited for classifying target
sentences as causal ones with respect to industry characteristics. Our dataset
and experimental codes are publicly available.
","2023-10-26","2310.16095v1.pdf"
"2310.16111","Saiteja Utpala","Saiteja Utpala, Sara Hooker, Pin Yu Chen","Locally Differentially Private Document Generation Using Zero Shot
  Prompting","Accepted at EMNLP 2023 (Findings)","","","","cs.CL cs.CR cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Numerous studies have highlighted the privacy risks associated with
pretrained large language models. In contrast, our research offers a unique
perspective by demonstrating that pretrained large language models can
effectively contribute to privacy preservation. We propose a locally
differentially private mechanism called DP-Prompt, which leverages the power of
pretrained large language models and zero-shot prompting to counter author
de-anonymization attacks while minimizing the impact on downstream utility.
When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),
we observe a notable reduction in the success rate of de-anonymization attacks,
showing that it surpasses existing approaches by a considerable margin despite
its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt
(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving
a 46\% reduction in author identification F1 score against static attackers and
a 26\% reduction against adaptive attackers. We conduct extensive experiments
across six open-source large language models, ranging up to 7 billion
parameters, to analyze various effects of the privacy-utility tradeoff.
","2023-10-26","2310.16111v1.pdf"
"2310.16112","Gregory Holste","Gregory Holste, Yiliang Zhou, Song Wang, Ajay Jaiswal, Mingquan Lin,
  Sherry Zhuge, Yuzhe Yang, Dongkyun Kim, Trong-Hieu Nguyen-Mau, Minh-Triet
  Tran, Jaehyup Jeong, Wongi Park, Jongbin Ryu, Feng Hong, Arsh Verma, Yosuke
  Yamagishi, Changhyun Kim, Hyeryeong Seo, Myungjoo Kang, Leo Anthony Celi,
  Zhiyong Lu, Ronald M. Summers, George Shih, Zhangyang Wang, Yifan Peng","Towards long-tailed, multi-label disease classification from chest
  X-ray: Overview of the CXR-LT challenge","","","","","cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Many real-world image recognition problems, such as diagnostic medical
imaging exams, are ""long-tailed"" $\unicode{x2013}$ there are a few common
findings followed by many more relatively rare conditions. In chest
radiography, diagnosis is both a long-tailed and multi-label problem, as
patients often present with multiple findings simultaneously. While researchers
have begun to study the problem of long-tailed learning in medical image
recognition, few have studied the interaction of label imbalance and label
co-occurrence posed by long-tailed, multi-label disease classification. To
engage with the research community on this emerging topic, we conducted an open
challenge, CXR-LT, on long-tailed, multi-label thorax disease classification
from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset
of over 350,000 CXRs, each labeled with at least one of 26 clinical findings
following a long-tailed distribution. We synthesize common themes of
top-performing solutions, providing practical recommendations for long-tailed,
multi-label medical image classification. Finally, we use these insights to
propose a path forward involving vision-language foundation models for few- and
zero-shot disease classification.
","2023-10-26","2310.16112v1.pdf"
"2310.16117","Abdelrahim Elmadany","Muhammad Abdul-Mageed, AbdelRahim Elmadany, Chiyu Zhang, El Moatez
  Billah Nagoudi, Houda Bouamor, Nizar Habash","NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task","arXiv admin note: text overlap with arXiv:2210.09582","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We describe the findings of the fourth Nuanced Arabic Dialect Identification
Shared Task (NADI 2023). The objective of NADI is to help advance
state-of-the-art Arabic NLP by creating opportunities for teams of researchers
to collaboratively compete under standardized conditions. It does so with a
focus on Arabic dialects, offering novel datasets and defining subtasks that
allow for meaningful comparisons between different approaches. NADI 2023
targeted both dialect identification (Subtask 1) and dialect-to-MSA machine
translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered
for the shared task, of whom 18 teams have participated (with 76 valid
submissions during test phase). Among these, 16 teams participated in Subtask
1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning
teams achieved 87.27
  F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3,
respectively. Results show that all three subtasks remain challenging, thereby
motivating future work in this area. We describe the methods employed by the
participating teams and briefly offer an outlook for NADI.
","2023-10-26","2310.16117v1.pdf"
"2310.16119","Ondrej Kobza","Ond\v{r}ej Kobza, Jan \v{C}uhel, Tommaso Gargiani, David Herel, Petr
  Marek (Faculty of Electrical Engineering, CTU in Prague)","Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for
  Enhancing SocialBot Conversations","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize
SocialBot Grand Challenge~5. Building upon previous versions of our system, we
introduce the NRG Barista and outline several innovative approaches for
integrating Barista into our SocialBot, improving the overall conversational
experience. Additionally, we extend our SocialBot to support multimodal
devices. This paper offers insights into the development of Alquist~5.0, which
meets evolving user expectations while maintaining empathetic and knowledgeable
conversational abilities across diverse topics.
","2023-10-26","2310.16119v1.pdf"
"2310.16135","Chenghao Yang","Chenghao Yang, Allyson Ettinger","Can You Follow Me? Testing Situational Understanding in ChatGPT","EMNLP 2023 Main Paper (Camera Ready)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Understanding sentence meanings and updating information states appropriately
across time -- what we call ""situational understanding"" (SU) -- is a critical
ability for human-like AI agents. SU is essential in particular for chat
models, such as ChatGPT, to enable consistent, coherent, and effective dialogue
between humans and AI. Previous works have identified certain SU limitations in
non-chatbot Large Language models (LLMs), but the extent and causes of these
limitations are not well understood, and capabilities of current chat-based
models in this domain have not been explored. In this work we tackle these
questions, proposing a novel synthetic environment for SU testing which allows
us to do controlled and systematic testing of SU in chat-oriented models,
through assessment of models' ability to track and enumerate environment
states. Our environment also allows for close analysis of dynamics of model
performance, to better understand underlying causes for performance patterns.
We apply our test to ChatGPT, the state-of-the-art chatbot, and find that
despite the fundamental simplicity of the task, the model's performance
reflects an inability to retain correct environment states across time. Our
follow-up analyses suggest that performance degradation is largely because
ChatGPT has non-persistent in-context memory (although it can access the full
dialogue history) and it is susceptible to hallucinated updates -- including
updates that artificially inflate accuracies. Our findings suggest overall that
ChatGPT is not currently equipped for robust tracking of situation states, and
that trust in the impressive dialogue performance of ChatGPT comes with risks.
We release the codebase for reproducing our test environment, as well as all
prompts and API responses from ChatGPT, at
https://github.com/yangalan123/SituationalTesting.
","2023-10-26","2310.16135v1.pdf"
"2310.16146","Alejandro Lozano","Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, and Nigam Shah","Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model
  System for Answering Medical Questions using Scientific Literature","Preprint of an article published in Pacific Symposium on Biocomputing
  copyright 2024 World Scientific Publishing Co., Singapore,
  http://psb.stanford.edu/","","","","cs.IR cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  The quickly-expanding nature of published medical literature makes it
challenging for clinicians and researchers to keep up with and summarize
recent, relevant findings in a timely manner. While several closed-source
summarization tools based on large language models (LLMs) now exist, rigorous
and systematic evaluations of their outputs are lacking. Furthermore, there is
a paucity of high-quality datasets and appropriate benchmark tasks with which
to evaluate these tools. We address these issues with four contributions: we
release Clinfo.ai, an open-source WebApp that answers clinical questions based
on dynamically retrieved scientific literature; we specify an information
retrieval and abstractive summarization task to evaluate the performance of
such retrieval-augmented LLM systems; we release a dataset of 200 questions and
corresponding answers derived from published systematic reviews, which we name
PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for
Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.
","2023-10-26","2310.16146v1.pdf"
"2310.16147","Wookje Han","Wookje Han, Jinsol Park, Kyungjae Lee","PreWoMe: Exploiting Presuppositions as Working Memory for Long Form
  Question Answering","11 pages 3 figures, Accepted to EMNLP 2023 (short)","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Information-seeking questions in long-form question answering (LFQA) often
prove misleading due to ambiguity or false presupposition in the question.
While many existing approaches handle misleading questions, they are tailored
to limited questions, which are insufficient in a real-world setting with
unpredictable input characteristics. In this work, we propose PreWoMe, a
unified approach capable of handling any type of information-seeking question.
The key idea of PreWoMe involves extracting presuppositions in the question and
exploiting them as working memory to generate feedback and action about the
question. Our experiment shows that PreWoMe is effective not only in tackling
misleading questions but also in handling normal ones, thereby demonstrating
the effectiveness of leveraging presuppositions, feedback, and action for
real-world QA settings.
","2023-10-26","2310.16147v1.pdf"
"2310.16152","Md Rafi Ur Rashid","Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana,
  Shagufta Mehnaz","FLTrojan: Privacy Leakage Attacks against Federated Language Models
  Through Selective Weight Tampering","22 pages (including bibliography and Appendix), Submitted to USENIX
  Security '24","","","","cs.CR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Federated learning (FL) is becoming a key component in many technology-based
applications including language modeling -- where individual FL participants
often have privacy-sensitive text data in their local datasets. However,
realizing the extent of privacy leakage in federated language models is not
straightforward and the existing attacks only intend to extract data regardless
of how sensitive or naive it is. To fill this gap, in this paper, we introduce
two novel findings with regard to leaking privacy-sensitive user data from
federated language models. Firstly, we make a key observation that model
snapshots from the intermediate rounds in FL can cause greater privacy leakage
than the final trained model. Secondly, we identify that privacy leakage can be
aggravated by tampering with a model's selective weights that are specifically
responsible for memorizing the sensitive training data. We show how a malicious
client can leak the privacy-sensitive data of some other user in FL even
without any cooperation from the server. Our best-performing method improves
the membership inference recall by 29% and achieves up to 70% private data
reconstruction, evidently outperforming existing attacks with stronger
assumptions of adversary capabilities.
","2023-10-26","2310.16152v1.pdf"
"2310.16154","Leonardo Petrini","Leonardo Petrini","Breaking the Curse of Dimensionality in Deep Neural Networks by Learning
  Invariant Representations","PhD Thesis @ EPFL","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Artificial intelligence, particularly the subfield of machine learning, has
seen a paradigm shift towards data-driven models that learn from and adapt to
data. This has resulted in unprecedented advancements in various domains such
as natural language processing and computer vision, largely attributed to deep
learning, a special class of machine learning models. Deep learning arguably
surpasses traditional approaches by learning the relevant features from raw
data through a series of computational layers.
  This thesis explores the theoretical foundations of deep learning by studying
the relationship between the architecture of these models and the inherent
structures found within the data they process. In particular, we ask What
drives the efficacy of deep learning algorithms and allows them to beat the
so-called curse of dimensionality-i.e. the difficulty of generally learning
functions in high dimensions due to the exponentially increasing need for data
points with increased dimensionality? Is it their ability to learn relevant
representations of the data by exploiting their structure? How do different
architectures exploit different data structures? In order to address these
questions, we push forward the idea that the structure of the data can be
effectively characterized by its invariances-i.e. aspects that are irrelevant
for the task at hand.
  Our methodology takes an empirical approach to deep learning, combining
experimental studies with physics-inspired toy models. These simplified models
allow us to investigate and interpret the complex behaviors we observe in deep
learning systems, offering insights into their inner workings, with the
far-reaching goal of bridging the gap between theory and practice.
","2023-10-26","2310.16154v1.pdf"
"2310.16176","Zhenzhen Liu","Zhenzhen Liu, Chao Wan, Varsha Kishore, Jin Peng Zhou, Minmin Chen,
  Kilian Q. Weinberger","Correction with Backtracking Reduces Hallucination in Summarization","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Abstractive summarization aims at generating natural language summaries of a
source document that are succinct while preserving the important elements.
Despite recent advances, neural text summarization models are known to be
susceptible to hallucinating (or more correctly confabulating), that is to
produce summaries with details that are not grounded in the source document. In
this paper, we introduce a simple yet efficient technique, CoBa, to reduce
hallucination in abstractive summarization. The approach is based on two steps:
hallucination detection and mitigation. We show that the former can be achieved
through measuring simple statistics about conditional word probabilities and
distance to context words. Further, we demonstrate that straight-forward
backtracking is surprisingly effective at mitigation. We thoroughly evaluate
the proposed method with prior art on three benchmark datasets for text
summarization. The results show that CoBa is effective and efficient in
reducing hallucination, and offers great adaptability and flexibility.
","2023-10-26","2310.16176v1.pdf"
"2310.16183","Md. Arid Hasan","Md. Arid Hasan, Firoj Alam, Anika Anjum, Shudipta Das, Afiyat Anjum","BLP 2023 Task 2: Sentiment Analysis","Accepted in BLP Workshop at EMNLP-23","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We present an overview of the BLP Sentiment Shared Task, organized as part of
the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is
defined as the detection of sentiment in a given piece of social media text.
This task attracted interest from 71 participants, among whom 29 and 30 teams
submitted systems during the development and evaluation phases, respectively.
In total, participants submitted 597 runs. However, a total of 15 teams
submitted system description papers. The range of approaches in the submitted
systems spans from classical machine learning models, fine-tuning pre-trained
models, to leveraging Large Language Model (LLMs) in zero- and few-shot
settings. In this paper, we provide a detailed account of the task setup,
including dataset development and evaluation setup. Additionally, we provide a
brief overview of the systems submitted by the participants. All datasets and
evaluation scripts from the shared task have been made publicly available for
the research community, to foster further research in this domain
","2023-10-26","2310.16183v1.pdf"
"2310.16197","Adithya Pratapa","Adithya Pratapa, Kevin Small, Markus Dreyer","Background Summarization of Event Timelines","EMNLP 2023 camera-ready","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generating concise summaries of news events is a challenging natural language
processing task. While journalists often curate timelines to highlight key
sub-events, newcomers to a news event face challenges in catching up on its
historical context. In this paper, we address this need by introducing the task
of background news summarization, which complements each timeline update with a
background summary of relevant preceding events. We construct a dataset by
merging existing timeline datasets and asking human annotators to write a
background summary for each timestep of each news event. We establish strong
baseline performance using state-of-the-art summarization systems and propose a
query-focused variant to generate background summaries. To evaluate background
summary quality, we present a question-answering-based evaluation metric,
Background Utility Score (BUS), which measures the percentage of questions
about a current event timestep that a background summary answers. Our
experiments show the effectiveness of instruction fine-tuned systems such as
Flan-T5, in addition to strong zero-shot performance using GPT-3.5.
","2023-10-26","2310.16197v1.pdf"
"2310.16226","Saurabh Garg","Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja
  Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri","TiC-CLIP: Continual Training of CLIP Models","","","","","cs.CV cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Keeping large foundation models up to date on latest data is inherently
expensive. To avoid the prohibitive costs of constantly retraining, it is
imperative to continually train these models. This problem is exacerbated by
the lack of any large scale continual learning benchmarks or baselines. We
introduce the first set of web-scale Time-Continual (TiC) benchmarks for
training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with
over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first
use our benchmarks to curate various dynamic evaluations to measure temporal
robustness of existing models. We show OpenAI's CLIP (trained on data up to
2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from
2021--2022 compared with more recently trained models in OpenCLIP repository.
We then study how to efficiently train models on time-continuous data. We
demonstrate that a simple rehearsal-based approach that continues training from
the last checkpoint and replays old data reduces compute by $2.5\times$ when
compared to the standard practice of retraining from scratch.
","2023-10-26","2310.16226v1.pdf"
"2310.16240","Raymond Li","Raymond Li, Gabriel Murray and Giuseppe Carenini","Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting
  Pre-trained Language Models","14 pages, 3 figures, Camera-Ready for EMNLP 2023 Findings (Long
  Paper)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  In this work, we propose a method that combines two popular research areas by
injecting linguistic structures into pre-trained language models in the
parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel
adapter modules encoding different linguistic structures are combined using a
novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates
are used to determine the importance of these modules at each layer of the
model. To reduce the number of parameters, we first train the model for a fixed
small number of steps before pruning the experts based on their importance
scores. Our experiment results with three different pre-trained models show
that our approach can outperform state-of-the-art PEFT methods with a
comparable number of parameters. In addition, we provide additional analysis to
examine the experts selected by each model at each layer to provide insights
for future studies.
","2023-10-26","2310.16240v1.pdf"
"2310.16261","Ting-Rui Chiang","Ting-Rui Chiang, Dani Yogatama","The Distributional Hypothesis Does Not Fully Explain the Benefits of
  Masked Language Model Pretraining","EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We analyze the masked language modeling pretraining objective function from
the perspective of the distributional hypothesis. We investigate whether better
sample efficiency and the better generalization capability of models pretrained
with masked language modeling can be attributed to the semantic similarity
encoded in the pretraining data's distributional property. Via a synthetic
dataset, our analysis suggests that distributional property indeed leads to the
better sample efficiency of pretrained masked language models, but does not
fully explain the generalization capability. We also conduct analyses over two
real-world datasets and demonstrate that the distributional property does not
explain the generalization ability of pretrained natural language models
either. Our results illustrate our limited understanding of model pretraining
and provide future research directions.
","2023-10-26","2310.16261v1.pdf"
"2310.16262","Eunice Jun","Eunice Jun, Edward Misback, Jeffrey Heer, Ren\'e Just","rTisane: Externalizing conceptual models for data analysis increases
  engagement with domain knowledge and improves statistical model quality","","","","","cs.HC cs.AI cs.PL stat.CO","http://creativecommons.org/licenses/by/4.0/","  Statistical models should accurately reflect analysts' domain knowledge about
variables and their relationships. While recent tools let analysts express
these assumptions and use them to produce a resulting statistical model, it
remains unclear what analysts want to express and how externalization impacts
statistical model quality. This paper addresses these gaps. We first conduct an
exploratory study of analysts using a domain-specific language (DSL) to express
conceptual models. We observe a preference for detailing how variables relate
and a desire to allow, and then later resolve, ambiguity in their conceptual
models. We leverage these findings to develop rTisane, a DSL for expressing
conceptual models augmented with an interactive disambiguation process. In a
controlled evaluation, we find that rTisane's DSL helps analysts engage more
deeply with and accurately externalize their assumptions. rTisane also leads to
statistical models that match analysts' assumptions, maintain analysis intent,
and better fit the data.
","2023-10-26","2310.16262v1.pdf"
"2310.16263","Jiexin Wang","Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam
  Jatowt, Yi Cai","Enhancing Large Language Models for Secure Code Generation: A
  Dataset-driven Study on Vulnerability Mitigation","","","","","cs.SE cs.AI cs.CL cs.CR","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have brought significant advancements to code
generation, benefiting both novice and experienced developers. However, their
training using unsanitized data from open-source repositories, like GitHub,
introduces the risk of inadvertently propagating security vulnerabilities. To
effectively mitigate this concern, this paper presents a comprehensive study
focused on evaluating and enhancing code LLMs from a software security
perspective. We introduce SecuCoGen\footnote{SecuCoGen has been uploaded as
supplemental material and will be made publicly available after publication.},
a meticulously curated dataset targeting 21 critical vulnerability types.
SecuCoGen comprises 180 samples and serves as the foundation for conducting
experiments on three crucial code-related tasks: code generation, code repair
and vulnerability classification, with a strong emphasis on security. Our
experimental results reveal that existing models often overlook security
concerns during code generation, leading to the generation of vulnerable code.
To address this, we propose effective approaches to mitigate the security
vulnerabilities and enhance the overall robustness of code generated by LLMs.
Moreover, our study identifies weaknesses in existing models' ability to repair
vulnerable code, even when provided with vulnerability information.
Additionally, certain vulnerability types pose challenges for the models,
hindering their performance in vulnerability classification. Based on these
findings, we believe our study will have a positive impact on the software
engineering community, inspiring the development of improved methods for
training and utilizing LLMs, thereby leading to safer and more trustworthy
model deployment.
","2023-10-26","2310.16263v1.pdf"
"2310.16269","Cristina Espa\~na-Bonet","Cristina Espa\~na-Bonet","Multilingual Coarse Political Stance Classification of Media. The
  Editorial Line of a ChatGPT and Bard Newspaper","To be published at EMNLP 2023 (Findings)","","","","cs.CL cs.AI cs.CY","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Neutrality is difficult to achieve and, in politics, subjective. Traditional
media typically adopt an editorial line that can be used by their potential
readers as an indicator of the media bias. Several platforms currently rate
news outlets according to their political bias. The editorial line and the
ratings help readers in gathering a balanced view of news. But in the advent of
instruction-following language models, tasks such as writing a newspaper
article can be delegated to computers. Without imposing a biased persona, where
would an AI-based news outlet lie within the bias ratings? In this work, we use
the ratings of authentic news outlets to create a multilingual corpus of news
with coarse stance annotations (Left and Right) along with automatically
extracted topic annotations. We show that classifiers trained on this data are
able to identify the editorial line of most unseen newspapers in English,
German, Spanish and Catalan. We then apply the classifiers to 101
newspaper-like articles written by ChatGPT and Bard in the 4 languages at
different time periods. We observe that, similarly to traditional newspapers,
ChatGPT editorial line evolves with time and, being a data-driven system, the
stance of the generated articles differs among languages.
","2023-10-26","2310.16269v1.pdf"
"2310.16270","Mansi Sakarvadia","Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel
  Hudson, Andr\'e Bauer, Kyle Chard, Ian Foster","Attention Lens: A Tool for Mechanistically Interpreting the Attention
  Head Information Retrieval Mechanism","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformer-based Large Language Models (LLMs) are the state-of-the-art for
natural language tasks. Recent work has attempted to decode, by reverse
engineering the role of linear layers, the internal mechanisms by which LLMs
arrive at their final predictions for text completion tasks. Yet little is
known about the specific role of attention heads in producing the final token
prediction. We propose Attention Lens, a tool that enables researchers to
translate the outputs of attention heads into vocabulary tokens via learned
attention-head-specific transformations called lenses. Preliminary findings
from our trained lenses indicate that attention heads play highly specialized
roles in language models. The code for Attention Lens is available at
github.com/msakarvadia/AttentionLens.
","2023-10-26","2310.16270v1.pdf"
"2310.16271","Jixiang Hong","Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan","CycleAlign: Iterative Distillation from Black-box LLM to White-box
  Models for Better Human Alignment","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Language models trained on large-scale corpus often generate content that is
harmful, toxic, or contrary to human preferences, making their alignment with
human values a critical concern. Reinforcement learning from human feedback
(RLHF) with algorithms like PPO is a prevalent approach for alignment but is
often complex, unstable, and resource-intensive. Recently, ranking-based
alignment methods have emerged, offering stability and effectiveness by
replacing the RL framework with supervised fine-tuning, but they are costly due
to the need for annotated data. Considering that existing large language models
(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,
researchers have begun to align the language model with human preference from
AI feedback. The common practices, which unidirectionally distill the
instruction-following responses from LLMs, are constrained by their bottleneck.
Thus we introduce CycleAlign to distill alignment capabilities from
parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
in an iterative manner. With in-context learning (ICL) as the core of the
cycle, the black-box models are able to rank the model-generated responses
guided by human-craft instruction and demonstrations about their preferences.
During iterative interaction, the white-box models also have a judgment about
responses generated by them. Consequently, the agreement ranking could be
viewed as a pseudo label to dynamically update the in-context demonstrations
and improve the preference ranking ability of black-box models. Through
multiple interactions, the CycleAlign framework could align the white-box model
with the black-box model effectively in a low-resource way. Empirical results
illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing
methods, and achieves the state-of-the-art performance in alignment with human
value.
","2023-10-26","2310.16271v1.pdf"
"2310.16301","Chao-Hong Tan","Chao-Hong Tan, Jia-Chen Gu, Zhen-Hua Ling","Is ChatGPT a Good Multi-Party Conversation Solver?","Accepted by Findings of EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have emerged as influential instruments within
the realm of natural language processing; nevertheless, their capacity to
handle multi-party conversations (MPCs) -- a scenario marked by the presence of
multiple interlocutors involved in intricate information exchanges -- remains
uncharted. In this paper, we delve into the potential of generative LLMs such
as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is
conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by
subjecting them to evaluation across three MPC datasets that encompass five
representative tasks. The findings reveal that ChatGPT's performance on a
number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results
portend a promising future. Additionally, we endeavor to bolster performance
through the incorporation of MPC structures, encompassing both speaker and
addressee architecture. This study provides an exhaustive evaluation and
analysis of applying generative LLMs to MPCs, casting a light upon the
conception and creation of increasingly effective and robust MPC agents.
Concurrently, this work underscores the challenges implicit in the utilization
of LLMs for MPCs, such as deciphering graphical information flows and
generating stylistically consistent responses.
","2023-10-26","2310.16301v1.pdf"
"2310.16314","Debanjan Mondal","Debanjan Mondal, Abhilasha Lodha, Ankita Sahoo, Beena Kumari","Understanding Code Semantics: An Evaluation of Transformer Models in
  Summarization","All authors are co-first authors and have equal contributions","","","","cs.LG","http://creativecommons.org/publicdomain/zero/1.0/","  This paper delves into the intricacies of code summarization using advanced
transformer-based language models. Through empirical studies, we evaluate the
efficacy of code summarization by altering function and variable names to
explore whether models truly understand code semantics or merely rely on
textual cues. We have also introduced adversaries like dead code and commented
code across three programming languages (Python, Javascript, and Java) to
further scrutinize the model's understanding. Ultimately, our research aims to
offer valuable insights into the inner workings of transformer-based LMs,
enhancing their ability to understand code and contributing to more efficient
software development practices and maintenance workflows.
","2023-10-26","2310.16314v1.pdf"
"2310.16329","Aviya Maimon","Aviya Maimon and Reut Tsarfaty","CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment
  of Coherence in Generated Texts","","","","","cs.CL cs.AI cs.DB","http://creativecommons.org/licenses/by/4.0/","  Coherence is a linguistic term that refers to the relations between small
textual units (sentences, propositions), which make the text logically
consistent and meaningful to the reader. With the advances of generative
foundational models in NLP, there is a pressing need to automatically assess
the human-perceived coherence of automatically generated texts. Up until now,
little work has been done on explicitly assessing the coherence of generated
texts and analyzing the factors contributing to (in)coherence. Previous work on
the topic used other tasks, e.g., sentence reordering, as proxies of coherence,
rather than approaching coherence detection heads on. In this paper, we
introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of
automatically generated texts. Our annotation protocol reflects two
perspectives; one is global, assigning a single coherence score, and the other
is incremental, scoring sentence by sentence. The incremental method produces
an (in)coherence score for each text fragment and also pinpoints reasons for
incoherence at that point. Our benchmark contains 500 automatically-generated
and human-annotated paragraphs, each annotated in both methods, by multiple
raters. Our analysis shows that the inter-annotator agreement in the
incremental mode is higher than in the holistic alternative, and our
experiments show that standard LMs fine-tuned for coherence detection show
varied performance on the different factors contributing to (in)coherence. All
in all, these models yield unsatisfactory performance, emphasizing the need for
developing more reliable methods for coherence assessment.
","2023-10-26","2310.16329v1.pdf"
"2310.16338","Alexander H. Liu","Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra,
  Wei-Ning Hsu","Generative Pre-training for Speech with Flow Matching","Preprint, under review","","","","eess.AS cs.CL cs.LG cs.SD","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
","2023-10-26","2310.16338v1.pdf"
"2310.16340","Zefan Wang","Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan,
  Lingfei Wu, Qingsong Wen","RCAgent: Cloud Root Cause Analysis by Autonomous Agents with
  Tool-Augmented Large Language Models","","","","","cs.SE cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language model (LLM) applications in cloud root cause analysis (RCA)
have been actively explored recently. However, current methods are still
reliant on manual workflow settings and do not unleash LLMs' decision-making
and environment interaction capabilities. We present RCAgent, a tool-augmented
LLM autonomous agent framework for practical and privacy-aware industrial RCA
usage. Running on an internally deployed model rather than GPT families,
RCAgent is capable of free-form data collection and comprehensive analysis with
tools. Our framework combines a variety of enhancements, including a unique
Self-Consistency for action trajectories, and a suite of methods for context
management, stabilization, and importing domain knowledge. Our experiments show
RCAgent's evident and consistent superiority over ReAct across all aspects of
RCA -- predicting root causes, solutions, evidence, and responsibilities -- and
tasks covered or uncovered by current rules, as validated by both automated
metrics and human evaluations. Furthermore, RCAgent has already been integrated
into the diagnosis and issue discovery workflow of the Real-time Compute
Platform for Apache Flink of Alibaba Cloud.
","2023-10-26","2310.16340v1.pdf"
"2310.16343","Xiang Chen","Xiang Chen and Xiaojun Wan","A Comprehensive Evaluation of Constrained Text Generation for Large
  Language Models","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Advancements in natural language generation (NLG) and large language models
(LLMs) have led to proficient text generation in various tasks. However,
integrating intricate constraints into neural text generation, due to LLMs'
opacity, remains challenging. This study investigates constrained text
generation for LLMs, where predefined constraints are applied during LLM's
generation process. Our research examines multiple LLMs, including ChatGPT and
GPT-4, categorizing constraints into lexical, structural, and relation-based
types. We also present various benchmarks to facilitate fair evaluation. The
study addresses some key research questions, including the extent of LLMs'
compliance with constraints. Results illuminate LLMs' capacity and deficiency
to incorporate constraints and provide insights for future developments in
constrained text generation. Codes and datasets will be released upon
acceptance.
","2023-10-26","2310.16343v1.pdf"
"2310.16350","Jiaxi Li","Xiaobing Sun, Jiaxi Li, Wei Lu","Unraveling Feature Extraction Mechanisms in Neural Networks","Accepted by EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The underlying mechanism of neural networks in capturing precise knowledge
has been the subject of consistent research efforts. In this work, we propose a
theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such
mechanisms. Specifically, considering the infinite network width, we
hypothesize the learning dynamics of target models may intuitively unravel the
features they acquire from training data, deepening our insights into their
internal mechanisms. We apply our approach to several fundamental models and
reveal how these models leverage statistical features during gradient descent
and how they are integrated into final decisions. We also discovered that the
choice of activation function can affect feature extraction. For instance, the
use of the \textit{ReLU} activation function could potentially introduce a bias
in features, providing a plausible explanation for its replacement with
alternative functions in recent pre-trained language models. Additionally, we
find that while self-attention and CNN models may exhibit limitations in
learning n-grams, multiplication-based models seem to excel in this area. We
verify these theoretical findings through experiments and find that they can be
applied to analyze language modeling tasks, which can be regarded as a special
variant of classification. Our contributions offer insights into the roles and
capacities of fundamental components within large language models, thereby
aiding the broader understanding of these complex systems.
","2023-10-27","2310.16350v1.pdf"
"2310.16355","Bowen Tan","Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong
  Chen, Eric Xing, Zhiting Hu","Redco: A Lightweight Tool to Automate Distributed Training of LLMs on
  Any GPU/TPUs","Released under Apache License 2.0 at
  https://github.com/tanyuqian/redco","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent progress of AI can be largely attributed to large language models
(LLMs). However, their escalating memory requirements introduce challenges for
machine learning (ML) researchers and engineers. Addressing this requires
developers to partition a large model to distribute it across multiple GPUs or
TPUs. This necessitates considerable coding and intricate configuration efforts
with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa.
These tools require users' expertise in machine learning systems (MLSys),
creating a bottleneck in LLM development, particularly for developers without
MLSys background. In this work, we present Redco, a lightweight and
user-friendly tool crafted to automate distributed training and inference for
LLMs, as well as to simplify ML pipeline development. The design of Redco
emphasizes two key aspects. Firstly, to automate model parallism, our study
identifies two straightforward rules to generate tensor parallel strategies for
any given LLM. Integrating these rules into Redco facilitates effortless
distributed LLM training and inference, eliminating the need of additional
coding or complex configurations. We demonstrate the effectiveness by applying
Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to
the size of 66B. Secondly, we propose a mechanism that allows for the
customization of diverse ML pipelines through the definition of merely three
functions, eliminating redundant and formulaic code like multi-host related
processing. This mechanism proves adaptable across a spectrum of ML algorithms,
from foundational language modeling to complex algorithms like meta-learning
and reinforcement learning. Consequently, Redco implementations exhibit much
fewer code lines compared to their official counterparts.
","2023-10-26","2310.16355v1.pdf"
"2310.16361","Zhiyu Chen","Besnik Fetahu, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi","InstructPTS: Instruction-Tuning LLMs for Product Title Summarization","Accepted by EMNLP 2023 (Industry Track)","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  E-commerce product catalogs contain billions of items. Most products have
lengthy titles, as sellers pack them with product attributes to improve
retrieval, and highlight key product aspects. This results in a gap between
such unnatural products titles, and how customers refer to them. It also limits
how e-commerce stores can use these seller-provided titles for recommendation,
QA, or review summarization.
  Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a
controllable approach for the task of Product Title Summarization (PTS).
Trained using a novel instruction fine-tuning strategy, our approach is able to
summarize product titles according to various criteria (e.g. number of words in
a summary, inclusion of specific phrases, etc.). Extensive evaluation on a
real-world e-commerce catalog shows that compared to simple fine-tuning of
LLMs, our proposed approach can generate more accurate product name summaries,
with an improvement of over 14 and 8 BLEU and ROUGE points, respectively.
","2023-10-26","2310.16361v1.pdf"
"2310.16379","Xiting Wang","Xiting Wang, Liming Jiang, Jose Hernandez-Orallo, Luning Sun, David
  Stillwell, Fang Luo, Xing Xie","Evaluating General-Purpose AI with Psychometrics","Work in progress","","","","cs.AI cs.CY","http://creativecommons.org/licenses/by/4.0/","  Artificial intelligence (AI) has witnessed an evolution from task-specific to
general-purpose systems that trend toward human versatility. As AI systems
begin to play pivotal roles in society, it is important to ensure that they are
adequately evaluated. Current AI benchmarks typically assess performance on
collections of specific tasks. This has drawbacks when used for assessing
general-purpose AI systems. First, it is difficult to predict whether AI
systems could complete a new task it has never seen or that did not previously
exist. Second, these benchmarks often focus on overall performance metrics,
potentially overlooking the finer details crucial for making informed
decisions. Lastly, there are growing concerns about the reliability of existing
benchmarks and questions about what is being measured. To solve these
challenges, this paper suggests that psychometrics, the science of
psychological measurement, should be placed at the core of evaluating
general-purpose AI. Psychometrics provides a rigorous methodology for
identifying and measuring the latent constructs that underlie performance
across multiple tasks. We discuss its merits, warn against potential pitfalls,
and propose a framework for putting it into practice. Finally, we explore
future opportunities to integrate psychometrics with AI.
","2023-10-26","2310.16379v1.pdf"
"2310.16393","Vipul Rathore","Vipul Rathore, Rajdeep Dhingra, Parag Singla, Mausam","ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source
  Ensembling of Language Adapters","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via
the use of language adapters (LAs). Most of the earlier works have explored
training with adapter of a single source (often English), and testing either
using the target LA or LA of another related language. Training target LA
requires unlabeled data, which may not be readily available for low resource
unseen languages: those that are neither seen by the underlying multilingual
language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data
for them. We posit that for more effective cross-lingual transfer, instead of
just one source LA, we need to leverage LAs of multiple (linguistically or
geographically related) source languages, both at train and test-time - which
we investigate via our novel neural architecture, ZGUL. Extensive
experimentation across four language groups, covering 15 unseen target
languages, demonstrates improvements of up to 3.2 average F1 points over
standard fine-tuning and other strong baselines on POS tagging and NER tasks.
We also extend ZGUL to settings where either (1) some unlabeled data or (2)
few-shot training examples are available for the target language. We find that
ZGUL continues to outperform baselines in these settings too.
","2023-10-26","2310.16393v1.pdf"
"2310.16401","See Hian Lee","See Hian Lee, Feng Ji, Kelin Xia and Wee Peng Tay","Graph Neural Networks with a Distribution of Parametrized Graphs","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Traditionally, graph neural networks have been trained using a single
observed graph. However, the observed graph represents only one possible
realization. In many applications, the graph may encounter uncertainties, such
as having erroneous or missing edges, as well as edge weights that provide
little informative value. To address these challenges and capture additional
information previously absent in the observed graph, we introduce latent
variables to parameterize and generate multiple graphs. We obtain the maximum
likelihood estimate of the network parameters in an Expectation-Maximization
(EM) framework based on the multiple graphs. Specifically, we iteratively
determine the distribution of the graphs using a Markov Chain Monte Carlo
(MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical
experiments demonstrate improvements in performance against baseline models on
node classification for heterogeneous graphs and graph regression on chemistry
datasets.
","2023-10-26","2310.16401v1.pdf"
"2310.16411","Alon Goldstein PhD","Alon Goldstein, Miriam Havin, Roi Reichart and Ariel Goldstein","Decoding Stumpers: Large Language Models vs. Human Problem-Solvers","","","","","cs.CL cs.HC","http://creativecommons.org/licenses/by/4.0/","  This paper investigates the problem-solving capabilities of Large Language
Models (LLMs) by evaluating their performance on stumpers, unique single-step
intuition problems that pose challenges for human solvers but are easily
verifiable. We compare the performance of four state-of-the-art LLMs
(Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our
findings reveal that the new-generation LLMs excel in solving stumpers and
surpass human performance. However, humans exhibit superior skills in verifying
solutions to the same problems. This research enhances our understanding of
LLMs' cognitive abilities and provides insights for enhancing their
problem-solving potential across various domains.
","2023-10-26","2310.16411v1.pdf"
"2310.16421","Qinyong Wang","Qinyong Wang, Zhenxiang Gao, Rong Xu","Graph Agent: Explicit Reasoning Agent for Graphs","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks
","2023-10-26","2310.16421v1.pdf"
"2310.16427","Zhen Wang","Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou
  Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu","PromptAgent: Strategic Planning with Language Models Enables
  Expert-level Prompt Optimization","34 pages, 10 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Highly effective, task-specific prompts are often heavily engineered by
experts to integrate detailed instructions and domain insights based on a deep
understanding of both instincts of large language models (LLMs) and the
intricacies of the target task. However, automating the generation of such
expert-level prompts remains elusive. Existing prompt optimization methods tend
to overlook the depth of domain knowledge and struggle to efficiently explore
the vast space of expert-level prompts. Addressing this, we present
PromptAgent, an optimization method that autonomously crafts prompts equivalent
in quality to those handcrafted by experts. At its core, PromptAgent views
prompt optimization as a strategic planning problem and employs a principled
planning algorithm, rooted in Monte Carlo tree search, to strategically
navigate the expert-level prompt space. Inspired by human-like trial-and-error
exploration, PromptAgent induces precise expert-level insights and in-depth
instructions by reflecting on model errors and generating constructive error
feedback. Such a novel framework allows the agent to iteratively examine
intermediate prompts (states), refine them based on error feedbacks (actions),
simulate future rewards, and search for high-reward paths leading to expert
prompts. We apply PromptAgent to 12 tasks spanning three practical domains:
BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing
it significantly outperforms strong Chain-of-Thought and recent prompt
optimization baselines. Extensive analyses emphasize its capability to craft
expert-level, detailed, and domain-insightful prompts with great efficiency and
generalizability.
","2023-10-26","2310.16427v1.pdf"
"2310.16436","Ge Zheng","Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang","DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning
  in Language Models","24 pages, 13 figures, to be published in NeurIPS 2023","","","","cs.CV cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A long-standing goal of AI systems is to perform complex multimodal reasoning
like humans. Recently, large language models (LLMs) have made remarkable
strides in such multi-step reasoning on the language modality solely by
leveraging the chain of thought (CoT) to mimic human thinking. However, the
transfer of these advancements to multimodal contexts introduces heightened
challenges, including but not limited to the impractical need for
labor-intensive annotation and the limitations in terms of flexibility,
generalizability, and explainability. To evoke CoT reasoning in multimodality,
this work first conducts an in-depth analysis of these challenges posed by
multimodality and presents two key insights: ""keeping critical thinking"" and
""letting everyone do their jobs"" in multimodal CoT reasoning. Furthermore, this
study proposes a novel DDCoT prompting that maintains a critical attitude
through negative-space prompting and incorporates multimodality into reasoning
by first dividing the reasoning responsibility of LLMs into reasoning and
recognition and then integrating the visual recognition capability of visual
models into the joint reasoning process. The rationales generated by DDCoT not
only improve the reasoning abilities of both large and small language models in
zero-shot prompting and fine-tuning learning, significantly outperforming
state-of-the-art methods but also exhibit impressive generalizability and
explainability.
","2023-10-27","2310.16436v1.pdf"
"2310.16450","Guanzheng Chen","Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing","CLEX: Continuous Length Extrapolation for Large Language Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transformer-based Large Language Models (LLMs) are pioneering advances in
many natural language processing tasks, however, their exceptional capabilities
are restricted within the preset context window of Transformer. Position
Embedding (PE) scaling methods, while effective in extending the context window
to a specific length, demonstrate either notable limitations in their
extrapolation abilities or sacrificing partial performance within the context
window. Length extrapolation methods, although theoretically capable of
extending the context window beyond the training sequence length, often
underperform in practical long-context applications. To address these
challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We
generalise the PE scaling approaches to model the continuous dynamics by
ordinary differential equations over the length scaling factor, thereby
overcoming the constraints of current PE scaling methods designed for specific
lengths. Moreover, by extending the dynamics to desired context lengths beyond
the training sequence length, CLEX facilitates the length extrapolation with
impressive performance in practical tasks. We demonstrate that CLEX can be
seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such
as LLaMA and GPT-NeoX, with negligible impact on training and inference
latency. Experimental results reveal that CLEX can effectively extend the
context window to over 4x or almost 8x training length, with no deterioration
in performance. Furthermore, when evaluated on the practical LongBench
benchmark, our model trained on a 4k length exhibits competitive performance
against state-of-the-art open-source models trained on context lengths up to
32k.
","2023-10-26","2310.16450v1.pdf"
"2310.16484","Max M\""uller-Eberstein","Max M\""uller-Eberstein, Rob van der Goot, Barbara Plank and Ivan Titov","Subspace Chronicles: How Linguistic Information Emerges, Shifts and
  Interacts during Language Model Training","Accepted at EMNLP 2023 (Findings)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Representational spaces learned via language modeling are fundamental to
Natural Language Processing (NLP), however there has been limited understanding
regarding how and when during training various types of linguistic information
emerge and interact. Leveraging a novel information theoretic probing suite,
which enables direct comparisons of not just task performance, but their
representational subspaces, we analyze nine tasks covering syntax, semantics
and reasoning, across 2M pre-training steps and five seeds. We identify
critical learning phases across tasks and time, during which subspaces emerge,
share information, and later disentangle to specialize. Across these phases,
syntactic knowledge is acquired rapidly after 0.5% of full training. Continued
performance improvements primarily stem from the acquisition of open-domain
knowledge, while semantics and reasoning tasks benefit from later boosts to
long-range contextualization and higher specialization. Measuring cross-task
similarity further reveals that linguistically related tasks share information
throughout training, and do so more during the critical phase of learning than
before or after. Our findings have implications for model interpretability,
multi-task learning, and learning from limited data.
","2023-10-26","2310.16484v1.pdf"
"2310.16492","Sangha Park","Sangha Park, Jisoo Mok, Dahuin Jung, Saehyung Lee, Sungroh Yoon","On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection","Accepted by NeurIPS 2023","","","","cs.CV cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Successful detection of Out-of-Distribution (OoD) data is becoming
increasingly important to ensure safe deployment of neural networks. One of the
main challenges in OoD detection is that neural networks output overconfident
predictions on OoD data, make it difficult to determine OoD-ness of data solely
based on their predictions. Outlier exposure addresses this issue by
introducing an additional loss that encourages low-confidence predictions on
OoD data during training. While outlier exposure has shown promising potential
in improving OoD detection performance, all previous studies on outlier
exposure have been limited to utilizing visual outliers. Drawing inspiration
from the recent advancements in vision-language pre-training, this paper
venture out to the uncharted territory of textual outlier exposure. First, we
uncover the benefits of using textual outliers by replacing real or virtual
outliers in the image-domain with textual equivalents. Then, we propose various
ways of generating preferable textual outliers. Our extensive experiments
demonstrate that generated textual outliers achieve competitive performance on
large-scale OoD and hard OoD benchmarks. Furthermore, we conduct empirical
analyses of textual outliers to provide primary criteria for designing
advantageous textual outliers: near-distribution, descriptiveness, and
inclusion of visual semantics.
","2023-10-26","2310.16492v1.pdf"
"2310.16494","Sebastian Koch","Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi,
  Timo Ropinski","Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph
  prediction","3DV 2024. Project page: https://kochsebastian.com/lang3dsg","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  D scene graphs are an emerging 3D scene representation, that models both the
objects present in the scene as well as their relationships. However, learning
3D scene graphs is a challenging task because it requires not only object
labels but also relationship annotations, which are very scarce in datasets.
While it is widely accepted that pre-training is an effective approach to
improve model performance in low data regimes, in this paper, we find that
existing pre-training methods are ill-suited for 3D scene graphs. To solve this
issue, we present the first language-based pre-training approach for 3D scene
graphs, whereby we exploit the strong relationship between scene graphs and
language. To this end, we leverage the language encoder of CLIP, a popular
vision-language model, to distill its knowledge into our graph-based network.
We formulate a contrastive pre-training, which aligns text embeddings of
relationships (subject-predicate-object triplets) and predicted 3D graph
features. Our method achieves state-of-the-art results on the main semantic 3D
scene graph benchmark by showing improved effectiveness over pre-training
baselines and outperforming all the existing fully supervised scene graph
prediction methods by a significant margin. Furthermore, since our scene graph
features are language-aligned, it allows us to query the language space of the
features in a zero-shot manner. In this paper, we show an example of utilizing
this property of the features to predict the room type of a scene without
further training.
","2023-10-26","2310.16494v1.pdf"
"2310.16499","Rujing Yao","Ou Wu and Rujing Yao","Data Optimization in Deep Learning: A Survey","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large-scale, high-quality data are considered an essential factor for the
successful application of many deep learning techniques. Meanwhile, numerous
real-world deep learning tasks still have to contend with the lack of
sufficient amounts of high-quality data. Additionally, issues such as model
robustness, fairness, and trustworthiness are also closely related to training
data. Consequently, a huge number of studies in the existing literature have
focused on the data aspect in deep learning tasks. Some typical data
optimization techniques include data augmentation, logit perturbation, sample
weighting, and data condensation. These techniques usually come from different
deep learning divisions and their theoretical inspirations or heuristic
motivations may seem unrelated to each other. This study aims to organize a
wide range of existing data optimization methodologies for deep learning from
the previous literature, and makes the effort to construct a comprehensive
taxonomy for them. The constructed taxonomy considers the diversity of split
dimensions, and deep sub-taxonomies are constructed for each dimension. On the
basis of the taxonomy, connections among the extensive data optimization
methods for deep learning are built in terms of four aspects. We probe into
rendering several promising and interesting future directions. The constructed
taxonomy and the revealed connections will enlighten the better understanding
of existing methods and the design of novel data optimization techniques.
Furthermore, our aspiration for this survey is to promote data optimization as
an independent subdivision of deep learning. A curated, up-to-date list of
resources related to data optimization in deep learning is available at
\url{https://github.com/YaoRujing/Data-Optimization}.
","2023-10-26","2310.16499v1.pdf"
"2310.16517","Mingfeng Xue","Mingfeng Xue, Dayiheng Liu, Kexin Yang, Guanting Dong, Wenqiang Lei,
  Zheng Yuan, Chang Zhou, Jingren Zhou","OccuQuest: Mitigating Occupational Bias for Inclusive Large Language
  Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  The emergence of large language models (LLMs) has revolutionized natural
language processing tasks. However, existing instruction-tuning datasets suffer
from occupational bias: the majority of data relates to only a few occupations,
which hampers the instruction-tuned LLMs to generate helpful responses to
professional queries from practitioners in specific fields. To mitigate this
issue and promote occupation-inclusive LLMs, we create an instruction-tuning
dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs
and 30,000+ dialogues covering over 1,000 occupations in 26 occupational
categories. We systematically request ChatGPT, organizing queries
hierarchically based on Occupation, Responsibility, Topic, and Question, to
ensure a comprehensive coverage of occupational specialty inquiries. By
comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we
observe that OccuQuest exhibits a more balanced distribution across
occupations. Furthermore, we assemble three test sets for comprehensive
evaluation, an occu-test set covering 25 occupational categories, an estate set
focusing on real estate, and an occu-quora set containing real-world questions
from Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which
significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and
WizardLM) on professional questions in GPT-4 and human evaluations. Notably, on
the occu-quora set, OccuLLaMA reaches a high win rate of 86.4\% against
WizardLM.
","2023-10-26","2310.16517v1.pdf"
"2310.16520","Yixin Liu","Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, Shirui Pan","Towards Self-Interpretable Graph-Level Anomaly Detection","23 pages; accepted to NeurIPS 2023","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit
notable dissimilarity compared to the majority in a collection. However,
current works primarily focus on evaluating graph-level abnormality while
failing to provide meaningful explanations for the predictions, which largely
limits their reliability and application scope. In this paper, we investigate a
new challenging problem, explainable GLAD, where the learning objective is to
predict the abnormality of each graph sample with corresponding explanations,
i.e., the vital subgraph that leads to the predictions. To address this
challenging problem, we propose a Self-Interpretable Graph aNomaly dETection
model (SIGNET for short) that detects anomalous graphs as well as generates
informative explanations simultaneously. Specifically, we first introduce the
multi-view subgraph information bottleneck (MSIB) framework, serving as the
design basis of our self-interpretable GLAD approach. This way SIGNET is able
to not only measure the abnormality of each graph based on cross-view mutual
information but also provide informative graph rationales by extracting
bottleneck subgraphs from the input graph and its dual hypergraph in a
self-supervised way. Extensive experiments on 16 datasets demonstrate the
anomaly detection capability and self-interpretability of SIGNET.
","2023-10-26","2310.16520v1.pdf"
"2310.16523","Preethi Lahoti","Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi,
  Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex
  Beutel, Jilin Chen","Improving Diversity of Demographic Representation in Large Language
  Models via Collective-Critiques and Self-Voting","To appear at EMNLP 2023 main conference","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  A crucial challenge for generative large language models (LLMs) is diversity:
when a user's prompt is under-specified, models may follow implicit assumptions
while generating a response, which may result in homogenization of the
responses, as well as certain demographic groups being under-represented or
even erased from the generated responses. In this paper, we formalize diversity
of representation in generative LLMs. We present evaluation datasets and
propose metrics to measure diversity in generated responses along people and
culture axes. We find that LLMs understand the notion of diversity, and that
they can reason and critique their own responses for that goal. This finding
motivated a new prompting technique called collective-critique and self-voting
(CCSV) to self-improve people diversity of LLMs by tapping into its diversity
reasoning capabilities, without relying on handcrafted examples or prompt
tuning. Extensive empirical experiments with both human and automated
evaluations show that our proposed approach is effective at improving people
and culture diversity, and outperforms all baseline methods by a large margin.
","2023-10-26","2310.16523v1.pdf"
"2310.16528","Jind\v{r}ich Libovick\'y","Jind\v{r}ich Helcl and Jind\v{r}ich Libovick\'y","CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task
  Information Retrieval","8 pages, 2 figures; System description paper at the MRL 2023 workshop
  at EMNLP 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present the Charles University system for the MRL~2023 Shared Task on
Multi-lingual Multi-task Information Retrieval. The goal of the shared task was
to develop systems for named entity recognition and question answering in
several under-represented languages. Our solutions to both subtasks rely on the
translate-test approach. We first translate the unlabeled examples into English
using a multilingual machine translation model. Then, we run inference on the
translated data using a strong task-specific model. Finally, we project the
labeled data back into the original language. To keep the inferred tags on the
correct positions in the original language, we propose a method based on
scoring the candidate positions using a label-sensitive translation model. In
both settings, we experiment with finetuning the classification models on the
translated data. However, due to a domain mismatch between the development data
and the shared task validation and test sets, the finetuned models could not
outperform our baselines.
","2023-10-26","2310.16528v1.pdf"
"2310.16534","Yang Wu","Yang Wu, Shilong Wang, Hao Yang, Tian Zheng, Hongbo Zhang, Yanyan
  Zhao, Bing Qin","An Early Evaluation of GPT-4V(ision)","Technical Report. Data are available at
  https://github.com/albertwy/GPT-4V-Evaluation","","","","cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we evaluate different abilities of GPT-4V including visual
understanding, language understanding, visual puzzle solving, and understanding
of other modalities such as depth, thermal, video, and audio. To estimate
GPT-4V's performance, we manually construct 656 test instances and carefully
evaluate the results of GPT-4V. The highlights of our findings are as follows:
(1) GPT-4V exhibits impressive performance on English visual-centric benchmarks
but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows
inconsistent refusal behavior when answering questions related to sensitive
traits such as gender, race, and age; (3) GPT-4V obtains worse results than
GPT-4 (API) on language understanding tasks including general language
understanding benchmarks and visual commonsense knowledge evaluation
benchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both
visual understanding and language understanding; (5) GPT-4V struggles to find
the nuances between two similar images and solve the easy math picture puzzles;
(6) GPT-4V shows non-trivial performance on the tasks of similar modalities to
image, such as video and thermal. Our experimental results reveal the ability
and limitations of GPT-4V and we hope our paper can provide some insights into
the application and research of GPT-4V.
","2023-10-26","2310.16534v1.pdf"
"2310.16535","Qingyuan Tian","Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, Yunshi Lan","R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought
  Reasoning in Large Language Models under Noisy Context","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the help of Chain-of-Thought (CoT) prompting, Large Language Models
(LLMs) have achieved remarkable performance on various reasoning tasks.
However, most of them have been evaluated under noise-free context and the
dilemma for LLMs to produce inaccurate results under the noisy context has not
been fully investigated. Existing studies utilize trigger sentences to
encourage LLMs to concentrate on the relevant information but the trigger has
limited effect on final answer prediction. Inspired by interactive CoT method,
where intermediate reasoning steps are promoted by multiple rounds of
interaction between users and LLMs, we propose a novel prompting method, namely
R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$
prompting interacts with LLMs to perform key sentence extraction, variable
declaration and answer prediction, which corresponds to a thought process of
reviewing, rephrasing and resolving. The responses generated at the last
interaction will perform as hints to guide toward the responses of the next
interaction. Our experiments show that R$^3$ prompting significantly
outperforms existing CoT prompting methods on five reasoning tasks under noisy
context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on
the reasoning tasks under noisy context compared to the most competitive
prompting baseline. More analyses and ablation studies show the robustness and
generalization of R$^3$ prompting method in solving reasoning tasks in LLMs
under noisy context.
","2023-10-26","2310.16535v1.pdf"
"2310.16538","Jaemin Shin","Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu,
  Jinho D. Choi, Sung-Ju Lee","FedTherapist: Mental Health Monitoring with User-Generated Linguistic
  Expressions on Smartphones via Federated Learning","Accepted to the 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2023)","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Psychiatrists diagnose mental disorders via the linguistic use of patients.
Still, due to data privacy, existing passive mental health monitoring systems
use alternative features such as activity, app usage, and location via mobile
devices. We propose FedTherapist, a mobile mental health monitoring system that
utilizes continuous speech and keyboard input in a privacy-preserving way via
federated learning. We explore multiple model designs by comparing their
performance and overhead for FedTherapist to overcome the complex nature of
on-device language model training on smartphones. We further propose a
Context-Aware Language Learning (CALL) methodology to effectively utilize
smartphones' large and noisy text for mental health signal sensing. Our
IRB-approved evaluation of the prediction of self-reported depression, stress,
anxiety, and mood from 46 participants shows higher accuracy of FedTherapist
compared with the performance with non-language features, achieving 0.15 AUROC
improvement and 8.21% MAE reduction.
","2023-10-26","2310.16538v1.pdf"
"2310.16568","Palak Jain","Palak Jain, Livio Baldini Soares, Tom Kwiatkowski","1-PAGER: One Pass Answer Generation and Evidence Retrieval","Accepted at EMNLP 2023 (Findings)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present 1-Pager the first system that answers a question and retrieves
evidence using a single Transformer-based model and decoding process. 1-Pager
incrementally partitions the retrieval corpus using constrained decoding to
select a document and answer string, and we show that this is competitive with
comparable retrieve-and-read alternatives according to both retrieval and
answer accuracy metrics. 1-Pager also outperforms the equivalent closed-book
question answering model, by grounding predictions in an evidence corpus. While
1-Pager is not yet on-par with more expensive systems that read many more
documents before generating an answer, we argue that it provides an important
step toward attributed generation by folding retrieval into the
sequence-to-sequence paradigm that is currently dominant in NLP. We also show
that the search paths used to partition the corpus are easy to read and
understand, paving a way forward for interpretable neural retrieval.
","2023-10-26","2310.16568v1.pdf"
"2310.16570","Paul Youssef","Paul Youssef, Osman Alperen Kora\c{s}, Meijie Li, J\""org
  Schl\""otterer, Christin Seifert","Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained
  Language Models","Accepted at EMNLP Findings 2023","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich
in world knowledge. This fact has sparked the interest of the community in
quantifying the amount of factual knowledge present in PLMs, as this explains
their performance on downstream tasks, and potentially justifies their use as
knowledge bases. In this work, we survey methods and datasets that are used to
probe PLMs for factual knowledge. Our contributions are: (1) We propose a
categorization scheme for factual probing methods that is based on how their
inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of
the datasets used for factual probing; (3) We synthesize insights about
knowledge retention and prompt optimization in PLMs, analyze obstacles to
adopting PLMs as knowledge bases and outline directions for future work.
","2023-10-26","2310.16570v1.pdf"
"2310.16582","Tianlong Li","Tianlong Li, Xiaoqing Zheng and Xuanjing Huang","Tailoring Personality Traits in Large Language Models via
  Unsupervisedly-Built Personalized Lexicons","Work in progress","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Personality plays a pivotal role in shaping human expression patterns, and
empowering and manipulating large language models (LLMs) with personality
traits holds significant promise in enhancing the user experience of LLMs.
However, prior approaches either rely on fine-tuning LLMs on a corpus enriched
with personalized expressions or necessitate the manual crafting of prompts to
induce LLMs to produce personalized responses. The former approaches demand
substantial time and resources for collecting sufficient training examples
while the latter might fail in enabling the precise manipulation of the
personality traits at a fine-grained level (e.g., achieving high agreeableness
while reducing openness). In this study, we introduce a novel approach for
tailoring personality traits within LLMs, allowing for the incorporation of any
combination of the Big Five factors (i.e., openness, conscientiousness,
extraversion, agreeableness, and neuroticism) in a pluggable manner. This is
achieved by employing a set of Unsupervisedly-Built Personalized Lexicons
(UBPL) that are utilized to adjust the probability of the next token predicted
by the original LLMs during the decoding phase. This adjustment encourages the
models to generate words present in the personalized lexicons while preserving
the naturalness of the generated texts. Extensive experimentation demonstrates
the effectiveness of our approach in finely manipulating LLMs' personality
traits. Furthermore, our method can be seamlessly integrated into other LLMs
without necessitating updates to their parameters.
","2023-10-26","2310.16582v1.pdf"
"2310.16639","Jessica Maria Echterhoff","Jessica Echterhoff, An Yan, Kyungtae Han, Amr Abdelraouf, Rohit Gupta,
  Julian McAuley","Driving through the Concept Gridlock: Unraveling Explainability
  Bottlenecks in Automated Driving","","","","","cs.CV cs.LG","http://creativecommons.org/licenses/by/4.0/","  Concept bottleneck models have been successfully used for explainable machine
learning by encoding information within the model with a set of human-defined
concepts. In the context of human-assisted or autonomous driving,
explainability models can help user acceptance and understanding of decisions
made by the autonomous vehicle, which can be used to rationalize and explain
driver or vehicle behavior. We propose a new approach using concept bottlenecks
as visual features for control command predictions and explanations of user and
vehicle behavior. We learn a human-understandable concept layer that we use to
explain sequential driving scenes while learning vehicle control commands. This
approach can then be used to determine whether a change in a preferred gap or
steering commands from a human (or autonomous vehicle) is led by an external
stimulus or change in preferences. We achieve competitive performance to latent
visual features while gaining interpretability within our model setup.
","2023-10-27","2310.16639v1.pdf"
"2310.16640","Niki Maria Foteinopoulou","Niki Maria Foteinopoulou, Ioannis Patras","EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression
  Recognition","10 pages, 3 figures","","","","cs.CV cs.HC","http://creativecommons.org/licenses/by/4.0/","  Facial Expression Recognition (FER) is a crucial task in affective computing,
but its conventional focus on the seven basic emotions limits its applicability
to the complex and expanding emotional spectrum. To address the issue of new
and unseen emotions present in dynamic in-the-wild FER, we propose a novel
vision-language model that utilises sample-level text descriptions (i.e.
captions of the context, expressions or emotional cues) as natural language
supervision, aiming to enhance the learning of rich latent representations, for
zero-shot classification. To test this, we evaluate using zero-shot
classification of the model trained on sample-level descriptions on four
popular dynamic FER datasets. Our findings show that this approach yields
significant improvements when compared to baseline methods. Specifically, for
zero-shot video FER, we outperform CLIP by over 10\% in terms of Weighted
Average Recall and 5\% in terms of Unweighted Average Recall on several
datasets. Furthermore, we evaluate the representations obtained from the
network trained using sample-level descriptions on the downstream task of
mental health symptom estimation, achieving performance comparable or superior
to state-of-the-art methods and strong agreement with human experts. Namely, we
achieve a Pearson's Correlation Coefficient of up to 0.85 on schizophrenia
symptom severity estimation, which is comparable to human experts' agreement.
The code is publicly available at: https://github.com/NickyFot/EmoCLIP.
","2023-10-26","2310.16640v1.pdf"
"2310.16654","Boda Lin","Boda Lin, Xinyi Zhou, Binghao Tang, Xiaocheng Gong, Si Li","ChatGPT is a Potential Zero-Shot Dependency Parser","10 pages","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Pre-trained language models have been widely used in dependency parsing task
and have achieved significant improvements in parser performance. However, it
remains an understudied question whether pre-trained language models can
spontaneously exhibit the ability of dependency parsing without introducing
additional parser structure in the zero-shot scenario. In this paper, we
propose to explore the dependency parsing ability of large language models such
as ChatGPT and conduct linguistic analysis. The experimental results
demonstrate that ChatGPT is a potential zero-shot dependency parser, and the
linguistic analysis also shows some unique preferences in parsing outputs.
","2023-10-26","2310.16654v1.pdf"
"2310.16656","Dani Valevski","Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, Yaniv
  Leviathan","A Picture is Worth a Thousand Words: Principled Recaptioning Improves
  Image Generation","","","","","cs.CV cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Text-to-image diffusion models achieved a remarkable leap in capabilities
over the last few years, enabling high-quality and diverse synthesis of images
from a textual prompt. However, even the most advanced models often struggle to
precisely follow all of the directions in their prompts. The vast majority of
these models are trained on datasets consisting of (image, caption) pairs where
the images often come from the web, and the captions are their HTML alternate
text. A notable example is the LAION dataset, used by Stable Diffusion and
other models. In this work we observe that these captions are often of low
quality, and argue that this significantly affects the model's capability to
understand nuanced semantics in the textual prompts. We show that by relabeling
the corpus with a specialized automatic captioning model and training a
text-to-image model on the recaptioned dataset, the model benefits
substantially across the board. First, in overall image quality: e.g. FID 14.84
vs. the baseline of 17.87, and 64.3% improvement in faithful image generation
according to human evaluation. Second, in semantic alignment, e.g. semantic
object accuracy 84.34 vs. 78.90, counting alignment errors 1.32 vs. 1.44 and
positional alignment 62.42 vs. 57.60. We analyze various ways to relabel the
corpus and provide evidence that this technique, which we call RECAP, both
reduces the train-inference discrepancy and provides the model with more
information per example, increasing sample efficiency and allowing the model to
better understand the relations between captions and images.
","2023-10-26","2310.16656v1.pdf"
"2310.16675","Jiechen Chen","Jiechen Chen, Sangwoo Park, and Osvaldo Simeone","Agreeing to Stop: Reliable Latency-Adaptive Decision Making via
  Ensembles of Spiking Neural Networks","Under review","","","","cs.NE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Spiking neural networks (SNNs) are recurrent models that can leverage
sparsity in input time series to efficiently carry out tasks such as
classification. Additional efficiency gains can be obtained if decisions are
taken as early as possible as a function of the complexity of the input time
series. The decision on when to stop inference and produce a decision must rely
on an estimate of the current accuracy of the decision. Prior work demonstrated
the use of conformal prediction (CP) as a principled way to quantify
uncertainty and support adaptive-latency decisions in SNNs. In this paper, we
propose to enhance the uncertainty quantification capabilities of SNNs by
implementing ensemble models for the purpose of improving the reliability of
stopping decisions. Intuitively, an ensemble of multiple models can decide when
to stop more reliably by selecting times at which most models agree that the
current accuracy level is sufficient. The proposed method relies on different
forms of information pooling from ensemble models, and offers theoretical
reliability guarantees. We specifically show that variational inference-based
ensembles with p-variable pooling significantly reduce the average latency of
state-of-the-art methods, while maintaining reliability guarantees.
","2023-10-26","2310.16675v1.pdf"
"2310.16681","Anthony Rios","Xingmeng Zhao, Tongnian Wang, Sheri Osborn, Anthony Rios","BabyStories: Can Reinforcement Learning Teach Baby Language Models to
  Write Better Stories?","Accepted to BabyLM workshop at CoNLL","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Language models have seen significant growth in the size of their corpus,
leading to notable performance improvements. Yet, there has been limited
progress in developing models that handle smaller, more human-like datasets. As
part of the BabyLM shared task, this study explores the impact of reinforcement
learning from human feedback (RLHF) on language models pretrained from scratch
with a limited training corpus. Comparing two GPT-2 variants, the larger model
performs better in storytelling tasks after RLHF fine-tuning. These findings
suggest that RLHF techniques may be more advantageous for larger models due to
their higher learning and adaptation capacity, though more experiments are
needed to confirm this finding. These insights highlight the potential benefits
of RLHF fine-tuning for language models within limited data, enhancing their
ability to maintain narrative focus and coherence while adhering better to
initial instructions in storytelling tasks. The code for this work is publicly
at https://github.com/Zephyr1022/BabyStories-UTSA.
","2023-10-26","2310.16681v1.pdf"
"2310.16685","Amanda Iaquinta","Amanda Ferrari Iaquinta, Gustavo Voltani von Atzingen","Detection of news written by the ChatGPT through authorship attribution
  performed by a Bidirectional LSTM model","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The large language based-model chatbot ChatGPT gained a lot of popularity
since its launch and has been used in a wide range of situations. This research
centers around a particular situation, when the ChatGPT is used to produce news
that will be consumed by the population, causing the facilitation in the
production of fake news, spread of misinformation and lack of trust in news
sources. Aware of these problems, this research aims to build an artificial
intelligence model capable of performing authorship attribution on news
articles, identifying the ones written by the ChatGPT. To achieve this goal, a
dataset containing equal amounts of human and ChatGPT written news was
assembled and different natural processing language techniques were used to
extract features from it that were used to train, validate and test three
models built with different techniques. The best performance was produced by
the Bidirectional Long Short Term Memory (LSTM) Neural Network model, achiving
91.57\% accuracy when tested against the data from the testing set.
","2023-10-26","2310.16685v1.pdf"
"2310.16712","Ganesh Jawahar","Ganesh Jawahar, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Dujian
  Ding","LLM Performance Predictors are good initializers for Architecture Search","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have become an integral component in solving a
wide range of NLP tasks. In this work, we explore a novel use case of using
LLMs to build performance predictors (PP): models that, given a specific deep
neural network architecture, predict its performance on a downstream task. We
design PP prompts for LLMs consisting of: (i) role: description of the role
assigned to the LLM, (ii) instructions: set of instructions to be followed by
the LLM to carry out performance prediction, (iii) hyperparameters: a
definition of each architecture-specific hyperparameter and (iv)
demonstrations: sample architectures along with their efficiency metrics and
'training from scratch' performance. For machine translation (MT) tasks, we
discover that GPT-4 with our PP prompts (LLM-PP) can predict the performance of
architecture with a mean absolute error matching the SOTA and a marginal
degradation in rank correlation coefficient compared to SOTA performance
predictors. Further, we show that the predictions from LLM-PP can be distilled
to a small regression model (LLM-Distill-PP). LLM-Distill-PP models
surprisingly retain the performance of LLM-PP largely and can be a
cost-effective alternative for heavy use cases of performance estimation.
Specifically, for neural architecture search (NAS), we propose a Hybrid-Search
algorithm for NAS (HS-NAS), which uses LLM-Distill-PP for the initial part of
search, resorting to the baseline predictor for rest of the search. We show
that HS-NAS performs very similar to SOTA NAS across benchmarks, reduces search
hours by 50% roughly, and in some cases, improves latency, GFLOPs, and model
size.
","2023-10-26","2310.16712v1.pdf"
"2310.16713","Tianwen Wei","Liu Yang, Haihua Yang, Wenjun Cheng, Lei Lin, Chenxia Li, Yifu Chen,
  Lunan Liu, Jianfei Pan, Tianwen Wei, Biye Li, Liang Zhao, Lijie Wang, Bo Zhu,
  Guoliang Li, Xuejie Wu, Xilin Luo, Rui Hu","SkyMath: Technical Report","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have shown great potential to solve varieties of
natural language processing (NLP) tasks, including mathematical reasoning. In
this work, we present SkyMath, a large language model for mathematics with 13
billion parameters. By applying self-compare fine-tuning, we have enhanced
mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K,
SkyMath outperforms all known open-source models of similar size and has
established a new SOTA performance.
","2023-10-27","2310.16713v1.pdf"
"2310.16730","Dong-Ki Kim","Dong-Ki Kim, Sungryull Sohn, Lajanugen Logeswaran, Dongsub Shim,
  Honglak Lee","MultiPrompter: Cooperative Prompt Optimization with Multi-Agent
  Reinforcement Learning","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, there has been an increasing interest in automated prompt
optimization based on reinforcement learning (RL). This approach offers
important advantages, such as generating interpretable prompts and being
compatible with black-box foundation models. However, the substantial prompt
space size poses challenges for RL-based methods, often leading to suboptimal
policy convergence. This paper introduces MultiPrompter, a new framework that
views prompt optimization as a cooperative game between prompters which take
turns composing a prompt together. Our cooperative prompt optimization
effectively reduces the problem size and helps prompters learn optimal prompts.
We test our method on the text-to-image task and show its ability to generate
higher-quality images than baselines.
","2023-10-26","2310.16730v1.pdf"
"2310.16731","Roshanak Mirzaee","Roshanak Mirzaee, Parisa Kordjamshidi","Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning","Accepted in EMNLP-Finding 2023","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Spatial reasoning over text is challenging as the models not only need to
extract the direct spatial information from the text but also reason over those
and infer implicit spatial relations. Recent studies highlight the struggles
even large language models encounter when it comes to performing spatial
reasoning over text. In this paper, we explore the potential benefits of
disentangling the processes of information extraction and reasoning in models
to address this challenge. To explore this, we design various models that
disentangle extraction and reasoning(either symbolic or neural) and compare
them with state-of-the-art(SOTA) baselines with no explicit design for these
parts. Our experimental results consistently demonstrate the efficacy of
disentangling, showcasing its ability to enhance models' generalizability
within realistic data domains.
","2023-10-26","2310.16731v1.pdf"
"2310.16738","Xi Wang","Xi Wang, Hossein A. Rahmani, Jiqun Liu, Emine Yilmaz","Improving Conversational Recommendation Systems via Bias Analysis and
  Language-Model-Enhanced Data Augmentation","Accepted by EMNLP 2023 (Findings)","","","","cs.CL cs.IR","http://creativecommons.org/licenses/by/4.0/","  Conversational Recommendation System (CRS) is a rapidly growing research area
that has gained significant attention alongside advancements in language
modelling techniques. However, the current state of conversational
recommendation faces numerous challenges due to its relative novelty and
limited existing contributions. In this study, we delve into benchmark datasets
for developing CRS models and address potential biases arising from the
feedback loop inherent in multi-turn interactions, including selection bias and
multiple popularity bias variants. Drawing inspiration from the success of
generative data via using language models and data augmentation techniques, we
present two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model
performance while mitigating biases. Through extensive experiments on ReDial
and TG-ReDial benchmark datasets, we show a consistent improvement of CRS
techniques with our data augmentation approaches and offer additional insights
on addressing multiple newly formulated biases.
","2023-10-26","2310.16738v1.pdf"
"2310.16746","Nafis Irtiza Tripto","Nafis Irtiza Tripto, Adaku Uchendu, Thai Le, Mattia Setzu, Fosca
  Giannotti, Dongwon Lee","HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis","9 pages, EMNLP-23 findings, 5 pages appendix, 6 figures, 17 tables","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Authorship Analysis, also known as stylometry, has been an essential aspect
of Natural Language Processing (NLP) for a long time. Likewise, the recent
advancement of Large Language Models (LLMs) has made authorship analysis
increasingly crucial for distinguishing between human-written and AI-generated
texts. However, these authorship analysis tasks have primarily been focused on
written texts, not considering spoken texts. Thus, we introduce the largest
benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark).
HANSEN encompasses meticulous curation of existing speech datasets accompanied
by transcripts, alongside the creation of novel AI-generated spoken text
datasets. Together, it comprises 17 human datasets, and AI-generated spoken
texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To
evaluate and demonstrate the utility of HANSEN, we perform Authorship
Attribution (AA) & Author Verification (AV) on human-spoken datasets and
conducted Human vs. AI spoken text detection using state-of-the-art (SOTA)
models. While SOTA methods, such as, character ngram or Transformer-based
model, exhibit similar AA & AV performance in human-spoken datasets compared to
written ones, there is much room for improvement in AI-generated spoken text
detection. The HANSEN benchmark is available at:
https://huggingface.co/datasets/HANSEN-REPO/HANSEN.
","2023-10-26","2310.16746v1.pdf"
"2310.16755","Naihao Deng","Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, Naihao
  Deng","HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning
  in Large Language Models","Accepted at Findings of EMNLP 2023","Findings of EMNLP 2023","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-sa/4.0/","  Theory of Mind (ToM) is the ability to reason about one's own and others'
mental states. ToM plays a critical role in the development of intelligence,
language understanding, and cognitive processes. While previous work has
primarily focused on first and second-order ToM, we explore higher-order ToM,
which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a
Higher Order Theory of Mind benchmark. Our experimental evaluation using
various Large Language Models (LLMs) indicates a decline in performance on
higher-order ToM tasks, demonstrating the limitations of current LLMs. We
conduct a thorough analysis of different failure cases of LLMs, and share our
thoughts on the implications of our findings on the future of NLP.
","2023-10-26","2310.16755v1.pdf"
"2310.16761","Ashim Gupta","Bhavuk Singhal, Ashim Gupta, Shivasankaran V P, Amrith Krishna","IntenDD: A Unified Contrastive Learning Approach for Intent Detection
  and Discovery","EMNLP 2023 Findings","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Identifying intents from dialogue utterances forms an integral component of
task-oriented dialogue systems. Intent-related tasks are typically formulated
either as a classification task, where the utterances are classified into
predefined categories or as a clustering task when new and previously unknown
intent categories need to be discovered from these utterances. Further, the
intent classification may be modeled in a multiclass (MC) or multilabel (ML)
setup. While typically these tasks are modeled as separate tasks, we propose
IntenDD, a unified approach leveraging a shared utterance encoding backbone.
IntenDD uses an entirely unsupervised contrastive learning strategy for
representation learning, where pseudo-labels for the unlabeled utterances are
generated based on their lexical features. Additionally, we introduce a
two-step post-processing setup for the classification tasks using modified
adsorption. Here, first, the residuals in the training data are propagated
followed by smoothing the labels both modeled in a transductive setting.
Through extensive evaluations on various benchmark datasets, we find that our
approach consistently outperforms competitive baselines across all three tasks.
On average, IntenDD reports percentage improvements of 2.32%, 1.26%, and 1.52%
in their respective metrics for few-shot MC, few-shot ML, and the intent
discovery tasks respectively.
","2023-10-26","2310.16761v1.pdf"
"2310.16763","Gabriel Mukobi","Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim, Gitta
  Kutyniok, Kush Bhatia, Silas Alberti","SuperHF: Supervised Iterative Learning from Human Feedback","Accepted to the Socially Responsible Language Modelling Research
  (SoLaR) workshop at NeurIPS 2023","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  While large language models demonstrate remarkable capabilities, they often
present challenges in terms of safety, alignment with human values, and
stability during training. Here, we focus on two prevalent methods used to
align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning
from Human Feedback (RLHF). SFT is simple and robust, powering a host of
open-source models, while RLHF is a more sophisticated method used in top-tier
models like ChatGPT but also suffers from instability and susceptibility to
reward hacking. We propose a novel approach, Supervised Iterative Learning from
Human Feedback (SuperHF), which seeks to leverage the strengths of both
methods. Our hypothesis is two-fold: that the reward model used in RLHF is
critical for efficient data use and model generalization and that the use of
Proximal Policy Optimization (PPO) in RLHF may not be necessary and could
contribute to instability issues. SuperHF replaces PPO with a simple supervised
loss and a Kullback-Leibler (KL) divergence prior. It creates its own training
data by repeatedly sampling a batch of model outputs and filtering them through
the reward model in an online learning regime. We then break down the reward
optimization problem into three components: robustly optimizing the training
rewards themselves, preventing reward hacking-exploitation of the reward model
that degrades model performance-as measured by a novel METEOR similarity
metric, and maintaining good performance on downstream evaluations. Our
experimental results show SuperHF exceeds PPO-based RLHF on the training
objective, easily and favorably trades off high reward with low reward hacking,
improves downstream calibration, and performs the same on our GPT-4 based
qualitative evaluation scheme all the while being significantly simpler to
implement, highlighting SuperHF's potential as a competitive language model
alignment technique.
","2023-10-26","2310.16763v1.pdf"
"2310.16776","Devleena Das","Devleena Das, Vivek Khetan","DEFT: Data Efficient Fine-Tuning for Large Language Models via
  Unsupervised Core-Set Selection","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances have led to the availability of many pre-trained language
models (PLMs); however, a question that remains is how much data is truly
needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT,
a data-efficient fine-tuning framework that leverages unsupervised core-set
selection to minimize the amount of data needed to fine-tune PLMs for
downstream tasks. We demonstrate the efficacy of our DEFT framework in the
context of text-editing LMs, and compare to the state-of-the art text-editing
model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT
models are just as accurate as CoEDIT while being finetuned on ~70% less data.
","2023-10-27","2310.16776v1.pdf"
"2310.16787","Niklas Muennighoff","Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien
  Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara,
  Kartik Perisetla, Xinyi (Alexis) Wu, Enrico Shippole, Kurt Bollacker,
  Tongshuang Wu, Luis Villa, Sandy Pentland, Deb Roy, Sara Hooker","The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing
  & Attribution in AI","30 pages (18 main), 6 figures, 5 tables","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The race to train language models on vast, diverse, and inconsistently
documented datasets has raised pressing concerns about the legal and ethical
risks for practitioners. To remedy these practices threatening data
transparency and understanding, we convene a multi-disciplinary effort between
legal and machine learning experts to systematically audit and trace 1800+ text
datasets. We develop tools and standards to trace the lineage of these
datasets, from their source, creators, series of license conditions,
properties, and subsequent use. Our landscape analysis highlights the sharp
divides in composition and focus of commercially open vs closed datasets, with
closed datasets monopolizing important categories: lower resource languages,
more creative tasks, richer topic variety, newer and more synthetic training
data. This points to a deepening divide in the types of data that are made
available under different license conditions, and heightened implications for
jurisdictional legal interpretations of copyright and fair use. We also observe
frequent miscategorization of licenses on widely used dataset hosting sites,
with license omission of 72%+ and error rates of 50%+. This points to a crisis
in misattribution and informed use of the most popular datasets driving many
recent breakthroughs. As a contribution to ongoing improvements in dataset
transparency and responsible use, we release our entire audit, with an
interactive UI, the Data Provenance Explorer, which allows practitioners to
trace and filter on data provenance for the most popular open source finetuning
data collections: www.dataprovenance.org.
","2023-10-26","2310.16787v1.pdf"
"2310.16789","Weijia Shi","Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu,
  Terra Blevins, Danqi Chen, Luke Zettlemoyer","Detecting Pretraining Data from Large Language Models","","","","","cs.CL cs.CR cs.LG","http://creativecommons.org/licenses/by/4.0/","  Although large language models (LLMs) are widely deployed, the data used to
train them is rarely disclosed. Given the incredible scale of this data, up to
trillions of tokens, it is all but certain that it includes potentially
problematic text such as copyrighted materials, personally identifiable
information, and test data for widely reported reference benchmarks. However,
we currently have no way to know which data of these types is included or in
what proportions. In this paper, we study the pretraining data detection
problem: given a piece of text and black-box access to an LLM without knowing
the pretraining data, can we determine if the model was trained on the provided
text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that
uses data created before and after model training to support gold truth
detection. We also introduce a new detection method Min-K% Prob based on a
simple hypothesis: an unseen example is likely to contain a few outlier words
with low probabilities under the LLM, while a seen example is less likely to
have words with such low probabilities. Min-K% Prob can be applied without any
knowledge about the pretraining corpus or any additional training, departing
from previous detection methods that require training a reference model on data
that is similar to the pretraining data. Moreover, our experiments demonstrate
that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous
methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book
detection, and contaminated downstream example detection, and find it a
consistently effective solution.
","2023-10-26","2310.16789v1.pdf"
"2310.16792","Lingda Li","Lingda Li, Thomas Flynn, Adolfy Hoisie","Learning Independent Program and Architecture Representations for
  Generalizable Performance Modeling","","","","","cs.LG cs.AR","http://creativecommons.org/licenses/by/4.0/","  This paper proposes PerfVec, a novel deep learning-based performance modeling
framework that learns high-dimensional, independent/orthogonal program and
microarchitecture representations. Once learned, a program representation can
be used to predict its performance on any microarchitecture, and likewise, a
microarchitecture representation can be applied in the performance prediction
of any program. Additionally, PerfVec yields a foundation model that captures
the performance essence of instructions, which can be directly used by
developers in numerous performance modeling related tasks without incurring its
training cost. The evaluation demonstrates that PerfVec is more general,
efficient, and accurate than previous approaches.
","2023-10-26","2310.16792v1.pdf"
"2310.16795","Elias Frantar","Elias Frantar and Dan Alistarh","QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Mixture-of-Experts (MoE) architectures offer a general solution to the high
inference costs of large language models (LLMs) via sparse routing, bringing
faster and more accurate models, at the cost of massive parameter counts. For
example, the SwitchTransformer-c2048 model has 1.6 trillion parameters,
requiring 3.2TB of accelerator memory to run efficiently, which makes practical
deployment challenging and expensive. In this paper, we present a solution to
this memory problem, in form of a new compression and execution framework
called QMoE. Specifically, QMoE consists of a scalable algorithm which
accurately compresses trillion-parameter MoEs to less than 1 bit per parameter,
in a custom format co-designed with bespoke GPU decoding kernels to facilitate
efficient end-to-end compressed inference, with minor runtime overheads
relative to uncompressed execution. Concretely, QMoE can compress the 1.6
trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x
compression, 0.8 bits per parameter) at only minor accuracy loss, in less than
a day on a single GPU. This enables, for the first time, the execution of a
trillion-parameter model on affordable commodity hardware, like a single server
with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead
relative to ideal uncompressed inference. The source code and compressed models
are available at github.com/IST-DASLab/qmoe.
","2023-10-26","2310.16795v1.pdf"
"2310.16803","Saiteja Utpala","Saiteja Utpala, Alex Gu, Pin Yu Chen","Language Agnostic Code Embeddings","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently, code language models have achieved notable advancements in
addressing a diverse array of essential code comprehension and generation
tasks. Yet, the field lacks a comprehensive deep dive and understanding of the
code embeddings of multilingual code models. In this paper, we present a
comprehensive study on multilingual code embeddings, focusing on the
cross-lingual capabilities of these embeddings across different programming
languages. Through probing experiments, we demonstrate that code embeddings
comprise two distinct components: one deeply tied to the nuances and syntax of
a specific language, and the other remaining agnostic to these details,
primarily focusing on semantics. Further, we show that when we isolate and
eliminate this language-specific component, we witness significant improvements
in downstream code retrieval tasks, leading to an absolute increase of up to
+17 in the Mean Reciprocal Rank (MRR).
","2023-10-26","2310.16803v1.pdf"
"2310.16809","Yongxin Shi","Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen,
  Chongyu Liu, Yuyi Zhang, Lianwen Jin","Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and
  In-depth Evaluation","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  This paper presents a comprehensive evaluation of the Optical Character
Recognition (OCR) capabilities of the recently released GPT-4V(ision), a Large
Multimodal Model (LMM). We assess the model's performance across a range of OCR
tasks, including scene text recognition, handwritten text recognition,
handwritten mathematical expression recognition, table structure recognition,
and information extraction from visually-rich document. The evaluation reveals
that GPT-4V performs well in recognizing and understanding Latin contents, but
struggles with multilingual scenarios and complex tasks. Based on these
observations, we delve deeper into the necessity of specialized OCR models and
deliberate on the strategies to fully harness the pretrained general LMMs like
GPT-4V for OCR downstream tasks. The study offers a critical reference for
future research in OCR with LMMs. Evaluation pipeline and results are available
at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.
","2023-10-26","2310.16809v1.pdf"
"2310.16810","Yongxin Zhou","Yongxin Zhou, Fabien Ringeval, Fran\c{c}ois Portet","Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT
  and GPT-4 for Dialogue Summarization","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  This study explores the capabilities of prompt-driven Large Language Models
(LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue
summarization. Experiments employed DialogSum (English social conversations)
and DECODA (French call center interactions), testing various prompts:
including prompts from existing literature and those from human summarization
guidelines, as well as a two-step prompt approach. Our findings indicate that
GPT models often produce lengthy summaries and deviate from human summarization
guidelines. However, using human guidelines as an intermediate step shows
promise, outperforming direct word-length constraint prompts in some cases. The
results reveal that GPT models exhibit unique stylistic tendencies in their
summaries. While BERTScores did not dramatically decrease for GPT outputs
suggesting semantic similarity to human references and specialised pre-trained
models, ROUGE scores reveal grammatical and lexical disparities between
GPT-generated and human-written summaries. These findings shed light on the
capabilities and limitations of GPT models in following human instructions for
dialogue summarization.
","2023-10-26","2310.16810v1.pdf"
"2310.16818","Jingxiang Sun","Jingxiang Sun and Bo Zhang and Ruizhi Shao and Lizhen Wang and Wen Liu
  and Zhenda Xie and Yebin Liu","DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion
  Prior","Project Page: https://mrtornado24.github.io/DreamCraft3D/","","","","cs.CV cs.CG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present DreamCraft3D, a hierarchical 3D content generation method that
produces high-fidelity and coherent 3D objects. We tackle the problem by
leveraging a 2D reference image to guide the stages of geometry sculpting and
texture boosting. A central focus of this work is to address the consistency
issue that existing works encounter. To sculpt geometries that render
coherently, we perform score distillation sampling via a view-dependent
diffusion model. This 3D prior, alongside several training strategies,
prioritizes the geometry consistency but compromises the texture fidelity. We
further propose Bootstrapped Score Distillation to specifically boost the
texture. We train a personalized diffusion model, Dreambooth, on the augmented
renderings of the scene, imbuing it with 3D knowledge of the scene being
optimized. The score distillation from this 3D-aware diffusion prior provides
view-consistent guidance for the scene. Notably, through an alternating
optimization of the diffusion prior and 3D scene representation, we achieve
mutually reinforcing improvements: the optimized 3D scene aids in training the
scene-specific diffusion model, which offers increasingly view-consistent
guidance for 3D optimization. The optimization is thus bootstrapped and leads
to substantial texture boosting. With tailored 3D priors throughout the
hierarchical generation, DreamCraft3D generates coherent 3D objects with
photorealistic renderings, advancing the state-of-the-art in 3D content
generation. Code available at https://github.com/deepseek-ai/DreamCraft3D.
","2023-10-27","2310.16818v1.pdf"
"2310.16825","Aaron Gokaslan","Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin,
  Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, Volodymyr
  Kuleshov","CommonCanvas: An Open Diffusion Model Trained with Creative-Commons
  Images","","","","","cs.CV cs.CY","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We assemble a dataset of Creative-Commons-licensed (CC) images, which we use
to train a set of open diffusion models that are qualitatively competitive with
Stable Diffusion 2 (SD2). This task presents two challenges: (1)
high-resolution CC images lack the captions necessary to train text-to-image
generative models; (2) CC images are relatively scarce. In turn, to address
these challenges, we use an intuitive transfer learning technique to produce a
set of high-quality synthetic captions paired with curated CC images. We then
develop a data- and compute-efficient training recipe that requires as little
as 3% of the LAION-2B data needed to train existing SD2 models, but obtains
comparable quality. These results indicate that we have a sufficient number of
CC images (~70 million) for training high-quality models. Our training recipe
also implements a variety of optimizations that achieve ~3X training speed-ups,
enabling rapid model iteration. We leverage this recipe to train several
high-quality text-to-image models, which we dub the CommonCanvas family. Our
largest model achieves comparable performance to SD2 on a human evaluation,
despite being trained on our CC dataset that is significantly smaller than
LAION and using synthetic captions for training. We release our models, data,
and code at
https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md
","2023-10-26","2310.16825v1.pdf"
"2310.16834","Aaron Lou","Aaron Lou, Chenlin Meng, Stefano Ermon","Discrete Diffusion Language Modeling by Estimating the Ratios of the
  Data Distribution","30 pages","","","","stat.ML cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Despite their groundbreaking performance for many generative modeling tasks,
diffusion models have fallen short on discrete data domains such as natural
language. Crucially, standard diffusion models rely on the well-established
theory of score matching, but efforts to generalize this to discrete structures
have not yielded the same empirical gains. In this work, we bridge this gap by
proposing score entropy, a novel discrete score matching loss that is more
stable than existing methods, forms an ELBO for maximum likelihood training,
and can be efficiently optimized with a denoising variant. We scale our Score
Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2,
achieving highly competitive likelihoods while also introducing distinct
algorithmic advantages. In particular, when comparing similarly sized SEDD and
GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of
and sometimes outperforming the baseline). Furthermore, SEDD models learn a
more faithful sequence distribution (around $4\times$ better compared to GPT-2
models with ancestral sampling as measured by large models), can trade off
compute for generation quality (needing only $16\times$ fewer network
evaluations to match GPT-2), and enables arbitrary infilling beyond the
standard left to right prompting.
","2023-10-26","2310.16834v1.pdf"
"2310.16836","Shih-Yang Liu","Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting
  Cheng","LLM-FP4: 4-Bit Floating-Point Quantized Transformers","EMNLP 2023 Main Conference","","","","cs.CL cs.AI cs.AR cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose LLM-FP4 for quantizing both weights and activations in large
language models (LLMs) down to 4-bit floating-point values, in a post-training
manner. Existing post-training quantization (PTQ) solutions are primarily
integer-based and struggle with bit widths below 8 bits. Compared to integer
quantization, floating-point (FP) quantization is more flexible and can better
handle long-tail or bell-shaped distributions, and it has emerged as a default
choice in many hardware platforms. One characteristic of FP quantization is
that its performance largely depends on the choice of exponent bits and
clipping range. In this regard, we construct a strong FP-PTQ baseline by
searching for the optimal quantization parameters. Furthermore, we observe a
high inter-channel variance and low intra-channel variance pattern in
activation distributions, which adds activation quantization difficulty. We
recognize this pattern to be consistent across a spectrum of transformer models
designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models.
To tackle this, we propose per-channel activation quantization and show that
these additional scaling factors can be reparameterized as exponential biases
of weights, incurring a negligible cost. Our method, for the first time, can
quantize both weights and activations in the LLaMA-13B to only 4-bit and
achieves an average score of 63.1 on the common sense zero-shot reasoning
tasks, which is only 5.8 lower than the full-precision model, significantly
outperforming the previous state-of-the-art by 12.7 points. Code is available
at: https://github.com/nbasyl/LLM-FP4.
","2023-10-26","2310.16836v1.pdf"
"2310.16837","Jiaxuan You","Zizhao Zhang, Yi Yang, Lutong Zou, He Wen, Tao Feng, Jiaxuan You","RDBench: ML Benchmark for Relational Databases","","","","","cs.LG cs.AI cs.DB cs.SI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Benefiting from high-quality datasets and standardized evaluation metrics,
machine learning (ML) has achieved sustained progress and widespread
applications. However, while applying machine learning to relational databases
(RDBs), the absence of a well-established benchmark remains a significant
obstacle to the development of ML. To address this issue, we introduce ML
Benchmark For Relational Databases (RDBench), a standardized benchmark that
aims to promote reproducible ML research on RDBs that include multiple tables.
RDBench offers diverse RDB datasets of varying scales, domains, and relational
structures, organized into 4 levels. Notably, to simplify the adoption of
RDBench for diverse ML domains, for any given database, RDBench exposes three
types of interfaces including tabular data, homogeneous graphs, and
heterogeneous graphs, sharing the same underlying task definition. For the
first time, RDBench enables meaningful comparisons between ML methods from
diverse domains, ranging from XGBoost to Graph Neural Networks, under RDB
prediction tasks. We design multiple classification and regression tasks for
each RDB dataset and report averaged results over the same dataset, further
enhancing the robustness of the experimental findings. RDBench is implemented
with DBGym, a user-friendly platform for ML research and application on
databases, enabling benchmarking new ML methods with RDBench at ease.
","2023-10-26","2310.16837v1.pdf"
"2310.16853","Tong Ye","Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu,
  Shouling Ji, Wenhai Wang","CP-BCS: Binary Code Summarization Guided by Control Flow Graph and
  Pseudo Code","EMNLP 2023 Main Conference","","","","cs.PL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Automatically generating function summaries for binaries is an extremely
valuable but challenging task, since it involves translating the execution
behavior and semantics of the low-level language (assembly code) into
human-readable natural language. However, most current works on understanding
assembly code are oriented towards generating function names, which involve
numerous abbreviations that make them still confusing. To bridge this gap, we
focus on generating complete summaries for binary functions, especially for
stripped binary (no symbol table and debug information in reality). To fully
exploit the semantics of assembly code, we present a control flow graph and
pseudo code guided binary code summarization framework called CP-BCS. CP-BCS
utilizes a bidirectional instruction-level control flow graph and pseudo code
that incorporates expert knowledge to learn the comprehensive binary function
execution behavior and logic semantics. We evaluate CP-BCS on 3 different
binary optimization levels (O1, O2, and O3) for 3 different computer
architectures (X86, X64, and ARM). The evaluation results demonstrate CP-BCS is
superior and significantly improves the efficiency of reverse engineering.
","2023-10-27","2310.16853v1.pdf"
"2310.16861","Zhe Li","Zhe Li and Zhangyang Gao and Cheng Tan and Stan Z. Li and Laurence T.
  Yang","General Point Model with Autoencoding and Autoregressive","","","","","cs.LG cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The pre-training architectures of large language models encompass various
types, including autoencoding models, autoregressive models, and
encoder-decoder models. We posit that any modality can potentially benefit from
a large language model, as long as it undergoes vector quantization to become
discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which
seamlessly integrates autoencoding and autoregressive tasks in point cloud
transformer. This model is versatile, allowing fine-tuning for downstream point
cloud representation tasks, as well as unconditional and conditional generation
tasks. GPM enhances masked prediction in autoencoding through various forms of
mask padding tasks, leading to improved performance in point cloud
understanding. Additionally, GPM demonstrates highly competitive results in
unconditional point cloud generation tasks, even exhibiting the potential for
conditional generation tasks by modifying the input's conditional information.
Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves
superior performance in point cloud understanding tasks. Furthermore, the
integration of autoregressive and autoencoding within the same transformer
underscores its versatility across different downstream tasks.
","2023-10-27","2310.16861v1.pdf"
"2310.16870","Juanwu Lu","Yunsheng Ma and Juanwu Lu and Can Cui and Sicheng ZHao and Xu Cao and
  Wenqian Ye and Ziran Wang","MACP: Efficient Model Adaptation for Cooperative Perception","Accepted by WACV 2024, 10 pages, 7 figures, 3 tables","","","","cs.CV cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception
capabilities of connected and automated vehicles (CAVs) by enabling information
sharing to ""see through the occlusions"", resulting in significant performance
improvements. However, developing and training complex multi-agent perception
models from scratch can be expensive and unnecessary when existing single-agent
models show remarkable generalization capabilities. In this paper, we propose a
new framework termed MACP, which equips a single-agent pre-trained model with
cooperation capabilities. We approach this objective by identifying the key
challenges of shifting from single-agent to cooperative settings, adapting the
model by freezing most of its parameters and adding a few lightweight modules.
We demonstrate in our experiments that the proposed framework can effectively
utilize cooperative observations and outperform other state-of-the-art
approaches in both simulated and real-world cooperative perception benchmarks
while requiring substantially fewer tunable parameters with reduced
communication costs. Our source code is available at
https://github.com/PurdueDigitalTwin/MACP.
","2023-10-27","2310.16870v1.pdf"
"2310.16872","Pavan Annangi Mr","Hariharan Ravishankar, Rohan Patil, Vikram Melapudi, Parminder Bhatia,
  Kass-Hout Taha, Pavan Annangi","SonoSAM -- Segment Anything on Ultrasound Images","","","","","eess.IV cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In this paper, we present SonoSAM - a promptable foundational model for
segmenting objects of interest on ultrasound images. Fine-tuned exclusively on
a rich, diverse set of objects from roughly 200k ultrasound image-mask pairs,
SonoSAM demonstrates state-of-the-art performance on 8 unseen ultrasound
data-sets, outperforming competing methods by a significant margin on all
metrics of interest. SonoSAM achieves average dice similarity score of more
than 90% on almost all test datasets within 2-6 clicks on an average, making it
a valuable tool for annotating ultrasound images. We also extend SonoSAM to 3-D
(2-D +t) applications and demonstrate superior performance making it a valuable
tool for generating dense annotations from ultrasound cine-loops. Further, to
increase practical utility of SonoSAM, we propose a two-step process of
fine-tuning followed by knowledge distillation to a smaller footprint model
without comprising the performance. We present detailed qualitative and
quantitative comparisons of SonoSAM with state-of-the art methods showcasing
efficacy of SonoSAM as one of the first reliable, generic foundational model
for ultrasound.
","2023-10-27","2310.16872v1.pdf"
"2310.16924","Nikita Mehandru","Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Elaine C Khoong, Ge Gao,
  Marine Carpuat, Niloufar Salehi","Physician Detection of Clinical Harm in Machine Translation: Quality
  Estimation Aids in Reliance and Backtranslation Identifies Critical Errors","EMNLP 2023","","","","cs.CL cs.HC","http://creativecommons.org/licenses/by/4.0/","  A major challenge in the practical use of Machine Translation (MT) is that
users lack guidance to make informed decisions about when to rely on outputs.
Progress in quality estimation research provides techniques to automatically
assess MT quality, but these techniques have primarily been evaluated in vitro
by comparison against human judgments outside of a specific context of use.
This paper evaluates quality estimation feedback in vivo with a human study
simulating decision-making in high-stakes medical settings. Using Emergency
Department discharge instructions, we study how interventions based on quality
estimation versus backtranslation assist physicians in deciding whether to show
MT outputs to a patient. We find that quality estimation improves appropriate
reliance on MT, but backtranslation helps physicians detect more clinically
harmful errors that QE alone often misses.
","2023-10-27","2310.16924v1.pdf"
"2310.16944","Alexander M. Rush","Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani,
  Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\'ementine
  Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush,
  and Thomas Wolf","Zephyr: Direct Distillation of LM Alignment","","","","","cs.LG cs.CL","http://creativecommons.org/licenses/by/4.0/","  We aim to produce a smaller language model that is aligned to user intent.
Previous research has shown that applying distilled supervised fine-tuning
(dSFT) on larger models significantly improves task accuracy; however, these
models are unaligned, i.e. they do not respond well to natural prompts. To
distill this property, we experiment with the use of preference data from AI
Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model,
we apply distilled direct preference optimization (dDPO) to learn a chat model
with significantly improved intent alignment. The approach requires only a few
hours of training without any additional sampling during fine-tuning. The final
result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B
parameter models, and requires no human annotation. In particular, results on
MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access
RLHF-based model. Code, models, data, and tutorials for the system are
available at https://github.com/huggingface/alignment-handbook.
","2023-10-27","2310.16944v1.pdf"
"2310.16955","Ananth Balashankar","Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin
  Chen, Alex Beutel","Break it, Imitate it, Fix it: Robustness by Generating Human-Like
  Attacks","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  Real-world natural language processing systems need to be robust to human
adversaries. Collecting examples of human adversaries for training is an
effective but expensive solution. On the other hand, training on synthetic
attacks with small perturbations - such as word-substitution - does not
actually improve robustness to human adversaries. In this paper, we propose an
adversarial training framework that uses limited human adversarial examples to
generate more useful adversarial examples at scale. We demonstrate the
advantages of this system on the ANLI and hate speech detection benchmark
datasets - both collected via an iterative, adversarial
human-and-model-in-the-loop procedure. Compared to training only on observed
human attacks, also training on our synthetic adversarial examples improves
model robustness to future rounds. In ANLI, we see accuracy gains on the
current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of
human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate
speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a
future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the
distribution of existing human adversaries, meanwhile, degrade robustness.
","2023-10-27","2310.16955v1.pdf"
"2310.16958","Pei Zhang","Pei Zhang, Logan Kearney, Debsindhu Bhowmik, Zachary Fox, Amit K.
  Naskar, John Gounley","Transferring a molecular foundation model for polymer property
  predictions","","","","","cs.LG physics.chem-ph","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Transformer-based large language models have remarkable potential to
accelerate design optimization for applications such as drug development and
materials discovery. Self-supervised pretraining of transformer models requires
large-scale datasets, which are often sparsely populated in topical areas such
as polymer science. State-of-the-art approaches for polymers conduct data
augmentation to generate additional samples but unavoidably incurs extra
computational costs. In contrast, large-scale open-source datasets are
available for small molecules and provide a potential solution to data scarcity
through transfer learning. In this work, we show that using transformers
pretrained on small molecules and fine-tuned on polymer properties achieve
comparable accuracy to those trained on augmented polymer datasets for a series
of benchmark prediction tasks.
","2023-10-27","2310.16958v1.pdf"
"2310.16959","Ananth Balashankar","Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin,
  Jilin Chen, Alex Beutel","Improving Few-shot Generalization of Safety Classifiers via Data
  Augmented Parameter-Efficient Fine-Tuning","","","","","cs.LG","http://creativecommons.org/licenses/by/4.0/","  As large language models (LLMs) are widely adopted, new safety issues and
policies emerge, to which existing safety classifiers do not generalize well.
If we have only observed a few examples of violations of a new safety rule, how
can we build a classifier to detect violations? In this paper, we study the
novel setting of domain-generalized few-shot learning for LLM-based text safety
classifiers. Unlike prior few-shot work, these new safety issues can be hard to
uncover and we do not get to choose the few examples. We demonstrate that
existing few-shot techniques do not perform well in this setting, and rather we
propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting
training data based on similar examples in prior existing rules. We empirically
show that our approach of similarity-based data-augmentation + prompt-tuning
(DAPT) consistently outperforms baselines that either do not rely on data
augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral
judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule
is loosely correlated with existing ones.
","2023-10-27","2310.16959v1.pdf"
"2310.16960","Fan Wu","Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran,
  Janardhan Kulkarni, Robert Sim","Privately Aligning Language Models with Reinforcement Learning","","","","","cs.LG cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.
","2023-10-27","2310.16960v1.pdf"
"2310.16990","Joel Ruben Antony Moniz","Leon Liyang Zhang, Jiarui Lu, Joel Ruben Antony Moniz, Aditya
  Kulkarni, Dhivya Piraviperumal, Tien Dung Tran, Nicholas Tzou, Hong Yu","STEER: Semantic Turn Extension-Expansion Recognition for Voice
  Assistants","EMNLP 2023 Industry Track","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the context of a voice assistant system, steering refers to the phenomenon
in which a user issues a follow-up command attempting to direct or clarify a
previous turn. We propose STEER, a steering detection model that predicts
whether a follow-up turn is a user's attempt to steer the previous command.
Constructing a training dataset for steering use cases poses challenges due to
the cold-start problem. To overcome this, we developed heuristic rules to
sample opt-in usage data, approximating positive and negative samples without
any annotation. Our experimental results show promising performance in
identifying steering intent, with over 95% accuracy on our sampled data.
Moreover, STEER, in conjunction with our sampling strategy, aligns effectively
with real-world steering scenarios, as evidenced by its strong zero-shot
performance on a human-graded evaluation set. In addition to relying solely on
user transcripts as input, we introduce STEER+, an enhanced version of the
model. STEER+ utilizes a semantic parse tree to provide more context on
out-of-vocabulary words, such as named entities that often occur at the
sentence boundary. This further improves model performance, reducing error rate
in domains where entities frequently appear, such as messaging. Lastly, we
present a data analysis that highlights the improvement in user experience when
voice assistants support steering use cases.
","2023-10-27","2310.16990v1.pdf"
"2310.16992","Sinclair Schneider","Sinclair Schneider, Florian Steuber, Joao A. G. Schneider, Gabi Dreo
  Rodosek","How well can machine-generated texts be identified and can language
  models be trained to avoid identification?","This paper has been accepted for the upcoming 57th Hawaii
  International Conference on System Sciences (HICSS-57)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  With the rise of generative pre-trained transformer models such as GPT-3,
GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated
ones has become important. We refined five separate language models to generate
synthetic tweets, uncovering that shallow learning classification algorithms,
like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.
  Shallow learning classifiers differ from human-based detection, especially
when using higher temperature values during text generation, resulting in a
lower detection rate. Humans prioritize linguistic acceptability, which tends
to be higher at lower temperature values. In contrast, transformer-based
classifiers have an accuracy of 0.9 and above. We found that using a
reinforcement learning approach to refine our generative models can
successfully evade BERT-based classifiers with a detection accuracy of 0.15 or
less.
","2023-10-27","2310.16992v1.pdf"
"2310.16995","Saptarshi Sengupta","Saptarshi Sengupta, Connor Heaton, Shreya Ghosh, Preslav Nakov,
  Prasenjit Mitra","Quality > Quantity: Synthetic Corpora from Foundation Models for
  Closed-Domain Extractive Question Answering","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Domain adaptation, the process of training a model in one domain and applying
it to another, has been extensively explored in machine learning. While
training a domain-specific foundation model (FM) from scratch is an option,
recent methods have focused on adapting pre-trained FMs for domain-specific
tasks. However, our experiments reveal that either approach does not
consistently achieve state-of-the-art (SOTA) results in the target domain. In
this work, we study extractive question answering within closed domains and
introduce the concept of targeted pre-training. This involves determining and
generating relevant data to further pre-train our models, as opposed to the
conventional philosophy of utilizing domain-specific FMs trained on a wide
range of data. Our proposed framework uses Galactica to generate synthetic,
``targeted'' corpora that align with specific writing styles and topics, such
as research papers and radiology reports. This process can be viewed as a form
of knowledge distillation. We apply our method to two biomedical extractive
question answering datasets, COVID-QA and RadQA, achieving a new benchmark on
the former and demonstrating overall improvements on the latter. Code available
at https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining/tree/main.
","2023-10-27","2310.16995v1.pdf"
"2310.17015","Anna Koufakou","Anna Koufakou, Diego Grisales, Ragy Costa de jesus, Oscar Fox","Data Augmentation for Emotion Detection in Small Imbalanced Text Data","Accepted paper at IEEE ICMLA 2023","","","","cs.CL","http://creativecommons.org/publicdomain/zero/1.0/","  Emotion recognition in text, the task of identifying emotions such as joy or
anger, is a challenging problem in NLP with many applications. One of the
challenges is the shortage of available datasets that have been annotated with
emotions. Certain existing datasets are small, follow different emotion
taxonomies and display imbalance in their emotion distribution. In this work,
we studied the impact of data augmentation techniques precisely when applied to
small imbalanced datasets, for which current state-of-the-art models (such as
RoBERTa) under-perform. Specifically, we utilized four data augmentation
methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and
ProtAugment) on three datasets that come from different sources and vary in
size, emotion categories and distributions. Our experimental results show that
using the augmented data when training the classifier model leads to
significant improvements. Finally, we conducted two case studies: a) directly
using the popular chat-GPT API to paraphrase text using different prompts, and
b) using external data to augment the training set. Results show the promising
potential of these methods.
","2023-10-27","2310.17015v1.pdf"
"2310.17017","Young Min Cho","Young Min Cho, Sunny Rai, Lyle Ungar, Jo\~ao Sedoc, Sharath Chandra
  Guntuku","An Integrative Survey on Mental Health Conversational Agents to Bridge
  Computer Science and Medical Perspectives","Accepted in EMNLP 2023 Main Conference, camera ready","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Mental health conversational agents (a.k.a. chatbots) are widely studied for
their potential to offer accessible support to those experiencing mental health
challenges. Previous surveys on the topic primarily consider papers published
in either computer science or medicine, leading to a divide in understanding
and hindering the sharing of beneficial knowledge between both domains. To
bridge this gap, we conduct a comprehensive literature review using the PRISMA
framework, reviewing 534 papers published in both computer science and
medicine. Our systematic review reveals 136 key papers on building mental
health-related conversational agents with diverse characteristics of modeling
and experimental design techniques. We find that computer science papers focus
on LLM techniques and evaluating response quality using automated metrics with
little attention to the application while medical papers use rule-based
conversational agents and outcome metrics to measure the health outcomes of
participants. Based on our findings on transparency, ethics, and cultural
heterogeneity in this review, we provide a few recommendations to help bridge
the disciplinary divide and enable the cross-disciplinary development of mental
health conversational agents.
","2023-10-27","2310.17017v1.pdf"
"2310.17019","K Zentner","K.R. Zentner, Ryan Julian, Brian Ichter, Gaurav S. Sukhatme","Conditionally Combining Robot Skills using Large Language Models","","","","","cs.LG cs.CL cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call ""Language-World,"" which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large Language Models (LLMs) and those
using Deep Reinforcement Learning. Second, we introduce a method we call Plan
Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of
high-level plans using end-to-end demonstrations. Using Language-World, we show
that PCBC is able to achieve strong performance in a variety of few-shot
regimes, often achieving task generalization with as little as a single
demonstration. We have made Language-World available as open-source software at
https://github.com/krzentner/language-world/.
","2023-10-27","2310.17019v1.pdf"
"2310.17022","Ahmad Beirami","Sidharth Mudgal and Jong Lee and Harish Ganapathy and YaGuang Li and
  Tao Wang and Yanping Huang and Zhifeng Chen and Heng-Tze Cheng and Michael
  Collins and Trevor Strohman and Jilin Chen and Alex Beutel and Ahmad Beirami","Controlled Decoding from Language Models","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  We propose controlled decoding (CD), a novel off-policy reinforcement
learning method to control the autoregressive generation from language models
towards high reward outcomes. CD solves an off-policy reinforcement learning
problem through a value function for the reward, which we call a prefix scorer.
The prefix scorer is used at inference time to steer the generation towards
higher reward outcomes. We show that the prefix scorer may be trained on
(possibly) off-policy data to predict the expected reward when decoding is
continued from a partially decoded response. We empirically demonstrate that CD
is effective as a control mechanism on Reddit conversations corpus. We also
show that the modularity of the design of CD makes it possible to control for
multiple rewards, effectively solving a multi-objective reinforcement learning
problem with no additional complexity. Finally, we show that CD can be applied
in a novel blockwise fashion at inference-time, again without the need for any
training-time changes, essentially bridging the gap between the popular
best-of-$K$ strategy and token-level reinforcement learning. This makes CD a
promising approach for alignment of language models.
","2023-10-27","2310.17022v1.pdf"
"2310.17034","Besnik Fetahu","Besnik Fetahu, Pedro Faustini, Giuseppe Castellucci, Anjie Fang, Oleg
  Rokhlenko, Shervin Malmasi","Follow-on Question Suggestion via Voice Hints for Voice Assistants","Accepted as Long Paper at EMNLP'23 Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The adoption of voice assistants like Alexa or Siri has grown rapidly,
allowing users to instantly access information via voice search. Query
suggestion is a standard feature of screen-based search experiences, allowing
users to explore additional topics. However, this is not trivial to implement
in voice-based settings. To enable this, we tackle the novel task of suggesting
questions with compact and natural voice hints to allow users to ask follow-up
questions.
  We define the task, ground it in syntactic theory and outline linguistic
desiderata for spoken hints. We propose baselines and an approach using
sequence-to-sequence Transformers to generate spoken hints from a list of
questions. Using a new dataset of 6681 input questions and human written hints,
we evaluated the models with automatic metrics and human evaluation. Results
show that a naive approach of concatenating suggested questions creates poor
voice hints. Our approach, which applies a linguistically-motivated pretraining
task was strongly preferred by humans for producing the most natural hints.
","2023-10-27","2310.17034v1.pdf"
"2310.17054","Yufei Tian","Yufei Tian, Felix Zhang, Nanyun Peng","BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs'
  Generation","EMNLP 2023","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) such as GPT-3 have demonstrated a strong
capability to generate coherent and contextually relevant text. However, amidst
their successes, a crucial issue persists: their generated outputs still lack
commonsense at times. Moreover, fine-tuning the entire LLM towards more
commonsensical outputs is computationally expensive if not infeasible. In this
paper, we present a computation-efficient framework that steers a frozen
Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,
producing a plausible output that incorporates a list of concepts in a
meaningful way). Specifically, we first construct a reference-free evaluator
that assigns a sentence with a commonsensical score by grounding the sentence
to a dynamic commonsense knowledge base from four different relational aspects.
We then use the scorer as the oracle for commonsense knowledge, and extend the
controllable generation method called NADO to train an auxiliary head that
guides a fixed PTLM to better satisfy the oracle. We test our framework on a
series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two
constrained concept-to-sentence benchmarks. Human evaluation results
demonstrate that our method consistently leads to the most commonsensical
outputs.
","2023-10-27","2310.17054v1.pdf"
"2310.17064","Tuhin Sahai","Hassen Saidi, Susmit Jha, Tuhin Sahai","math-PVS: A Large Language Model Framework to Map Scientific
  Publications to PVS Theories","","","","","cs.AI cs.CL cs.LG cs.LO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
  While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery.
","2023-10-27","2310.17064v1.pdf"
"2310.17086","Deqing Fu","Deqing Fu, Tian-Qi Chen, Robin Jia, Vatsal Sharan","Transformers Learn Higher-Order Optimization Methods for In-Context
  Learning: A Study with Linear Models","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Transformers are remarkably good at in-context learning (ICL) -- learning
from demonstrations without parameter updates -- but how they perform ICL
remains a mystery. Recent work suggests that Transformers may learn in-context
by internally running Gradient Descent, a first-order optimization method. In
this paper, we instead demonstrate that Transformers learn to implement
higher-order optimization methods to perform ICL. Focusing on in-context linear
regression, we show that Transformers learn to implement an algorithm very
similar to Iterative Newton's Method, a higher-order optimization method,
rather than Gradient Descent. Empirically, we show that predictions from
successive Transformer layers closely match different iterations of Newton's
Method linearly, with each middle layer roughly computing 3 iterations. In
contrast, exponentially more Gradient Descent steps are needed to match an
additional Transformers layer; this suggests that Transformers have an
comparable rate of convergence with high-order methods such as Iterative
Newton, which are exponentially faster than Gradient Descent. We also show that
Transformers can learn in-context on ill-conditioned data, a setting where
Gradient Descent struggles but Iterative Newton succeeds. Finally, we show
theoretical results which support our empirical findings and have a close
correspondence with them: we prove that Transformers can implement $k$
iterations of Newton's method with $\mathcal{O}(k)$ layers.
","2023-10-27","2310.17086v1.pdf"
"2310.17110","Zeyang Zhang","Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu,
  Wenwu Zhu","LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?","","","","","cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  In an era marked by the increasing adoption of Large Language Models (LLMs)
for various tasks, there is a growing focus on exploring LLMs' capabilities in
handling web data, particularly graph data. Dynamic graphs, which capture
temporal network evolution patterns, are ubiquitous in real-world web data.
Evaluating LLMs' competence in understanding spatial-temporal information on
dynamic graphs is essential for their adoption in web applications, which
remains unexplored in the literature. In this paper, we bridge the gap via
proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic
graphs, to the best of our knowledge, for the first time. Specifically, we
propose the LLM4DyG benchmark, which includes nine specially designed tasks
considering the capability evaluation of LLMs from both temporal and spatial
dimensions. Then, we conduct extensive experiments to analyze the impacts of
different data generators, data statistics, prompting techniques, and LLMs on
the model performance. Finally, we propose Disentangled Spatial-Temporal
Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal
understanding abilities. Our main observations are: 1) LLMs have preliminary
spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph
tasks show increasing difficulties for LLMs as the graph size and density
increase, while not sensitive to the time span and data generation mechanism,
3) the proposed DST2 prompting method can help to improve LLMs'
spatial-temporal understanding abilities on dynamic graphs for most tasks. The
data and codes will be open-sourced at publication time.
","2023-10-27","2310.17110v1.pdf"
"2310.17119","Farima Fatahi Bayat","Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi,
  Samira Khorshidi, Fei Wu, Ihab F. Ilyas, Yunyao Li","FLEEK: Factual Error Detection and Correction with Evidence Retrieved
  from External Knowledge","EMNLP 2023 (Demonstration Track)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Detecting factual errors in textual information, whether generated by large
language models (LLM) or curated by humans, is crucial for making informed
decisions. LLMs' inability to attribute their claims to external knowledge and
their tendency to hallucinate makes it difficult to rely on their responses.
Humans, too, are prone to factual errors in their writing. Since manual
detection and correction of factual errors is labor-intensive, developing an
automatic approach can greatly reduce human effort. We present FLEEK, a
prototype tool that automatically extracts factual claims from text, gathers
evidence from external knowledge sources, evaluates the factuality of each
claim, and suggests revisions for identified errors using the collected
evidence. Initial empirical evaluation on fact error detection (77-85\% F1)
shows the potential of FLEEK. A video demo of FLEEK can be found at
https://youtu.be/NapJFUlkPdQ.
","2023-10-27","2310.17119v1.pdf"
"2310.17121","Go Kamoda","Go Kamoda, Benjamin Heinzerling, Keisuke Sakaguchi, Kentaro Inui","Test-time Augmentation for Factual Probing","12 pages, 4 figures, accepted to EMNLP 2023 Findings (short paper)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Factual probing is a method that uses prompts to test if a language model
""knows"" certain world knowledge facts. A problem in factual probing is that
small changes to the prompt can lead to large changes in model output. Previous
work aimed to alleviate this problem by optimizing prompts via text mining or
fine-tuning. However, such approaches are relation-specific and do not
generalize to unseen relation types. Here, we propose to use test-time
augmentation (TTA) as a relation-agnostic method for reducing sensitivity to
prompt variations by automatically augmenting and ensembling prompts at test
time. Experiments show improved model calibration, i.e., with TTA, model
confidence better reflects prediction accuracy. Improvements in prediction
accuracy are observed for some models, but for other models, TTA leads to
degradation. Error analysis identifies the difficulty of producing high-quality
prompt variations as the main challenge for TTA.
","2023-10-27","2310.17121v1.pdf"
"2310.17130","Hongcheng Guo","Hongcheng Guo, Boyang Wang, Jiaqi Bai, Jiaheng Liu, Jian Yang, Zhoujun
  Li","M2C: Towards Automatic Multimodal Manga Complement","EMNLP2023. arXiv admin note: text overlap with arXiv:2210.15461","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Multimodal manga analysis focuses on enhancing manga understanding with
visual and textual features, which has attracted considerable attention from
both natural language processing and computer vision communities. Currently,
most comics are hand-drawn and prone to problems such as missing pages, text
contamination, and aging, resulting in missing comic text content and seriously
hindering human comprehension. In other words, the Multimodal Manga Complement
(M2C) task has not been investigated, which aims to handle the aforementioned
issues by providing a shared semantic space for vision and language
understanding. To this end, we first propose the Multimodal Manga Complement
task by establishing a new M2C benchmark dataset covering two languages. First,
we design a manga argumentation method called MCoT to mine event knowledge in
comics with large language models. Then, an effective baseline FVP-M$^{2}$
using fine-grained visual prompts is proposed to support manga complement.
Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for
Multimodal Mange Complement.
","2023-10-27","2310.17130v1.pdf"
"2310.17133","Li Bei","Yuxin Zuo, Bei Li, Chuanhao Lv, Tong Zheng, Tong Xiao, Jingbo Zhu","Incorporating Probing Signals into Multimodal Machine Translation via
  Visual Question-Answering Pairs","Findings of EMNLP2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents an in-depth study of multimodal machine translation
(MMT), examining the prevailing understanding that MMT systems exhibit
decreased sensitivity to visual information when text inputs are complete.
Instead, we attribute this phenomenon to insufficient cross-modal interaction,
rather than image information redundancy. A novel approach is proposed to
generate parallel Visual Question-Answering (VQA) style pairs from the source
text, fostering more robust cross-modal interaction. Using Large Language
Models (LLMs), we explicitly model the probing signal in MMT to convert it into
VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask
learning framework is introduced to incorporate explicit probing signals from
the dataset into the MMT training process. Experimental results on two
widely-used benchmarks demonstrate the effectiveness of this novel approach.
Our code and data would be available at:
\url{https://github.com/libeineu/MMT-VQA}.
","2023-10-27","2310.17133v1.pdf"
"2310.17140","Justin Chiu","Justin T. Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander
  M. Rush, Daniel Fried","Symbolic Planning and Code Generation for Grounded Dialogue","Accepted to EMNLP 2023","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.
","2023-10-27","2310.17140v1.pdf"
"2310.17143","Zhicheng Lin","Zhicheng Lin","Supercharging academic writing with generative AI: framework,
  techniques, and caveats","14 pages, 2 figures, 1 table, 1 box","","","","cs.CY cs.CL","http://creativecommons.org/licenses/by/4.0/","  Academic writing is an indispensable yet laborious part of the research
enterprise. This Perspective maps out principles and methods for using
generative artificial intelligence (AI), specifically large language models
(LLMs), to elevate the quality and efficiency of academic writing. We introduce
a human-AI collaborative framework that delineates the rationale (why), process
(how), and nature (what) of AI engagement in writing. The framework pinpoints
both short-term and long-term reasons for engagement and their underlying
mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals
the role of AI throughout the writing process, conceptualized through a
two-stage model for human-AI collaborative writing, and the nature of AI
assistance in writing, represented through a model of writing-assistance types
and levels. Building on this framework, we describe effective prompting
techniques for incorporating AI into the writing routine (outlining, drafting,
and editing) as well as strategies for maintaining rigorous scholarship,
adhering to varied journal policies, and avoiding overreliance on AI.
Ultimately, the prudent integration of AI into academic writing can ease the
communication burden, empower authors, accelerate discovery, and promote
diversity in science.
","2023-10-27","2310.17143v1.pdf"
"2310.17157","Zichang Liu","Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song,
  Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen","Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","","Proceedings of the 40th International Conference on Machine
  Learning, 2023, 919","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) with hundreds of billions of parameters have
sparked a new wave of exciting AI applications. However, they are
computationally expensive at inference time. Sparsity is a natural approach to
reduce this cost, but existing methods either require costly retraining, have
to forgo LLM's in-context learning ability, or do not yield wall-clock time
speedup on modern hardware. We hypothesize that contextual sparsity, which are
small, input-dependent sets of attention heads and MLP parameters that yield
approximately the same output as the dense model for a given input, can address
these issues. We show that contextual sparsity exists, that it can be
accurately predicted, and that we can exploit it to speed up LLM inference in
wall-clock time without compromising LLM's quality or in-context learning
ability. Based on these insights, we propose DejaVu, a system that uses a
low-cost algorithm to predict contextual sparsity on the fly given inputs to
each layer, along with an asynchronous and hardware-aware implementation that
speeds up LLM inference. We validate that DejaVu can reduce the inference
latency of OPT-175B by over 2X compared to the state-of-the-art
FasterTransformer, and over 6X compared to the widely used Hugging Face
implementation, without compromising model quality. The code is available at
https://github.com/FMInference/DejaVu.
","2023-10-27","2310.17157v1.pdf"
"2310.17162","Liwei Lin","Liwei Lin, Gus Xia, Junyan Jiang, and Yixiao Zhang","Content-based Controls For Music Large Language Modeling","","","","","cs.AI cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent years have witnessed a rapid growth of large-scale language models in
the domain of music audio. Such models enable end-to-end generation of
higher-quality music, and some allow conditioned generation using text
descriptions. However, the control power of text controls on music is
intrinsically limited, as they can only describe music indirectly through
meta-data (such as singers and instruments) or high-level representations (such
as genre and emotion). We aim to further equip the models with direct and
content-based controls on innate music languages such as pitch, chords and drum
track. To this end, we contribute Coco-Mulla, a content-based control method
for music large language modeling. It uses a parameter-efficient fine-tuning
(PEFT) method tailored for Transformer-based audio models. Experiments show
that our approach achieved high-quality music generation with low-resource
semi-supervised learning, tuning with less than 4% parameters compared to the
original model and training on a small dataset with fewer than 300 songs.
Moreover, our approach enables effective content-based controls, and we
illustrate the control power via chords and rhythms, two of the most salient
features of music audio. Furthermore, we show that by combining content-based
controls and text descriptions, our system achieves flexible music variation
generation and style transfer. Our source codes and demos are available online.
","2023-10-27","2310.17162v1.pdf"
"2310.17163","Yingwen Wu","Yingwen Wu, Tao Li, Xinwen Cheng, Jie Yang, Xiaolin Huang","Low-Dimensional Gradient Helps Out-of-Distribution Detection","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Detecting out-of-distribution (OOD) samples is essential for ensuring the
reliability of deep neural networks (DNNs) in real-world scenarios. While
previous research has predominantly investigated the disparity between
in-distribution (ID) and OOD data through forward information analysis, the
discrepancy in parameter gradients during the backward process of DNNs has
received insufficient attention. Existing studies on gradient disparities
mainly focus on the utilization of gradient norms, neglecting the wealth of
information embedded in gradient directions. To bridge this gap, in this paper,
we conduct a comprehensive investigation into leveraging the entirety of
gradient information for OOD detection. The primary challenge arises from the
high dimensionality of gradients due to the large number of network parameters.
To solve this problem, we propose performing linear dimension reduction on the
gradient using a designated subspace that comprises principal components. This
innovative technique enables us to obtain a low-dimensional representation of
the gradient with minimal information loss. Subsequently, by integrating the
reduced gradient with various existing detection score functions, our approach
demonstrates superior performance across a wide range of detection tasks. For
instance, on the ImageNet benchmark, our method achieves an average reduction
of 11.15% in the false positive rate at 95% recall (FPR95) compared to the
current state-of-the-art approach. The code would be released.
","2023-10-27","2310.17163v1.pdf"
"2310.17166","Taejun Yun","Taejun Yun, Jinhyeon Kim, Deokyeong Kang, Seong Hoon Lim, Jihoon Kim,
  Taeuk Kim","X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity","Accepted to EMNLP 2023 (Findings)","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Cross-lingual transfer (XLT) is an emergent ability of multilingual language
models that preserves their performance on a task to a significant extent when
evaluated in languages that were not included in the fine-tuning process. While
English, due to its widespread usage, is typically regarded as the primary
language for model adaption in various tasks, recent studies have revealed that
the efficacy of XLT can be amplified by selecting the most appropriate source
languages based on specific conditions. In this work, we propose the
utilization of sub-network similarity between two languages as a proxy for
predicting the compatibility of the languages in the context of XLT. Our
approach is model-oriented, better reflecting the inner workings of foundation
models. In addition, it requires only a moderate amount of raw text from
candidate languages, distinguishing it from the majority of previous methods
that rely on external resources. In experiments, we demonstrate that our method
is more effective than baselines across diverse tasks. Specifically, it shows
proficiency in ranking candidates for zero-shot XLT, achieving an improvement
of 4.6% on average in terms of NDCG@3. We also provide extensive analyses that
confirm the utility of sub-networks for XLT prediction.
","2023-10-27","2310.17166v1.pdf"
"2310.17191","Jiahai Feng","Jiahai Feng, Jacob Steinhardt","How do Language Models Bind Entities in Context?","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  To correctly use in-context information, language models (LMs) must bind
entities to their attributes. For example, given a context describing a ""green
square"" and a ""blue circle"", LMs must bind the shapes to their respective
colors. We analyze LM representations and identify the binding ID mechanism: a
general mechanism for solving the binding problem, which we observe in every
sufficiently large model from the Pythia and LLaMA families. Using causal
interventions, we show that LMs' internal activations represent binding
information by attaching binding ID vectors to corresponding entities and
attributes. We further show that binding ID vectors form a continuous subspace,
in which distances between binding ID vectors reflect their discernability.
Overall, our results uncover interpretable strategies in LMs for representing
symbolic knowledge in-context, providing a step towards understanding general
in-context reasoning in large-scale LMs.
","2023-10-27","2310.17191v1.pdf"
"2310.17217","Chenze Shao","Chenze Shao and Zhengrui Ma and Min Zhang and Yang Feng","Beyond MLE: Convex Learning for Text Generation","NeurIPS 2023","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.
","2023-10-27","2310.17217v1.pdf"
"2310.17228","Anirudh Khatry","Anirudh Khatry, Sumit Gulwani, Priyanshu Gupta, Vu Le, Ananya Singha,
  Mukul Singh, Gust Verbruggen","TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World","Accepted for EMNLP-Findings, 2023","","","","cs.AI cs.CL cs.SE","http://creativecommons.org/licenses/by/4.0/","  Target similarity tuning (TST) is a method of selecting relevant examples in
natural language (NL) to code generation through large language models (LLMs)
to improve performance. Its goal is to adapt a sentence embedding model to have
the similarity between two NL inputs match the similarity between their
associated code outputs. In this paper, we propose different methods to apply
and improve TST in the real world. First, we replace the sentence transformer
with embeddings from a larger model, which reduces sensitivity to the language
distribution and thus provides more flexibility in synthetic generation of
examples, and we train a tiny model that transforms these embeddings to a space
where embedding similarity matches code similarity, which allows the model to
remain a black box and only requires a few matrix multiplications at inference
time. Second, we how to efficiently select a smaller number of training
examples to train the TST model. Third, we introduce a ranking-based evaluation
for TST that does not require end-to-end code generation experiments, which can
be expensive to perform.
","2023-10-27","2310.17228v1.pdf"
"2310.17230","Alex Tamkin","Alex Tamkin, Mohammad Taufeeque, Noah D. Goodman","Codebook Features: Sparse and Discrete Interpretability for Neural
  Networks","","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Understanding neural networks is challenging in part because of the dense,
continuous nature of their hidden states. We explore whether we can train
neural networks to have hidden states that are sparse, discrete, and more
interpretable by quantizing their continuous features into what we call
codebook features. Codebook features are produced by finetuning neural networks
with vector quantization bottlenecks at each layer, producing a network whose
hidden features are the sum of a small number of discrete vector codes chosen
from a larger codebook. Surprisingly, we find that neural networks can operate
under this extreme bottleneck with only modest degradation in performance. This
sparse, discrete bottleneck also provides an intuitive way of controlling
neural network behavior: first, find codes that activate when the desired
behavior is present, then activate those same codes during generation to elicit
that behavior. We validate our approach by training codebook Transformers on
several different datasets. First, we explore a finite state machine dataset
with far more hidden states than neurons. In this setting, our approach
overcomes the superposition problem by assigning states to distinct codes, and
we find that we can make the neural network behave as if it is in a different
state by activating the code for that state. Second, we train Transformer
language models with up to 410M parameters on two natural language datasets. We
identify codes in these models representing diverse, disentangled concepts
(ranging from negative emotions to months of the year) and find that we can
guide the model to generate different topics by activating the appropriate
codes during inference. Overall, codebook features appear to be a promising
unit of analysis and control for neural networks and interpretability. Our
codebase and models are open-sourced at
https://github.com/taufeeque9/codebook-features.
","2023-10-27","2310.17230v1.pdf"
"2310.17233","Ping Guo","Ping Guo, Xiangpeng Wei, Yue Hu, Baosong Yang, Dayiheng Liu, Fei
  Huang, Jun Xie","EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual
  Representation Learning","Accepted by NeurIPS 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Expressing universal semantics common to all languages is helpful in
understanding the meanings of complex and culture-specific sentences. The
research theme underlying this scenario focuses on learning universal
representations across languages with the usage of massive parallel corpora.
However, due to the sparsity and scarcity of parallel data, there is still a
big challenge in learning authentic ``universals'' for any two languages. In
this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm,
to learn (X)Cross-lingual universals with the aid of excessive multilingual
non-parallel data. EMMA-X unifies the cross-lingual representation learning
task and an extra semantic relation prediction task within an EM framework.
Both the extra semantic classifier and the cross-lingual sentence encoder
approximate the semantic relation of two sentences, and supervise each other
until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly
introduced benchmark containing 12 widely studied cross-lingual tasks that
fully depend on sentence-level representations. Results reveal that EMMA-X
achieves state-of-the-art performance. Further geometric analysis of the built
representation space with three requirements demonstrates the superiority of
EMMA-X over advanced models.
","2023-10-27","2310.17233v1.pdf"
"2310.17238","Wei Liu","Zhaohui Yan, Songlin Yang, Wei Liu, Kewei Tu","Joint Entity and Relation Extraction with Span Pruning and Hypergraph
  Neural Networks","Accepted to Proceedings of EMNLP, 2023","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Entity and Relation Extraction (ERE) is an important task in information
extraction. Recent marker-based pipeline models achieve state-of-the-art
performance, but still suffer from the error propagation issue. Also, most of
current ERE models do not take into account higher-order interactions between
multiple entities and relations, while higher-order modeling could be
beneficial.In this work, we propose HyperGraph neural network for ERE
($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based
pipleline model). To alleviate error propagation,we use a high-recall pruner
mechanism to transfer the burden of entity identification and labeling from the
NER module to the joint module of our model. For higher-order modeling, we
build a hypergraph, where nodes are entities (provided by the span pruner) and
relations thereof, and hyperedges encode interactions between two different
relations or between a relation and its associated subject and object entities.
We then run a hypergraph neural network for higher-order inference by applying
message passing over the built hypergraph. Experiments on three widely used
benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant
improvements over the previous state-of-the-art PL-marker.
","2023-10-27","2310.17238v1.pdf"
"2310.17261","Dongkyun Kim","Dongkyun Kim, Mingi Kwon, Youngjung Uh","Attribute Based Interpretable Evaluation Metrics for Generative Models","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  When the training dataset comprises a 1:1 proportion of dogs to cats, a
generative model that produces 1:1 dogs and cats better resembles the training
species distribution than another model with 3:1 dogs and cats. Can we capture
this phenomenon using existing metrics? Unfortunately, we cannot, because these
metrics do not provide any interpretability beyond ""diversity"". In this
context, we propose a new evaluation protocol that measures the divergence of a
set of generated images from the training set regarding the distribution of
attribute strengths as follows. Single-attribute Divergence (SaD) measures the
divergence regarding PDFs of a single attribute. Paired-attribute Divergence
(PaD) measures the divergence regarding joint PDFs of a pair of attributes.
They provide which attributes the models struggle. For measuring the attribute
strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures
the cosine similarity between image and text vectors with heterogeneous initial
points. With SaD and PaD, we reveal the following about existing generative
models. ProjectedGAN generates implausible attribute relationships such as a
baby with a beard even though it has competitive scores of existing metrics.
Diffusion models struggle to capture diverse colors in the datasets. The larger
sampling timesteps of latent diffusion model generate the more minor objects
including earrings and necklaces. Stable Diffusion v1.5 better captures the
attributes than v2.1. Our metrics lay a foundation for explainable evaluations
of generative models.
","2023-10-27","2310.17261v1.pdf"
"2310.17271","Katerina Margatina","Ahmed Alajrami and Katerina Margatina and Nikolaos Aletras","Understanding the Role of Input Token Characters in Language Models: How
  Does Information Loss Affect Performance?","To appear at EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Understanding how and what pre-trained language models (PLMs) learn about
language is an open challenge in natural language processing. Previous work has
focused on identifying whether they capture semantic and syntactic information,
and how the data or the pre-training objective affects their performance.
However, to the best of our knowledge, no previous work has specifically
examined how information loss in input token characters affects the performance
of PLMs. In this study, we address this gap by pre-training language models
using small subsets of characters from individual tokens. Surprisingly, we find
that pre-training even under extreme settings, i.e. using only one character of
each token, the performance retention in standard NLU benchmarks and probing
tasks compared to full-token models is high. For instance, a model pre-trained
only on single first characters from tokens achieves performance retention of
approximately $90$\% and $77$\% of the full-token model in SuperGLUE and GLUE
tasks, respectively.
","2023-10-27","2310.17271v1.pdf"
"2310.17303","Daniil Tiapkin","Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines,
  Alexey Naumov, Pierre Perrault, Michal Valko, Pierre Menard","Demonstration-Regularized RL","","","","","stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Incorporating expert demonstrations has empirically helped to improve the
sample efficiency of reinforcement learning (RL). This paper quantifies
theoretically to what extent this extra information reduces RL's sample
complexity. In particular, we study the demonstration-regularized reinforcement
learning that leverages the expert demonstrations by KL-regularization for a
policy learned by behavior cloning. Our findings reveal that using
$N^{\mathrm{E}}$ expert demonstrations enables the identification of an optimal
policy at a sample complexity of order
$\widetilde{\mathcal{O}}(\mathrm{Poly}(S,A,H)/(\varepsilon^2 N^{\mathrm{E}}))$
in finite and $\widetilde{\mathcal{O}}(\mathrm{Poly}(d,H)/(\varepsilon^2
N^{\mathrm{E}}))$ in linear Markov decision processes, where $\varepsilon$ is
the target precision, $H$ the horizon, $A$ the number of action, $S$ the number
of states in the finite case and $d$ the dimension of the feature space in the
linear case. As a by-product, we provide tight convergence guarantees for the
behaviour cloning procedure under general assumptions on the policy classes.
Additionally, we establish that demonstration-regularized methods are provably
efficient for reinforcement learning from human feedback (RLHF). In this
respect, we provide theoretical evidence showing the benefits of
KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid
pessimism injection by employing computationally feasible regularization to
handle reward estimation uncertainty, thus setting our approach apart from the
prior works.
","2023-10-27","2310.17303v1.pdf"
"2310.17312","Vijini Liyanage","Vijini Liyanage and Davide Buscaldi","An Ensemble Method Based on the Combination of Transformers with
  Convolutional Neural Networks to Detect Artificially Generated Text","In Proceedings of the 21st Annual Workshop of the Australasian
  Language Technology Association (ALTA 2023)","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Thanks to the state-of-the-art Large Language Models (LLMs), language
generation has reached outstanding levels. These models are capable of
generating high quality content, thus making it a challenging task to detect
generated text from human-written content. Despite the advantages provided by
Natural Language Generation, the inability to distinguish automatically
generated text can raise ethical concerns in terms of authenticity.
Consequently, it is important to design and develop methodologies to detect
artificial content. In our work, we present some classification models
constructed by ensembling transformer models such as Sci-BERT, DeBERTa and
XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate
that the considered ensemble architectures surpass the performance of the
individual transformer models for classification. Furthermore, the proposed
SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared
task 2023 data.
","2023-10-27","2310.17312v1.pdf"
"2310.17316","Shuai Yang","Shuai Yang, Zhifei Chen, Pengguang Chen, Xi Fang, Shu Liu, Yingcong
  Chen","Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with
  Rich Semantics","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Defect inspection is paramount within the closed-loop manufacturing system.
However, existing datasets for defect inspection often lack precision and
semantic granularity required for practical applications. In this paper, we
introduce the Defect Spectrum, a comprehensive benchmark that offers precise,
semantic-abundant, and large-scale annotations for a wide range of industrial
defects. Building on four key industrial benchmarks, our dataset refines
existing annotations and introduces rich semantic details, distinguishing
multiple defect types within a single image. Furthermore, we introduce
Defect-Gen, a two-stage diffusion-based generator designed to create
high-quality and diverse defective images, even when working with limited
datasets. The synthetic images generated by Defect-Gen significantly enhance
the efficacy of defect inspection models. Overall, The Defect Spectrum dataset
demonstrates its potential in defect inspection research, offering a solid
platform for testing and refining advanced models.
","2023-10-27","2310.17316v1.pdf"
"2310.17342","Hanchong Zhang","Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, Kai Yu","ACT-SQL: In-Context Learning for Text-to-SQL with
  Automatically-Generated Chain-of-Thought","","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.
","2023-10-27","2310.17342v1.pdf"
"2310.17353","Yong Cao","Yong Cao, Yova Kementchedjhieva, Ruixiang Cui, Antonia Karamolegkou,
  Li Zhou, Megan Dare, Lucia Donatelli, Daniel Hershcovich","Cultural Adaptation of Recipes","Accepted to TACL","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Building upon the considerable advances in Large Language Models (LLMs), we
are now equipped to address more sophisticated tasks demanding a nuanced
understanding of cross-cultural contexts. A key example is recipe adaptation,
which goes beyond simple translation to include a grasp of ingredients,
culinary techniques, and dietary preferences specific to a given culture. We
introduce a new task involving the translation and cultural adaptation of
recipes between Chinese and English-speaking cuisines. To support this
investigation, we present CulturalRecipes, a unique dataset comprised of
automatically paired recipes written in Mandarin Chinese and English. This
dataset is further enriched with a human-written and curated test set. In this
intricate task of cross-cultural recipe adaptation, we evaluate the performance
of various methods, including GPT-4 and other LLMs, traditional machine
translation, and information retrieval techniques. Our comprehensive analysis
includes both automatic and human evaluation metrics. While GPT-4 exhibits
impressive abilities in adapting Chinese recipes into English, it still lags
behind human expertise when translating English recipes into Chinese. This
underscores the multifaceted nature of cultural adaptations. We anticipate that
these insights will significantly contribute to future research on
culturally-aware language models and their practical application in culturally
diverse contexts.
","2023-10-27","2310.17353v1.pdf"
"2310.17372","Antonio Valerio Miceli Barone","Antonio Valerio Miceli-Barone, Alex Lascarides, Craig Innes","Dialogue-based generation of self-driving simulation scenarios using
  Large Language Models","12 pages, 6 figures, SpLU-RoboNLP 2023","","","","cs.AI cs.CL cs.RO","http://creativecommons.org/licenses/by/4.0/","  Simulation is an invaluable tool for developing and evaluating controllers
for self-driving cars. Current simulation frameworks are driven by
highly-specialist domain specific languages, and so a natural language
interface would greatly enhance usability. But there is often a gap, consisting
of tacit assumptions the user is making, between a concise English utterance
and the executable code that captures the user's intent. In this paper we
describe a system that addresses this issue by supporting an extended
multimodal interaction: the user can follow up prior instructions with
refinements or revisions, in reaction to the simulations that have been
generated from their utterances so far. We use Large Language Models (LLMs) to
map the user's English utterances in this interaction into domain-specific
code, and so we explore the extent to which LLMs capture the context
sensitivity that's necessary for computing the speaker's intended message in
discourse.
","2023-10-27","2310.17372v1.pdf"
"2310.17386","Pierre Ablin","Anastasia Ivanova and Pierre Ablin","A Challenge in Reweighting Data with Bilevel Optimization","","","","","stat.ML cs.LG","http://creativecommons.org/licenses/by/4.0/","  In many scenarios, one uses a large training set to train a model with the
goal of performing well on a smaller testing set with a different distribution.
Learning a weight for each data point of the training set is an appealing
solution, as it ideally allows one to automatically learn the importance of
each training point for generalization on the testing set. This task is usually
formalized as a bilevel optimization problem. Classical bilevel solvers are
based on a warm-start strategy where both the parameters of the models and the
data weights are learned at the same time. We show that this joint dynamic may
lead to sub-optimal solutions, for which the final data weights are very
sparse. This finding illustrates the difficulty of data reweighting and offers
a clue as to why this method is rarely used in practice.
","2023-10-27","2310.17386v1.pdf"
"2310.17389","Zi Lin","Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang,
  Jingbo Shang","ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in
  Real-World User-AI Conversation","","EMNLP findings 2023","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite remarkable advances that large language models have achieved in
chatbots, maintaining a non-toxic user-AI interactive environment has become
increasingly critical nowadays. However, previous efforts in toxicity detection
have been mostly based on benchmarks derived from social media content, leaving
the unique challenges inherent to real-world user-AI interactions
insufficiently explored. In this work, we introduce ToxicChat, a novel
benchmark based on real user queries from an open-source chatbot. This
benchmark contains the rich, nuanced phenomena that can be tricky for current
toxicity detection models to identify, revealing a significant domain
difference compared to social media content. Our systematic evaluation of
models trained on existing toxicity datasets has shown their shortcomings when
applied to this unique domain of ToxicChat. Our work illuminates the
potentially overlooked challenges of toxicity detection in real-world user-AI
conversations. In the future, ToxicChat can be a valuable resource to drive
further advancements toward building a safe and healthy environment for user-AI
interactions.
","2023-10-27","2310.17389v1.pdf"
"2310.17407","Vladim\'ir Havl\'ik","Vladim\'ir Havl\'ik","Meaning and understanding in large language models","20 pages","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Can a machine understand the meanings of natural language? Recent
developments in the generative large language models (LLMs) of artificial
intelligence have led to the belief that traditional philosophical assumptions
about machine understanding of language need to be revised. This article
critically evaluates the prevailing tendency to regard machine language
performance as mere syntactic manipulation and the simulation of understanding,
which is only partial and very shallow, without sufficient referential
grounding in the world. The aim is to highlight the conditions crucial to
attributing natural language understanding to state-of-the-art LLMs, where it
can be legitimately argued that LLMs not only use syntax but also semantics,
their understanding not being simulated but duplicated; and determine how they
ground the meanings of linguistic expressions.
","2023-10-27","2310.17407v1.pdf"
"2310.17413","Anas Belfathi","Anas Belfathi, Nicolas Hernandez, Laura Monceaux","Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases","","JURIX 2023 - The 36th International Conference on Legal Knowledge
  and Information System, Maastricht, the Netherlands","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  We propose a comprehensive study of one-stage elicitation techniques for
querying a large pre-trained generative transformer (GPT-3.5-turbo) in the
rhetorical role prediction task of legal cases. This task is known as requiring
textual context to be addressed. Our study explores strategies such as zero-few
shots, task specification with definitions and clarification of annotation
ambiguities, textual context and reasoning with general prompts and specific
questions. We show that the number of examples, the definition of labels, the
presentation of the (labelled) textual context and specific questions about
this context have a positive influence on the performance of the model. Given
non-equivalent test set configurations, we observed that prompting with a few
labelled examples from direct context can lead the model to a better
performance than a supervised fined-tuned multi-class classifier based on the
BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach
the performance of the best systems = 86%) in the LegalEval 2023 task which, on
the other hand, require dedicated resources, architectures and training.
","2023-10-27","2310.17413v1.pdf"
"2310.17415","Yang Tan","Yang Tan, Mingchen Li, Pan Tan, Ziyi Zhou, Huiqun Yu, Guisheng Fan,
  Liang Hong","PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word
  Tokenization on Downstream Applications","46 pages, 4figures, 9 tables","","","","cs.CL cs.AI q-bio.BM","http://creativecommons.org/licenses/by/4.0/","  Large protein language models are adept at capturing the underlying
evolutionary information in primary structures, offering significant practical
value for protein engineering. Compared to natural language models, protein
amino acid sequences have a smaller data volume and a limited combinatorial
space. Choosing an appropriate vocabulary size to optimize the pre-trained
model is a pivotal issue. Moreover, despite the wealth of benchmarks and
studies in the natural language community, there remains a lack of a
comprehensive benchmark for systematically evaluating protein language model
quality. Given these challenges, PETA trained language models with 14 different
vocabulary sizes under three tokenization methods. It conducted thousands of
tests on 33 diverse downstream datasets to assess the models' transfer learning
capabilities, incorporating two classification heads and three random seeds to
mitigate potential biases. Extensive experiments indicate that vocabulary sizes
between 50 and 200 optimize the model, whereas sizes exceeding 800
detrimentally affect the model's representational performance. Our code, model
weights and datasets are available at
https://github.com/ginnm/ProteinPretraining.
","2023-10-27","2310.17415v1.pdf"
"2310.17419","You-Ming Chang","You-Ming Chang, Chen Yeh, Wei-Chen Chiu, Ning Yu","AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image
  Detectors","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep generative models can create remarkably photorealistic fake images while
raising concerns about misinformation and copyright infringement, known as
deepfake threats. Deepfake detection technique is developed to distinguish
between real and fake images, where the existing methods typically learn
classifiers in the image domain or various feature domains. However, the
generalizability of deepfake detection against emerging and more advanced
generative models remains challenging. In this paper, being inspired by the
zero-shot advantages of Vision-Language Models (VLMs), we propose a novel
approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve
the deepfake detection accuracy over unseen data. We formulate deepfake
detection as a visual question answering problem, and tune soft prompts for
InstructBLIP to answer the real/fake information of a query image. We conduct
full-spectrum experiments on datasets from 3 held-in and 13 held-out generative
models, covering modern text-to-image generation, image editing and image
attacks. Results demonstrate that (1) the deepfake detection accuracy can be
significantly and consistently improved (from 58.8% to 91.31%, in average
accuracy over unseen data) using pretrained vision-language models with prompt
tuning; (2) our superior performance is at less cost of trainable parameters,
resulting in an effective and efficient solution for deepfake detection. Code
and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.
","2023-10-27","2310.17419v1.pdf"
"2310.17428","Rishav Hada","Rishav Hada, Agrima Seth, Harshita Diddee, Kalika Bali","''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT
  Generated English Text","Camera-ready version in EMNLP 2023","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Language serves as a powerful tool for the manifestation of societal belief
systems. In doing so, it also perpetuates the prevalent biases in our society.
Gender bias is one of the most pervasive biases in our society and is seen in
online and offline discourses. With LLMs increasingly gaining human-like
fluency in text generation, gaining a nuanced understanding of the biases these
systems can generate is imperative. Prior work often treats gender bias as a
binary classification task. However, acknowledging that bias must be perceived
at a relative scale; we investigate the generation and consequent receptivity
of manual annotators to bias of varying degrees. Specifically, we create the
first dataset of GPT-generated English text with normative ratings of gender
bias. Ratings were obtained using Best--Worst Scaling -- an efficient
comparative annotation framework. Next, we systematically analyze the variation
of themes of gender biases in the observed ranking and show that
identity-attack is most closely related to gender bias. Finally, we show the
performance of existing automated models trained on related concepts on our
dataset.
","2023-10-27","2310.17428v1.pdf"
"2310.17471","Zhiheng Guo","Xiang Chen, Zhiheng Guo, Xijun Wang, Howard H. Yang, Chenyuan Feng,
  Junshen Su, Sihui Zheng, Tony Q. S. Quek","Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End
  Collaboration","8 pages, 4 figures, 1 table","","","","cs.IT cs.DC cs.LG cs.NI eess.SP math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Future wireless communication networks are in a position to move beyond
data-centric, device-oriented connectivity and offer intelligent, immersive
experiences based on task-oriented connections, especially in the context of
the thriving development of pre-trained foundation models (PFM) and the
evolving vision of 6G native artificial intelligence (AI). Therefore,
redefining modes of collaboration between devices and servers and constructing
native intelligence libraries become critically important in 6G. In this paper,
we analyze the challenges of achieving 6G native AI from the perspectives of
data, intelligence, and networks. Then, we propose a 6G native AI framework
based on foundation models, provide a customization approach for intent-aware
PFM, present a construction of a task-oriented AI toolkit, and outline a novel
cloud-edge-end collaboration paradigm. As a practical use case, we apply this
framework for orchestration, achieving the maximum sum rate within a wireless
communication system, and presenting preliminary evaluation results. Finally,
we outline research directions for achieving native AI in 6G.
","2023-10-27","2310.17471v1.pdf"
"2310.17490","Sukmin Cho","Sukmin Cho, Jeong yeon Seo, Soyeong Jeong, Jong C. Park","Improving Zero-shot Reader by Reducing Distractions from Irrelevant
  Documents in Open-Domain Question Answering","Findings of EMNLP 2023 Camera Ready","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
","2023-10-27","2310.17490v1.pdf"
"2310.17491","Wenhan Yu","Terence Jie Chua, Wenhan Yu, Jun Zhao, Kwok-Yan Lam","FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine
  Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation
  Models with Mobile Edge Computing","","","","","cs.LG cs.NI","http://creativecommons.org/licenses/by/4.0/","  The emergence of foundation models, including language and vision models, has
reshaped AI's landscape, offering capabilities across various applications.
Deploying and fine-tuning these large models, like GPT-3 and BERT, presents
challenges, especially in the current foundation model era. We introduce
Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning
(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we
expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses
adapters, emulators, and PEFT for federated model tuning, enhancing model
privacy and memory efficiency. Adapters adjust pre-trained models, while
emulators give a compact representation of original models, addressing both
privacy and efficiency. Adaptable to various neural networks, our approach also
uses deep reinforcement learning for hyper-parameter optimization. We tested
FedPEAT in a unique scenario with a server participating in collaborative
federated tuning, showcasing its potential in tackling foundation model
challenges.
","2023-10-27","2310.17491v1.pdf"
"2310.17492","Wenhan Yu","Wenhan Yu, Terence Jie Chua, Jun Zhao","Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation
  Models: A Multi-Agent Deep Reinforcement Learning Approach","","","","","cs.AI cs.DC cs.LG cs.NI","http://creativecommons.org/licenses/by/4.0/","  The efficient deployment and fine-tuning of foundation models are pivotal in
contemporary artificial intelligence. In this study, we present a
groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation
models, specifically designed to enhance local task performance on user
equipment (UE). Central to our approach is the innovative Emulator-Adapter
architecture, segmenting the foundation model into two cohesive modules. This
design not only conserves computational resources but also ensures adaptability
and fine-tuning efficiency for downstream tasks. Additionally, we introduce an
advanced resource allocation mechanism that is fine-tuned to the needs of the
Emulator-Adapter structure in decentralized settings. To address the challenges
presented by this system, we employ a hybrid multi-agent Deep Reinforcement
Learning (DRL) strategy, adept at handling mixed discrete-continuous action
spaces, ensuring dynamic and optimal resource allocations. Our comprehensive
simulations and validations underscore the practical viability of our approach,
demonstrating its robustness, efficiency, and scalability. Collectively, this
work offers a fresh perspective on deploying foundation models and balancing
computational efficiency with task proficiency.
","2023-10-27","2310.17492v1.pdf"
"2310.17512","Jindong Wang","Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao
  Chen, Xing Xie","CompeteAI: Understanding the Competition Behaviors in Large Language
  Model-based Agents","Technical report; 21 pages","","","","cs.AI cs.CL cs.HC cs.MA","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) have been widely used as agents to complete
different tasks, such as personal assistance or event planning. While most work
has focused on cooperation and collaboration between agents, little work
explores competition, another important mechanism that fosters the development
of society and economy. In this paper, we seek to examine the competition
behaviors in LLM-based agents. We first propose a general framework to study
the competition between agents. Then, we implement a practical competitive
environment using GPT-4 to simulate a virtual town with two types of agents,
including restaurant agents and customer agents. Specifically, restaurant
agents compete with each other to attract more customers, where the competition
fosters them to transform, such as cultivating new operating strategies. The
results of our experiments reveal several interesting findings ranging from
social learning to Matthew Effect, which aligns well with existing sociological
and economic theories. We believe that competition between agents deserves
further investigation to help us understand society better. The code will be
released soon.
","2023-10-27","2310.17512v1.pdf"
"2310.17513","Yuchen Zeng","Yuchen Zeng, Kangwook Lee","The Expressive Power of Low-Rank Adaptation","40 pages,5 figures","","","","cs.LG cs.AI cs.CL stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
","2023-10-27","2310.17513v1.pdf"
"2310.17526","Qusai Khraisha","Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch,
  Kristin Hadfield","Can large language models replace humans in the systematic review
  process? Evaluating GPT-4's efficacy in screening and extracting data from
  peer-reviewed and grey literature in multiple languages","9 pages, 2 figures, 1 table","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4's capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
","2023-10-27","2310.17526v1.pdf"
"2310.17533","Anaelia Ovalle","Anaelia Ovalle","Decoding The Digital Fuk\'u: Deciphering Colonial Legacies to Critically
  Assess ChatGPT in Dominican Education","","","","","cs.CY","http://creativecommons.org/licenses/by/4.0/","  Educational disparities within the Dominican Republic (DR) have long-standing
origins rooted in economic, political, and social inequity. Addressing these
challenges has necessarily called for capacity building with respect to
educational materials, high-quality instruction, and structural resourcing.
Generative AI tools like ChatGPT have begun to pique the interest of Dominican
educators due to their perceived potential to bridge these educational gaps.
However, a substantial body of AI fairness literature has documented ways AI
disproportionately reinforces power dynamics reflective of jurisdictions
driving AI development and deployment policies, collectively termed the AI
Global North. As such, indiscriminate adoption of this technology for DR
education, even in part, risks perpetuating forms of digital coloniality.
Therefore, this paper centers embracing AI-facilitated educational reform by
critically examining how AI-driven tools like ChatGPT in DR education may
replicate facets of digital colonialism. We provide a concise overview of
20th-century Dominican education reforms following the 1916 US occupation.
Then, we employ identified neocolonial aspects historically shaping Dominican
education to interrogate the perceived advantages of ChatGPT for contemporary
Dominican education, as outlined by a Dominican scholar. This work invites AI
Global North & South developers, stakeholders, and Dominican leaders alike to
exercise a relational contextualization of data-centric epistemologies like
ChatGPT to reap its transformative benefits while remaining vigilant of
safeguarding Dominican digital sovereignty.
","2023-10-27","2310.17533v1.pdf"
"2310.17551","Jing Yao","Xiaoyuan Yi, Jing Yao, Xiting Wang and Xing Xie","Unpacking the Ethical Value Alignment in Big Models","","","","","cs.CY cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Big models have greatly advanced AI's ability to understand, generate, and
manipulate information and content, enabling numerous applications. However, as
these models become increasingly integrated into everyday life, their inherent
ethical values and potential biases pose unforeseen risks to society. This
paper provides an overview of the risks and challenges associated with big
models, surveys existing AI ethics guidelines, and examines the ethical
implications arising from the limitations of these models. Taking a normative
ethics perspective, we propose a reassessment of recent normative guidelines,
highlighting the importance of collaborative efforts in academia to establish a
unified and universal AI ethics framework. Furthermore, we investigate the
moral inclinations of current mainstream LLMs using the Moral Foundation
theory, analyze existing alignment algorithms, and outline the unique
challenges encountered in aligning ethical values within them. To address these
challenges, we introduce a novel conceptual paradigm for aligning the ethical
values of big models and discuss promising research directions for alignment
criteria, evaluation, and method, representing an initial step towards the
interdisciplinary construction of the ethically aligned AI
  This paper is a modified English version of our Chinese paper
https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended
to help non-Chinese native speakers better understand our work.
","2023-10-27","2310.17551v1.pdf"
"2310.17555","Huihan Liu","Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov,
  Ching-An Cheng","Interactive Robot Learning from Verbal Correction","","","","","cs.RO cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  The ability to learn and refine behavior after deployment has become ever
more important for robots as we design them to operate in unstructured
environments like households. In this work, we design a new learning system
based on large language model (LLM), OLAF, that allows everyday users to teach
a robot using verbal corrections when the robot makes mistakes, e.g., by saying
""Stop what you're doing. You should move closer to the cup."" A key feature of
OLAF is its ability to update the robot's visuomotor neural policy based on the
verbal feedback to avoid repeating mistakes in the future. This is in contrast
to existing LLM-based robotic systems, which only follow verbal commands or
corrections but not learn from them. We demonstrate the efficacy of our design
in experiments where a user teaches a robot to perform long-horizon
manipulation tasks both in simulation and on physical hardware, achieving on
average 20.0% improvement in policy success rate. Videos and more results are
at https://ut-austin-rpl.github.io/olaf/
","2023-10-27","2310.17555v1.pdf"
"2310.17567","Dingli Yu","Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh
  Goyal, Sanjeev Arora","Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models","","","","","cs.CL cs.AI cs.LG cs.NE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With LLMs shifting their role from statistical modeling of language to
serving as general-purpose AI agents, how should LLM evaluations change?
Arguably, a key ability of an AI agent is to flexibly combine, as needed, the
basic skills it has learned. The capability to combine skills plays an
important role in (human) pedagogy and also in a paper on emergence phenomena
(Arora & Goyal, 2023).
  This work introduces Skill-Mix, a new evaluation to measure ability to
combine skills. Using a list of $N$ skills the evaluator repeatedly picks
random subsets of $k$ skills and asks the LLM to produce text combining that
subset of skills. Since the number of subsets grows like $N^k$, for even modest
$k$ this evaluation will, with high probability, require the LLM to produce
text significantly different from any text in the training set. The paper
develops a methodology for (a) designing and administering such an evaluation,
and (b) automatic grading (plus spot-checking by humans) of the results using
GPT-4 as well as the open LLaMA-2 70B model.
  Administering a version of to popular chatbots gave results that, while
generally in line with prior expectations, contained surprises. Sizeable
differences exist among model capabilities that are not captured by their
ranking on popular LLM leaderboards (""cramming for the leaderboard"").
Furthermore, simple probability calculations indicate that GPT-4's reasonable
performance on $k=5$ is suggestive of going beyond ""stochastic parrot"" behavior
(Bender et al., 2021), i.e., it combines skills in ways that it had not seen
during training.
  We sketch how the methodology can lead to a Skill-Mix based eco-system of
open evaluations for AI capabilities of future models.
","2023-10-27","2310.17567v1.pdf"
"2310.17588","Guangliang Liu","Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson and
  Rongrong Wang","PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven
  Perturbed Gradient Descent","Accepted to EMNLP23 main","","","","cs.LG cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Fine-tuning pretrained language models (PLMs) for downstream tasks is a
large-scale optimization problem, in which the choice of the training algorithm
critically determines how well the trained model can generalize to unseen test
data, especially in the context of few-shot learning. To achieve good
generalization performance and avoid overfitting, techniques such as data
augmentation and pruning are often applied. However, adding these
regularizations necessitates heavy tuning of the hyperparameters of
optimization algorithms, such as the popular Adam optimizer. In this paper, we
propose a two-stage fine-tuning method, PAC-tuning, to address this
optimization challenge. First, based on PAC-Bayes training, PAC-tuning directly
minimizes the PAC-Bayes generalization bound to learn proper parameter
distribution. Second, PAC-tuning modifies the gradient by injecting noise with
the variance learned in the first stage into the model parameters during
training, resulting in a variant of perturbed gradient descent (PGD). In the
past, the few-shot scenario posed difficulties for PAC-Bayes training because
the PAC-Bayes bound, when applied to large models with limited training data,
might not be stringent. Our experimental results across 5 GLUE benchmark tasks
demonstrate that PAC-tuning successfully handles the challenges of fine-tuning
tasks and outperforms strong baseline methods by a visible margin, further
confirming the potential to apply PAC training for any other settings where the
Adam optimizer is currently used for training.
","2023-10-27","2310.17588v1.pdf"
"2310.17589","Yucheng Li","Yucheng Li","An Open Source Data Contamination Report for Llama Series Models","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Data contamination in language model evaluation is increasingly prevalent as
the popularity of large language models. It allows models to ""cheat"" via
memorisation instead of displaying true capabilities. Therefore, contamination
analysis has became an crucial part of reliable model evaluation to validate
results. However, existing contamination analysis is usually conducted
internally by LLM developers and often lacks transparency and completeness.
This paper present an open source data contamination reports for the Llama
series models. We analyse six popular multi-choice QA benchmarks and quantify
their overlapping with the training set of Llama. Various levels of
contamination ranging from 1\% to 8.7\% are found across benchmarks. Our
comparison also reveals that Llama models can gain over 5\% higher accuracy on
contaminated subsets versus clean subsets. Data and code are available at:
https://github.com/liyucheng09/Contamination_Detector.
","2023-10-27","2310.17589v1.pdf"
"2310.17591","Venkata S Govindarajan","Venkata S Govindarajan, Juan Diego Rodriguez, Kaj Bostrom, Kyle
  Mahowald","Lil-Bevo: Explorations of Strategies for Training Language Models in
  More Humanlike Ways","Proceedings of the BabyLM Challenge","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained
our masked language models with three ingredients: an initial pretraining with
music data, training on shorter sequences before training on longer ones, and
masking specific tokens to target some of the BLiMP subtasks. Overall, our
baseline models performed above chance, but far below the performance levels of
larger LLMs trained on more data. We found that training on short sequences
performed better than training on longer sequences.Pretraining on music may
help performance marginally, but, if so, the effect seems small. Our targeted
Masked Language Modeling augmentation did not seem to improve model performance
in general, but did seem to help on some of the specific BLiMP tasks that we
were targeting (e.g., Negative Polarity Items). Training performant LLMs on
small amounts of data is a difficult but potentially informative task. While
some of our techniques showed some promise, more work is needed to explore
whether they can improve performance more than the modest gains here. Our code
is available at https://github.com/venkatasg/Lil-Bevo and out models at
https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a
","2023-10-27","2310.17591v1.pdf"
"2310.17606","Owen Henkel","Owen Henkel, Hannah Horne-Robinson, Libby Hills, Bill Roberts, Joshua
  McGrane","Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in
  Ghana","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  This paper reports on a set of three recent experiments utilizing large-scale
speech models to evaluate the oral reading fluency (ORF) of students in Ghana.
While ORF is a well-established measure of foundational literacy, assessing it
typically requires one-on-one sessions between a student and a trained
evaluator, a process that is time-consuming and costly. Automating the
evaluation of ORF could support better literacy instruction, particularly in
education contexts where formative assessment is uncommon due to large class
sizes and limited resources. To our knowledge, this research is among the first
to examine the use of the most recent versions of large-scale speech models
(Whisper V2 wav2vec2.0) for ORF assessment in the Global South.
  We find that Whisper V2 produces transcriptions of Ghanaian students reading
aloud with a Word Error Rate of 13.5. This is close to the model's average WER
on adult speech (12.8) and would have been considered state-of-the-art for
children's speech transcription only a few years ago. We also find that when
these transcriptions are used to produce fully automated ORF scores, they
closely align with scores generated by expert human graders, with a correlation
coefficient of 0.96. Importantly, these results were achieved on a
representative dataset (i.e., students with regional accents, recordings taken
in actual classrooms), using a free and publicly available speech model out of
the box (i.e., no fine-tuning). This suggests that using large-scale speech
models to assess ORF may be feasible to implement and scale in lower-resource,
linguistically diverse educational contexts.
","2023-10-27","2310.17606v1.pdf"
"2310.17623","Yonatan Oren","Yonatan Oren and Nicole Meister and Niladri Chatterji and Faisal
  Ladhak and Tatsunori B. Hashimoto","Proving Test Set Contamination in Black Box Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models are trained on vast amounts of internet data, prompting
concerns and speculation that they have memorized public benchmarks. Going from
speculation to proof of contamination is challenging, as the pretraining data
used by proprietary models are often not publicly accessible. We show that it
is possible to provide provable guarantees of test set contamination in
language models without access to pretraining data or model weights. Our
approach leverages the fact that when there is no data contamination, all
orderings of an exchangeable benchmark should be equally likely. In contrast,
the tendency for language models to memorize example order means that a
contaminated language model will find certain canonical orderings to be much
more likely than others. Our test flags potential contamination whenever the
likelihood of a canonically ordered benchmark dataset is significantly higher
than the likelihood after shuffling the examples. We demonstrate that our
procedure is sensitive enough to reliably prove test set contamination in
challenging situations, including models as small as 1.4 billion parameters, on
small test sets of only 1000 examples, and datasets that appear only a few
times in the pretraining corpus. Using our test, we audit five popular publicly
accessible language models for test set contamination and find little evidence
for pervasive contamination.
","2023-10-27","2310.17623v1.pdf"
"2310.17630","Heng Yang","Heng Yang, Ke Li","InstOptima: Evolutionary Multi-objective Instruction Optimization via
  Large Language Model-based Instruction Operators","Accepted by EMNLP Findings","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instruction-based language modeling has received significant attention in
pretrained language models. However, the efficiency of instruction engineering
remains low and hinders the development of instruction studies. Recent studies
have focused on automating instruction generation, but they primarily aim to
improve performance without considering other crucial objectives that impact
instruction quality, such as instruction length and perplexity. Therefore, we
propose a novel approach (i.e., InstOptima) that treats instruction generation
as an evolutionary multi-objective optimization problem. In contrast to text
edition-based methods, our approach utilizes a large language model (LLM) to
simulate instruction operators, including mutation and crossover. Furthermore,
we introduce an objective-guided mechanism for these operators, allowing the
LLM to comprehend the objectives and enhance the quality of the generated
instructions. Experimental results demonstrate improved fine-tuning performance
and the generation of a diverse set of high-quality instructions.
","2023-10-27","2310.17630v1.pdf"
"2310.17631","Lianghui Zhu","Lianghui Zhu, Xinggang Wang, Xinlong Wang","JudgeLM: Fine-tuned Large Language Models are Scalable Judges","30 pages, 23 figures","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Evaluating Large Language Models (LLMs) in open-ended scenarios is
challenging because existing benchmarks and metrics can not measure them
comprehensively. To address this problem, we propose to fine-tune LLMs as
scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in
open-ended benchmarks. We first propose a comprehensive, large-scale,
high-quality dataset containing task seeds, LLMs-generated answers, and
GPT-4-generated judgments for fine-tuning high-performance judges, as well as a
new benchmark for evaluating the judges. We train JudgeLM at different scales
from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its
capabilities and behaviors. We then analyze the key biases in fine-tuning LLM
as a judge and consider them as position bias, knowledge bias, and format bias.
To address these issues, JudgeLM introduces a bag of techniques including swap
augmentation, reference support, and reference drop, which clearly enhance the
judge's performance. JudgeLM obtains the state-of-the-art judge performance on
both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM
is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8
A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an
agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM
also demonstrates extended capabilities in being judges of the single answer,
multimodal models, multiple answers, and multi-turn chat.
","2023-10-27","2310.17631v1.pdf"
"2310.17639","Eric Bigelow","Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka,
  Tomer D. Ullman","In-Context Learning Dynamics with Random Binary Sequences","","","","","cs.AI cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) trained on huge corpora of text datasets
demonstrate complex, emergent capabilities, achieving state-of-the-art
performance on tasks they were not explicitly trained for. The precise nature
of LLM capabilities is often mysterious, and different prompts can elicit
different capabilities through in-context learning. We propose a Cognitive
Interpretability framework that enables us to analyze in-context learning
dynamics to understand latent concepts in LLMs underlying behavioral patterns.
This provides a more nuanced understanding than success-or-failure evaluation
benchmarks, but does not require observing internal activations as a
mechanistic interpretation of circuits would. Inspired by the cognitive science
of human randomness perception, we use random binary sequences as context and
study dynamics of in-context learning by manipulating properties of context
data, such as sequence length. In the latest GPT-3.5+ models, we find emergent
abilities to generate pseudo-random numbers and learn basic formal languages,
with striking in-context learning dynamics where model outputs transition
sharply from pseudo-random behaviors to deterministic repetition.
","2023-10-27","2310.17639v1.pdf"
