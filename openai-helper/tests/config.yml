model: "gpt-4o-mini"
temperature: 1.0
max_tokens: 1024
top_p: 1
frequency_penalty: 0
presence_penalty: 0
response_format: {"type": "json_object"}

system_prompt: |
  You are an expert research librarian. You are precise and can analyze the structure of papers very well. You return information in json format.


user_prompt: |
  Extract the title and authors and affiliations from the first page of a scientific paper. 

  Use the following step-by-step instructions to respond to user inputs.

  Extract the title and authors from the first page of a scientific paper. The paper text will snipped will be delimited by triple quotes. Geolocate each author affiliation with latitude and longitude.

  The output should have the following format:

  { "title": "The paper's title",
    "authors": [
      {
        "name": "Yong Ren",
        "email": null,
        "affiliations": [ "list of indices" ]
      }
    ],
    "affiliations": [ 
      {
        "index": "the index", 
        "name": "The affiliation name", 
        "longitude": "the longitude", 
        "latitude": "the latitude" 
      } 
    ]
  }

  '''
  Attention Is All You Need
  Ashish Vaswani∗
  Google Brain
  avaswani@google.com
  Noam Shazeer∗
  Google Brain
  noam@google.com
  Niki Parmar∗
  Google Research
  nikip@google.com
  Jakob Uszkoreit∗
  Google Research
  usz@google.com
  Llion Jones∗
  Google Research
  llion@google.com
  Aidan N. Gomez∗ †
  University of Toronto
  aidan@cs.toronto.edu
  Łukasz Kaiser∗
  Google Brain
  lukaszkaiser@google.com
  Illia Polosukhin∗ ‡
  illia.polosukhin@gmail.com
  Abstract
  The dominant sequence transduction models are based on complex recurrent or
  convolutional neural networks that include an encoder and a decoder. The best
  performing models also connect the encoder and decoder through an attention
  mechanism. We propose a new simple network architecture, the Transformer,
  based solely on attention mechanisms, dispensing with recurrence and convolutions
  entirely. Experiments on two machine translation tasks show these models to
  be superior in quality while being more parallelizable and requiring significantly
  less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
  to-German translation task, improving over the existing best results, including
  ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
  our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
  training for 3.5 days on eight GPUs, a small fraction of the training costs of the
  best models from the literature. We show that the Transformer generalizes well to
  other tasks by applying it successfully to English constituency parsing both with
  large and limited training data
  '''



