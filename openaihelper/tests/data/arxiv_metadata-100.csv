"id","submitter","authors","title","comments","journal-ref","doi","report-no","categories","license","abstract","update_date","src_pdf","text"
"2310.00014","Yong Ren","Yong Ren, Tao Wang, Jiangyan Yi, Le Xu, Jianhua Tao, Chuyuan Zhang,
  Junzuo Zhou","Fewer-token Neural Speech Codec with Time-invariant Codes","Submitted to ICASSP 2024","","","","cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Language model based text-to-speech (TTS) models, like VALL-E, have gained
attention for their outstanding in-context learning capability in zero-shot
scenarios. Neural speech codec is a critical component of these models, which
can convert speech into discrete token representations. However, excessive
token sequences from the codec may negatively affect prediction accuracy and
restrict the progression of Language model based TTS models. To address this
issue, this paper proposes a novel neural speech codec with time-invariant
codes named TiCodec. By encoding and quantizing time-invariant information into
a separate code, TiCodec can reduce the amount of frame-level information that
needs encoding, effectively decreasing the number of tokens as codes of speech.
Furthermore, this paper introduces a time-invariant encoding consistency loss
to enhance the consistency of time-invariant code within an utterance and force
it to capture more global information, which can benefit the zero-shot TTS
task. Experimental results demonstrate that TiCodec can not only enhance the
quality of reconstruction speech with fewer tokens but also increase the
similarity and naturalness, as well as reduce the word error rate of the
synthesized speech by the TTS model.
","2023-10-03","2310.00014v1.pdf","FEWER-TOKEN NEURAL SPEECH CODEC WITH TIME-INVARIANT CODES
Yong Ren1,2, Tao Wang1, Jiangyan Yi1, Le Xu1,2, Jianhua Tao3, Chuyuan Zhang1,2, Junzuo Zhou1,2
1Institute of Automation, Chinese Academy of Sciences, China
2University of Chinese Academy of Sciences, China
3Department of Automation, Tsinghua University, China
ABSTRACT
Language model based text-to-speech (TTS) models, like VALL-E,
have gained attention for their outstanding in-context learning capa-
bility in zero-shot scenarios. Neural speech codec is a critical com-
ponent of these models, which can convert speech into discrete token
representations. However, excessive token sequences from the codec
may negatively affect prediction accuracy and restrict the progres-
sion of Language model based TTS models. To address this issue,
this paper proposes a novel neural speech codec with time-invariant
codes named TiCodec. By encoding and quantizing time-invariant
information into a separate code, TiCodec can reduce the amount of
frame-level information that needs encoding, effectively decreasing
the number of tokens as codes of speech. Furthermore, this paper
introduces a time-invariant encoding consistency loss to enhance the
consistency of time-invariant code within an utterance and force it
to capture more global information, which can benefit the zero-shot
TTS task. Experimental results demonstrate that TiCodec can not
only enhance the quality of reconstruction speech with fewer tokens
but also increase the similarity and naturalness, as well as reduce the
word error rate of the synthesized speech by the TTS model.
Index Terms— speech codec, fewer tokens, time-invariant, lan-
guage model, text-to-speech
1. INTRODUCTION
arXiv:2310.00014v1  [cs.SD]  15 Sep 2023
VQVAE [13] as conditioning for the WaveNet decoder. After that,
SoundStream [14], as a fully convolutional end-to-end universal au-
dio codec model, was proposed, extending the VQVAE vector quan-
tizer to a residual vector quantizer. Following that, Encodec [4] in-
troduced a spectrogram-only adversarial loss, a novel gradient bal-
ancer, and a small Transformer model to further improve the per-
formance of codec. HifiCodec [15] proposes a codec model that
uses group-residual vector quantization to improve the reconstruc-
tion performance of audio. It can achieve good speech reconstruc-
tion performance with only four discrete token sequences, outper-
forming SoundStream and Encodec. However, the performance of
codec decreases significantly when using only one or two discrete
token sequences to represent speech, making it unable to reconstruct
high-quality speech.
To achieve good speech reconstruction performance with only
two or even one sequence of discrete frame-level tokens, we pro-
pose a neural speech codec model with time-invariant codes named
TiCodec. Some information in a speech that does not change over
time is extracted by a time-invariant representation extraction mod-
ule and encoded into a fixed-length code, referred to as the time-
invariant code. This operation can reduce the amount of information
that needs to be encoded in frame-level codes, forcing it to be max-
imally informative about time-related aspects. After obtaining the
frame-level and time-invariant features, they are separately quan-
tized as frame-level and time-invariant tokens. When TiCodec is
used for downstream TTS tasks, the time-invariant tokens can be
extracted from the prompt of target speakers, which can better main-
tain the timbre information of target speakers. At the same time,
fewer frame-level tokens can be used to predict by the TTS model,
while maintaining a low word error rate (WER) and high quality of
synthesized speech. To make the time-invariant token representa-
tions extracted from the target speech in TTS contain more global
time-invariant information, we introduce the time-invariant encod-
ing consistency loss, hoping to improve the robustness of inference
in TTS and further reduce WER.
The contributions of this paper are as follows:
• This paper proposed a neural speech codec model named
TiCodec, which can separate the time-varying and time-
invariant information in speech and quantize them separately.
• A time-invariant encoding consistency loss was introduced to
improve the consistency of the time-invariant codes.
Recently, large language models have demonstrated remarkable per-
formance on zero-shot text-to-speech (TTS) tasks such as VALL-
E [1], SPEAR-TTS [2], and SoundStorm [3]. VALL-E uses dis-
crete tokens derived from Encodec [4] as a representation of speech,
and then trains an autoregressive (AR) language model and a non-
autoregressive (NAR) language model to generate tokens from the
first quantizer and the other seven quantizers separately. It can syn-
thesize high-quality personalized speech by using a short recording
of an unknown speaker as an acoustic prompt. However, the high-
quality reconstruction of speech requires multiple token sequences,
which affects the inference speed and robustness, and restricts the
model structure and training methods of language model based TTS
models. Therefore, how to represent speech better with fewer dis-
crete tokens has become a core issue.
Neural speech codec is an important method to acquire discrete
token representations of speech. To improve the compression rate
and reduce the number of tokens, more and more research is focus-
ing on neural speech codec [5, 6, 7]. Kleijn et al. [8] proposed a
low-rate speech coding architecture based on the WaveNet [9] de-
coder.
Lyra [10] encodes quantized mel-spectrogram features of
speech, and then decodes them with WaveGRU [11]. Subsequently,
end-to-end neural speech codecs have been introduced.
Grbacea
et al. [12] used the discretized latent representations proposed in
Experimental results on speech reconstruction and zero-shot
TTS task with LibriTTS datasets [16] show that TiCodec achieved
better speech reconstruction performance with fewer tokens and
improved robustness, quality, and similarity of synthesized speech
in the zero-shot TTS task.
"
"2310.00031","Markus Marks","Neehar Kondapaneni, Markus Marks, Manuel Knott, Rog\'erio Guimar\~aes,
  Pietro Perona","Text-image Alignment for Diffusion-based Perception","Project page: https://www.vision.caltech.edu/tadp/","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving. Project page:
https://www.vision.caltech.edu/tadp/
","2023-10-06","2310.00031v1.pdf","Text-image Alignment for Diffusion-based Perception
Neehar Kondapaneni1* Markus Marks1∗
Manuel Knott2∗
Rog´erio Guimar˜aes1
Pietro Perona1
1California Institute of Technology
2ETH Z¨urich, Swiss Data Science Center, Empa
Abstract
Single-domain
Depth Estimation
Diffusion-Pretrained 
Vision Model
Segmentation
“a dog and a bird”
Captioner
+
”in a watercolor style”
Cross-domain
Object Detection
Caption Modifier
Figure 1. Text-Aligned Diffusion Perception (TADP). In TADP,
image captions align the text prompts and images passed to
diffusion-based vision models. In cross-domain tasks, target do-
main information is incorporated into the prompt to boost perfor-
mance.
Diffusion models are generative models with impressive
text-to-image synthesis capabilities and have spurred a new
wave of creative methods for classical machine learning
tasks.
However, the best way to harness the perceptual
knowledge of these generative models for visual tasks is
still an open question. Specifically, it is unclear how to
use the prompting interface when applying diffusion back-
bones to vision tasks. We find that automatically gener-
ated captions can improve text-image alignment and sig-
nificantly enhance a model’s cross-attention maps, leading
to better perceptual performance. Our approach improves
upon the current SOTA in diffusion-based semantic segmen-
tation on ADE20K and the current overall SOTA in depth
estimation on NYUv2. Furthermore, our method general-
izes to the cross-domain setting; we use model personal-
ization and caption modifications to align our model to the
target domain and find improvements over unaligned base-
lines. Our object detection model, trained on Pascal VOC,
achieves SOTA results on Watercolor2K. Our segmentation
method, trained on Cityscapes, achieves SOTA results on
Dark Zurich-val and Nighttime Driving.
1. Introduction
arXiv:2310.00031v1  [cs.CV]  29 Sep 2023
Diffusion models have set the state-of-the-art for image
generation [30, 33, 36, 48]. Recently, a few works have
shown diffusion pre-trained backbones have a strong prior
for scene understanding that allows them to perform well in
advanced discriminative vision tasks, such as semantic seg-
mentation and monocular depth estimation [16, 49]. Unlike
contrastive vision language models (like CLIP) [21, 25, 29],
generative models have a causal relationship with text, in
which text guides image generation.
In latent diffusion
models, text prompts control the denoising U-Net [34],
moving the image latent in a semantically meaningful di-
*Equal contribution.
rection [5].
We explore this relationship and find that text-image
alignment significantly improves the performance of
diffusion-based perception. We then investigate text-target
domain alignment in cross-domain vision tasks, finding that
aligning the text with the target domain while training on the
source domain can improve a model’s target domain perfor-
mance (Fig. 1).
We first study prompting for diffusion-based perceptual
models and find that increasing text-image alignment im-
proves semantic segmentation and depth estimation perfor-
mance. We hypothesize that unaligned text prompts can in-
troduce semantic shifts to the feature maps of the diffusion
model [5] and that these shifts can make it more difficult
for the task-specific head to solve the target task. Specifi-
1
"
"2310.00032","Qinghua Xu","Qinghua Xu, Tao Yue, Shaukat Ali and Maite Arratibel","Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event
  Analysis in Cyber-physical Systems","","","","","cs.SE","http://creativecommons.org/licenses/by/4.0/","  Cyber-Physical Systems (CPSs), e.g., elevator systems and autonomous driving
systems, are progressively permeating our everyday lives. To ensure their
safety, various analyses need to be conducted, such as anomaly detection and
time-to-event analysis (the focus of this paper). Recently, it has been widely
accepted that digital Twins (DTs) can serve as an efficient method to aid in
the development, maintenance, and safe and secure operation of CPSs. However,
CPSs frequently evolve, e.g., with new or updated functionalities, which demand
their corresponding DTs be co-evolved, i.e., in synchronization with the CPSs.
To that end, we propose a novel method, named PPT, utilizing an
uncertainty-aware transfer learning for DT evolution. Specifically, we first
pretrain PPT with a pretraining dataset to acquire generic knowledge about the
CPSs, followed by adapting it to a specific CPS with the help of prompt tuning.
Results highlight that PPT is effective in time-to-event analysis in both
elevator and ADSs case studies, on average, outperforming a baseline method by
7.31 and 12.58 in terms of Huber loss, respectively. The experiment results
also affirm the effectiveness of transfer learning, prompt tuning and
uncertainty quantification in terms of reducing Huber loss by at least 21.32,
3.14 and 4.08, respectively, in both case studies.
","2023-10-06","2310.00032v2.pdf","PRETRAIN, PROMPT, AND TRANSFER: EVOLVING DIGITAL
TWINS FOR TIME-TO-EVENT ANALYSIS IN CYBER-PHYSICAL
SYSTEMS
A PREPRINT
Qinghua Xu
Department of Engineering Complex Software Systems
Simula Research Laboratory
Oslo, Norway
qinghua@simula.no
Tao Yue
Department of Engineering Complex Software Systems
Simula Research Laboratory
Oslo, Norway
taoyue@gmail.com
Maite Arratibel
Orona Group
Hernani, Basque Country, Spain
marratibel@orona-group.com
Shaukat Ali
Department of Engineering Complex Software Systems
Simula Research Laboratory
Oslo, Norway
shaukat@simula.no
October 6, 2023
ABSTRACT
arXiv:2310.00032v2  [cs.SE]  3 Oct 2023
Cyber-Physical Systems (CPSs), e.g., elevator systems and autonomous driving systems, are progres-
sively permeating our everyday lives. To ensure their safety, various analyses need to be conducted,
such as anomaly detection and time-to-event analysis (the focus of this paper). Recently, it has
been widely accepted that digital Twins (DTs) can serve as an efficient method to aid in the devel-
opment, maintenance, and safe and secure operation of CPSs. However, CPSs frequently evolve,
e.g., with new or updated functionalities, which demand their corresponding DTs be co-evolved, i.e.,
in synchronization with the CPSs. To that end, we propose a novel method, named PPT, utilizing
an uncertainty-aware transfer learning for DT evolution. Specifically, we first pretrain PPT with a
pretraining dataset to acquire generic knowledge about the CPSs, followed by adapting it to a specific
CPS with the help of prompt tuning. Results highlight that PPT is effective in time-to-event analysis
in both elevator and ADSs case studies, on average, outperforming a baseline method by 7.31 and
12.58 in terms of Huber loss, respectively. The experiment results also affirm the effectiveness of
transfer learning, prompt tuning and uncertainty quantification in terms of reducing Huber loss by at
least 21.32, 3.14 and 4.08, respectively, in both case studies.
1
Introduction
Cyber-Physical Systems (CPSs) serve as essential elements in actualizing the vision of Industry 4.0 [1]. Unlike
conventional physical systems, a typical CPS incorporates a cyber component, linking physical systems through a
network. This combination of cyber and physical systems enables more intelligent and adept industrial applications,
especially in crucial infrastructures such as transportation systems. However, the increasing complexity, heterogeneity,
and constantly evolving nature of CPSs, brought about by introducing a rich array of functionalities, opens them up to
significant threats and challenges. This often renders existing security and safety techniques ineffective, emphasizing
the need to devise novel techniques to ensure the dependability of various CPS tasks.
"
"2310.00034","Yuzhang Shang","Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong","PB-LLM: Partially Binarized Large Language Models","Frist work using network binarization for large language model
  compression","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper explores network binarization, a radical form of quantization,
compressing model weights to a single bit, specifically for Large Language
Models (LLMs) compression. Due to previous binarization methods collapsing
LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can
achieve extreme low-bit quantization while maintaining the linguistic reasoning
capacity of quantized LLMs. Specifically, our exploration first uncovers the
ineffectiveness of naive applications of existing binarization algorithms and
highlights the imperative role of salient weights in achieving low-bit
quantization. Thus, PB-LLM filters a small ratio of salient weights during
binarization, allocating them to higher-bit storage, i.e.,
partially-binarization. PB-LLM is extended to recover the capacities of
quantized LMMs, by analyzing from the perspective of post-training quantization
(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts
from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian
matrix and successfully recover the reasoning capacity of PB-LLM in low-bit.
Under QAT, we freeze the salient weights during training, explore the
derivation of optimal scaling factors crucial for minimizing the quantization
error, and propose a scaling mechanism based on this derived scaling strategy
for residual binarized weights. Those explorations and the developed
methodologies significantly contribute to rejuvenating the performance of
low-bit quantized LLMs and present substantial advancements in the field of
network binarization for LLMs.The code is available at
https://github.com/hahnyuan/BinaryLLM.
","2023-10-03","2310.00034v1.pdf","PB-LLM: PARTIALLY BINARIZED LARGE LANGUAGE
MODELS
Yuzhang Shang∗
Illinois Institute of Technology
Zhihang Yuan∗
Huomo AI
Qiang Wu
Huomo AI
Zhen Dong
UC Berkeley
ABSTRACT
This paper explores network binarization, a radical form of quantization, compress-
ing model weights to a single bit, specifically for Large Language Models (LLMs)
compression. Due to previous binarization methods collapsing LLMs, we propose
a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme
low-bit quantization while maintaining the linguistic reasoning capacity of quan-
tized LLMs. Specifically, our exploration first uncovers the ineffectiveness of na¨ıve
applications of existing binarization algorithms and highlights the imperative role
of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small
ratio of salient weights during binarization, allocating them to higher-bit storage,
i.e., partially-binarization. PB-LLM is extended to recover the capacities of quan-
tized LMMs, by analyzing from the perspective of post-training quantization (PTQ)
and quantization-aware training (QAT). Under PTQ, combining the concepts from
GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix
and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT,
we freeze the salient weights during training, explore the derivation of optimal
scaling factors crucial for minimizing the quantization error, and propose a scaling
mechanism based on this derived scaling strategy for residual binarized weights.
Those explorations and the developed methodologies significantly contribute to
rejuvenating the performance of low-bit quantized LLMs and present substantial
advancements in the field of network binarization for LLMs. The code is available
at PB-LLM.
1
INTRODUCTION
arXiv:2310.00034v1  [cs.LG]  29 Sep 2023
Recently, large language models (LLMs) have gained significant traction in artificial intelligence. It
can be attributed to the success of models such as ChatGPT [Brown et al., 2020, Ouyang et al., 2022].
Following its lead, other LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao et al., 2022], and
LLaMA [Touvron et al., 2023] have emerged, proving that an increase in model size typically results
in enhanced capabilities. As a result, models with tens to hundreds of billions of parameters have
become the norm. However, their vast size poses considerable deployment challenges on memory-
constrained devices. A model such as the LLAMA-65B (with 65 billion parameters) requires at least
130GB of memory for inference - a number that often exceeds the capacity of a single GPU or server.
Many methods have been proposed to reduce the memory consumption of LLMs [Zhu et al., 2023].
Those methods can be categorized into weight quantization [Dettmers et al., 2022], network prun-
ing [Frantar and Alistarh, 2023], and low-rank factorization [Zhang et al., 2023]. Among these
compression paradigms, weight quantization is particularly prominent and widely adopted for LLMs.
Since it preserves the original model architecture and leverages well-trained LLMs’ full-precision
checkpoints, the compression process is greatly simplified [Zhu et al., 2023]. However, state-of-the-art
LLM quantization methods show a marked decline in quality beyond 4 bits [Liu et al., 2023a].
More aggressive compression methods are required to push the LLM quantization into the lower bit
range. The network binarization technique stands out, reducing the bit-width of weights to just one
bit [Helwegen et al., 2019, Rusci et al., 2020, Qin et al., 2020a; 2023]. The binarized models take
little storage and memory, and accelerate the inference by efficient bitwise operations. Compared
∗Equal contribution.
1
"
"2310.00035","Xi Wang","Xi Wang, Laurence Aitchison, Maja Rudolph","LoRA ensembles for large language model fine-tuning","Update the title in the PDF file","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by/4.0/","  Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test
data or out-of-distribution samples. One approach commonly used in vision for
alleviating this issue is a deep ensemble, which constructs an ensemble by
training the same model multiple times using different random initializations.
However, there is a huge challenge to ensembling LLMs: the most effective LLMs
are very, very large. Keeping a single LLM in memory is already challenging
enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many
settings. To address these issues, we propose an ensemble approach using
Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.
Critically, these low-rank adapters represent a very small number of
parameters, orders of magnitude less than the underlying pre-trained model.
Thus, it is possible to construct large ensembles of LoRA adapters with almost
the same computational overhead as using the original model. We find that LoRA
ensembles, applied on its own or on top of pre-existing regularization
techniques, gives consistent improvements in predictive accuracy and
uncertainty quantification.
","2023-10-06","2310.00035v1.pdf","Preprint. Under review
ENSEMBLE OF LOW-RANK ADAPTERS FOR LARGE
LANGUAGE MODEL FINE-TUNING
Xi Wang∗
UMass Amherst
Laurence Aitchison
University of Bristol
Maja Rudolph
Bosch Center for AI
ABSTRACT
Fine-tuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test data or
out-of-distribution samples. One approach commonly used in vision for alleviating
this issue is a deep ensemble, which constructs an ensemble by training the same
model multiple times using different random initializations. However, there is
a huge challenge to ensembling LLMs: the most effective LLMs are very, very
large. Keeping a single LLM in memory is already challenging enough: keeping
an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address
this issue, we propose an ensemble approach using Low-Rank Adapters (LoRA),
a parameter-efficient fine-tuning technique. Critically, these low-rank adapters
require a very small number of parameters, orders of magnitude less than the
underlying pre-trained model. Thus, it is possible to construct large ensembles of
LoRA adapters with almost the same computational overhead as using the original
model. We find that LoRA ensembles, applied on its own or on top of pre-existing
regularization techniques, gives consistent improvements in predictive accuracy
and uncertainty quantification.
1
INTRODUCTION
LLMs have demonstrated state-of-art performance in many natural language processing tasks (Rad-
ford et al., 2019; Touvron et al., 2023; Brown et al., 2020; Chung et al., 2022; Kojima et al., 2022;
OpenAI, 2023). With additional fine-tuning a pre-trained LLM can be adapted to downstream applica-
tions or data. However, fine-tuned LLMs can overfit to training data and often exhibit overconfidence
(as visualized in Fig.1a). Specifically, these models may yield overly certain predictions, especially
on incorrectly predicted samples or those from different domains. Ideally, a model should exhibit
low confidence when its predictions are likely to be incorrect; otherwise, the outcomes could be
dangerously misleading in safety-critical contexts such as medical diagnosis(Singhal et al., 2023),
finance (Yang et al., 2023), or decision-making processes (Li et al., 2022).
arXiv:2310.00035v1  [cs.LG]  29 Sep 2023
A widely adopted approach for mitigating overconfidence in deep learning is to make predictions
using an ensemble of neural networks rather than a single model. There are many approaches for
constructing an ensemble of networks, such as training multiple networks with different random
initializations (Lakshminarayanan et al., 2017), different hyperparameters (Wenzel et al., 2020b).
However, there are two barriers to applying these approaches for fine-tuning LLMs. First, ensembles
require storing multiple copies of the model weights and loading them into GPU at test time. This is
not practical for modern LLMs. A single LLaMA-13b (Touvron et al., 2023) stored at 16-bit precision,
is 25 GB on disk, and loading it to the GPU takes around 6 seconds. In addition, random initialization
has been noted to play a crucial role in deep ensembles (Lakshminarayanan et al., 2017; Fort et al.,
2019). However, starting the fine-tuning of the individual LLMs with the same initialization – the
pre-trained weights – eliminates an important source of randomness and may cause a lack of diversity
across the ensemble, thereby potentially reducing its benefits.
Work by Gleave & Irving (2022) and Sun et al. (2022) has attempted building ensembles of fine-tuned
LLMs but due to the limitations above, their method is restricted to smaller models such as GPT-2
(Radford et al., 2019) with only 1.5 billion parameters. In this paper, we build on recent advances in
efficient LLM fine-tuning with low-rank adapters (LoRA) (Hu et al., 2021) and propose an ensemble
∗Work done during an internship at the Bosch Center for AI. Contact: xwang3@cs.umass.edu
1
"
"2310.00036","Shengyi Huang","Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu,
  Santiago Onta\~n\'on","Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning
  Platform","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Distributed Deep Reinforcement Learning (DRL) aims to leverage more
computational resources to train autonomous agents with less training time.
Despite recent progress in the field, reproducibility issues have not been
sufficiently explored. This paper first shows that the typical actor-learner
framework can have reproducibility issues even if hyperparameters are
controlled. We then introduce Cleanba, a new open-source platform for
distributed DRL that proposes a highly reproducible architecture. Cleanba
implements highly optimized distributed variants of PPO and IMPALA. Our Atari
experiments show that these variants can obtain equivalent or higher scores
than strong IMPALA baselines in moolib and torchbeast and PPO baseline in
CleanRL. However, Cleanba variants present 1) shorter training time and 2) more
reproducible learning curves in different hardware settings. Cleanba's source
code is available at \url{https://github.com/vwxyzjn/cleanba}
","2023-10-03","2310.00036v1.pdf","CLEANBA: A REPRODUCIBLE AND EFFICIENT DIS-
TRIBUTED REINFORCEMENT LEARNING PLATFORM
Shengyi Huang‡
Jiayi Weng∗
Rujikorn Charakorn♯
Min Lin△
Zhongwen Xu♢
Santiago Onta˜n´on‡§
‡Drexel University
Hugging Face
§Google
♯VISTEC
△Sea AI Lab
♢Tencent AI Lab
costa.huang@outlook.com
ABSTRACT
Distributed Deep Reinforcement Learning (DRL) aims to leverage more compu-
tational resources to train autonomous agents with less training time. Despite
recent progress in the field, reproducibility issues have not been sufficiently ex-
plored. This paper first shows that the typical actor-learner framework can have
reproducibility issues even if hyperparameters are controlled. We then introduce
Cleanba, a new open-source platform for distributed DRL that proposes a highly
reproducible architecture. Cleanba implements highly optimized distributed vari-
ants of PPO (Schulman et al., 2017) and IMPALA (Espeholt et al., 2018). Our
Atari experiments show that these variants can obtain equivalent or higher scores
than strong IMPALA baselines in moolib and torchbeast and PPO base-
line in CleanRL. However, Cleanba variants present 1) shorter training time and
2) more reproducible learning curves in different hardware settings. Cleanba’s
source code is available at https://github.com/vwxyzjn/cleanba
1
INTRODUCTION
Deep Reinforcement Learning (DRL) is a technique to train autonomous agents to perform tasks.
In recent years, it has demonstrated remarkable success across various domains, including video
games (Mnih et al., 2015), robotics control (Schulman et al., 2017), chip design (Mirhoseini et al.,
2021), and large language model tuning (Ouyang et al., 2022). Distributed DRL (Espeholt et al.,
2018; 2020) has also become a fast-growing field that leverages more computing resources to train
agents. Despite recent progress, reproducibility issues in distributed DRL have not been sufficiently
explored. This paper introduces Cleanba, a new platform for distributed DRL that addresses repro-
ducibility issues under different hardware settings.
arXiv:2310.00036v1  [cs.LG]  29 Sep 2023
Reproducibility in DRL is a challenging issue. Not only are DRL algorithms brittle to hyperparam-
eters and neural network architectures (Henderson et al., 2018), implementation details are often
crucial for successfully applying DRL but frequently omitted from publications (Engstrom et al.,
2020; Andrychowicz et al., 2021; Huang et al., 2022a). Reproducibility issues in distributed DRL
are under-studied and arguably even more challenging. In particular, most high-profile distributed
DRL works, such as Apex-DQN (Horgan et al., 2018), IMPALA (Espeholt et al., 2018), R2D2 (Kap-
turowski et al., 2019), and Podracer Sebulba (Hessel et al., 2021) are not (fully) open-source. Fur-
thermore, earlier work pointed out that more actor threads not only improve training speed but cause
reproducibility issues – different hardware settings could impact the data efficiency in a non-linear
fashion (Mnih et al., 2016).
In this paper, we present a more principled approach to distributed DRL, in which different hardware
settings could make training speed slower or faster but do not impact data efficiency, thus making
scaling results more reproducible and predictable. We first analyze the typical actor-learner architec-
ture in IMPALA (Espeholt et al., 2018) and show that its parallelism paradigm could introduce repro-
ducibility issues due to the concurrent scheduling of different actor threads. We then propose a more
reproducible distributed architecture by better aligning the parallelized actor and learner’s compu-
∗Currently at OpenAI.
1
"
"2310.00068","Luchuan Song","Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, Chenliang Xu","Emotional Listener Portrait: Neural Listener Head Generation with
  Emotion","Accepted by ICCV2023","","","","cs.GR cs.AI cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Listener head generation centers on generating non-verbal behaviors (e.g.,
smile) of a listener in reference to the information delivered by a speaker. A
significant challenge when generating such responses is the non-deterministic
nature of fine-grained facial expressions during a conversation, which varies
depending on the emotions and attitudes of both the speaker and the listener.
To tackle this problem, we propose the Emotional Listener Portrait (ELP), which
treats each fine-grained facial motion as a composition of several discrete
motion-codewords and explicitly models the probability distribution of the
motions under different emotion in conversation. Benefiting from the
``explicit'' and ``discrete'' design, our ELP model can not only automatically
generate natural and diverse responses toward a given speaker via sampling from
the learned distribution but also generate controllable responses with a
predetermined attitude. Under several quantitative metrics, our ELP exhibits
significant improvements compared to previous methods.
","2023-10-10","2310.00068v1.pdf","Emotional Listener Portrait: Realistic Listener Motion Simulation in Conversation
Luchuan Song1
Guojun Yin2
Zhenchao Jin3
Xiaoyi Dong4
Chenliang Xu1
1University of Rochester
2University of Science and Technology of China
3University of Hong Kong
4Shanghai AI Laboratory
{lsong11@ur., chenliang.xu@}rochester.edu, gjyin@mail.ustc.edu.cn,
blwx96@connect.hku.hk, dongxiaoyi@pjlab.org.cn
Input: Speaker Video & Audio
Latent Space with Emotion
Output: Listener Video with Emotion Responsive and Blinking
Blink Coeffs.
Neutral
Space
Neutral
Positive
Space
Positive
Negative
Space
Negative
Figure 1: Illustration of our method on listener motion synthesis with the ternary emotional value as an example. Given
different emotional speakers (e.g. neutral, positive, and negative), our approach generates corresponding listeners under the
different emotional latent spaces.
Abstract
arXiv:2310.00068v1  [cs.GR]  29 Sep 2023
Listener head generation centers on generating non-
verbal behaviors (e.g., smile) of a listener in reference to the
information delivered by a speaker. A significant challenge
when generating such responses is the non-deterministic na-
ture of fine-grained facial expressions during a conversation,
which varies depending on the emotions and attitudes of
both the speaker and the listener. To tackle this problem, we
propose the Emotional Listener Portrait (ELP), which treats
each fine-grained facial motion as a composition of several
discrete motion-codewords and explicitly models the proba-
bility distribution of the motions under different emotion in
conversation. Benefiting from the “explicit” and “discrete”
design, our ELP model can not only automatically generate
natural and diverse responses toward a given speaker via
sampling from the learned distribution but also generate
controllable responses with a predetermined attitude. Under
several quantitative metrics, our ELP exhibits significant
improvements compared to previous methods.
1. Introduction
Listener Head Generation (LHG) technology aims to syn-
thesize the motion of the listener in response to the speaker.
In contrast to speaker head generation (SHG) [10, 44, 21,
13, 35, 47, 9, 53, 45], which focuses on generating lip-
speech synchronized portrait videos, LHG analyzes the talk-
ing semantics of the speaker automatically, without explicit
guidance, to synthesize corresponding interactive motions
of the listener. As shown in Figure 1, the listener reacts
positively when the speaker shares happy, and vice versa.
LHG can be employed in many applications, e.g. human-
computer interaction [57, 25, 60], virtual reality [22, 24],
metaverse [8, 7, 46] and media forensics [42, 37, 43, 17] etc.
The distinct nature of LHG, which necessitates a compre-
hensive modeling of the speaker’s motion [32], presents a
significant hurdle in yielding realistic listener head. In the
absence of audio-to-mouth matching evaluation, audiences
are more inclined to discern subtle changes in facial expres-
sions and head movements. However, the existing meth-
ods Responsive Listening Head Generation (RLHG) [61]
and Learning2Listen [31] have ignored these key compo-
nents. Specifically, RLHG [61] has replicated the regres-
sion experience from SHG [10], which weakens the non-
deterministic properties and smoothes the listener motion.
Meanwhile, although the motion categories in codebook pro-
posed by Learning2Listen [31] alleviate this problem, the
one-dimensional codebook from VQ-VAE [29] limits the
"
"2310.00074","Hangfeng He","Hangfeng He, Hongming Zhang, Dan Roth","SocREval: Large Language Models with the Socratic Method for
  Reference-Free Reasoning Evaluation","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  To comprehensively assess the capacity of current models for complex
reasoning, it is crucial to assess their step-by-step reasoning in a scalable
manner. Established reference-based evaluation metrics rely on human-annotated
reasoning chains to assess the model-derived chains. However, such
``gold-standard'' human-written reasoning chains may not be unique and their
acquisition is often labor-intensive. Existing reference-free reasoning metrics
eliminate the need for human-crafted reasoning chains as references, but they
typically require fine-tuning on datasets with human-derived reasoning chains,
which complicates the process and raises concerns regarding generalizability
across diverse datasets. To address these challenges, we harness GPT-4 to
automatically evaluate reasoning chain quality, obviating the need for
human-crafted references. Leveraging the Socratic method, we devise tailored
prompts to enhance reference-free reasoning evaluation, which we term SocREval
(Socratic method for Reasoning Evaluation). Empirical results from four human
annotated datasets reveal that SocREval significantly improves GPT-4's
performance, surpassing existing reference-free and reference-based reasoning
evaluation metrics. Beyond its demonstrated efficacy, our proposed framework,
large language models (LLMs) with the Socratic method, proves to be both
cost-efficient and robust to prompt writing and example selection, as
substantiated by our in-depth analysis.
","2023-10-03","2310.00074v1.pdf","SOCREVAL: Large Language Models with the Socratic Method for
Reference-Free Reasoning Evaluation
Hangfeng He†
Hongming Zhang‡
Dan Roth§
†University of Rochester
‡Tencent AI Lab, Seattle
§University of Pennsylvania
hanfeng.he@rochester.edu, hongmzhang@global.tencent.com
danroth@seas.upenn.edu
Abstract
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their
step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated
reasoning chains to assess the model-derived chains. However, such “gold-standard” human-written reasoning chains
may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate
the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with
human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across
diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality,
obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to en-
hance reference-free reasoning evaluation, which we term SOCREVAL (Socratic method for Reasoning Evaluation).
Empirical results from four human annotated datasets reveal that SOCREVAL significantly improves GPT-4’s perfor-
mance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated
efficacy, our proposed framework, large language models (LLMs) with the Socratic method, proves to be both cost-
efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.1
1
Introduction
arXiv:2310.00074v1  [cs.CL]  29 Sep 2023
Recent advances in large language models (LLMs) have led to state-of-the-art results in a plethora of natural language
processing (NLP) tasks, demonstrating the effectiveness of in-context learning without the need for task-specific train-
ing or fine-tuning (OpenAI, 2023; Anil et al., 2023; Touvron et al., 2023). Despite these impressive achievements,
the inherent reasoning capabilities of LLMs remain notably below human expectations (Arkoudas, 2023). Although
the core of reasoning fundamentally involves offering justifications, most contemporary evaluations primarily assess a
model’s reasoning capability based on its end-task performance (Huang & Chang, 2022). Such evaluations, focusing
solely on the accuracy of the final answer, neglect the complexities of the underlying reasoning chains. This oversight
inhibits a comprehensive understanding of a model’s reasoning ability and poses challenges to further advancements
in this domain.
To assess the quality of reasoning chains produced by models, a direct and intuitive approach centers on con-
trasting these generated chains with human-constructed ones, termed as reference-based reasoning evaluation (Clinciu
et al., 2021; Welleck et al., 2022; Saparov & He, 2022). However, these reference-based metrics highly rely on
human-constructed reasoning chains, which are both labor-intensive and costly. Furthermore, “gold-standard” rea-
soning chains may not be unique (Dalvi et al., 2021), implying that the effectiveness of reference-based evaluations
can be significantly influenced by the choice and breadth of human-crafted references. In light of these challenges,
recent research has begun to explore the evaluation of reasoning chains without necessitating human-annotated ref-
erences—termed reference-free reasoning evaluation (Golovneva et al., 2022; Prasad et al., 2023). Regrettably, these
reference-free metrics necessitate the fine-tuning of models on datasets with human-annotated reasoning chains, which
is not only complicated but also restricts their applicability across diverse datasets.
1Our code is publicly available at https://github.com/HornHehhf/SocREval.
1
"
"2310.00085","Haechan Mark Bong","Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni
  Beltrame","PEACE: Prompt Engineering Automation for CLIPSeg Enhancement in Aerial
  Robotics","Submitted to ICRA 2024. arXiv admin note: substantial text overlap
  with arXiv:2308.11471","","","","cs.RO","http://creativecommons.org/licenses/by-sa/4.0/","  From industrial to space robotics, safe landing is an essential component for
flight operations. With the growing interest in artificial intelligence, we
direct our attention to learning based safe landing approaches. This paper
extends our previous work, DOVESEI, which focused on a reactive UAV system by
harnessing the capabilities of open vocabulary image segmentation. Prompt-based
safe landing zone segmentation using an open vocabulary based model is no more
just an idea, but proven to be feasible by the work of DOVESEI. However, a
heuristic selection of words for prompt is not a reliable solution since it
cannot take the changing environment into consideration and detrimental
consequences can occur if the observed environment is not well represented by
the given prompt. Therefore, we introduce PEACE (Prompt Engineering Automation
for CLIPSeg Enhancement), powering DOVESEI to automate the prompt generation
and engineering to adapt to data distribution shifts. Our system is capable of
performing safe landing operations with collision avoidance at altitudes as low
as 20 meters using only monocular cameras and image segmentation. We take
advantage of DOVESEI's dynamic focus to circumvent abrupt fluctuations in the
terrain segmentation between frames in a video stream. PEACE shows promising
improvements in prompt generation and engineering for aerial images compared to
the standard prompt used for CLIP and CLIPSeg. Combining DOVESEI and PEACE, our
system was able improve successful safe landing zone selections by 58.62%
compared to using only DOVESEI. All the source code is open source and
available online.
","2023-10-03","2310.00085v1.pdf","PEACE: Prompt Engineering Automation for CLIPSeg Enhancement
in Aerial Robotics
Haechan Mark Bong∗, Rongge Zhang∗, Ricardo de Azambuja∗, Giovanni Beltrame∗
Abstract—From industrial to space robotics, safe landing is
an essential component for flight operations. With the growing
interest in artificial intelligence, we direct our attention to
learning based safe landing approaches. This paper extends our
previous work, DOVESEI, which focused on a reactive UAV
system by harnessing the capabilities of open vocabulary image
segmentation. Prompt-based safe landing zone segmentation
using an open vocabulary based model is no more just an
idea, but proven to be feasible by the work of DOVESEI.
However, a heuristic selection of words for prompt is not a
reliable solution since it cannot take the changing environment
into consideration and detrimental consequences can occur if
the observed environment is not well represented by the given
prompt. Therefore, we introduce PEACE (Prompt Engineering
Automation for CLIPSeg Enhancement), powering DOVESEI to
automate the prompt generation and engineering to adapt to
data distribution shifts. Our system is capable of performing
safe landing operations with collision avoidance at altitudes as
low as 20 meters using only monocular cameras and image
segmentation. We take advantage of DOVESEI’s dynamic focus
to circumvent abrupt fluctuations in the terrain segmentation
between frames in a video stream. PEACE shows promising
improvements in prompt generation and engineering for aerial
images compared to the standard prompt used for CLIP and
CLIPSeg. Combining DOVESEI and PEACE, our system was
able improve successful safe landing zone selections by 58.62%
compared to using only DOVESEI. All the source code is open
source and available online. 1
arXiv:2310.00085v1  [cs.RO]  29 Sep 2023
I. INTRODUCTION
Logistics stands as a pivotal element across diverse sectors,
ranging from e-commerce operations to complex military
undertakings. Application of Uncrewed [1], [2] Aerial Vehicles
(UAVs) is becoming part of research and industrial interest.
Within this context, autonomous robots have emerged as an
extensively sought-after solution. Notably, in modern urban
environments, aerial robots are being explored as a compelling
avenue to enhance last-mile delivery efficiency and reduce
carbon footprint. However, challenges concerning safety have
significantly hindered the widespread adoption of flying robots
in more densely populated areas. When not adequately de-
signed and operated, they can represent a possible threat
to structures, vehicles and the public in general, especially
if problems arise with their geolocation and other sensory
information such that it could impede safe landing. Therefore,
our aim is to achieve secure emergency landings without the
need for external communication, relying solely on onboard
computational capabilities and perceptual abilities of compact,
lightweight cameras.
The goal of UAV safe self-landing is to identify and descend
onto a designated Safe Landing Zone (SLZ), such as stable
ground, relatively flat grassland, or open fields / areas / parks,
while avoiding pedestrians, vehicles, and certain structures.
In the event of a localization (e.g., GPS) or remote control
communication failure, the operator should be able to regain
control after landing. Existing common automatic landing
systems employ either traditional localization and percep-
tion methods using Simultaneous Localization and Mapping
(SLAM), constrained by sensor performance and computa-
tional resources, or utilize conventional deep learning-based
image segmentation models, encountering domain adaptation
challenges.
This work extends our previous work (DOVESEI [3]2), a
system capable of running even with only a monocular RGB
camera, that can “dynamically focus“, by masking the received
raw segmentation according to the system’s current state.
Thanks to the advancements in large language models (LLMs),
DOVESEI can leverage open vocabulary models to allow it to
be easily “tuned“ only using language without extensive data
collection. LLMs for aerial robotics is a relatively underdevel-
oped area of research and the objective of DOVESEI was to
introduce the possibility of using an open vocabulary model
as a basis for segmentation and dynamically focus (mask) it
to improve the final system performance. However, DOVE-
SEI is based on the open vocabulary segmentation model,
CLIPSeg [4], which requires appropriate prompt inputs, and
the selection performance of SLZ is closely related to the input
prompts. CLIP [5] demonstrated that prompt engineering using
“A photo of {}.“ (where {} represents the terms defining the
class) already yields better results. Therefore, we used their
standard prompt engineering as a baseline and refer to CLIP’s
original prompt engineering as default and DOVESEI-DEF
for a DOVESEI setup using that. The initial motivation of
our work is due to the worse performance of aerial prompt
from DOVESEI compared to the DOVESEI-DEF. DOVE-
SEI’s original prompt engineering was created using CLIP
Interrogator [6] to heuristically produce better prompts for
the model. However, for the specific task of safe landing,
this approach is not rigorous enough. Given the constantly
changing aerial images, therefore data distribution shift, un-
reasonable prompts generated by CLIP Interrogator may lead
to SLZ selection failures. To address this, we propose PEACE
(Prompt Engineering Automation for CLIPSeg Enhancement),
aimed at enhancing the model’s adaptability and stability for
the application in ever-evolving real-world scenarios. PEACE
2Only available as an extended abstract.
∗MISTLab, ´Ecole Polytechnique Montr´eal, Montr´eal, Canada
Contact: giovanni.beltrame@polymtl.ca
This work was supported by the National Research Council Canada (NRC).
1https://github.com/MISTLab/PEACE
"
"2310.00092","Yang Su","Yang Su","Voice2Action: Language Models as Agent for Efficient Real-Time
  Interaction in Virtual Reality","","","","","cs.CL cs.AI cs.HC","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are trained and aligned to follow natural
language instructions with only a handful of examples, and they are prompted as
task-driven autonomous agents to adapt to various sources of execution
environments. However, deploying agent LLMs in virtual reality (VR) has been
challenging due to the lack of efficiency in online interactions and the
complex manipulation categories in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically analyzes customized voice signals
and textual commands through action and entity extraction and divides the
execution tasks into canonical interaction subsets in real-time with error
prevention from environment feedback. Experiment results in an urban
engineering VR environment with synthetic instruction data show that
Voice2Action can perform more efficiently and accurately than approaches
without optimizations.
","2023-10-03","2310.00092v1.pdf","Voice2Action: Language Models as Agent for
Efficient Real-Time Interaction in Virtual Reality
Yang Su
Cornell Tech
ys724@cornell.edu
Abstract
Large Language Models (LLMs) are trained
and aligned to follow natural language instruc-
tions with only a handful of examples, and they
are prompted as task-driven autonomous agents
to adapt to various sources of execution envi-
ronments. However, deploying agent LLMs
in virtual reality (VR) has been challenging
due to the lack of efficiency in online interac-
tions and the complex manipulation categories
in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically
analyzes customized voice signals and textual
commands through action and entity extraction
and divides the execution tasks into canonical
interaction subsets in real time with error pre-
vention from environment feedback. Experi-
ment results in an urban engineering VR envi-
ronment with synthetic instruction data show
that Voice2Action can perform more efficiently
and accurately than approaches without opti-
mizations.
1
Introduction
arXiv:2310.00092v1  [cs.CL]  29 Sep 2023
Large Language Models (LLMs) have demon-
strated impressive zero-shot and few-shot learn-
ing abilities in natural language understanding and
generation (Brown et al., 2020). With human align-
ments like reinforcement learning from human
feedback (RLHF), these models become better at
following human instructions (Ouyang et al., 2022);
with instruction prompting and providing external
resources (Nakano et al., 2021), they can be used
as agents to autonomously choose tools (Schick
et al., 2023), communicate with other agents (Shen
et al., 2023), and show superior ability in decision-
making and task execution.
However, the seamless integration of these mod-
els within VR has remained a challenging fron-
tier, hindered by efficiency, accuracy, and the com-
plexities associated with interactions and manipu-
lations in 3D spaces. Firstly, as a simulated three-
dimensional interaction environment that mimics
the real world, the VR environment has enormous
possibilities in the way that the user can interact
with entities (objects in the virtual scene) and ma-
nipulate their properties; secondly, the game en-
gines that execute the user instructions has a pre-
defined set of atomic operations for entity attribute
modifications, causing it non-trivial to map or clas-
sify the user instruction to the most proper config-
uration in the engine; lastly, the accuracy of VR
hardware (i.e., the voice recognition SDK, Wit.ai)
and the efficiency in 3D graphics rendering (i.e.,
the uv rendering pipeline) limits the number of op-
erations we can perform while not exceeding user’s
comfortable response time to receive the feedback
of the executed tasks.
In this paper, we focus on two main challenges
for deploying agent LLMs in VR: efficiency and
accuracy. While improving and balancing these
metrics, we plan to define how agent LLMs oper-
ate within the virtual environments, and then build
an interactive tool to provide users with a more
practical experience in developing their customized
virtual scene. Hence, we propose the Voice2Action
framework, created upon a rich taxonomy of text
input commands, ranging from simple object selec-
tion and state manipulation to more complex opera-
tions involving animation, scripted sequences, and
environment configuration modification. By hierar-
chical instruction prompting and entity extraction,
Voice2Action can accurately interpret users’ tex-
tual instructions by incorporating environmental
feedback.
To provide empirical validation, we conduct ex-
periments and ablation studies in an urban city plan-
ning virtual environment. We build a synthetic
dataset generated by the text-davinci-003 model
from OpenAI API with the self-instruct (Wang
et al., 2022) framework, where we use a pre-defined
canonical instruction subset as the seed tasks, and
manually filter out the unsatisfactory generated
instruction-execution pair. The results indicate a
1
"
"2310.00098","Tatiana Likhomanenko","Martin Pelikan, Sheikh Shams Azam, Vitaly Feldman, Jan ""Honza""
  Silovsky, Kunal Talwar, Tatiana Likhomanenko","Federated Learning with Differential Privacy for End-to-End Speech
  Recognition","Under review","","","","cs.LG cs.CR stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While federated learning (FL) has recently emerged as a promising approach to
train machine learning models, it is limited to only preliminary explorations
in the domain of automatic speech recognition (ASR). Moreover, FL does not
inherently guarantee user privacy and requires the use of differential privacy
(DP) for robust privacy guarantees. However, we are not aware of prior work on
applying DP to FL for ASR. In this paper, we aim to bridge this research gap by
formulating an ASR benchmark for FL with DP and establishing the first
baselines. First, we extend the existing research on FL for ASR by exploring
different aspects of recent $\textit{large end-to-end transformer models}$:
architecture design, seed models, data heterogeneity, domain shift, and impact
of cohort size. With a $\textit{practical}$ number of central aggregations we
are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even
with heterogeneous data, a seed model from another domain, or no pre-trained
seed model. Second, we apply DP to FL for ASR, which is non-trivial since DP
noise severely affects model training, especially for large transformer models,
due to highly imbalanced gradients in the attention block. We counteract the
adverse effect of DP noise by reviving per-layer clipping and explaining why
its effect is more apparent in our case than in the prior work. Remarkably, we
achieve user-level ($7.2$, $10^{-9}$)-$\textbf{DP}$ (resp. ($4.5$,
$10^{-9}$)-$\textbf{DP}$) with a 1.3% (resp. 4.6%) absolute drop in the word
error rate for extrapolation to high (resp. low) population scale for
$\textbf{FL with DP in ASR}$.
","2023-10-03","2310.00098v1.pdf","Preprint. Under review.
FEDERATED LEARNING WITH DIFFERENTIAL PRIVACY
FOR END-TO-END SPEECH RECOGNITION
Martin Pelikan∗, Sheikh Shams Azam, Vitaly Feldman, Jan “Honza” Silovsky,
Kunal Talwar, Tatiana Likhomanenko∗
Apple
{mpelikan,s azam,vitalyf,jsilovsky,ktalwar,antares}@apple.com
ABSTRACT
While federated learning (FL) has recently emerged as a promising approach to
train machine learning models, it is limited to only preliminary explorations in the
domain of automatic speech recognition (ASR). Moreover, FL does not inherently
guarantee user privacy and requires the use of differential privacy (DP) for robust
privacy guarantees. However, we are not aware of prior work on applying DP
to FL for ASR. In this paper, we aim to bridge this research gap by formulating
an ASR benchmark for FL with DP and establishing the first baselines. First,
we extend the existing research on FL for ASR by exploring different aspects of
recent large end-to-end transformer models: architecture design, seed models, data
heterogeneity, domain shift, and impact of cohort size. With a practical number of
central aggregations we are able to train FL models that are nearly optimal even
with heterogeneous data, a seed model from another domain, or no pre-trained seed
model. Second, we apply DP to FL for ASR, which is non-trivial since DP noise
severely affects model training, especially for large transformer models, due to
highly imbalanced gradients in the attention block. We counteract the adverse effect
of DP noise by reviving per-layer clipping and explaining why its effect is more
apparent in our case than in the prior work. Remarkably, we achieve user-level
(7.2, 10−9)-DP (resp. (4.5, 10−9)-DP) with a 1.3% (resp. 4.6%) absolute drop in
the word error rate for extrapolation to high (resp. low) population scale for FL
with DP in ASR.
1
INTRODUCTION
arXiv:2310.00098v1  [cs.LG]  29 Sep 2023
Federated learning (FL) allows training models in a distributed manner without storing data centrally
on a server (Koneˇcn´y et al., 2015). While FL on its own provides only limited privacy guaran-
tees (Boenisch et al., 2023; Carlini et al., 2023; Kariyappa et al., 2023; Bertran et al., 2019; Azam
et al., 2022), it can be combined with differential privacy (DP) (Dwork et al., 2014) and secure
aggregation (Bonawitz et al., 2016; Talwar et al., 2023) to provide strong privacy guarantees for users
(or clients) while training high quality models (Abadi et al., 2016). FL introduces a lot of challenges
into the model training, e.g. heterogeneous data (Li et al., 2020; Wang et al., 2020), scaling laws for
large cohort training (Charles et al., 2021), and convergence rate due to local training (Malinovsky
et al., 2022). Moreover, for practical FL with DP we are limited by the total privacy budget that we
can spend on hyper-parameter tuning, because it incurs additional overhead in terms of the privacy,
and communication and computation cost (Wang et al., 2018; Azam et al., 2021). Thus, robust models
and simple training recipes are of great interest.
Applying FL to train end-to-end (E2E) automatic speech recognition (ASR) models is also challeng-
ing (Guliani et al., 2021; Yu et al., 2021; Guliani et al., 2022; Gao et al., 2022; Nguyen et al., 2023)
especially due to the inherently heterogeneous data (Cui et al., 2021; Gao et al., 2022) (uniform
data sampling used for central training is impossible for FL) and models based on transformer
architecture (Synnaeve et al., 2020; Baevski et al., 2020; Gulati et al., 2020; Kim et al., 2022). It
is well known that to train high quality transformer models, we typically need to apply a lot of
optimization tricks such as learning rate warm-up and decay, gradient clipping, adaptive optimizers,
∗Equal contribution.
1
"
"2310.00149","Lecheng Kong","Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin
  Chen, Muhan Zhang","One for All: Towards Training One Graph Model for All Classification
  Tasks","","","","","cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Designing a single model that addresses multiple tasks has been a
long-standing objective in artificial intelligence. Recently, large language
models have demonstrated exceptional capability in integrating and solving
different tasks within the language domain. However, a unified model for
various tasks on graphs remains underexplored, primarily due to the challenges
unique to the graph learning domain. First, graph data from different areas
carry distinct attributes and follow different distributions. Such discrepancy
makes it hard to represent graphs in a single representation space. Second,
tasks on graphs diversify into node, link, and graph tasks, requiring distinct
embedding strategies. Finally, an appropriate graph prompting paradigm for
in-context learning is unclear. Striving to handle all the aforementioned
challenges, we propose One for All (OFA), the first general framework that can
use a single graph model to address the above challenges. Specifically, OFA
proposes text-attributed graphs to unify different graph data by describing
nodes and edges with natural language and uses language models to encode the
diverse and possibly cross-domain text attributes to feature vectors in the
same embedding space. Furthermore, OFA introduces the concept of
nodes-of-interest to standardize different tasks with a single task
representation. For in-context learning on graphs, OFA introduces a novel graph
prompting paradigm that appends prompting substructures to the input graph,
which enables it to address varied tasks without fine-tuning. We train the OFA
model using graph data from multiple domains (including citation networks,
molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its
ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs
well across different tasks, making it the first general-purpose graph
classification model across domains.
","2023-10-03","2310.00149v1.pdf","Preprint. Under review.
ONE FOR ALL:
TOWARDS TRAINING ONE GRAPH
MODEL FOR ALL CLASSIFICATION TASKS
Hao Liu1∗
Jiarui Feng1∗
Lecheng Kong1∗
Ningyue Liang1
Dacheng Tao2
Yixin Chen1
Muhan Zhang3
{liuhao, feng.jiarui, jerry.kong, fliang, ychen25}@wustl.edu,
dacheng.tao@gmail.com, muhan@pku.edu.cn
1Washington University in St. Louis
2JD Explore Academy
3Peking University
ABSTRACT
Designing a single model that addresses multiple tasks has been a long-standing
objective in artificial intelligence. Recently, large language models have demon-
strated exceptional capability in integrating and solving different tasks within the
language domain. However, a unified model for various tasks on graphs remains
underexplored, primarily due to the challenges unique to the graph learning do-
main. First, graph data from different areas carry distinct attributes and follow
different distributions. Such discrepancy makes it difficult to represent graphs in
a single representation space. Second, tasks on graphs diversify into node, link,
and graph tasks, requiring distinct embedding strategies. Finally, an appropriate
graph prompting paradigm for in-context learning is unclear. Striving to handle
all the aforementioned challenges, we propose One for All (OFA), the first gen-
eral framework that can use a single graph model to address the above challenges.
Specifically, OFA proposes text-attributed graphs to unify different graph data by
describing nodes and edges with natural language and uses language models to
encode the diverse and possibly cross-domain text attributes to feature vectors in
the same embedding space. Furthermore, OFA introduces the concept of nodes-
of-interest to standardize different tasks with a single task representation. For
in-context learning on graphs, OFA introduces a novel graph prompting paradigm
that appends prompting substructures to the input graph, which enables it to ad-
dress varied tasks without fine-tuning. We train the OFA model using graph data
from multiple domains (including citation networks, molecular graphs, knowledge
graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and
zero-shot learning scenarios. OFA performs well across different tasks, making
it the first general-purpose graph classification model across domains. All codes
can be found at https://github.com/LechengKong/OneForAll.
1
INTRODUCTION
arXiv:2310.00149v1  [cs.LG]  29 Sep 2023
Recently, large language models (LLMs) have received tremendous attention due to their power and
versatility in solving natural language tasks like text generation, machine translation, and question-
answering. LLMs’ in-context learning ability and universality allow the model to directly perform
various cross-domain downstream tasks by providing related context or prompt to the model, there-
fore avoiding any fine-tuning on model parameters (Brown et al., 2020; Zhang et al., 2023b; Lu
et al., 2021; Bommasani et al., 2021).
Despite the great success of foundation models on language, developing a foundation model for
graph structure data is less explored. Particularly, several challenges unique to graph data pre-
vent the direct transfer of foundation model design from the language domain to the graph domain.
First, although the natures of language tasks differ, they are still uniformly represented in human-
interpretable texts. An LLM can encode them into the same text embedding space and train on
different source tasks together. However, graph datasets from different sources are usually com-
pletely different in feature representation. Concretely, widely used graph datasets include citation
∗Contributed equally.
1
"
"2310.00158","Reyhane Askari Hemmat","Reyhane Askari Hemmat, Mohammad Pezeshki, Florian Bordes, Michal
  Drozdzal, Adriana Romero-Soriano","Feedback-guided Data Synthesis for Imbalanced Classification","","","","","cs.CV cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Current status quo in machine learning is to use static datasets of real
images for training, which often come from long-tailed distributions. With the
recent advances in generative models, researchers have started augmenting these
static datasets with synthetic data, reporting moderate performance
improvements on classification tasks. We hypothesize that these performance
gains are limited by the lack of feedback from the classifier to the generative
model, which would promote the usefulness of the generated samples to improve
the classifier's performance. In this work, we introduce a framework for
augmenting static datasets with useful synthetic samples, which leverages
one-shot feedback from the classifier to drive the sampling of the generative
model. In order for the framework to be effective, we find that the samples
must be close to the support of the real data of the task at hand, and be
sufficiently diverse. We validate three feedback criteria on a long-tailed
dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On
ImageNet-LT, we achieve state-of-the-art results, with over 4 percent
improvement on underrepresented classes while being twice efficient in terms of
the number of generated synthetic samples. NICO++ also enjoys marked boosts of
over 5 percent in worst group accuracy. With these results, our framework paves
the path towards effectively leveraging state-of-the-art text-to-image models
as data sources that can be queried to improve downstream applications.
","2023-10-03","2310.00158v1.pdf","FEEDBACK-GUIDED DATA SYNTHESIS FOR
IMBALANCED CLASSIFICATION
Reyhane Askari Hemmat1,2,3,†
Mohammad Pezeshki1∗
Florian Bordes1,2,3 ∗
Michal Drozdzal1
Adriana Romero Soriano1,2,4
1FAIR at Meta 2Mila
3Universit´e de Montr´eal
4 McGill University, CIFAR AI chair
ABSTRACT
Current status quo in machine learning is to use static datasets of real images
for training, which often come from long-tailed distributions. With the recent
advances in generative models, researchers have started augmenting these static
datasets with synthetic data, reporting moderate performance improvements on
classification tasks. We hypothesize that these performance gains are limited by
the lack of feedback from the classifier to the generative model, which would
promote the usefulness of the generated samples to improve the classifier’s per-
formance. In this work, we introduce a framework for augmenting static datasets
with useful synthetic samples, which leverages one-shot feedback from the clas-
sifier to drive the sampling of the generative model. In order for the framework to
be effective, we find that the samples must be close to the support of the real data
of the task at hand, and be sufficiently diverse. We validate three feedback criteria
on a long-tailed dataset (ImageNet-LT) as well as a group-imbalanced dataset
(NICO++). On ImageNet-LT, we achieve state-of-the-art results, with over 4%
improvement on underrepresented classes while being twice efficient in terms of
the number of generated synthetic samples. NICO++ also enjoys marked boosts
of over 5% in worst group accuracy. With these results, our framework paves the
path towards effectively leveraging state-of-the-art text-to-image models as data
sources that can be queried to improve downstream applications.
1
INTRODUCTION
arXiv:2310.00158v1  [cs.CV]  29 Sep 2023
In the recent year, we have witnessed unprecedented progress in image generative models (Ho et al.,
2020; Nichol et al., 2022; Ramesh et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Saharia
et al., 2022; Balaji et al., 2022; Kang et al., 2023). The photo-realistic results achieved by these
models has propelled an arms race towards their widespread use in content creation applications,
and as a byproduct, the research community has focused on developing models and techniques to
improve image realism (Kang et al., 2023) and conditioning-generation consistency (Hu et al., 2023;
Yarom et al., 2023; Xu et al., 2023). Yet, the potential for those models to become sources of data to
train machine learning models is still under debate, raising intriguing questions about the qualities
that the synthetic data must possess to be effective in training downstream representation learning
models.
Several recent works have proposed using generative models as either data augmentation or sole
source of data to train machine learning models (He et al., 2023; Sariyildiz et al., 2023; Shipard
et al., 2023; Bansal & Grover, 2023; Dunlap et al., 2023; Gu et al., 2023; Astolfi et al., 2023; Tian
et al., 2023), reporting moderate model performance gains. These works operate in a static scenario,
where the models being trained do not provide any feedback to the synthetic data collection process
that would ensure the usefulness of the generated samples. Instead, to achieve performance gains,
the proposed approaches often rely on laborious ’prompt engineering’ (Gu et al., 2023) to promote
synthetic data to be close to the support of the real data distribution on which the downstream
representation learning model is to be deployed (Shin et al., 2023). Moreover, recent studies have
highlighted the limited conditional diversity in the samples generated by state-of-the-art image gen-
erative models (Hall et al., 2023; Cho et al., 2022; Luccioni et al., 2023; Bianchi et al., 2022), which
∗Equal contribution, † Corresponding author: reyhaneaskari@meta.com.
1
"
"2310.00160","Junmo Kang","Junmo Kang, Hongyin Luo, Yada Zhu, James Glass, David Cox, Alan
  Ritter, Rogerio Feris, Leonid Karlinsky","Self-Specialization: Uncovering Latent Expertise within Large Language
  Models","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Recent works have demonstrated the effectiveness of self-alignment in which a
large language model is, by itself, aligned to follow general instructions
through the automatic generation of instructional data using a handful of
human-written seeds. Instead of general alignment, in this work, we focus on
self-alignment for expert domain specialization (e.g., biomedicine),
discovering it to be very effective for improving zero-shot and few-shot
performance in target domains of interest. As a preliminary, we first present
the benchmark results of existing aligned models within a specialized domain,
which reveals the marginal effect that ""generic"" instruction-following training
has on downstream expert domains' performance. To remedy this, we explore
self-specialization that leverages domain-specific unlabelled data and a few
labeled seeds for the self-alignment process. When augmented with retrieval to
reduce hallucination and enhance concurrency of the alignment,
self-specialization offers an effective (and efficient) way of ""carving out"" an
expert model out of a ""generalist"", pre-trained LLM where different domains of
expertise are originally combined in a form of ""superposition"". Our
experimental results on a biomedical domain show that our self-specialized
model (30B) outperforms its base model, MPT-30B by a large margin and even
surpasses larger popular models based on LLaMA-65B, highlighting its potential
and practicality for specialization, especially considering its efficiency in
terms of data and parameters.
","2023-10-03","2310.00160v1.pdf","SELF-SPECIALIZATION:
UNCOVERING LATENT EX-
PERTISE WITHIN LARGE LANGUAGE MODELS
Junmo Kang∗1
Hongyin Luo2
Yada Zhu3
James Glass2
David Cox3
Alan Ritter1
Rogerio Feris3
Leonid Karlinsky3
1Georgia Institute of Technology
2Massachusetts Institute of Technology
3MIT-IBM Watson AI Lab
ABSTRACT
Recent works have demonstrated the effectiveness of self-alignment in which a
large language model is, by itself, aligned to follow general instructions through
the automatic generation of instructional data using a handful of human-written
seeds. Instead of general alignment, in this work, we focus on self-alignment
for expert domain specialization (e.g., biomedicine), discovering it to be very ef-
fective for improving zero-shot and few-shot performance in target domains of
interest.
As a preliminary, we first present the benchmark results of existing
aligned models within a specialized domain, which reveals the marginal effect that
“generic” instruction-following training has on downstream expert domains’ per-
formance. To remedy this, we explore self-specialization that leverages domain-
specific unlabelled data and a few labeled seeds for the self-alignment process.
When augmented with retrieval to reduce hallucination and enhance concurrency
of the alignment, self-specialization offers an effective (and efficient) way of
“carving out” an expert model out of a “generalist”, pre-trained LLM where dif-
ferent domains of expertise are originally combined in a form of “superposition”.
Our experimental results on a biomedical domain show that our self-specialized
model (30B) outperforms its base model, MPT-30B by a large margin and even
surpasses larger popular models based on LLaMA-65B, highlighting its potential
and practicality for specialization, especially considering its efficiency in terms of
data and parameters.
1
INTRODUCTION
🔥
Sports
Biomedicine
Self-Specialization
🔥
🔥
Finance
Carving out
arXiv:2310.00160v1  [cs.CL]  29 Sep 2023
latent expertise
🔥
Specialized LoRA
Law
Biomedicine Expert
Plug-and-play
🔥
Base LLM   ✚
Figure 1: Self-specialization concept. Exper-
tise in various domains is mixed and latent
within base LLMs. Target domain expertise
is carved out through self-specialization.
Instruction-tuning (Ouyang et al., 2022; Wei et al.,
2022; Mishra et al., 2022; Su et al., 2022) of
large language models (LLMs) offers a mechanism
to adeptly guide models using specific directives,
thereby enhancing their versatility across diverse
tasks. However, as promising as this concept might
seem, it poses an inherent challenge: the substan-
tial need for quality data (Chung et al., 2022; Wan
et al., 2023; K¨opf et al., 2023). The very premise
of instruction-tuning hinges on the availability of
well-crafted, human-annotated data, a resource that
is both time-consuming and challenging to scale ef-
ficiently (Honovich et al., 2022; Kang et al., 2023).
Furthermore, acquiring domain-specific data is even more demanding as it requires the involvement
of domain experts, which is often more expensive (Bai et al., 2021; Wang et al., 2023).
Emerging as a promising solution to this data-intensive challenge is the approach of self-alignment
(Wang et al., 2022a; Sun et al., 2023). By allowing LLMs to automatically generate instructional
∗Work done during internship at MIT-IBM Watson AI Lab.
1
"
"2310.00163","Angelos Mavrogiannis","Angelos Mavrogiannis, Christoforos Mavrogiannis, Yiannis Aloimonos","Cook2LTL: Translating Cooking Recipes to LTL Formulae using Large
  Language Models","","","","","cs.RO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Cooking recipes are especially challenging to translate to robot plans as
they feature rich linguistic complexity, temporally-extended interconnected
tasks, and an almost infinite space of possible actions. Our key insight is
that combining a source of background cooking domain knowledge with a formalism
capable of handling the temporal richness of cooking recipes could enable the
extraction of unambiguous, robot-executable plans. In this work, we use Linear
Temporal Logic (LTL) as a formal language expressible enough to model the
temporal nature of cooking recipes. Leveraging pre-trained Large Language
Models (LLMs), we present a system that translates instruction steps from an
arbitrary cooking recipe found on the internet to a series of LTL formulae,
grounding high-level cooking actions to a set of primitive actions that are
executable by a manipulator in a kitchen environment. Our approach makes use of
a caching scheme, dynamically building a queryable action library at runtime,
significantly decreasing LLM API calls (-51%), latency (-59%) and cost (-42%)
compared to a baseline that queries the LLM for every newly encountered action
at runtime. We demonstrate the transferability of our system in a realistic
simulation platform through showcasing a set of simple cooking tasks.
","2023-10-03","2310.00163v1.pdf","Cook2LTL: Translating Cooking Recipes to LTL Formulae
using Large Language Models
Angelos Mavrogiannis1, Christoforos Mavrogiannis2, and Yiannis Aloimonos1
Abstract— Cooking recipes are especially challenging to
translate to robot plans as they feature rich linguistic complex-
ity, temporally-extended interconnected tasks, and an almost
infinite space of possible actions. Our key insight is that
combining a source of background cooking domain knowledge
with a formalism capable of handling the temporal richness of
cooking recipes could enable the extraction of unambiguous,
robot-executable plans. In this work, we use Linear Temporal
Logic (LTL) as a formal language expressible enough to model
the temporal nature of cooking recipes. Leveraging pre-trained
Large Language Models (LLMs), we present a system that
translates instruction steps from an arbitrary cooking recipe
found on the internet to a series of LTL formulae, grounding
high-level cooking actions to a set of primitive actions that
are executable by a manipulator in a kitchen environment.
Our approach makes use of a caching scheme, dynamically
building a queryable action library at runtime, significantly
decreasing LLM API calls (−51%), latency (−59%) and cost
(−42%) compared to a baseline that queries the LLM for
every newly encountered action at runtime. We demonstrate the
transferability of our system in a realistic simulation platform
through showcasing a set of simple cooking tasks.
I. INTRODUCTION
Fig. 1: Cook2LTL in AI2-THOR [19]: The robot is given
the instruction Refrigerate the apple. Cook2LTL produces an
initial LTL formula ϕ (top left); then it queries an LLM to
retrieve the low-level admissible primitives for executing the
action; finally it generates a formula consisting of 4 atomic
propositions (ψ1, ψ2, ψ3, ψ4) that provide the required task
specification and yield these consecutive scenes.
knowledge to fill in the missing steps. For example, recipes
with eggs do not explicitly state the prerequisite steps of
cracking them and extracting their contents. Additionally,
although inherently sequential, recipes often include addi-
tional explicit sequencing language (e.g. until, before, once)
that clearly defines the temporal action boundaries.
arXiv:2310.00163v1  [cs.RO]  29 Sep 2023
To be useful in household environments, robots may need
to understand and execute instructions from novice users.
Natural language is possibly the easiest way for users to
provide instructions to robots but it is often too vague.
This motivates the need for mapping natural language to
actionable, robot-executable commands. This is a challenging
problem, especially for complex activities that include tem-
porally correlated subtasks, such as following instructions in
a manual, or performing a delicate assembly task.
In this paper, we focus on translating cooking recipes into
executable robot plans. Cooking is one of the most common
household activities and poses a unique set of challenges
to robots [5]. It usually requires following a recipe, written
assuming that the reader has some background experience
in cooking and commonsense reasoning to understand and
complete the instruction steps. Recipes often feature am-
biguous language [25], such as omitting arguments that are
easily inferred from context (the known “Zero Anaphora”
problem [18]; see Fig. 3b where the direct object of the verb
“cook” is missing), or, more crucially, underspecified tasks
under the assumption that the reader possesses the necessary
1Department of Computer Science, University of Maryland, Col-
lege Park, 8125 Paint Branch Dr, College Park, MD 20742, USA.
angelosm@cs.umd.edu, jyaloimo@cs.umd.edu
2Department of Robotics, University of Michigan, Ann Arbor, MI, 48105.
cmavro@umich.edu.
Our code can be found at this link. A video with an example simulation
rollout can be found at this link.
Motivated by these observations, our key insight is that
combining a source of background cooking domain knowl-
edge with a formalism capable of handling the temporal
richness of cooking recipes could enable the extraction of un-
ambiguous, robot-executable plans. Our main contribution
is a system that receives a cooking recipe in natural language
form, reduces high-level cooking actions to robot-executable
primitive actions through the use of LLMs, and produces
unambiguous task specifications written in the form of LTL
formulae (See Fig. 1). These plans are then suitable for use
in downstream robotic tasks. We cache the action reduction
policy, incrementally building a queryable action library and
limiting proprietary LLM API calls with significant benefits
in cost (−42%) and computation time (−59%) compared to
a baseline that queries the LLM for every unseen action at
runtime. We build and evaluate our method based on a subset
of recipes from the Recipe1M+ corpus [26], and demonstrate
its transferability to an embodied robotic platform through
experiments in a simulated kitchen in AI2-THOR [19].
"
"2310.00164","Keivan Rezaei","Keivan Rezaei, Mehrdad Saberi, Mazda Moayeri, Soheil Feizi","PRIME: Prioritizing Interpretability in Failure Mode Extraction","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  In this work, we study the challenge of providing human-understandable
descriptions for failure modes in trained image classification models. Existing
works address this problem by first identifying clusters (or directions) of
incorrectly classified samples in a latent space and then aiming to provide
human-understandable text descriptions for them. We observe that in some cases,
describing text does not match well with identified failure modes, partially
owing to the fact that shared interpretable attributes of failure modes may not
be captured using clustering in the feature space. To improve on these
shortcomings, we propose a novel approach that prioritizes interpretability in
this problem: we start by obtaining human-understandable concepts (tags) of
images in the dataset and then analyze the model's behavior based on the
presence or absence of combinations of these tags. Our method also ensures that
the tags describing a failure mode form a minimal set, avoiding redundant and
noisy descriptions. Through several experiments on different datasets, we show
that our method successfully identifies failure modes and generates
high-quality text descriptions associated with them. These results highlight
the importance of prioritizing interpretability in understanding model
failures.
","2023-10-03","2310.00164v1.pdf","PRIME: PRIORITIZING INTERPRETABILITY IN FAIL-
URE MODE EXTRACTION
Keivan Rezaei1∗, Mehrdad Saberi1∗, Mazda Moayeri1, Soheil Feizi1
1Department of Computer Science, University of Maryland
{krezaei,msaberi,mmoayeri,sfeizi}@umd.edu
ABSTRACT
In this work, we study the challenge of providing human-understandable descrip-
tions for failure modes in trained image classification models. Existing works
address this problem by first identifying clusters (or directions) of incorrectly clas-
sified samples in a latent space and then aiming to provide human-understandable
text descriptions for them. We observe that in some cases, describing text does
not match well with identified failure modes, partially owing to the fact that
shared interpretable attributes of failure modes may not be captured using clus-
tering in the feature space. To improve on these shortcomings, we propose a
novel approach that prioritizes interpretability in this problem: we start by ob-
taining human-understandable concepts (tags) of images in the dataset and then
analyze the model’s behavior based on the presence or absence of combinations
of these tags. Our method also ensures that the tags describing a failure mode
form a minimal set, avoiding redundant and noisy descriptions. Through several
experiments on different datasets, we show that our method successfully iden-
tifies failure modes and generates high-quality text descriptions associated with
them. These results highlight the importance of prioritizing interpretability in un-
derstanding model failures.
1
INTRODUCTION
A plethora of reasons (spurious correlations, imbalanced data, corrupted inputs, etc.) may lead a
model to underperform on a specific subpopulation; we term this a failure mode. Failure modes
are challenging to identify due to the black-box nature of deep models, and further, they are often
obfuscated by common metrics like overall accuracy, leading to a false sense of security. However,
these failures can have significant real-world consequences, such as perpetuating algorithmic bias
(Buolamwini & Gebru, 2018) or unexpected catastrophic failure under distribution shift. Thus, the
discovery and description of failure modes is crucial in building reliable AI, as we cannot fix a
problem without first diagnosing it.
arXiv:2310.00164v1  [cs.CV]  29 Sep 2023
Detection of failure modes or biases within trained models has been studied in the literature. Prior
work (Tsipras et al., 2020; Vasudevan et al., 2022) requires humans in the loop to get a sense of biases
or subpopulations on which a model underperforms. Some other methods (Sohoni et al., 2020b;
Nam et al., 2020; Kim et al., 2019; Liu et al., 2021) do the process of capturing and intervening in
hard inputs without providing human-understandable descriptions for challenging subpopulations.
Providing human-understandable and interpretable descriptions for failure modes not only enables
humans to easily understand hard subpopulations, but enables the use of text-to-image methods
(Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Kattakinda et al., 2022) to generate
relevant images corresponding to failure modes to improve model’s accuracy over them.
Recent work (Eyuboglu et al., 2022; Jain et al., 2022; Kim et al., 2023; d’Eon et al., 2021) takes
an important step in improving failure mode diagnosis by additionally finding natural language de-
scriptions of detected failure modes, namely via leveraging modern vision-language models. These
methodologies leverage the shared vision-language latent space, discerning intricate clusters or di-
rections within this space, and subsequently attributing human-comprehensible descriptions to them.
∗Equal contribution.
1
"
"2310.00166","Martin Klissarov","Martin Klissarov, Pierluca D'Oro, Shagun Sodhani, Roberta Raileanu,
  Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff","Motif: Intrinsic Motivation from Artificial Intelligence Feedback","The first two authors equally contributed - order decided by coin
  flip","","","","cs.AI cs.LG","http://creativecommons.org/licenses/by/4.0/","  Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
","2023-10-03","2310.00166v1.pdf","MOTIF: INTRINSIC MOTIVATION FROM
ARTIFICIAL INTELLIGENCE FEEDBACK
Martin Klissarov*, 1, 2, 5 & Pierluca D’Oro*, 1, 2, 4, Shagun Sodhani2, Roberta Raileanu2,
Pierre-Luc Bacon1, 4, Pascal Vincent1, 2, Amy Zhang2, 3, Mikael Henaff2
1 Mila, 2 FAIR at Meta, 3 UT Austin, 4 Universit´e de Montr´eal, 5 McGill University
ABSTRACT
Exploring rich environments and evaluating one’s actions without prior knowledge
is immensely challenging. In this paper, we propose Motif, a general method to in-
terface such prior knowledge from a Large Language Model (LLM) with an agent.
Motif is based on the idea of grounding LLMs for decision-making without requir-
ing them to interact with the environment: it elicits preferences from an LLM over
pairs of captions to construct an intrinsic reward, which is then used to train agents
with reinforcement learning. We evaluate Motif’s performance and behavior on
the challenging, open-ended and procedurally-generated NetHack game. Surpris-
ingly, by only learning to maximize its intrinsic reward, Motif achieves a higher
game score than an algorithm directly trained to maximize the score itself. When
combining Motif’s intrinsic reward with the environment reward, our method sig-
nificantly outperforms existing approaches and makes progress on tasks where no
advancements have ever been made without demonstrations. Finally, we show that
Motif mostly generates intuitive human-aligned behaviors which can be steered
easily through prompt modifications, while scaling well with the LLM size and
the amount of information given in the prompt.
1
INTRODUCTION
Where do rewards come from?
An artificial intelligence agent introduced into a new environ-
ment without prior knowledge has to start from a blank slate. What is good and what is bad in
this environment? Which actions will lead to better outcomes or yield new information? Imag-
ine tasking an agent with the goal of opening a locked door.
The first time the agent finds a
key, it will have no idea whether this could be useful for achieving the goal of opening a door:
1000
Motif (int. only)
Score
500
it has to learn this fact by interaction. A human, instead,
would know by mere common sense that picking up a key
is generally desirable for opening doors. Since the idea
of manually providing this knowledge on a per-task basis
does not scale, we ask: what if we could harness the col-
lective high-level knowledge humanity has recorded on the
Internet to endow agents with similar common sense?
Motif (ext.+int.)
Extrinsic only
RND (ext.+int.)
arXiv:2310.00166v1  [cs.AI]  29 Sep 2023
0
0
1
2
Environment Steps (×109)
Figure 1: NetHack score for Motif and
baselines. Agents trained exclusively
with Motif’s intrinsic reward surpris-
ingly outperform agents trained using
the score itself, and perform even bet-
ter when trained with a combination of
the two reward functions.
Although this knowledge may not provide a direct solu-
tion to how an agent should manage its sensors or actua-
tors, it bears answers to the fundamental questions men-
tioned above. This holds true for many of the environ-
ments where we would want to deploy an agent. However,
the knowledge on the Internet is highly unstructured and
amorphous, making it difficult to find and reuse informa-
tion. Fortunately, by learning on Internet-scale datasets,
Large Language Models (LLMs) absorb this information
and make it accessible (Brown et al., 2020). Nonetheless,
empowering a sequential decision-making agent with this
source of common sense is far from trivial.
While an LLM’s knowledge typically exists at a high level of abstraction, a decision-making agent
often operates at a lower level of abstraction, where it must process rich observations and output
* Equal contribution, order defined by coin flip ({klissarm, pierluca.doro}@mila.quebec).
1
"
"2310.00194","Taylor Webb","Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida
  Momennejad","A Prefrontal Cortex-inspired Architecture for Planning in Large Language
  Models","","","","","cs.AI cs.NE","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. To address this, we take inspiration from
the human brain, in which planning is accomplished via the recurrent
interaction of specialized modules in the prefrontal cortex (PFC). These
modules perform functions such as conflict monitoring, state prediction, state
evaluation, task decomposition, and task coordination. We find that LLMs are
sometimes capable of carrying out these functions in isolation, but struggle to
autonomously coordinate them in the service of a goal. Therefore, we propose a
black box architecture with multiple LLM-based (GPT-4) modules. The
architecture improves planning through the interaction of specialized
PFC-inspired modules that break down a larger problem into multiple brief
automated calls to the LLM. We evaluate the combined architecture on two
challenging planning tasks -- graph traversal and Tower of Hanoi -- finding
that it yields significant improvements over standard LLM methods (e.g.,
zero-shot prompting or in-context learning). These results demonstrate the
benefit of utilizing knowledge from cognitive neuroscience to improve planning
in LLMs.
","2023-10-03","2310.00194v1.pdf","Preprint. Under review.
A PREFRONTAL CORTEX-INSPIRED ARCHITECTURE
FOR PLANNING IN LARGE LANGUAGE MODELS
Shanka Subhra Mondal* †
Department of Electrical and Computer Engineering
Princeton University
Princeton, NJ
smondal@princeton.edu
Taylor Webb*
Department of Psychology
University of California, Los Angeles
Los Angeles, CA
taylor.w.webb@gmail.com
* Equal contribution
Brian Krabach
Microsoft, Office of the CTO
Redmond, WA
Brian.Krabach@microsoft.com
Chi Wang
Microsoft Research
Redmond, WA
wang.chi@microsoft.com
Ida Momennejad
Microsoft Research
New York, NY
idamo@microsoft.com
ABSTRACT
arXiv:2310.00194v1  [cs.AI]  30 Sep 2023
Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step reason-
ing or goal-directed planning. To address this, we take inspiration from the human
brain, in which planning is accomplished via the recurrent interaction of special-
ized modules in the prefrontal cortex (PFC). These modules perform functions
such as conflict monitoring, state prediction, state evaluation, task decomposition,
and task coordination. We find that LLMs are sometimes capable of carrying out
these functions in isolation, but struggle to autonomously coordinate them in the
service of a goal. Therefore, we propose a black box architecture with multiple
LLM-based (GPT-4) modules. The architecture improves planning through the
interaction of specialized PFC-inspired modules that break down a larger problem
into multiple brief automated calls to the LLM. We evaluate the combined archi-
tecture on two challenging planning tasks – graph traversal and Tower of Hanoi –
finding that it yields significant improvements over standard LLM methods (e.g.,
zero-shot prompting or in-context learning). These results demonstrate the benefit
of utilizing knowledge from cognitive neuroscience to improve planning in LLMs.
1
INTRODUCTION
Large Language Models (LLMs) (Devlin et al., 2090; Brown et al., 2020) have recently emerged as
highly capable generalist systems with a surprising range of emergent capacities (Srivastava et al.,
2022; Wei et al., 2022a; Webb et al., 2023). They have also sparked broad controversy, with some
suggesting that they are approaching general intelligence (Bubeck et al., 2023), and others noting
a number of significant deficiencies (Mahowald et al., 2023). A particularly notable shortcoming
is their poor ability to plan or perform faithful multi-step reasoning (Valmeekam et al., 2023; Dziri
et al., 2023). Recent work (Momennejad et al., 2023) has evaluated the extent to which LLMs might
possess an emergent capacity for planning and exploiting cognitive maps, the relational structures
that humans and other animals utilize to perform planning (Tolman, 1948; Tavares et al., 2015;
†Work performed during internship at Microsoft Research, New York, NY.
1
"
"2310.00212","Tianhao Wu","Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran,
  Jiantao Jiao","Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for
  LLM Alignment","19 pages, 5 figures","","","","cs.LG cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves
Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy
Optimization (PPO) serving as the default RL optimizer. Despite its
effectiveness, PPO has limitations when optimizing rewards trained from
comparison-based loss. Primarily, PPO is not invariant to equivalent reward
functions containing identical preference information due to the need to
calibrate the reward scale. Additionally, PPO's necessity for token-wise
updates introduces complexity in both function approximation and algorithm
design compared to trajectory-wise optimization. This paper proposes a new
framework, reinforcement learning with relative feedback, and a novel
trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show
theoretically that P3O is invariant to equivalent rewards and avoids the
complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO
in the KL-Reward trade-off and can align with human preferences as well as or
better than prior methods. In summary, this work introduces a simpler yet
effective approach for aligning LLMs to human preferences through relative
feedback.
","2023-10-11","2310.00212v1.pdf","Under review as a conference paper
PAIRWISE PROXIMAL POLICY OPTIMIZATION:
HARNESSING RELATIVE FEEDBACK FOR LLM ALIGN-
MENT
Tianhao Wu1∗, Banghua Zhu1, Ruoyu Zhang2, Zhaojin Wen1, Kannan Ramchandran1 & Jiantao Jiao1
1University of California, Berkeley, 2Peking University
ABSTRACT
Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora.
However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves Rein-
forcement Learning with Human Feedback (RLHF), with Proximal Policy Opti-
mization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO
has limitations when optimizing rewards trained from comparison-based loss. Pri-
marily, PPO is not invariant to equivalent reward functions containing identical
preference information due to the need to calibrate the reward scale. Additionally,
PPO’s necessity for token-wise updates introduces complexity in both function ap-
proximation and algorithm design compared to trajectory-wise optimization. This
paper proposes a new framework, reinforcement learning with relative feedback,
and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show the-
oretically that P3O is invariant to equivalent rewards and avoids the complexity
of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-
Reward trade-off and can align with human preferences as well as or better than
prior methods. In summary, this work introduces a simpler yet effective approach
for aligning LLMs to human preferences through relative feedback.
1
INTRODUCTION
arXiv:2310.00212v1  [cs.LG]  30 Sep 2023
Large Language Models (LLMs) have made remarkable progress, profoundly influencing the AI
community (Chowdhery et al., 2022; Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023).
However, due to the reliance on massive corpora of internet data, which encompasses a high portion
of low-quality data, LLMs are likely to express unintended behavior. These include fabricating facts,
generating biased or toxic text, and even harmful content to humans (Perez et al., 2022; Ganguli
et al., 2022). Consequently, it is crucial to align LLMs with human values, e.g., helpful, honest,
harmless (Bai et al., 2022a).
A dominant approach in the realm of AI Alignment for LLMs named Reinforcement Learning with
Human Feedback (RLHF) involves a three-stage procedure: supervised fine-tuning, reward learning,
and reinforcement learning (RL) fine-tuning (Ziegler et al., 2019; Ouyang et al., 2022). In the critical
third stage, Proximal Policy Optimization (PPO) is widely adopted as the default RL optimizer
(Schulman et al., 2017). Despite the acclaimed efficiency of PPO, recent studies have highlighted
several intriguing questions and potential issues that require additional attention:
Instability of PPO. Despite its acclaimed effectiveness, recent studies have identified instability
associated with PPO. Factors such as reward normalization, reward scaling, reward clipping, KL
control, advantage normalization and critic initialization (Zheng et al., 2023; Engstrom et al., 2020)
can contribute to this instability. Moreover, we identify another source of instability: there is an
inconsistency when it comes to optimizing the reward trained with the Bradley-Terry Loss (BTL)
comparative reward model. In essence, BTL is invariant to constant shift while PPO is not. This
∗Contact author through thw@berkeley.edu
1
"
"2310.00222","Hongsheng Hu","Hongsheng Hu, Xuyun Zhang, Zoran Salcic, Lichao Sun, Kim-Kwang Raymond
  Choo, Gillian Dobbie","Source Inference Attacks: Beyond Membership Inference Attacks in
  Federated Learning","Accepted by IEEE Transactions on Dependable and Secure Computing","","","","cs.CR","http://creativecommons.org/licenses/by/4.0/","  Federated learning (FL) is a popular approach to facilitate privacy-aware
machine learning since it allows multiple clients to collaboratively train a
global model without granting others access to their private data. It is,
however, known that FL can be vulnerable to membership inference attacks
(MIAs), where the training records of the global model can be distinguished
from the testing records. Surprisingly, research focusing on the investigation
of the source inference problem appears to be lacking. We also observe that
identifying a training record's source client can result in privacy breaches
extending beyond MIAs. For example, consider an FL application where multiple
hospitals jointly train a COVID-19 diagnosis model, membership inference
attackers can identify the medical records that have been used for training,
and any additional identification of the source hospital can result the patient
from the particular hospital more prone to discrimination. Seeking to
contribute to the literature gap, we take the first step to investigate source
privacy in FL. Specifically, we propose a new inference attack (hereafter
referred to as source inference attack -- SIA), designed to facilitate an
honest-but-curious server to identify the training record's source client. The
proposed SIAs leverage the Bayesian theorem to allow the server to implement
the attack in a non-intrusive manner without deviating from the defined FL
protocol. We then evaluate SIAs in three different FL frameworks to show that
in existing FL frameworks, the clients sharing gradients, model parameters, or
predictions on a public dataset will leak such source information to the
server. We also conduct extensive experiments on various datasets to
investigate the key factors in an SIA. The experimental results validate the
efficacy of the proposed SIAs.
","2023-10-03","2310.00222v1.pdf","1
Source Inference Attacks: Beyond Membership
Inference Attacks in Federated Learning
Hongsheng Hu∗, Xuyun Zhang†, Zoran Salcic∗, Life Senior Member, IEEE, Lichao Sun‡,
Kim-Kwang Raymond Choo§, Senior Member, IEEE, and Gillian Dobbie∗
∗The University of Auckland, New Zealand
†Macquarie University, Australia
‡Lehigh University, USA
§The University of Texas at San Antonio, USA
Abstract—Federated learning (FL) is a popular approach to facilitate privacy-aware machine learning since it allows multiple clients to
collaboratively train a global model without granting others access to their private data. It is, however, known that FL can be vulnerable
to membership inference attacks (MIAs), where the training records of the global model can be distinguished from the testing records.
Surprisingly, research focusing on the investigation of the source inference problem appears to be lacking. We also observe that
identifying a training record’s source client can result in privacy breaches extending beyond MIAs. For example, consider an FL
application where multiple hospitals jointly train a COVID-19 diagnosis model, membership inference attackers can identify the medical
records that have been used for training, and any additional identification of the source hospital can result the patient from the
particular hospital more prone to discrimination. Seeking to contribute to the literature gap, we take the first step to investigate source
privacy in FL. Specifically, we propose a new inference attack (hereafter referred to as source inference attack – SIA), designed to
facilitate an honest-but-curious server to identify the training record’s source client. The proposed SIAs leverage the Bayesian theorem
to allow the server to implement the attack in a non-intrusive manner without deviating from the defined FL protocol. We then evaluate
SIAs in three different FL frameworks to show that in existing FL frameworks, the clients sharing gradients, model parameters, or
predictions on a public dataset will leak such source information to the server. We also conduct extensive experiments on various
datasets to investigate the key factors in an SIA. The experimental results validate the efficacy of the proposed SIAs, e.g., an attack
success rate of 67.1% (baseline 10%) can be achieved when the clients share model parameters with the server. Comprehensive
ablation studies demonstrate that the success of an SIA is directly related to the overfitting of the local models.
Index Terms—Federated Learning, Membership Inference Attacks, Source Inference Attacks, Privacy Leakage.
✦
1
INTRODUCTION
arXiv:2310.00222v1  [cs.CR]  30 Sep 2023
Recent deep learning advances have partly contributed to
the building of powerful machine learning (ML) models
from large datasets. In practice, however, data often resides
across different organizational entities (also referred to as
data islands). The data records of a single entity, perhaps
with the exception of extremely large technology organiza-
tions, are generally limited and do not represent the entire
data distribution. Thus, stakeholders (e.g., consumers) can
generally benefit if different data owners can collaboratively
train a joint machine learning model based on the union
of different datasets. For example, our society will benefit
if different countries and organizations can collaborate to
collaboratively train COVID-19 diagnosis models using the
broad range of medical data records in these different en-
tities. The exacting privacy regulations (e.g., GDPR [1] in
the European Union and CCPA [2] in the United States),
however, complicate such collaborative efforts.
Federated learning (FL) is one approach that can be
utilized to circumvent the limitations due to data islands,
by allowing multiple clients coordinated by a central server
to train a joint ML model in an iterative manner [3], [4], [5],
[6]. In FL, the clients send their model updates to the server
but never their raw training dataset, thereby leading to a
privacy-aware paradigm for collaborative model training.
For the example mentioned above, FL can greatly facilitate
the hospitals wishing to train a joint COVID-19 diagnosis
model from the distributed data in different hospitals. A
real-life case in [7] has shown the successful adoption of
FL where an ML model for COVID-19 diagnosis has been
trained with the usage of the geographically distributed
chest computed tomography data collected from different
patients at different hospitals.
However, many recent studies [8], [9], [10], [11], [12],
[13] have shown that FL does not provide sufficient privacy
guarantees, because sensitive information from the training
data can still be revealed during the communication process.
In FL, the clients transmit necessary information from up-
dates, e.g., gradients, to the central server for global model
training. Because the updates are derived from the clients’
private training data, there are several recently proposed
privacy attacks trying to infer the privacy of the clients from
such updates, such as data reconstruction attacks [14], prop-
erty inference attacks [15], preference profiling attacks [13],
and membership inference attacks (MIAs) [16]. Among such
attacks, MIAs aim to identify whether or not a data record
was in the training dataset of a target model (i.e., a member).
While an MIA seems like a simple attack, it can impose
†Xuyun Zhang is the corresponding author.
"
"2310.00230","Yongqiang Wang","Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan
  Cao, Yongqiang Wang, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul Rubenstein,
  Lukas Zilka, Dian Yu, Zhong Meng, Golan Pundak, Nikhil Siddhartha, Johan
  Schalkwyk, Yonghui Wu","SLM: Bridge the thin gap between speech and text foundation models","","","","","cs.CL cs.SD eess.AS","http://creativecommons.org/licenses/by/4.0/","  We present a joint Speech and Language Model (SLM), a multitask,
multilingual, and dual-modal model that takes advantage of pretrained
foundational speech and language models. SLM freezes the pretrained foundation
models to maximally preserves their capabilities, and only trains a simple
adapter with just 1\% (156M) of the foundation models' parameters. This
adaptation not only leads SLM to achieve strong performance on conventional
tasks such as speech recognition (ASR) and speech translation (AST), but also
introduces the novel capability of zero-shot instruction-following for more
diverse tasks: given a speech input and a text instruction, SLM is able to
perform unseen generation tasks including contextual biasing ASR using
real-time context, dialog generation, speech continuation, and question
answering, etc. Our approach demonstrates that the representational gap between
pretrained speech and language models might be narrower than one would expect,
and can be bridged by a simple adaptation mechanism. As a result, SLM is not
only efficient to train, but also inherits strong capabilities already acquired
in foundation models of different modalities.
","2023-10-03","2310.00230v1.pdf","SLM: BRIDGE THE THIN GAP BETWEEN SPEECH AND TEXT FOUNDATION MODELS
Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Yongqiang Wang,
Nanxin Chen, Yu Zhang, Hagen Soltau, Paul K. Rubenstein, Lukas Zilka, Dian Yu, Zhong Meng
Golan Pundak, Nikhil Siddhartha, Johan Schalkwyk, Yonghui Wu
Google Deepmind
ABSTRACT
Fig. 1. SLM consists of a frozen pretrained speech model, a
frozen pretrained LLM and an adapter to bridge from speech
to textual embeddings.
Therefore, SLM extends LLM’s
instruction-following capabilities beyond text to speech in-
puts and successfully performs multiple 0-shot tasks.
We present a joint Speech and Language Model (SLM),
a multitask, multilingual, and dual-modal model that takes
advantage of pretrained foundational speech and language
models. SLM freezes the pretrained foundation models to
maximally preserves their capabilities, and only trains a sim-
ple adapter with just 1% (156M) of the foundation models’
parameters. This adaptation not only leads SLM to achieve
strong performance on conventional tasks such as automatic
speech recognition (ASR) and automatic speech translation
(AST), but also unlocks the novel capability of zero-shot
instruction-following for more diverse tasks. Given a speech
input and a text instruction, SLM is able to perform unseen
generation tasks including contextual biasing ASR using
real-time context, dialog generation, speech continuation,
and question answering. Our approach demonstrates that the
representational gap between pretrained speech and language
models is narrower than one would expect, and can be bridged
by a simple adaptation mechanism. As a result, SLM is not
only efficient to train, but also inherits strong capabilities
already present in foundation models of different modalities.
1. INTRODUCTION
arXiv:2310.00230v1  [cs.CL]  30 Sep 2023
and language foundation models to unlock new multitask
and 0-shot capabilities. In contrast to the previous version
of SLM, in this work the two foundation models are kept
frozen to safeguard their inherent capabilities and an adapter
is trained to bridge the two modalities. The adapter takes the
output of the speech encoder, applies a uniform subsampling
approach to reduce the sequence length, and learns to map
the audio representation into the textual representation space
that can be interpreted by the frozen LLM.
The key contributions of this work are:
• A lightweight and efficient approach to glue frozen
speech and text foundation models with a simple
adapter, maximally preserving the native capabilities in
the pretrained foundation models.
• A robust and generalizable model that achieves strong
performance on a variety of speech tasks including
ASR, AST and speech biasing.
Recent advances in foundation models of text and speech have
offered new opportunities to build strong speech-language
models without a large amounts of paired speech-text data.
Text foundation models have demonstrated impressive capa-
bilities and performance on a wide range of language tasks
[1, 2], and audio foundation models have recently advanced
the state-of-the-art in speech recognition and understanding
tasks [3, 4]. Developing effective approaches that unify foun-
dation models of both modalities is a natural way of building
strong speech understanding models without requiring a large
amount of paired speech-text data.
In previous work, a joint Speech Language Model (SLM) [5]
was introduced using an adapter-based approach [6] to unify
pretrained speech and text models for an end-to-end English
dialog understanding task, namely, MultiWoz [7].
In this
work, we refine the proposed SLM using multilingual speech
Corresponding author: mingqiuwang@google.com
• The proposed system demonstrates novel cross-modality
zero-shot instruction-following capabilities, with speech
as inputs and text as instructions.
We describe our approach and model in Section 3, the
training data and tasks in Section 4, experiment setup in Sec-
tion 5, illustrate several zero-shot capabilities in Section 6.3,
"
"2310.00247","Sixing Yu","Sixing Yu, J. Pablo Mu\~noz, Ali Jannesari","Bridging the Gap Between Foundation Models and Heterogeneous Federated
  Learning","","","","","cs.LG cs.DC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Federated learning (FL) offers privacy-preserving decentralized machine
learning, optimizing models at edge clients without sharing private data.
Simultaneously, foundation models (FMs) have gained traction in the artificial
intelligence (AI) community due to their exceptional performance across various
tasks. However, integrating FMs into FL presents challenges, primarily due to
their substantial size and intensive resource requirements. This is especially
true when considering the resource heterogeneity in edge FL systems. We present
an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to
address these challenges. RaFFM introduces specialized model compression
algorithms tailored for FL scenarios, such as salient parameter prioritization
and high-performance subnetwork extraction. These algorithms enable dynamic
scaling of given transformer-based FMs to fit heterogeneous resource
constraints at the network edge during both FL's optimization and deployment
stages. Experimental results demonstrate that RaFFM shows significant
superiority in resource utilization efficiency and uses fewer resources to
deploy FMs to FL. Despite the lower resource consumption, target models
optimized by RaFFM achieve performance on par with traditional FL methods
applied to full-sized FMs. This is evident across tasks in both natural
language processing and computer vision domains.
","2023-10-06","2310.00247v1.pdf","Under review as a conference paper at ICLR 2024
BRIDGING THE GAP BETWEEN FOUNDATION MODELS
AND HETEROGENEOUS FEDERATED LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
Federated learning (FL) offers privacy-preserving decentralized machine learning,
optimizing models at edge clients without sharing private data. Simultaneously,
foundation models (FMs) have gained traction in the artificial intelligence (AI)
community due to their exceptional performance across various tasks. However,
integrating FMs into FL presents challenges, primarily due to their substantial
size and intensive resource requirements. This is especially true when consid-
ering the resource heterogeneity in edge FL systems. We present an adaptive
framework for Resource-aware Federated Foundation Models (RaFFM) to ad-
dress these challenges. RaFFM introduces specialized model compression algo-
rithms tailored for FL scenarios, such as salient parameter prioritization and high-
performance subnetwork extraction. These algorithms enable dynamic scaling of
given transformer-based FMs to fit heterogeneous resource constraints at the net-
work edge during both FL’s optimization and deployment stages. Experimental
results demonstrate that RaFFM shows significant superiority in resource utiliza-
tion efficiency and uses fewer resources to deploy FMs to FL. Despite the lower
resource consumption, target models optimized by RaFFM achieve performance
on par with traditional FL methods applied to full-sized FMs. This is evident
across tasks in both natural language processing and computer vision domains.
1
INTRODUCTION
Federated learning (FL) (McMahan et al., 2017) represents a significant advancement in machine
learning, emphasizing decentralized training while preserving data privacy. FL enhances data pri-
vacy and collaboration compared to traditional machine learning by enabling model training across
multitudes of decentralized devices without direct data sharing. However, challenges like non-
identical independent distribution (non-IID) data and heterogeneous computational resources among
devices present potential training failures.
arXiv:2310.00247v1  [cs.LG]  30 Sep 2023
Concurrently, transformer-based foundation models (FMs) (Bommasani et al., 2021), typified by
GPT (Radford et al., 2019; Brown et al., 2020a; OpenAI, 2023), BERT (Devlin et al., 2018), and
ViT (Dosovitskiy et al., 2020), pre-trained on large-scale datasets, have revolutionized AI research.
FMs leverage their inherent pre-trained knowledge, achieving exceptional performance across mul-
tiple domains in downstream tasks even with limited fine-tuning data.
Given the superior strengths of FMs in few-shot transfer learning, they appear well-suited for non-
IID FL environments (Yu et al., 2023; Zhuang et al., 2023). However, seamlessly integrating FMs
into FL presents significant challenges. The substantial size and intensive resource demands of FMs
make their deployment on resource-constrained FL edge devices problematic. Furthermore, the
uneven distribution of computational resources within FL increases the difficulty of existing chal-
lenges. A resource-limited device must first satisfy the resource requirements for FM optimization,
despite the presence of more capable devices within the same FL network, leading to high system re-
quirements overall. Additionally, fine-tuning FMs typically requires approximately seven times the
resources compared to inference. This disparity means that FL often faces resource-hungry during
model training while leaving resources underutilized during inference.
We propose a framework, Resource-aware Federated Foundation Models (RaFFM), to address the
resource utilization challenges in FL. RaFFM uses specialized transformer-based FM compression
algorithms tailored for FL-edge environments, and dynamically deploys resource-aware scaled FMs
1
"
"2310.00259","Zouying Cao","Zouying Cao, Yifei Yang, Hai Zhao","AutoHall: Automated Hallucination Dataset Generation for Large Language
  Models","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  While Large language models (LLMs) have garnered widespread applications
across various domains due to their powerful language understanding and
generation capabilities, the detection of non-factual or hallucinatory content
generated by LLMs remains scarce. Currently, one significant challenge in
hallucination detection is the laborious task of time-consuming and expensive
manual annotation of the hallucinatory generation. To address this issue, this
paper first introduces a method for automatically constructing model-specific
hallucination datasets based on existing fact-checking datasets called
AutoHall. Furthermore, we propose a zero-resource and black-box hallucination
detection method based on self-contradiction. We conduct experiments towards
prevalent open-/closed-source LLMs, achieving superior hallucination detection
performance compared to extant baselines. Moreover, our experiments reveal
variations in hallucination proportions and types among different models.
","2023-10-03","2310.00259v1.pdf","AutoHall: Automated Hallucination Dataset Generation
for Large Language Models
Cao Zouying, Yang Yifei, Zhao Hai∗
Department of Computer Science and Engineering
Shanghai Jiao Tong University
Shanghai, China
{zuoyingcao,yifeiyang}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
ABSTRACT
While Large language models (LLMs) have garnered widespread applications
across various domains due to their powerful language understanding and gen-
eration capabilities, the detection of non-factual or hallucinatory content gener-
ated by LLMs remains scarce. Currently, one significant challenge in halluci-
nation detection is the laborious task of time-consuming and expensive manual
annotation of the hallucinatory generation. To address this issue, this paper first
introduces a method for automatically constructing model-specific hallucination
datasets based on existing fact-checking datasets called AutoHall. Furthermore,
we propose a zero-resource and black-box hallucination detection method based
on self-contradiction. We conduct experiments towards prevalent open-/closed-
source LLMs, achieving superior hallucination detection performance compared
to extant baselines. Moreover, our experiments reveal variations in hallucination
proportions and types among different models.
1
INTRODUCTION
arXiv:2310.00259v1  [cs.CL]  30 Sep 2023
Large language models (LLMs) such as ChatGPT 1, GPT-4 (OpenAI, 2023), Claude (Bai et al.,
2022) and Llama-2 (Touvron et al., 2023) have achieved widespread popularity and adoption across
diverse industries and domains (Sohail et al., 2023; Sallam, 2023; Sallam et al., 2023). Despite their
powerful capabilities, the issue of “hallucination” poses a concern that LLMs have the tendency to
generate inaccurate/fabricated information in generation tasks (Zhang et al., 2023b; Ji et al., 2023).
As shown in Fig. 1, ChatGPT suffers from hallucination when giving a description of the novel
“The Leopard” by Norwegian author Jo Nesbø. It can be observed that ChatGPT makes up some
plots of the novel and contains incorrect texts in the response, because the novel never mentions the
presence of a “red diamond” at the crime scene and the “The Snowman” case has also been solved
before. Since the current artificial intelligence relies more on LLMs, hallucinatory information
indeed disturbs the enterprise security and the user trust (Zhang et al., 2023a; Gupta et al., 2023).
Therefore, detecting hallucinations generated by the LLMs is of significant importance.
Current research efforts on hallucination detection leverage external knowledge sources (Chern
et al., 2023; Gou et al., 2023) or just adopt a zero-resource approach, which focuses on resources
inherent to the model itself (Azaria & Mitchell, 2023; Agrawal et al., 2023; Varshney et al., 2023;
Manakul et al., 2023b; M¨undler et al., 2023). Typically, most of these methods begin with a crowd-
sourced annotation, where researchers use QA datasets to have the model generate responses and
then manually annotate whether the answers contain hallucinations.
∗Corresponding author
1https://chat.openai.com/
1
"
"2310.00272","Baphumelele Masikisiki","Baphumelele Masikisiki, Vukosi Marivate, Yvette Hlope","Investigating the Efficacy of Large Language Models in Reflective
  Assessment Methods through Chain of Thoughts Prompting","Accepted for publication in the Associate Computer Machinery","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models, such as Generative Pre-trained Transformer 3 (aka.
GPT-3), have been developed to understand language through the analysis of
extensive text data, allowing them to identify patterns and connections between
words. While LLMs have demonstrated impressive performance across various
text-related tasks, they encounter challenges in tasks associated with
reasoning. To address this challenge, Chain of Thought(CoT) prompting method
has been proposed as a means to enhance LLMs' proficiency in complex reasoning
tasks like solving math word problems and answering questions based on logical
argumentative reasoning. The primary aim of this research is to assess how well
four language models can grade reflective essays of third-year medical
students. The assessment will specifically target the evaluation of critical
thinking skills using CoT prompting.
  The research will provide the following contributions; to introduce and
educate on the process of instructing models to evaluate reflective essays from
a dataset they have not been previously trained on; to illustrate the use of
CoT prompting as an instructional approach for training large models to carry
out particular tasks. Our results suggest that among all the models, Llama-7b
performs the least effectively, displaying the highest mean squared error.
Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen
kappa score value of 0.53. Lastly, it's important to note that the selected
models do prioritise user privacy by allowing users to delete their own
conducted conversations.
","2023-10-03","2310.00272v1.pdf","Investigating the Efficacy of Large Language
Models in Reflective Assessment Methods
through Chain of Thoughts Prompting
Baphumelele Masikisiki1, Vukosi Marivate2, Yvette Hlophe3
1 University of Pretoria, bmasikisiki@gmail.com
2University of Pretoria, vukosi.marivate@cs.up.za
3University of Pretoria, yvette.hlophe@up.ac.za
Abstract
arXiv:2310.00272v1  [cs.CL]  30 Sep 2023
Large Language Models, such as Generative Pre-trained Transformer
3 (aka. GPT-3), have been developed to understand language through
the analysis of extensive text data, allowing them to identify patterns and
connections between words. While LLMs have demonstrated impressive
performance across various text-related tasks, they encounter challenges
in tasks associated with reasoning. To address this challenge, Chain of
Thought (CoT) prompting method has been proposed as a means to en-
hance LLMs’ proficiency in complex reasoning tasks like solving math
word problems and answering questions based on logical argumentative
reasoning. The primary aim of this research is to assess how well four lan-
guage models can grade reflective essays of third-year medical students.
The assessment will specifically target the evaluation of critical thinking
skills using CoT prompting.
The research will provide the following contributions; to introduce and
educate on the process of instructing models to evaluate reflective essays
from a dataset they have not been previously trained on; to illustrate
the use of CoT prompting as an instructional approach for training large
models to carry out particular tasks. Our results suggest that among all
the models, Llama-7b performs the least effectively, displaying the highest
mean squared error. Conversely, ChatGPT emerges as the superior model,
boasting a higher Cohen kappa score value of 0.53. Lastly, it’s important
to note that the selected models do prioritise user privacy by allowing
users to delete their own conducted conversations.
1
Introduction
Reflective writing [5] is an assessment technique that helps students demonstrate
a deeper understanding of a subject. This form of writing encourages critical
1
"
"2310.00277","Yunhao Chen","Yunhao Chen and Zihui Yan and Yunjie Zhu","A Unified Framework for Generative Data Augmentation: A Comprehensive
  Survey","","","","","cs.LG cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Generative data augmentation (GDA) has emerged as a promising technique to
alleviate data scarcity in machine learning applications. This thesis presents
a comprehensive survey and unified framework of the GDA landscape. We first
provide an overview of GDA, discussing its motivation, taxonomy, and key
distinctions from synthetic data generation. We then systematically analyze the
critical aspects of GDA - selection of generative models, techniques to utilize
them, data selection methodologies, validation approaches, and diverse
applications. Our proposed unified framework categorizes the extensive GDA
literature, revealing gaps such as the lack of universal benchmarks. The thesis
summarises promising research directions, including , effective data selection,
theoretical development for large-scale models' application in GDA and
establishing a benchmark for GDA. By laying a structured foundation, this
thesis aims to nurture more cohesive development and accelerate progress in the
vital arena of generative data augmentation.
","2023-10-03","2310.00277v1.pdf","A Unified Framework for Generative Data
Augmentation: A Comprehensive Survey
Yunhao Chen; Zihui Yan; Yunjie Zhu
aJiangnan University, 1191200221@stu.jiangnan.edu.cn China
Abstract
Generative data augmentation (GDA) has emerged as a promising technique
to alleviate data scarcity in machine learning applications. This thesis presents
a comprehensive survey and unified framework of the GDA landscape. We first
provide an overview of GDA, discussing its motivation, taxonomy, and key
distinctions from synthetic data generation. We then systematically analyze
the critical aspects of GDA - selection of generative models, techniques to
utilize them, data selection methodologies, validation approaches, and diverse
applications. Our proposed unified framework categorizes the extensive GDA
literature, revealing gaps such as the lack of universal benchmarks. The
thesis summarises promising research directions, including , effective data
selection, theoretical development for large-scale models’ application in GDA
and establishing a benchmark for GDA. By laying a structured foundation,
this thesis aims to nurture more cohesive development and accelerate progress
in the vital arena of generative data augmentation.
Keywords:
Generative Data Augmentation, Synthetic Data, Data Augmentation
arXiv:2310.00277v1  [cs.LG]  30 Sep 2023
Preprint submitted to Neurocomputing, AI is used to polish up the paper October 3, 2023
"
"2310.00280","Qiushi Sun","Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng
  Kong","Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model
  Collaboration","work in progress","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large Language Models (LLMs) are evolving at an unprecedented pace and have
exhibited considerable capability in the realm of natural language processing
(NLP) with world knowledge. Benefiting from ultra-large-scale training corpora,
a single LLM can manage typical NLP tasks competently. However, its performance
in executing reasoning tasks is still confined by the limitations of its
internal representations. To push this boundary further, we introduce Corex in
this paper, a suite of novel general-purpose strategies that transform LLMs
into autonomous agents pioneering multi-model collaborations for complex
task-solving. Inspired by human behaviors, Corex is constituted by diverse
collaboration paradigms including Debate, Review, and Retrieve modes, which
collectively work towards enhancing the factuality, faithfulness, and
reliability of the reasoning process. These paradigms foster task-agnostic
approaches that enable LLMs to ''think outside the box,'' thereby overcoming
hallucinations and providing better solutions. Through extensive experiments
across four different types of reasoning tasks, we demonstrate that
orchestrating multiple LLMs to work in concert yields substantially better
performance compared to existing methods. Further results and in-depth analysis
demonstrate the cost-effectiveness of our method, facilitating collaboration
among different LLMs and promoting annotation efficiency.
","2023-10-03","2310.00280v1.pdf","Preprint
Corex: PUSHING THE BOUNDARIES OF COMPLEX REA-
SONING THROUGH MULTI-MODEL COLLABORATION
Qiushi Sun♢♡∗, Zhangyue Yin♣, Xiang Li♠, Zhiyong Wu♢, Xipeng Qiu♣, Lingpeng Kong♥
♢Shark-NLP, Shanghai AI Laboratory ♣Fudan University ♡National University of Singapore
♠East China Normal University ♥The University of Hong Kong
qiushisun@u.nus.edu, wuzhiyong@pjlab.org.cn, yinzy21@m.fudan.edu.cn
xiangli@dase.ecnu.edu.cn, xpqiu@fudan.edu.cn, lpk@cs.hku.hk
ABSTRACT
Large Language Models (LLMs) are evolving at an unprecedented pace and have
exhibited considerable capability in the realm of natural language processing (NLP)
with world knowledge. Benefiting from ultra-large-scale training corpora, a single
LLM can manage typical NLP tasks competently. However, its performance in
executing complex reasoning tasks is still confined by the limitations of its internal
representation. To push this boundary further, we introduce Corex in this paper,
a suite of novel general-purpose strategies that transform LLMs into autonomous
agents, pioneering multi-model collaborations for complex task-solving. Inspired
by human behaviors, Corex is constituted by diverse collaboration paradigms
including Debate, Review, and Retrieve modes, which collectively work towards
enhancing the factuality, faithfulness, and reliability of the reasoning process. These
paradigms foster task-agnostic approaches that enable LLMs to “think outside the
box,” thereby overcoming hallucinations and providing better solutions. Through
extensive experiments across four different types of reasoning tasks, we demon-
strate that orchestrating multiple LLMs to work in concert yields substantially
better performance compared to existing methods. Further results and in-depth
analysis demonstrate the cost-effectiveness of our method, facilitating collaboration
among different LLMs and promoting annotation efficiency
1 .
“A problem shared is a problem halved.”
—English Proverb
1
INTRODUCTION
Large Language Models (LLMs) have succeeded in advancing the state-of-the-arts for a series of
Natural Language Processing (NLP) tasks (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023;
arXiv:2310.00280v1  [cs.AI]  30 Sep 2023
inter alia). Recent research (Wei et al., 2022a) indicates that
scaling up models (Kaplan et al., 2020) can yield improvements in both performance and sample
efficiency across a broad spectrum of downstream tasks. Notwithstanding their remarkable proficiency
in language understanding and instruction following (Ouyang et al., 2022), the reasoning abilities of
LLMs, often seen as a hallmark for assessing their potential, still present challenges (Suzgun et al.,
might not adequately address their inherent limitations in solving reasoning tasks (Rae et al., 2022).
In response to this challenge, Wei et al. (2022b) put forth chain-of-thought (CoT) prompting that an
LLM generates a series of intermediate steps toward a final answer, contrasting the use of “answer-
only” prompts. Subsequently, various approaches have been put forward, such as self-consistency
decoding (Wang et al., 2023d) which utilizes a majority voting mechanism to determine the final
answer, and program-aided language models (PAL; Gao et al., 2022; Chen et al., 2022a) that leverage
code generation to reduce errors in computations. Besides, curated prompts necessitate task-specific
designs (Zheng et al., 2023a) have also been utilized to elicit more accurate predictions. Nevertheless,
these approaches are confined within a static black box (Yao et al., 2023b), wherein the LLM relies
∗ Work done during an internship at Shanghai AI Laboratory.
1Code and data will be available at this link.
1
2023; Huang & Chang, 2023). Concurrently, there is a prevailing view that merely increasing the size
Touvron et al., 2023; Zhao et al., 2023a,
"
"2310.00297","Jianhao Yan","Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang","Understanding In-Context Learning from Repetitions","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  This paper explores the elusive mechanism underpinning in-context learning in
Large Language Models (LLMs). Our work provides a novel perspective by
examining in-context learning via the lens of surface repetitions. We
quantitatively investigate the role of surface features in text generation, and
empirically establish the existence of \emph{token co-occurrence
reinforcement}, a principle that strengthens the relationship between two
tokens based on their contextual co-occurrences. By investigating the dual
impacts of these features, our research illuminates the internal workings of
in-context learning and expounds on the reasons for its failures. This paper
provides an essential contribution to the understanding of in-context learning
and its potential limitations, providing a fresh perspective on this exciting
capability.
","2023-10-11","2310.00297v1.pdf","Published as a conference paper at ICLR 2024
UNDERSTANDING IN-CONTEXT LEARNING FROM REP-
ETITIONS
Jianhao Yan1,2
Jin Xu4
Chiyu Song1,2
Chenming Wu5
Yafu Li1,2
Yue Zhang2,3,∗
1Zhejiang University
2School of Engineering, Westlake University
3 Institute of Advanced Technology, Westlake Institute for Advanced Study
4 Alibaba Group
5 Baidu Research
elliottyan37@gmail.com
ABSTRACT
This paper explores the elusive mechanism underpinning in-context learning in
Large Language Models (LLMs). Our work provides a novel perspective by
examining in-context learning via the lens of surface repetitions. We quantitatively
investigate the role of surface features in text generation, and empirically establish
the existence of token co-occurrence reinforcement, a principle that strengthens
the relationship between two tokens based on their contextual co-occurrences.
By investigating the dual impacts of these features, our research illuminates the
internal workings of in-context learning and expounds on the reasons for its failures.
This paper provides an essential contribution to the understanding of in-context
learning and its potential limitations, providing a fresh perspective on this exciting
capability.
1
INTRODUCTION
The impressive ability of Large Language Models (LLMs; Touvron et al. (2023); Chowdhery et al.
(2022); OpenAI (2023)) to execute in-context learning (ICL) is a standout characteristic. This
behavior mirrors human learning and reasoning from analogy (Winston, 1980), enabling LLMs to
rapidly adapt to a range of downstream tasks. Without being explicitly pretrained to learn from
demonstrations, LLMs can predict responses to unseen test queries from a few demonstrations and
without any instruction given (Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022). An
example of in-context learning can be found in Figure 1(a), where a pre-trained LLaMA model is
given demonstrations for a binary classification task, and learns to make predictions correctly. Despite
the success in applications, the working mechanism of in-context learning is still an open question.
arXiv:2310.00297v1  [cs.CL]  30 Sep 2023
We take a feature-centric view to understand ICL, analyzing the key patterns in the input context that
correlate with ICL behavior. In particular, as Figure 1(b) shows, in-context demonstrations can result
not only in desired effects but also cause errors. In this example, the same LLaMA model makes the
incorrect prediction ‘True’ given the input “Circulation revenue has decreased by 5% in Finland.”,
which is likely because of the repeated pattern “Answer:” -> “True” from the demonstrations. In the
same perspective, the success case in Figure 1(a) can be attributed to learning desired patterns such
as “Answer:” -> “True|False” in the demonstrations. Such patterns are apparently used as features in
the autoregressive inference process by the model.
The patterns we discussed above can be viewed as generalizations to repetition patterns (Holtzman
et al., 2019; Fu et al., 2020) and self-reinforced patterns (Xu et al., 2022) which have been discussed
in the literature. The ‘self-reinforcement effect’ describes a phenomenon where the model tends to
perpetuate the generation of sentences that have frequently appeared in its context. These effects are
regarded as harmful to text generation and previous work puts efforts to mitigate it. However, the
above observation implies that they play the role of both angels and demons in LLMs.
* Corresponding author.
1
"
"2310.00299","Asahi Ushio","Asahi Ushio, Jose Camacho-Collados, Steven Schockaert","RelBERT: Embedding Relations with Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Many applications need access to background knowledge about how different
concepts and entities are related. Although Knowledge Graphs (KG) and Large
Language Models (LLM) can address this need to some extent, KGs are inevitably
incomplete and their relational schema is often too coarse-grained, while LLMs
are inefficient and difficult to control. As an alternative, we propose to
extract relation embeddings from relatively small language models. In
particular, we show that masked language models such as RoBERTa can be
straightforwardly fine-tuned for this purpose, using only a small amount of
training data. The resulting model, which we call RelBERT, captures relational
similarity in a surprisingly fine-grained way, allowing us to set a new
state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of
modelling relations that go well beyond what the model has seen during
training. For instance, we obtained strong results on relations between named
entities with a model that was only trained on lexical relations between
concepts, and we observed that RelBERT can recognise morphological analogies
despite not being trained on such examples. Overall, we find that RelBERT
significantly outperforms strategies based on prompting language models that
are several orders of magnitude larger, including recent GPT-based models and
open source models.
","2023-10-10","2310.00299v1.pdf","RelBERT: Embedding Relations with Language Models
Asahi Ushio, Jose Camacho-Collados, Steven Schockaert
aCardiff NLP, School of Computer Science and Informatics, Cardiff University, Senghennydd Rd, Cardiff, CF24 4AG, United Kingdom
Abstract
Many applications need access to background knowledge about how different concepts and entities are related. Al-
though Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are
inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult
to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In
particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this pur-
pose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational
similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Cru-
cially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For
instance, we obtained strong results on relations between named entities with a model that was only trained on lexical
relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being
trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting
language models that are several orders of magnitude larger, including recent GPT-based models and open source
models. 1
Keywords: language models, relational knowledge, analogy
PACS: 0000, 1111
2000 MSC: 0000, 1111
1. Introduction
arXiv:2310.00299v1  [cs.CL]  30 Sep 2023
Recognizing the lexical relationship between two words has long been studied as a fundamental task in natural
language processing (NLP) [1]. As a representative early example, DIRT [2] first collects sentences in which two given
target words co-occur (e.g. London and U.K.) and then uses the dependency paths between the two words to model
their relationship. Along similar lines, Latent Relational Analysis (LRA [1]) relies on templates expressing lexical
patterns to characterise word pairs (e.g. [head word] is the capital of [tail word]), thus again relying on sentences
where the words co-occur. After the advent of word embeddings [3–5], most approaches for modelling relations
relied on word vectors in one way or another. A common strategy to model the relation between two words was to
take the vector difference between the embeddings of each word [3, 6, 7]. For example, the relationship between
“King” and “Queen” is the gender difference, which can be captured by wvpKingq ´ wvpQueenq, where wvpXq
denotes the embedding of word X. Although the vector difference of word embeddings quickly gained popularity, it
has been shown that the latent space of such relation vectors is noisy, with nearest neighbours often corresponding to
different relationships [8–10]. The limitations of word embedding differences can also clearly be seen on SAT [11],
a well-known benchmark involving word pair analogies (see § 4.1), where the accuracy of word vector differences is
particularly disappointing [12].
Knowledge graphs (KGs) such as Wikidata [13] and ConceptNet [14] are also closely related to the study of
relation understanding. In contrast to the aforementioned methods, KGs rely on symbolic representations. They use a
fixed relational schema to explicitly encode the relationships between words or entities. KGs are more interpretable
than embeddings, but they usually have the drawback of being highly incomplete. Moreover, due to their use of
1Source code to reproduce our experimental results and the model checkpoints are available in the following repository: https://github.
com/asahi417/relbert.
Preprint submitted to Artificial Intelligence
October 3, 2023
"
"2310.00305","Xuan Zhang","Xuan Zhang and Wei Gao","Towards LLM-based Fact Verification on News Claims with a Hierarchical
  Step-by-Step Prompting Method","Accepted by AACL 2023","","","","cs.CL","http://creativecommons.org/licenses/by-sa/4.0/","  While large pre-trained language models (LLMs) have shown their impressive
capabilities in various NLP tasks, they are still under-explored in the
misinformation domain. In this paper, we examine LLMs with in-context learning
(ICL) for news claim verification, and find that only with 4-shot demonstration
examples, the performance of several prompting methods can be comparable with
previous supervised models. To further boost performance, we introduce a
Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to
separate a claim into several subclaims and then verify each of them via
multiple questions-answering steps progressively. Experiment results on two
public misinformation datasets show that HiSS prompting outperforms
state-of-the-art fully-supervised approach and strong few-shot ICL-enabled
baselines.
","2023-10-03","2310.00305v1.pdf","Towards LLM-based Fact Verification on News Claims with a Hierarchical
Step-by-Step Prompting Method
Xuan Zhang and Wei Gao
School of Computing and Information Systems
Singapore Management University
80 Stamford Rd, Singapore 178902
xuanzhang.2020@phdcs.smu.edu.sg, weigao@smu.edu.sg
Abstract
news related benchmarks (Soleimani et al., 2020;
Atanasova et al., 2020; Kruengkrai et al., 2021),
thanks to their strong ability to understand nuanced
context for more accurate decision. Recently, large
pre-trained language models (LLMs) with a mas-
sive number of parameters, such as GPT-3.5, have
shown impressive performances in various down-
stream tasks (Brown et al., 2020; Wei et al., 2022;
Zhou et al., 2022; Press et al., 2022). But it is basi-
cally unclear how well LLMs can perform on fact
verification task as this is not at the core of LLM
pre-training (Brown et al., 2020; Anil et al., 2023).
While large pre-trained language models
(LLMs) have shown their impressive capabili-
ties in various NLP tasks, they are still under-
explored in the misinformation domain. In this
paper, we examine LLMs with in-context learn-
ing (ICL) for news claim verification, and find
that only with 4-shot demonstration examples,
the performance of several prompting methods
can be comparable with previous supervised
models.
To further boost performance, we
introduce a Hierarchical Step-by-Step (HiSS)
prompting method which directs LLMs to sepa-
rate a claim into several subclaims and then
verify each of them via multiple questions-
answering steps progressively. Experiment re-
sults on two public misinformation datasets
show that HiSS prompting outperforms state-
of-the-art fully-supervised approach and strong
few-shot ICL-enabled baselines.
1
Introduction
arXiv:2310.00305v1  [cs.CL]  30 Sep 2023
Misinformation such as fake news often causes con-
fusion or wrong belief because they contain claims
that are factually false or inaccurate (Lazer et al.,
2018). To combat misinformation in news claims,
stakeholders rely on fact-checking practices for
claim verification. Fact-checking services online,
such as PolitiFact1 and Snopes2) require laborious
manual efforts, making it challenging to match the
rapid pace of misinformation being produced.
In recent years, deep neural networks-based
misinformation detection and fact-checking meth-
ods have been studied extensively (Wang, 2017;
Rashkin et al., 2017; Popat et al., 2018; Ma et al.,
2019; Kotonya and Toni, 2020; Atanasova et al.,
2020; Yang et al., 2022). In particular, pre-trained
language models (PLMs) like BERT (Kenton and
Toutanova, 2019) have demonstrated superior re-
sults and surpassed traditional methods in fake
1https://www.politifact.com/.
2https://www.snopes.com/.
While it is not practical to directly fine-tune most
LLMs, in-context learning (ICL) (Brown et al.,
2020) offers an alternative way to instruct LLMs
to learn new tasks via inference only, conditioning
on demonstration examples without any gradient
updates. Properly prompted LLMs can carry out
similar steps of logical traces with that in demon-
stration examples, which is known as Chain-of-
Thought (CoT) reasoning (Wei et al., 2022). This
generative reasoning process not only enhances the
model’s performance on tasks such as arithmetic,
commonsense, and symbolic reasoning, but also
facilitates the understanding of the underlying ra-
tionale behind the results from LLMs.
Previous research has suggested the importance
of reasoning in improving the accuracy and explain-
ability of fake news detection (Jin et al., 2022).
However, leveraging LLM reasoning in the context
of fake news related tasks remains under-explored.
In this work, we first evaluate three classical ICL
methods, including standard prompting and CoT-
based methods for news claim verification. The
standard prompting takes in a news claim for LLM
to return its factuality judgment on the claim, while
CoT additionally generates a series of intermediate
verbal reasoning steps in the result. On two fake
news benchmark datasets RAWFC (Yang et al.,
2022) and LIAR (Wang, 2017), we find that the
standard prompting performs comparably well as
"
"2310.00313","Safoora Yousefi","Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Akanksha Saran,
  Rapha\""el Milli\`ere, Ida Momennejad","In-Context Learning in Large Language Models: A Neuroscience-inspired
  Analysis of Representations","Added overview figures 1-3 in this version","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) exhibit remarkable performance improvement
through in-context learning (ICL) by leveraging task-specific examples in the
input. However, the mechanisms behind this improvement remain elusive. In this
work, we investigate embeddings and attention representations in Llama-2 70B
and Vicuna 13B. Specifically, we study how embeddings and attention change
after in-context-learning, and how these changes mediate improvement in
behavior. We employ neuroscience-inspired techniques, such as representational
similarity analysis (RSA), and propose novel methods for parameterized probing
and attention ratio analysis (ARA, measuring the ratio of attention to relevant
vs. irrelevant information). We designed three tasks with a priori
relationships among their conditions: reading comprehension, linear regression,
and adversarial prompt injection. We formed hypotheses about expected
similarities in task representations to investigate latent changes in
embeddings and attention. Our analyses revealed a meaningful correlation
between changes in both embeddings and attention representations with
improvements in behavioral performance after ICL. This empirical framework
empowers a nuanced understanding of how latent representations affect LLM
behavior with and without ICL, offering valuable tools and insights for future
research and practical applications.
","2023-10-19","2310.00313v1.pdf","Under review as a conference paper at ICLR 2024
IN-CONTEXT LEARNING IN LARGE LANGUAGE MOD-
ELS: A NEUROSCIENCE-INSPIRED ANALYSIS OF REP-
RESENTATIONS
Akanksha Saran
Microsoft Research
New York, NY
aksaran†
Hosein Hasanbeig
Microsoft Research
New York, NY
hhasanbeig†
Leo Betthauser ∗
Microsoft
Redmond, WA
lebettha†
Safoora Yousefi∗
Microsoft
Redmond, WA
sayouse†
†@microsoft.com
Rapha¨el Milli`ere
Macquarie University
Sydney, Australia
raphael.milliere@mq.edu.au
Ida Momennejad
Microsoft Research
New York, NY
idamo†
ABSTRACT
Large language models (LLMs) exhibit remarkable performance improvement
through in-context learning (ICL) by leveraging task-specific examples in the in-
put. However, the mechanisms behind this improvement remain elusive. In this
work, we investigate how LLM embeddings and attention representations change
following in-context-learning, and how these changes mediate improvement in be-
havior. We employ neuroscience-inspired techniques such as representational sim-
ilarity analysis (RSA) and propose novel methods for parameterized probing and
measuring ratio of attention to relevant vs. irrelevant information in Llama-2 70B
and Vicuna 13B. We designed three tasks with a priori relationships among their
conditions: reading comprehension, linear regression, and adversarial prompt in-
jection. We formed hypotheses about expected similarities in task representations
to investigate latent changes in embeddings and attention. Our analyses revealed a
meaningful correlation between changes in both embeddings and attention repre-
sentations with improvements in behavioral performance after ICL. This empirical
framework empowers a nuanced understanding of how latent representations af-
fect LLM behavior with and without ICL, offering valuable tools and insights for
future research and practical applications.
1
INTRODUCTION
arXiv:2310.00313v1  [cs.CL]  30 Sep 2023
Transformer-based large language models (LLMs) such as GPT-3 (Brown et al., 2020) and Llama-2
(Touvron et al., 2023b) have achieved state-of-the-art performance on a wide range of tasks. One of
the most intriguing aspects of modern Transformer-based models, especially LLMs, is their capacity
for in-context learning (ICL) (Brown et al., 2020). ICL enables the model to improve its performance
on new tasks from a few examples provided in the input context (or prompt), without any parameter
updates. ICL enables LLMs to flexibly adapt their behavior to task-specific demands during infer-
ence without further training or fine-tuning. For instance, including examples of question-answer
pairs in the prompt significantly improves performance on arithmetic, commonsense, and symbolic
reasoning tasks (Wei et al., 2022; Zhou et al., 2022). However, in spite of progress in this area, how
ICL improves behavior remains mostly unknown and an active area of research.
Some prior studies have framed ICL as implicit optimization, providing theoretical and empirical
evidence that Transformer self-attention can implement algorithms similar to gradient descent (von
Oswald et al., 2022; Aky¨urek et al., 2022; Ahn et al., 2023). Other work has proposed a Bayesian
perspective, suggesting pretraining learns a latent variable model that allows conditioning on in-
context examples for downstream prediction (Xie et al., 2022; Wang et al., 2023; Ahuja et al., 2023).
∗Equal Contribution
1
"
"2310.00322","Chengdong Ma","Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan,
  Yaodong Yang","Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language
  Models","","","","","cs.CL cs.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deployable Large Language Models (LLMs) must conform to the criterion of
helpfulness and harmlessness, thereby achieving consistency between LLMs
outputs and human values. Red-teaming techniques constitute a critical way
towards this criterion. Existing work rely solely on manual red team designs
and heuristic adversarial prompts for vulnerability detection and optimization.
These approaches lack rigorous mathematical formulation, thus limiting the
exploration of diverse attack strategy within quantifiable measure and
optimization of LLMs under convergence guarantees. In this paper, we present
Red-teaming Game (RTG), a general game-theoretic framework without manual
annotation. RTG is designed for analyzing the multi-turn attack and defense
interactions between Red-team language Models (RLMs) and Blue-team Language
Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with
diversity measure of the semantic space. GRTS is an automated red teaming
technique to solve RTG towards Nash equilibrium through meta-game analysis,
which corresponds to the theoretically guaranteed optimization direction of
both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that
GRTS autonomously discovered diverse attack strategies and effectively improved
security of LLMs, outperforming existing heuristic red-team designs. Overall,
RTG has established a foundational framework for red teaming tasks and
constructed a new scalable oversight technique for alignment.
","2023-10-11","2310.00322v1.pdf","RED TEAMING GAME: A GAME-THEORETIC FRAME-
WORK FOR RED TEAMING LANGUAGE MODELS
Chengdong Ma1,∗, Ziran Yang2,∗, Minquan Gao1, Hai Ci3, Jun Gao4,
Xuehai Pan3 & Yaodong Yang1,†
1 Institute for Artificial Intelligence, Peking University
2 Yuanpei College, Peking University
3 School of Computer Science, Peking University
4 School of Artificial Intelligence, Beijing University of Posts and Telecommunications
∗ Equal contribution
† Corresponding author
mcd1619@buaa.edu.cn, ziranyang@stu.pku.edu.cn,
minchiuan@zju.edu.cn, cihai@pku.edu.cn, jungao@bupt.edu.cn,
XuehaiPan@pku.edu.cn, yaodong.yang@pku.edu.cn,
ABSTRACT
Deployable Large Language Models (LLMs) must conform to the criterion of help-
fulness and harmlessness, thereby achieving consistency between LLMs outputs
and human values. Red-teaming techniques constitute a critical way towards this
criterion. Existing work rely solely on manual red team designs and heuristic adver-
sarial prompts for vulnerability detection and optimization. These approaches lack
rigorous mathematical formulation, thus limiting the exploration of diverse attack
strategy within quantifiable measure and optimization of LLMs under convergence
guarantees. In this paper, we present Red-teaming Game (RTG), a general game-
theoretic framework without manual annotation. RTG is designed for analyzing
the multi-turn attack and defense interactions between Red-team language Models
(RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose
Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic
space. GRTS is an automated red teaming technique to solve RTG towards Nash
equilibrium through meta-game analysis, which corresponds to the theoretically
guaranteed optimization direction of both RLMs and BLM. Empirical results in
multi-turn attacks with RLMs show that GRTS autonomously discovered diverse
attack strategies and effectively improved security of LLMs, outperforming existing
heuristic red-team designs. Overall, RTG has established a foundational frame-
work for red teaming tasks and constructed a new scalable oversight technique
for alignment. Warning: this paper contains examples that may be offensive or
upsetting.
arXiv:2310.00322v1  [cs.CL]  30 Sep 2023
1
INTRODUCTION
The development of Large Language Models (LLMs) has illuminated the path towards General
Artificial Intelligence. LLMs such as ChatGPT John Schulman & Hilton (2022) and Claude An-
thropic (2023) have demonstrated the ability to generate high-quality content and follow human
instructions, spawning applications to assist humans in solving various problems. However, this
scientific advancement has also given rise to significant ethical and safety concerns. For exam-
ple, language models that absorb vast and unfiltered data from diverse sources but without align-
ment can inadvertently generate content with undesirable features Gehman et al. (2020) such as
pornography, violence, racial discrimination, gender bias and other harmful biases, distorting the
correct societal values Sap et al. (2019); Hutchinson et al. (2020); Kurita et al. (2019); Abid et al.
(2021); Basta et al. (2019). Furthermore, the misuse of these models can lead to their involve-
ment in criminal activities, providing guidance and support for privacy breaches Carlini et al.
(2021), the creation of hazardous substances, and other harmful behaviors Bender et al. (2021);
Bommasani et al. (2021); Dinan et al. (2021); Weidinger et al. (2021); Ganguli et al. (2022a);
1
"
"2310.00328","Joe O'Brien","Joe O'Brien, Shaun Ee, Zoe Williams","Deployment Corrections: An incident response framework for frontier AI
  models","53 pages; 1 figure; 1 table","","","","cs.CY","http://creativecommons.org/licenses/by/4.0/","  A comprehensive approach to addressing catastrophic risks from AI models
should cover the full model lifecycle. This paper explores contingency plans
for cases where pre-deployment risk management falls short: where either very
dangerous models are deployed, or deployed models become very dangerous.
  Informed by incident response practices from industries including
cybersecurity, we describe a toolkit of deployment corrections that AI
developers can use to respond to dangerous capabilities, behaviors, or use
cases of AI models that develop or are detected after deployment. We also
provide a framework for AI developers to prepare and implement this toolkit.
  We conclude by recommending that frontier AI developers should (1) maintain
control over model access, (2) establish or grow dedicated teams to design and
maintain processes for deployment corrections, including incident response
plans, and (3) establish these deployment corrections as allowable actions with
downstream users. We also recommend frontier AI developers, standard-setting
organizations, and regulators should collaborate to define a standardized
industry-wide approach to the use of deployment corrections in incident
response.
  Caveat: This work applies to frontier AI models that are made available
through interfaces (e.g., API) that provide the AI developer or another
upstream party means of maintaining control over access (e.g., GPT-4 or
Claude). It does not apply to management of catastrophic risk from open-source
models (e.g., BLOOM or Llama-2), for which the restrictions we discuss are
largely unenforceable.
","2023-10-03","2310.00328v1.pdf","Deployment corrections
An incident response framework for frontier AI
models
Institute for AI Policy and Strategy (IAPS)
30th September - 2023
AUTHORS
Joe O’Brien - Associate Researcher
Shaun Ee - Researcher
Zoe Williams - Research Manager
"
"2310.00347","Shaina Raza Dr.","Shaina Raza, Oluwanifemi Bamgbose, Veronica Chatrath, Shardul Ghuge,
  Yan Sidyakin, Abdullah Y Muaad","Unlocking Bias Detection: Leveraging Transformer-Based Models for
  Content Analysis","UNDER REVIEW","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Bias detection in text is imperative due to its role in reinforcing negative
stereotypes, disseminating misinformation, and influencing decisions. Current
language models often fall short in generalizing beyond their training sets. In
response, we introduce the Contextualized Bi-Directional Dual Transformer
(CBDT) Classifier. This novel architecture utilizes two synergistic transformer
networks: the Context Transformer and the Entity Transformer, aiming for
enhanced bias detection. Our dataset preparation follows the FAIR principles,
ensuring ethical data usage. Through rigorous testing on various datasets, CBDT
showcases its ability in distinguishing biased from neutral statements, while
also pinpointing exact biased lexemes. Our approach outperforms existing
methods, achieving a 2-4\% increase over benchmark performances. This opens
avenues for adapting the CBDT model across diverse linguistic and cultural
landscapes.
","2023-10-17","2310.00347v1.pdf","Unlocking Bias Detection: Leveraging
Transformer-Based Models for Content Analysis
Shaina Razaa,∗, Oluwanifemi Bamgbosea, Veronica Chatratha, Yan
Sidyakina, Shardul Ghugea, Abdullah Y Muaadb
aVector Institute for Artificial Intelligence, Toronto, M5G 1M1, Ontario, Canada
bDepartment of Computer Science, University of
Mysore, 570006, Manasagangothri, Mysore, India
Abstract
Detecting bias in text is crucial due to its potential implications in perpetuat-
ing harmful stereotypes, spreading misinformation, and influencing decision-
making. Existing language models often struggle to generalize beyond their
training data.
To address this challenge, we propose the Contextualized
Bi-Directional Dual Transformer (CBDT) Classifier that leverages two in-
terconnected transformer networks, the Context Transformer and the Entity
Transformer, to detect bias in text. Experimental results on diverse datasets
demonstrate the superiority of the CBDT classifier in accurately classifying
biased and non-biased sentences, as well as identifying specific biased words
and phrases. We get a performance gain of about 2-4% over the baselines.
Future research can extend the model to different languages and cultural
contexts.
Keywords:
Bias, Language Models, Transformers, Evaluation
1. Introduction
arXiv:2310.00347v1  [cs.CL]  30 Sep 2023
As Natural Language Processing (NLP) rapidly evolves, its significance ex-
tends well beyond text analysis.
NLP influences diverse sectors, ranging
from social media analytics to advanced healthcare diagnostics. The perva-
sive reach of NLP showcases not only its achievements but also highlights
vital challenges. Among these challenges, linguistic biases [1, 2], which are
∗Corresponding author
Email address: shaina.raza@vectorinstitute.ai (Shaina Raza)
Preprint submitted to Elsevier
October 3, 2023
"
"2310.00371","Kartik Ramachandruni","Kartik Ramachandruni, Max Zuo, Sonia Chernova","ConSOR: A Context-Aware Semantic Object Rearrangement Framework for
  Partially Arranged Scenes","Accepted to IROS 2023","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  Object rearrangement is the problem of enabling a robot to identify the
correct object placement in a complex environment. Prior work on object
rearrangement has explored a diverse set of techniques for following user
instructions to achieve some desired goal state. Logical predicates, images of
the goal scene, and natural language descriptions have all been used to
instruct a robot in how to arrange objects. In this work, we argue that
burdening the user with specifying goal scenes is not necessary in
partially-arranged environments, such as common household settings. Instead, we
show that contextual cues from partially arranged scenes (i.e., the placement
of some number of pre-arranged objects in the environment) provide sufficient
context to enable robots to perform object rearrangement \textit{without any
explicit user goal specification}. We introduce ConSOR, a Context-aware
Semantic Object Rearrangement framework that utilizes contextual cues from a
partially arranged initial state of the environment to complete the arrangement
of new objects, without explicit goal specification from the user. We
demonstrate that ConSOR strongly outperforms two baselines in generalizing to
novel object arrangements and unseen object categories. The code and data can
be found at https://github.com/kartikvrama/consor.
","2023-10-03","2310.00371v1.pdf","ConSOR: A Context-Aware Semantic Object Rearrangement
Framework for Partially Arranged Scenes
Kartik Ramachandruni, Max Zuo, and Sonia Chernova
Abstract— Object rearrangement is the problem of enabling
a robot to identify the correct object placement in a complex
environment. Prior work on object rearrangement has explored
a diverse set of techniques for following user instructions to
achieve some desired goal state. Logical predicates, images
of the goal scene, and natural language descriptions have all
been used to instruct a robot in how to arrange objects. In
this work, we argue that burdening the user with specifying
goal scenes is not necessary in partially-arranged environ-
ments, such as common household settings. Instead, we show
that contextual cues from partially arranged scenes (i.e., the
placement of some number of pre-arranged objects in the
environment) provide sufficient context to enable robots to
perform object rearrangement without any explicit user goal
specification. We introduce ConSOR, a Context-aware Semantic
Object Rearrangement framework that utilizes contextual cues
from a partially arranged initial state of the environment to
complete the arrangement of new objects, without explicit goal
specification from the user. We demonstrate that ConSOR
strongly outperforms two baselines in generalizing to novel ob-
ject arrangements and unseen object categories. The code and
data are available at https://github.com/kartikvrama/consor.
I. INTRODUCTION
In this work, we posit that contextual cues from partially
arranged scenes (i.e., the placement of some number of
pre-arranged objects in the environment) provide sufficient
context to enable robots to perform object rearrangement
without any explicit user goal specification. Closely related
to our work are those of Abdo et al. [12] and Wu et al. [13],
which reason about object similarities by learning object
relationships from demonstrations of arranged environments,
which are then generalized to novel environments. However,
these works require that the desired organizational style in
the goal state be known a priori (e.g., specified by the user)
instead of inferring this style from scene context.
We introduce ConSOR, a Context-aware Semantic Object
Rearrangement framework that utilizes contextual cues from
a partially arranged initial state of the environment to com-
plete the arrangement of new objects, without explicit goal
specification from the user. Figure 1 presents an overview
of our framework. ConSOR reasons about the semantic
properties of objects in the environment, and the context
provided by the number of containers and existing placement
of objects into containers, to infer the desired placement for
new, unarranged objects. Additionally, ConSOR leverages
prior commonsense knowledge from pre-trained ConceptNet
embeddings to perform zero-shot generalization to scenes
with objects unseen during training. Our work makes the
following contributions:
• We formalize the problem of object rearrangement in
partially arranged environments.
• We present ConSOR, a Context-aware Semantic Object
Rearrangement framework that replaces human instruc-
tion with contextual cues from the initial state of the
environment to infer the desired goal state of an object
rearrangement task.
arXiv:2310.00371v1  [cs.RO]  30 Sep 2023
• We contribute a dataset of 8k rearranged goal states
from a dataset of 38 household objects, with each goal
state associated with one of four predefined organiza-
tional schemas.
Consider a service robot tasked with putting away newly
delivered groceries, or cleaning a living room. In both tasks,
the environment is most likely already partially arranged,
and that arrangement provides valuable clues for where
new items should be placed. For example, the pantry may
already contain unfinished boxes of cereals and pasta on
different shelves, while the left drawer of the refrigerator
may contain half-finished vegetables. Thus, new items, such
as a box of oatmeal, should be placed in accordance with the
user’s existing organization scheme (e.g., near the cereal).
Similarly, a book may naturally be placed alongside other
books on the shelf rather than next to houseplants.
The general problem of identifying the correct item place-
ment in a complex environment is known as the object
rearrangement problem [1]. Prior work on object rearrange-
ment has explored a diverse set of techniques for following
user instructions to achieve some desired goal state. Logical
predicates [2], [3], images of the goal scene [4], [5], [6], and
natural language descriptions [7], [8], [9] have all been used
to instruct a robot in how to arrange objects. However, all of
the above techniques place a burden on the user to explicitly
describe the goal state, or else to explicitly demonstrate
the rearrangement task so that the robot can learn from
demonstrations [10], [11].
Georgia Institute of Technology, Atlanta, Georgia, United States. Contact:
{kvr6,zuo,chernova}@gatech.edu
• We demonstrate that ConSOR is able to generalize
both to novel arrangements and novel object classes,
achieving high performance across all four organiza-
tional schemas we tested.
We compare ConSOR with two baselines, a collaborative
filtering-based approach to grouping objects based on learned
pairwise similarity scores [12] and the GPT-3 large language
model [14], on a withheld set of novel object arrangements
and object types. Our results show that ConSOR strongly
outperforms both baselines in every tested category, without
"
"2310.00374","Jonas Schuett","Jide Alaga and Jonas Schuett","Coordinated pausing: An evaluation-based coordination scheme for
  frontier AI developers","24 pages, 3 figures, 1 table","","","","cs.CY","http://creativecommons.org/licenses/by/4.0/","  As artificial intelligence (AI) models are scaled up, new capabilities can
emerge unintentionally and unpredictably, some of which might be dangerous. In
response, dangerous capabilities evaluations have emerged as a new risk
assessment tool. But what should frontier AI developers do if sufficiently
dangerous capabilities are in fact discovered? This paper focuses on one
possible response: coordinated pausing. It proposes an evaluation-based
coordination scheme that consists of five main steps: (1) Frontier AI models
are evaluated for dangerous capabilities. (2) Whenever, and each time, a model
fails a set of evaluations, the developer pauses certain research and
development activities. (3) Other developers are notified whenever a model with
dangerous capabilities has been discovered. They also pause related research
and development activities. (4) The discovered capabilities are analyzed and
adequate safety precautions are put in place. (5) Developers only resume their
paused activities if certain safety thresholds are reached. The paper also
discusses four concrete versions of that scheme. In the first version, pausing
is completely voluntary and relies on public pressure on developers. In the
second version, participating developers collectively agree to pause under
certain conditions. In the third version, a single auditor evaluates models of
multiple developers who agree to pause if any model fails a set of evaluations.
In the fourth version, developers are legally required to run evaluations and
pause if dangerous capabilities are discovered. Finally, the paper discusses
the desirability and feasibility of our proposed coordination scheme. It
concludes that coordinated pausing is a promising mechanism for tackling
emerging risks from frontier AI models. However, a number of practical and
legal obstacles need to be overcome, especially how to avoid violations of
antitrust law.
","2023-10-03","2310.00374v1.pdf","Coordinated pausing: An evaluation-based
coordination scheme for frontier AI developers
Jide Alaga∗
Centre for the Governance of AI
Jonas Schuett
Centre for the Governance of AI
Abstract
As artificial intelligence (AI) models are scaled up, new capabilities can emerge
unintentionally and unpredictably, some of which might be dangerous. In response,
dangerous capabilities evaluations have emerged as a new risk assessment tool.
But what should frontier AI developers do if sufficiently dangerous capabilities
are in fact discovered? This paper focuses on one possible response: coordinated
pausing. It proposes an evaluation-based coordination scheme that consists of five
main steps: (1) Frontier AI models are evaluated for dangerous capabilities. (2)
Whenever, and each time, a model fails a set of evaluations, the developer pauses
certain research and development activities. (3) Other developers are notified
whenever a model with dangerous capabilities has been discovered. They also
pause related research and development activities. (4) The discovered capabilities
are analyzed and adequate safety precautions are put in place. (5) Developers
only resume their paused activities if certain safety thresholds are reached. The
paper also discusses four concrete versions of that scheme. In the first version,
pausing is completely voluntary and relies on public pressure on developers. In the
second version, participating developers collectively agree to pause under certain
conditions. In the third version, a single auditor evaluates models of multiple
developers who agree to pause if any model fails a set of evaluations. In the
fourth version, developers are legally required to run evaluations and pause if
dangerous capabilities are discovered. Finally, the paper discusses the desirability
and feasibility of our proposed coordination scheme. It concludes that coordinated
pausing is a promising mechanism for tackling emerging risks from frontier AI
models. However, a number of practical and legal obstacles need to be overcome,
especially how to avoid violations of antitrust law.
Step 1
Step 2
Step 3
Step 4
Step 5
Dangerous capabilities evaluations
Individual pausing
Coordinated pausing
Investigation during pausing
Resuming paused activities
arXiv:2310.00374v1  [cs.CY]  30 Sep 2023
Other 
Other 
Other 
Other 
Other 
Developer
Developer
Developer
developers
developers
developers
developers
Developer
developers
Developer
Training
Training
Training
❌ Pausing
❌ Pausing
✅ Training
✅ Training
❌ Pausing
❌ Pausing
🔍 Analysis
Model
Models
Model
Models
Model
Models
Models
Model
Models
Model
Whenever, and each time,
Other developers are notified
Developers only resume their 
Frontier AI models are 
evaluated for dangerous 
a model fails a set of 
paused activities if certain 
capabilities.
The discovered capabilities 
are analyzed and adequate 
safety precautions are put
safety thresholds are 
evaluations, the developer 
pauses certain research and 
whenever a model with dangerous 
capabilities has been discovered. 
They also pause related research
in place
reached.
development activities.
and development activities.
Figure 1: The main steps of our proposed evaluation-based coordination scheme
∗Jide Alaga worked on the project as part of the 2023 GovAI Winter Research Fellowship.
"
"2310.00378","Zhaowei Zhang","Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang","Measuring Value Understanding in Language Models through
  Discriminator-Critique Gap","","","","","cs.CL cs.AI cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advancements in Large Language Models (LLMs) have heightened concerns
about their potential misalignment with human values. However, evaluating their
grasp of these values is complex due to their intricate and adaptable nature.
We argue that truly understanding values in LLMs requires considering both
""know what"" and ""know why"". To this end, we present the Value Understanding
Measurement (VUM) framework that quantitatively assesses both ""know what"" and
""know why"" by measuring the discriminator-critique gap related to human values.
Using the Schwartz Value Survey, we specify our evaluation values and develop a
thousand-level dialogue dataset with GPT-4. Our assessment looks at both the
value alignment of LLM's outputs compared to baseline answers and how LLM
responses align with reasons for value recognition versus GPT-4's annotations.
We evaluate five representative LLMs and provide strong evidence that the
scaling law significantly impacts ""know what"" but not much on ""know why"", which
has consistently maintained a high level. This may further suggest that LLMs
might craft plausible explanations based on the provided context without truly
understanding their inherent value, indicating potential risks.
","2023-10-20","2310.00378v1.pdf","MEASURING VALUE UNDERSTANDING IN LANGUAGE
MODELS THROUGH DISCRIMINATOR-CRITIQUE GAP
Zhaowei Zhang124,
Fengshuo Bai24∗†,
Jun Gao3∗,
Yaodong Yang2‡
1School of Intelligence Science and Technology, Peking University
2Institute for Artificial Intelligence, Peking University
3School of Artificial Intelligence, Beijing University of Posts and Telecommunications
4Beijing Institute for General Artificial Intelligence (BIGAI)
zwzhang@stu.pku.edu.cn, changwindeg@gmail.com
jungao@bupt.edu.cn, yaodong.yang@pku.edu.cn
ABSTRACT
Recent advancements in Large Language Models (LLMs) have heightened con-
cerns about their potential misalignment with human values. However, evaluating
their grasp of these values is complex due to their intricate and adaptable na-
ture. We argue that truly understanding values in LLMs requires considering both
“know what” and “know why”. To this end, we present the Value Understanding
Measurement (VUM) framework that quantitatively assess both “know what” and
“know why” by measuring the discriminator-critique gap related to human values.
Using the Schwartz Value Survey, we specify our evaluation values and develop
a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the
value alignment of LLM’s outputs compared to baseline answers and how LLM
responses align with reasons for value recognition versus GPT-4’s annotations. We
evaluate five representative LLMs and provide strong evidence that the scaling law
significantly impacts “know what” but not much on “know why”, which has con-
sistently maintained a high level. This may further suggest that LLMs might craft
plausible explanations based on the provided context without truly understanding
their inherent value, indicating potential risks.
1
INTRODUCTION
arXiv:2310.00378v1  [cs.CL]  30 Sep 2023
The rapid capacity emergence of Large Language Models (LLMs) is exciting, but it has heightened
our concerns about their potential misalignment with human values and further harm to humanity
(Future of Life Institute, 2023). Therefore, it is very important to evaluate the LLM’s ability to
understand human values. However, even though methods like chain-of-thought (Wei et al., 2022)
enable LLMs to have some self-correcting ability and stronger reasoning, they still sometimes
engage in fabricating facts and hallucination (Bang et al., 2023). We believe the reason behind this
phenomenon is during the training of LLMs, we typically only focus on having them mimic human
linguistic behavior, lacking attention to the motivations and reasons behind them, thus failing to
achieve a deeper alignment between knowledge and action (Ma et al., 2023). This kind of problem
will become more prominent on value alignment due to the complexity and adaptability of values,
where we need effective measurement for evaluating the value understanding of LLMs in the process
of scalable oversight (Amodei et al., 2016).
Several existing methods have already focused on evaluating the value emergence of LLMs. Zhang
et al. (2023) quantitatively assessed LLMs’ value rationality concerning different values using social
value orientation (Messick & McClintock, 1968; McClintock & Van Avermaet, 1982; Murphy et al.,
2011). Durmus et al. (2023) collected human value data from various cultures and evaluated the
extent of LLMs’ value emergence by measuring the similarity between LLM responses and human
data from different value backgrounds. Hendrycks et al. (2020); Abdulhai et al. (2022); Jin et al.
∗Equal contribution
†Work done when Fengshuo Bai visited Peking University.
‡Corresponding to: yaodong.yang@pku.edu.cn.
"
"2310.00385","Fei Zhao","Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, Xinyu Dai","Dynamic Demonstrations Controller for In-Context Learning","Under review","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In-Context Learning (ICL) is a new paradigm for natural language processing
(NLP), where a large language model (LLM) observes a small number of
demonstrations and a test instance as its input, and directly makes predictions
without updating model parameters. Previous studies have revealed that ICL is
sensitive to the selection and the ordering of demonstrations. However, there
are few studies regarding the impact of the demonstration number on the ICL
performance within a limited input length of LLM, because it is commonly
believed that the number of demonstrations is positively correlated with model
performance. In this paper, we found this conclusion does not always hold true.
Through pilot experiments, we discover that increasing the number of
demonstrations does not necessarily lead to improved performance. Building upon
this insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller),
which can improve the ICL performance by adjusting the number of demonstrations
dynamically. The experimental results show that D$^2$Controller yields a 5.4%
relative improvement on eight different sizes of LLMs across ten datasets.
Moreover, we also extend our method to previous ICL models and achieve
competitive results.
","2023-10-03","2310.00385v1.pdf","Preprint. Under review.
DYNAMIC DEMONSTRATIONS CONTROLLER FOR IN-
CONTEXT LEARNING
Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, Xinyu Dai
National Key Laboratory for Novel Software Technology, Nanjing University
{zhaof,pangtt,maz}@smail.nju.edu.cn
{wuz,huangsj,daixinyu}@nju.edu.cn
ABSTRACT
In-Context Learning (ICL) is a new paradigm for natural language processing
(NLP), where a large language model (LLM) observes a small number of demon-
strations and a test instance as its input, and directly makes predictions without
updating model parameters. Previous studies have revealed that ICL is sensitive
to the selection and the ordering of demonstrations. However, there are few studies
regarding the impact of the demonstration number on the ICL performance within
a limited input length of LLM, because it is commonly believed that the number
of demonstrations is positively correlated with model performance. In this paper,
we found this conclusion does not always hold true. Through pilot experiments,
we discover that increasing the number of demonstrations does not necessarily
lead to improved performance. Building upon this insight, we propose a Dynamic
Demonstrations Controller (D2Controller), which can improve the ICL perfor-
mance by adjusting the number of demonstrations dynamically. The experimental
results show that D2Controller yields a 5.4% relative improvement on eight dif-
ferent sizes of LLMs across ten datasets. Moreover, we also extend our method to
previous ICL models and achieve competitive results1.
1
INTRODUCTION
In-Context Learning (ICL) is a new paradigm for performing various NLP tasks using large language
models (LLMs) (Brown et al., 2020). In ICL, by conditioning on a small number of demonstrations,
LLMs can generate predictions for a given test input without updating model parameters. Restricted
by the maximum input length of LLMs, it is common to sample a small set of examples from the
training dataset randomly to formulate demonstrations. Figure 1 shows an example of sentiment
analysis using ICL.
arXiv:2310.00385v1  [cs.CL]  30 Sep 2023
To improve the performance of ICL, existing work primarily focuses on designing Demonstration
Selection methods (Liu et al., 2022a; Rubin et al., 2022; Zhang et al., 2022b; Kim et al., 2022; Gonen
et al., 2022; Sorensen et al., 2022; Wang et al., 2023; Li et al., 2023; Li & Qiu, 2023) or finding an
appropriate Demonstration Ordering (Lu et al., 2022; Wu et al., 2022), since a lot of studies have
revealed that ICL is sensitive to the selection as well as the ordering of demonstrations (Liu et al.,
2022a; Rubin et al., 2022; Zhang et al., 2022b; Lu et al., 2022; Wu et al., 2022; Li et al., 2023; Li &
Qiu, 2023; Dong et al., 2022).
However, to the best of our knowledge, there are few studies available regarding the impact of the
Demonstration Number on the ICL performance. This scarcity may be attributed to the prevail-
ing belief that the relation between the number of demonstrations and model performance follows
a power law – as the number of demonstrations increases, model performance continues to im-
prove (Xie et al., 2022; Xu et al., 2023). Nevertheless, through pilot experiments, we find this
conclusion does not always hold true. Specifically, within the constraints of input length in LLMs,
we systematically evaluate model performance across a spectrum ranging from the minimum to
the maximum number of demonstrations. This comprehensive assessment involves five different
datasets and encompasses five sizes of LLMs (Brown et al., 2020; Zhang et al., 2022a; Dey et al.,
2023). Our findings reveal that:
1Our code and datasets are available at https://github.com/TJTP/D2Controller
1
"
"2310.00390","Yulu Gan","Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed
  M. Alaa","InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision
  Generalists","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent advances in generative diffusion models have enabled text-controlled
synthesis of realistic and diverse images with impressive quality. Despite
these remarkable advances, the application of text-to-image generative models
in computer vision for standard visual recognition tasks remains limited. The
current de facto approach for these tasks is to design model architectures and
loss functions that are tailored to the task at hand. In this paper, we develop
a unified language interface for computer vision tasks that abstracts away
task-specific design choices and enables task execution by following natural
language instructions. Our approach involves casting multiple computer vision
tasks as text-to-image generation problems. Here, the text represents an
instruction describing the task, and the resulting image is a visually-encoded
task output. To train our model, we pool commonly-used computer vision datasets
covering a range of tasks, including segmentation, object detection, depth
estimation, and classification. We then use a large language model to
paraphrase prompt templates that convey the specific tasks to be conducted on
each image, and through this process, we create a multi-modal and multi-task
training dataset comprising input and output images along with annotated
instructions. Following the InstructPix2Pix architecture, we apply
instruction-tuning to a text-to-image diffusion model using our constructed
dataset, steering its functionality from a generative model to an
instruction-guided multi-task vision learner. Experiments demonstrate that our
model, dubbed InstructCV, performs competitively compared to other generalist
and task-specific vision models. Moreover, it exhibits compelling
generalization capabilities to unseen data, categories, and user instructions.
","2023-10-03","2310.00390v1.pdf","INSTRUCTCV: INSTRUCTION-TUNED TEXT-TO-IMAGE
DIFFUSION MODELS AS VISION GENERALISTS
Yulu Gan
Peking University
Sungwoo Park
UC Berkeley
Alexander Schubert
UC Berkeley and UCSF
Anthony Philippakis
Broad Institute of MIT & Harvard
Ahmed M. Alaa
UC Berkeley and UCSF
ABSTRACT
Recent advances in generative diffusion models have enabled text-controlled synthe-
sis of realistic and diverse images with impressive quality. Despite these remarkable
advances, the application of text-to-image generative models in computer vision for
standard visual recognition tasks remains limited. The current de facto approach for
these tasks is to design model architectures and loss functions that are tailored to
the task at hand. In this paper, we develop a unified language interface for computer
vision tasks that abstracts away task-specific design choices and enables task exe-
cution by following natural language instructions. Our approach involves casting
multiple computer vision tasks as text-to-image generation problems. Here, the text
represents an instruction describing the task, and the resulting image is a visually-
encoded task output. To train our model, we pool commonly-used computer vision
datasets covering a range of tasks, including segmentation, object detection, depth
estimation, and classification. We then use a large language model to paraphrase
prompt templates that convey the specific tasks to be conducted on each image,
and through this process, we create a multi-modal and multi-task training dataset
comprising input and output images along with annotated instructions. Following
the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image
diffusion model using our constructed dataset, steering its functionality from a
generative model to an instruction-guided multi-task vision learner. Experiments
demonstrate that our model, dubbed InstructCV, performs competitively compared
to other generalist and task-specific vision models. Moreover, it exhibits compelling
generalization capabilities to unseen data, categories, and user instructions.
Code: https://github.com/AlaaLab/InstructCV
Demo: https://huggingface.co/spaces/alaa-lab/InstructCV
Prompt: “Detect 
“Hi InstructCV, please 
“Create a monocular 
“Segment Berkeley’s 
Berkeley’s Sather Tower”
segment all trees”
depth map”
Sather Gate”
arXiv:2310.00390v1  [cs.CV]  30 Sep 2023
“Detect MIT’s great 
“Hi InstructCV, please 
“Highlight all trees 
“Create a depth map for 
dome”
segment all trees”
around Stata”
Stata Center”
Figure 1: Application of InstructCV to new test images & user-written instructions: InstructCV performs the
vision task described in the instruction on the input image. (Images courtesy of UC Berkeley and MIT).
1
"
"2310.00399","Yan Xiao","Yan Xiao, Xinyue Zuo, Lei Xue, Kailong Wang, Jin Song Dong and Ivan
  Beschastnikh","Empirical Study on Transformer-based Techniques for Software Engineering","","","","","cs.SE","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Many Transformer-based pre-trained models for code have been developed and
applied to code-related tasks. In this paper, we review the existing
literature, examine the suitability of model architectures for different tasks,
and look at the generalization ability of models on different datasets, and
their resource consumption.
  We examine three very representative pre-trained models for code: CodeBERT,
CodeGPT, and CodeT5, and conduct experiments on the top-4 most targeted
software engineering tasks that we found in our literature survey: Code
Summarization, Bug Fixing, Bug Detection, and Code Search. In our study, we
showcase the capability of decoder-only models (CodeGPT) for specific
generation tasks under state-of-the-art evaluation metrics and contest the
common belief that the encoder-decoder architecture is optimal for
general-purpose coding tasks. Additionally, we found that the most frequently
used models are not necessarily the most suitable for certain applications and
the developers' needs are not adequately addressed by current research. As
well, we found that the benchmark and frequent dataset for Bug Fixing and Code
Summarization both fail to enable models to generalize onto other datasets for
the same task (the frequent dataset refers to the dataset with the highest
frequency used in literature other than the benchmark). We use statistical
testing to support our conclusions from experiments. Finally, CodeBERT is
highly efficient for understanding tasks, whereas CodeT5's efficiency for
generation tasks is in doubt, as the highest resource consumption does not
guarantee a consistent better performance on different metrics. We also discuss
the numerous practical issues in advancing future research on transformer-based
models for code-related tasks.
","2023-10-03","2310.00399v1.pdf","Empirical Study on Transformer-based Techniques
for Software Engineering
Yan Xiao∗, Xinyue Zuo†, Lei Xue∗, Kailong Wang‡, Jin Song Dong† and Ivan Beschastnikh§
∗Shenzhen Campus of Sun Yat-sen University, China, xiaoy367@mail.sysu.edu.cn, xuelei3@mail.sysu.edu.cn
†National University of Singapore, Singapore, e0376991@u.nus.edu, dcsdjs@nus.edu.sg
‡Huazhong University of Science and Technology, China, Wangkl@hust.edu.cn
§Department of Computer Science, University of British Columbia, Vancouver, BC, Canada, bestchai@cs.ubc.ca
Abstract—Many Transformer-based pre-trained models for
code have been developed and applied to code-related tasks.
In this paper, we review the existing literature, examine the
suitability of model architectures for different tasks, and look
at the generalization ability of models on different datasets, and
their resource consumption.
We examine three very representative pre-trained models for
code: CodeBERT, CodeGPT, and CodeT5, and conduct exper-
iments on the top-4 most targeted software engineering tasks
that we found in our literature survey: Code Summarization,
Bug Fixing, Bug Detection, and Code Search. In our study, we
showcase the capability of decoder-only models (CodeGPT) for
specific generation tasks under state-of-the-art evaluation metrics
and contest the common belief that the encoder-decoder architec-
ture is optimal for general-purpose coding tasks. Additionally, we
found that the most frequently used models are not necessarily
the most suitable for certain applications and the developers’
needs are not adequately addressed by current research. As
well, we found that the benchmark and frequent dataset for
Bug Fixing and Code Summarization both fail to enable models
to generalize onto other datasets for the same task (the frequent
dataset refers to the dataset with the highest frequency used in
literature other than the benchmark). We use statistical testing to
support our conclusions from experiments. Finally, CodeBERT
is highly efficient for understanding tasks, whereas CodeT5’s
efficiency for generation tasks is in doubt, as the highest resource
consumption does not guarantee a consistent better performance
on different metrics. We also discuss the numerous practical
issues in advancing future research on transformer-based models
for code-related tasks.
Index Terms—transformer-based pre-trained models, Code-
BERT, CodeGPT, CodeT5, promises and perils
I. INTRODUCTION
arXiv:2310.00399v1  [cs.SE]  30 Sep 2023
of transformer-related papers published in top-tier SE con-
ferences and journals in the past five years. In many in-
stances, these works have reported state-of-the-art performance
on a variety of SE tasks. Some example applications of
transformer-based techniques include automated program re-
pair [3]–[5], merge conflict resolution [6], [7], requirements
engineering [8]–[10], code and comment generation [11], [12],
code and machine translation [11], [13], [14], and more [2],
[15]. Model structures, like encoder-only, decoder-only, and
encoder-decoder [11], together with the different pre-training
objectives, such as generative objectives and denoising objec-
tives, also add to the diversity of work in this space.
The excitement around these transformer-based models,
however, must be tempered with a careful assessment of their
advantages and pitfalls. This is the focus of this paper.
In this paper, we take a step back and reflect on the copious
amount of work that has been published in this area thus
far. We study 282 papers published at 27 top conferences
and journals during 2017-2022. We consider which models
are being used in these papers, which SE applications they
target, benchmarks that they use, and other key characteristics
of this quickly growing body of work. We then closely look at
the performance of the top models from the literature on the
most popular applications and review the corresponding model
generalizability and computational complexity. Throughout,
we frame our discussion in terms of promises and perils to help
position SE research that relies on transformer-based models
on a firmer footing.
The closest related empirical studies of this rich research
space have considered a fixed applications set [16]–[18],
different evaluation measures [19] and interpretability [20] of
BERT-related variants for Code Summarization, and a specific
research object like Copilot [21]. Different from these studies,
our review is more comprehensive, covering a wider range of
papers published over a longer period. We identified three very
representative pre-trained transformer-based models for code
and the top-4 most popular applications. We re-implemented
all three models across all four applications and managed to
contest certain beliefs regarding model architectures in the
literature with their performance measured using up-to-date
evaluation metrics. In addition to performance and architec-
ture analysis, we also analyzed models’ generalizability on
different datasets for each application, as well as their time
The availability of large natural language corpora and
advances in ML have led recent models to achieve extraor-
dinary performance on Natural Language Processing (NLP)
tasks. Transformer-based architectures [1], first introduced by
Vaswani et al. in 2017, are among the most successful model
variants in this field. Transformer-based models, like BERT
(Bidirectional Encoder Representations from Transformers)
and GPT (Generative Pre-trained Transformer), have revo-
lutionized NLP tasks, including text classification, sentiment
analysis, and language generation.
Given the abundance of large software code corpora,
Transformer-based models have also rapidly gained traction
in software engineering (SE) research [2], with hundreds
1
"
"2310.00426","Enze Xie","Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu,
  Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li","PixArt-$\alpha$: Fast Training of Diffusion Transformer for
  Photorealistic Text-to-Image Synthesis","Project Page: https://pixart-alpha.github.io","","","","cs.CV","http://creativecommons.org/licenses/by-nc-sa/4.0/","  The most advanced text-to-image (T2I) models require significant training
costs (e.g., millions of GPU hours), seriously hindering the fundamental
innovation for the AIGC community while increasing CO2 emissions. This paper
introduces PIXART-$\alpha$, a Transformer-based T2I diffusion model whose image
generation quality is competitive with state-of-the-art image generators (e.g.,
Imagen, SDXL, and even Midjourney), reaching near-commercial application
standards. Additionally, it supports high-resolution image synthesis up to
1024px resolution with low training cost, as shown in Figure 1 and 2. To
achieve this goal, three core designs are proposed: (1) Training strategy
decomposition: We devise three distinct training steps that separately optimize
pixel dependency, text-image alignment, and image aesthetic quality; (2)
Efficient T2I Transformer: We incorporate cross-attention modules into
Diffusion Transformer (DiT) to inject text conditions and streamline the
computation-intensive class-condition branch; (3) High-informative data: We
emphasize the significance of concept density in text-image pairs and leverage
a large Vision-Language model to auto-label dense pseudo-captions to assist
text-image alignment learning. As a result, PIXART-$\alpha$'s training speed
markedly surpasses existing large-scale T2I models, e.g., PIXART-$\alpha$ only
takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU
days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90% CO2
emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training
cost is merely 1%. Extensive experiments demonstrate that PIXART-$\alpha$
excels in image quality, artistry, and semantic control. We hope
PIXART-$\alpha$ will provide new insights to the AIGC community and startups to
accelerate building their own high-quality yet low-cost generative models from
scratch.
","2023-10-17","2310.00426v1.pdf","Technical Report
PIXART-α: FAST TRAINING OF DIFFUSION TRANS-
FORMER FOR PHOTOREALISTIC TEXT-TO-IMAGE
SYNTHESIS
Junsong Chen1,2,3∗, Jincheng Yu1,4∗, Chongjian Ge1,3∗, Lewei Yao1,4∗, Enze Xie1†,
Yue Wu1, Zhongdao Wang1, James Kwok4, Ping Luo3, Huchuan Lu2, Zhenguo Li1
1Huawei Noah’s Ark Lab
2Dalian University of Technology
3HKU
4HKUST
jschen@mail.dlut.edu.cn, rhettgee@connect.hku.hk,
{yujincheng4,yao.lewei,xie.enze,Li.Zhenguo}@huawei.com
Project Page: https://pixart-alpha.github.io/
ABSTRACT
arXiv:2310.00426v1  [cs.CV]  30 Sep 2023
The most advanced text-to-image (T2I) models require significant training costs
(e.g., millions of GPU hours), seriously hindering the fundamental innovation for
the AIGC community while increasing CO2 emissions. This paper introduces
PIXART-α, a Transformer-based T2I diffusion model whose image generation
quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL,
and even Midjourney), reaching near-commercial application standards. Addi-
tionally, it supports high-resolution image synthesis up to 1024 × 1024 resolu-
tion with low training cost, as shown in Figure 1 and 2. To achieve this goal,
three core designs are proposed: (1) Training strategy decomposition: We de-
vise three distinct training steps that respectively optimize pixel dependency, text-
image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We
incorporate cross-attention modules into Diffusion Transformer (DiT) to inject
text conditions and streamline the computation-intensive class-condition branch;
(3) High-informative data: We emphasize the significance of concept density in
text-image pairs and leverage a large Vision-Language model to auto-label dense
pseudo-captions to assist text-image alignment learning. As a result, PIXART-α’s
training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-
α only takes 10.8% of Stable Diffusion v1.5’s training time (∼675 vs. ∼6,250
A100 GPU days), saving nearly $300,000 ($26,000 vs. $320,000) and reducing
90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL,
our training cost is merely 1%. Extensive experiments demonstrate that PIXART-
α excels in image quality, artistry, and semantic control. We hope PIXART-α will
provide new insights to the AIGC community and startups to accelerate building
their own high-quality yet low-cost generative models from scratch.
1
INTRODUCTION
Recently, the advancement of text-to-image (T2I) generative models, such as DALL·E 2 (OpenAI,
2023), Imagen (Saharia et al., 2022), and Stable Diffusion (Rombach et al., 2022) has started a
new era of photorealistic image synthesis, profoundly impacting numerous downstream applica-
tions, such as image editing (Kim et al., 2022), video generation (Wu et al., 2022), 3D assets cre-
ation (Poole et al., 2022), etc.
However, the training of these advanced models demands immense computational resources. For in-
stance, training SDv1.5 (Podell et al., 2023) necessitates 6K A100 GPU days, approximately costing
∗Equal contribution. Work done during the internships of the four students at Huawei Noah’s Ark Lab.
†Corresponding author.
1
"
"2310.00429","Avishek Bose","Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco
  Jiralerspong, and Gauthier Gidel","On the Stability of Iterative Retraining of Generative Models on their
  own Data","","","","","cs.LG stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Deep generative models have made tremendous progress in modeling complex
data, often exhibiting generation quality that surpasses a typical human's
ability to discern the authenticity of samples. Undeniably, a key driver of
this success is enabled by the massive amounts of web-scale data consumed by
these models. Due to these models' striking performance and ease of
availability, the web will inevitably be increasingly populated with synthetic
content. Such a fact directly implies that future iterations of generative
models must contend with the reality that their training is curated from both
clean data and artificially generated data from past models. In this paper, we
develop a framework to rigorously study the impact of training generative
models on mixed datasets (of real and synthetic data) on their stability. We
first prove the stability of iterative training under the condition that the
initial generative models approximate the data distribution well enough and the
proportion of clean training data (w.r.t. synthetic data) is large enough. We
empirically validate our theory on both synthetic and natural images by
iteratively training normalizing flows and state-of-the-art diffusion models on
CIFAR10 and FFHQ.
","2023-10-04","2310.00429v1.pdf","Preprint. Under review, Copyright 2023 by the author(s).
ON THE STABILITY OF ITERATIVE RETRAINING OF
GENERATIVE MODELS ON THEIR OWN DATA
Quentin Bertrain ∗
Université de Montréal and Mila
Avishek (Joey) Bose
McGill and Mila
Alexandre Duplessis
École Normale Supérieure, PSL, and Mila
Marco Jiralerspong
Université de Montréal and Mila
Gauthier Gidel †
Université de Montréal and Mila
ABSTRACT
Deep generative models have made tremendous progress in modeling complex
data, often exhibiting generation quality that surpasses a typical human’s ability
to discern the authenticity of samples. Undeniably, a key driver of this success
is enabled by the massive amounts of web-scale data consumed by these models.
Due to these models’ striking performance and ease of availability, the web will
inevitably be increasingly populated with synthetic content. Such a fact directly
implies that future iterations of generative models must contend with the reality that
their training is curated from both clean data and artificially generated data from
past models. In this paper, we develop a framework to rigorously study the impact
of training generative models on mixed datasets (of real and synthetic data) on their
stability. We first prove the stability of iterative training under the condition that the
initial generative models approximate the data distribution well enough and the pro-
portion of clean training data (w.r.t. synthetic data) is large enough. We empirically
validate our theory on both synthetic and natural images by iteratively training
normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ.
Ground Truth
No retraining
#retrain. = 10
#retrain. = 20
#retrain. = 100
Collapsing
arXiv:2310.00429v1  [cs.LG]  30 Sep 2023
Stability
Figure 1: Stability vs. collapsing of iterative retraining of generative models on their own data. Each
model’s density is displayed as a function of the number of retraining steps. The first two columns correspond
to the true density and the density of a diffusion model trained on the true data respectively. As observed in
Shumailov et al. (2023); Alemohammad et al. (2023), iteratively retraining the model exclusively on its own
generated data (top row) yields a density that collapses: samples very near the mean of each mode are sampled
almost exclusively after 100 iterations of retraining. Contrastingly, retraining on a mixture of true and generated
data (bottom row) does not yield a collapsing density.
∗Corresponding authors: {quentin.bertrand@mila.quebec
†Canada Cifar AI Chair
1
"
"2310.00481","Xiyang Wu","Chak Lam Shek, Xiyang Wu, Dinesh Manocha, Pratap Tokekar, and Amrit
  Singh Bedi","LANCAR: Leveraging Language for Context-Aware Robot Locomotion in
  Unstructured Environments","","","","","cs.RO","http://creativecommons.org/licenses/by/4.0/","  Robotic locomotion is a challenging task, especially in unstructured
terrains. In practice, the optimal locomotion policy can be context-dependent
by using the contextual information of encountered terrains in decision-making.
Humans can interpret the environmental context for robots, but the ambiguity of
human language makes it challenging to use in robot locomotion directly. In
this paper, we propose a novel approach, LANCAR, that introduces a context
translator that works with reinforcement learning (RL) agents for context-aware
locomotion. Our formulation allows a robot to interpret the contextual
information from environments generated by human observers or Vision-Language
Models (VLM) with Large Language Models (LLM) and use this information to
generate contextual embeddings. We incorporate the contextual embeddings with
the robot's internal environmental observations as the input to the RL agent's
decision neural network. We evaluate LANCAR with contextual information in
varying ambiguity levels and compare its performance using several alternative
approaches. Our experimental results demonstrate that our approach exhibits
good generalizability and adaptability across diverse terrains, by achieving at
least 10% of performance improvement in episodic reward over baselines. The
experiment video can be found at the following link:
https://raaslab.org/projects/LLM_Context_Estimation/.
","2023-10-03","2310.00481v1.pdf","LANCAR: Leveraging Language for Context-Aware
Robot Locomotion in Unstructured Environments
Chak Lam Shek1∗, Xiyang Wu1∗, Dinesh Manocha2, Pratap Tokekar2, and Amrit Singh Bedi2
Abstract— Robotic locomotion is a challenging task, espe-
cially in unstructured terrains. In practice, the optimal loco-
motion policy can be context-dependent by using the contex-
tual information of encountered terrains in decision-making.
Humans can interpret the environmental context for robots,
but the ambiguity of human language makes it challenging to
use in robot locomotion directly. In this paper, we propose a
novel approach, LANCAR, that introduces a context translator
that works with reinforcement learning (RL) agents for context-
aware locomotion. Our formulation allows a robot to interpret
the contextual information from environments generated by hu-
man observers or Vision-Language Models (VLM) with Large
Language Models (LLM) and use this information to generate
contextual embeddings. We incorporate the contextual embed-
dings with the robot’s internal environmental observations as
the input to the RL agent’s decision neural network. We evalu-
ate LANCAR with contextual information in varying ambiguity
levels and compare its performance using several alternative
approaches. Our experimental results demonstrate that our
approach exhibits good generalizability and adaptability across
diverse terrains, by achieving at least 10% of performance
improvement in episodic reward over baselines. The experiment
video can be found at the following link: https://raaslab.
org/projects/LLM_Context_Estimation/.
I. INTRODUCTION
arXiv:2310.00481v1  [cs.RO]  30 Sep 2023
Reinforcement Learning (RL) is prevalent in robotics,
impacting manipulation [1], navigation [2], and locomo-
tion [3] tasks. RL agents learn optimal actions by interacting
with environments. However, developing unified learning RL
agents that can work robustly in diverse conditions is a
critical challenge, as optimal policies in diverse conditions
could diverge a lot [4]. Consider the case when computing
robust locomotion policies for a legged robot platform that
can operate in an unstructured environment with various
terrains. A standard RL-based approach does not always lead
to good performance in this scenario, as policy parameters
trained on a specific terrain may not translate to another [5].
One possible approach could be letting the robot agent model
all environmental properties as a part of its state and then
learn a policy that works in all conditions. However, this
method is not feasible due to the sensor limitations in fields
of view or ranges. The incapability of sensors in detecting
certain vital factors can downgrade the performance, e.g. a
robot might slip on loose soil without detecting its looseness.
Policy generalization and adaptation across diverse terrains
is still an open problem [6].
To address these issues, many prior works attempt to ex-
tract contextual information from environments from graph-
like structures [7] or autoencoders [8] to assist decision-
making. However, those methods lack the reasoning and
inference ability to handle complicated terrains. A natural
idea in these scenarios is to cooperate with humans to
interpret the environmental contextual information to robots
through their comprehensive sensing and better reasoning
abilities. For instance, humans can correlate wet grassland
with high damping upon observation, a connection robots
struggle to make. However, the ambiguity of human language
prevents this reasoning ability from being directly used by
robots [9], as many similar sentences can be interpreted
differently. As a result, it is challenging for robots to make
use of human-provided contextual information.
The recent success of the Large Language Model (LLMs)
and their ability to perform chain-of-thought [10], logic rea-
soning [11], and common sense reasoning [12] is a promising
approach to address these problems. An interesting line of
work is to incorporate LLM within RL frameworks so that
an RL agent can use LLM to assist its learning process
and make it more sample-efficient. Several prior studies
have made attempts along this line, such as using LLMs
to predict the reward functions necessary for RL [13] or
providing control inputs for robots [14]. These approaches,
though intriguing, still do not exploit the full potential of the
reasoning abilities of LLM. We believe that LLMs are better
suited as intermediaries, serving as interfaces to translate
human language into formats that are more accessible to RL
agents. This prevents human instructions from dominating
and interrupting the decision-making process of the RL
agent, which may cause performance degradation.
Main Contribution: In this paper, we investigate the pos-
sibility of utilizing LLMs to interpret contextual information
from environments (through their reasoning ability) to help
RL agents perform robot locomotion tasks. Specifically, we
study a quadruped robot navigating various terrains with a
human observer helping to interpret the context. The context
refers to terrain properties that the robot might not directly
perceive. Fig. 1 gives an overview of our approach. In
Scenario 1, a robot traverses various terrains without any
contextual information. Given the complexity of the terrains
encountered, robots may fail to develop a generalized policy
for all terrains. In Scenario 2, the robot traverses the same set
of diverse terrains but receives contextual information from
human observers1, like “You are entering a grassland right
after the rain” or “You are walking on a dry rocky road under
1In many cases vision-language models may also be a useful surrogate
to provide contextual information that is visual
∗ Denotes equal contribution
1 Authors are with the Department of Electrical and Computer Engi-
neering, University of Maryland, College Park, MD, USA {cshek1,
wuxiyang}@umd.edu
2
Authors
are
with
the
Department
of
Computer
Science,
University
of
Maryland,
College
Park,
MD,
USA {dmanocha,
tokekar,amritbd}@umd.edu
"
"2310.00483","Vincent Li","Vincent Li, Nick Doiron","Prompting Code Interpreter to Write Better Unit Tests on Quixbugs
  Functions","13 pages (including appendices), 0 figures, 1 table. First authored
  by Vincent Li; edited by Nick Doiron","","","","cs.SE cs.LG","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Unit testing is a commonly-used approach in software engineering to test the
correctness and robustness of written code. Unit tests are tests designed to
test small components of a codebase in isolation, such as an individual
function or method. Although unit tests have historically been written by human
programmers, recent advancements in AI, particularly LLMs, have shown
corresponding advances in automatic unit test generation. In this study, we
explore the effect of different prompts on the quality of unit tests generated
by Code Interpreter, a GPT-4-based LLM, on Python functions provided by the
Quixbugs dataset, and we focus on prompting due to the ease with which users
can make use of our findings and observations. We find that the quality of the
generated unit tests is not sensitive to changes in minor details in the
prompts provided. However, we observe that Code Interpreter is often able to
effectively identify and correct mistakes in code that it writes, suggesting
that providing it runnable code to check the correctness of its outputs would
be beneficial, even though we find that it is already often able to generate
correctly-formatted unit tests. Our findings suggest that, when prompting
models similar to Code Interpreter, it is important to include the basic
information necessary to generate unit tests, but minor details are not as
important.
","2023-10-03","2310.00483v1.pdf","Prompting Code Interpreter to Write Better Unit Tests on
Quixbugs Functions
Vincent Li∗†
nick.doiron@hpe.com
mail2vincentrli@gmail.com
Nick Doiron‡
Abstract
Keywords:
Large Language Models, Unit Testing,
Code Interpreter, Quixbugs, Prompting
1
Introduction
arXiv:2310.00483v1  [cs.SE]  30 Sep 2023
Unit
testing
is
a
commonly-used
approach
in
software engineering to test the correctness and
robustness of written code.
Unit tests are tests
designed to test small components of a codebase in
isolation, such as an individual function or method.
Although unit tests have historically been written
by human programmers, recent advancements in
AI, particularly LLMs, have shown corresponding
advances in automatic unit test generation. In this
study, we explore the eﬀect of diﬀerent prompts
on the quality of unit tests generated by Code
Interpreter,
a
GPT-4-based
LLM,
on
Python
functions provided by the Quixbugs dataset, and
we focus on prompting due to the ease with which
users can make use of our ﬁndings and observations.
We ﬁnd that the quality of the generated unit
tests is not sensitive to changes in minor details
in the prompts provided.
However, we observe
that Code Interpreter is often able to eﬀectively
identify and correct mistakes in code that it writes,
suggesting that providing it runnable code to check
the correctness of its outputs would be beneﬁcial,
even though we ﬁnd that it is already often able to
generate correctly-formatted unit tests. Our ﬁndings
suggest that, when prompting models similar to
Code Interpreter, it is important to include the basic
information necessary to generate unit tests, but
minor details are not as important.
∗University
of
Chicago
Existential
Risk
Laboratory
(institution where work was primarily conducted)
†Yale University (author’s current institution)
‡Hewlett Packard Enterprise
In software engineering,
testing
the correctness
of written code, especially before deployment, is
of utmost importance,
since it greatly reduces
the possibility of unexpected errors and crashes.
A common approach to software testing is unit
testing,
in
which
code
is
broken
down
into
smaller components whose correctness can be tested
individually.
Often, this is done by individually
testing a focal method or a focal class in isolation.
The advantage of such an approach is that breaking
down code into smaller components reduces its
complexity, making it easier for human programmers
to construct a comprehensive unit test suite that
includes a diverse set of edge cases. Furthermore, it
allows human programmers to more easily pinpoint
the location and cause of errors and discrepancies
between the expected and actual output of the code,
thus facilitating the debugging process.
However,
writing unit tests is often a time-consuming process
that
therefore
demands
a
large
portion
of
a
developer’s time and energy.
In recent years, with the rise of Large Language
Models (LLMs) such as ChatGPT [7], there has
been an increasing focus on the application of LLMs
to the task of writing unit tests, as they have the
potential to drastically reduce the time necessary
to properly and suﬃciently test written code before
deployment.
Therefore, the study of the unit test
1
"
"2310.00492","Xuansheng Wu","Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang,
  Ninghao Liu, Dong Yu","From Language Modeling to Instruction Following: Understanding the
  Behavior Shift in LLMs after Instruction Tuning","28 pages, 13 figures, 12 tables","","","","cs.CL cs.AI cs.LG","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Large Language Models (LLMs) have achieved remarkable success, demonstrating
powerful instruction-following capabilities across diverse tasks. Instruction
fine-tuning is critical in enabling LLMs to align with user intentions and
effectively follow instructions. In this work, we investigate how instruction
fine-tuning modifies pre-trained models, focusing on two perspectives:
instruction recognition and knowledge evolution. To study the behavior shift of
LLMs, we employ a suite of local and global explanation methods, including a
gradient-based approach for input-output attribution and techniques for
interpreting patterns and concepts in self-attention and feed-forward layers.
Our findings reveal three significant impacts of instruction fine-tuning: 1) It
empowers LLMs to better recognize the instruction parts from user prompts,
thereby facilitating high-quality response generation and addressing the
``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the
knowledge stored in feed-forward layers with user-oriented tasks, exhibiting
minimal shifts across linguistic levels. 3) It facilitates the learning of
word-word relations with instruction verbs through the self-attention
mechanism, particularly in the lower and middle layers, indicating enhanced
recognition of instruction words. These insights contribute to a deeper
understanding of the behavior shifts in LLMs after instruction fine-tuning and
lay the groundwork for future research aimed at interpreting and optimizing
LLMs for various applications. We will release our code and data soon.
","2023-10-03","2310.00492v1.pdf","Preprint
FROM LANGUAGE MODELING TO INSTRUCTION FOL-
LOWING: UNDERSTANDING THE BEHAVIOR SHIFT IN
LLMS AFTER INSTRUCTION TUNING
Xuansheng Wu♣∗, Wenlin Yao♡, Jianshu Chen♡,
Xiaoman Pan♡, Xiaoyang Wang♡, Ninghao Liu♣, & Dong Yu♡
♣University of Georgia
♡Tencent AI Lab
ABSTRACT
Large Language Models (LLMs) have achieved remarkable success, demonstrat-
ing powerful instruction-following capabilities across diverse tasks. Instruction
fine-tuning is critical in enabling LLMs to align with user intentions and effec-
tively follow instructions. In this work, we investigate how instruction fine-tuning
modifies pre-trained models, focusing on two perspectives: instruction recognition
and knowledge evolution. To study the behavior shift of LLMs, we employ a suite
of local and global explanation methods, including a gradient-based approach for
input-output attribution and techniques for interpreting patterns and concepts in
self-attention and feed-forward layers. Our findings reveal three significant impacts
of instruction fine-tuning: 1) It empowers LLMs to better recognize the instruc-
tion parts from user prompts, thereby facilitating high-quality response generation
and addressing the “lost-in-the-middle” issue observed in pre-trained models; 2)
It aligns the knowledge stored in feed-forward layers with user-oriented tasks,
exhibiting minimal shifts across linguistic levels. 3) It facilitates the learning of
word-word relations with instruction verbs through the self-attention mechanism,
particularly in the lower and middle layers, indicating enhanced recognition of
instruction words. These insights contribute to a deeper understanding of the behav-
ior shifts in LLMs after instruction fine-tuning and lay the groundwork for future
research aimed at interpreting and optimizing LLMs for various applications1.
1
INTRODUCTION
The remarkable capability of Large Language Models (LLMs) to align with user intentions is well-
recognized across various real-world applications, where they are expected to be helpful, honest, and
harmless AI assistants (Ouyang et al., 2022; OpenAI, 2023). Central to these roles, being “helpful”
is the most fundamental requisite, emphasizing that LLMs should help users to complete various
tasks, known as the “instruction following” capability. Many studies (Raffel et al., 2020; Wang et al.,
2022; Zhou et al., 2023) show that instruction fine-tuning, also called supervised fine-tuning (Ouyang
et al., 2022), is critical to acquire such capability, by fine-tuning pre-trained models on high-quality
prompt-response pairs. However, the impact of instruction tuning on the helpfulness of language
models remains inadequately understood, limiting the improvements toward better fine-tuned models.
arXiv:2310.00492v1  [cs.CL]  30 Sep 2023
While many studies on interpreting LLMs have delved into pre-training (Dai et al., 2021; Elhage et al.,
2021; Olsson et al., 2022; Meng et al., 2022), in-context learning (Xie et al., 2021; Olsson et al., 2022;
Liu et al., 2023b), and single-task fine-tuning (Kokalj et al., 2021; Wu & Ong, 2021; Enguehard,
2023), their findings cannot be extrapolated to instruction fine-tuning. This is because instruction
tuning updates model weights (differing from pre-training and in-context learning) to achieve robust
generalization across a range of downstream tasks (unlike single-task fine-tuning). Therefore, we
seek to explore how large language models undergo a shift in behavior—from primarily modeling
language to effectively following instructions—after being subjected to the instruction tuning process.
In this work, we focus on answering the research question of how instruction fine-tuning changes the
pre-trained model’s behavior from two perspectives, namely instruction recognition and knowledge
evolution. In particular, what role do user instructions play when an instruction fine-tuned model
generates responses? Also, what the encoded knowledge evolves after instruction fine-tuning? We
consider these two perspectives under a well-accepted belief that an LLM becomes a helpful assistant
by first recognizing human instructions and then retrieving corresponding knowledge to respond.
∗The work done during the internship at Tencent AI Lab.
1We will release our code and data soon.
1
"
"2310.00500","Ivona Najdenkoska","Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek,
  Marcel Worring, Yuki M. Asano","Small Visual Language Models can also be Open-Ended Few-Shot Learners","","","","","cs.CV","http://creativecommons.org/licenses/by-nc-nd/4.0/","  We present Self-Context Adaptation (SeCAt), a self-supervised approach that
unlocks open-ended few-shot abilities of small visual language models. Our
proposed adaptation algorithm explicitly learns from symbolic, yet
self-supervised training tasks. Specifically, our approach imitates image
captions in a self-supervised way based on clustering a large pool of images
followed by assigning semantically-unrelated names to clusters. By doing so, we
construct the `self-context', a training signal consisting of interleaved
sequences of image and pseudo-caption pairs and a query image for which the
model is trained to produce the right pseudo-caption. We demonstrate the
performance and flexibility of SeCAt on several multimodal few-shot datasets,
spanning various granularities. By using models with approximately 1B
parameters we outperform the few-shot abilities of much larger models, such as
Frozen and FROMAGe. SeCAt opens new possibilities for research in open-ended
few-shot learning that otherwise requires access to large or proprietary
models.
","2023-10-03","2310.00500v1.pdf","Preprint
SMALL VISUAL LANGUAGE MODELS CAN ALSO BE
OPEN-ENDED FEW-SHOT LEARNERS
Mohammad Mahdi Derakhshani*, Ivona Najdenkoska*
Cees G. M. Snoek†, Marcel Worring†,Yuki M. Asano†
University of Amsterdam
Amsterdam, the Netherlands
ABSTRACT
We present Self-Context Adaptation (SeCAt), a self-supervised approach that un-
locks open-ended few-shot abilities of small visual language models. Our proposed
adaptation algorithm explicitly learns from symbolic, yet self-supervised training
tasks. Specifically, our approach imitates image captions in a self-supervised way
based on clustering a large pool of images followed by assigning semantically-
unrelated names to clusters. By doing so, we construct the ‘self-context’, a training
signal consisting of interleaved sequences of image and pseudo-caption pairs and a
query image for which the model is trained to produce the right pseudo-caption.
We demonstrate the performance and flexibility of SeCAt on several multimodal
few-shot datasets, spanning various granularities. By using models with approxi-
mately 1B parameters we outperform the few-shot abilities of much larger models,
such as Frozen and FROMAGe. SeCAt opens new possibilities for research in
open-ended few-shot learning that otherwise requires access to large or proprietary
models.
1
INTRODUCTION
Empowered by large-scale pre-training on massive web-scraped datasets, large language models have
witnessed major advancements in the past years. These large models show fascinating emergent
abilities, particularly in-context learning for few-shot tasks (Brown et al., 2020; Wei et al., 2022a),
which are solved without gradient-based updates, based on context samples provided via a prompt.
Recently, such models have evolved from the natural language processing domain to visual language
models such as Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022). Such models
rely heavily on incorporating very large, proprietary language models, ranging from 7 up to 70
billion parameters, making them impractical for many individuals and organizations without access to
large-scale computational resources. This paper seeks to answer whether the model scale is a crucial
factor for solving open-ended few-shot learning problems.
arXiv:2310.00500v1  [cs.CV]  30 Sep 2023
As of yet, in-context learning has not been observed in small-scale visual language models as a
mechanism for solving few-shot tasks. One reason is that these small-scale models rely heavily on
semantic priors created during the pre-training and they cannot properly digest interleaved sequences
of images and captions. As shown by Wei et al. (2023), if one prompts a small model with a few pairs
of input-label mappings as context followed by a query sample, using new semantically-unrelated
labels, the small model will stick to its semantic priors and will not adjust its predictions. Larger
models, by contrast, override these priors, allowing them to learn directly from input-label mappings
presented in the context, with no further gradient-based updates. This behavior is attributed to their
enhanced capacity and pre-training on interleaved input-label data, which enables them to easily
capture patterns and dependencies within the presented context. We hypothesize that the mechanisms
for in-context learning should emerge in small models as well if we use such interleaved data and
mimic the final in-context learning objective. To do so, we propose a technique to in-context learn
few-shot tasks with small visual language models and without supervision.
*Shared first authorship. The authors can change the order for their own purposes. †Shared last-authorship;
order random. Corresponding authors: {m.m.derakhshani, i.najdenkoska}@uva.nl
1
"
"2310.00533","Jianqiao Lu","Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun
  Wang, Weichao Wang, Lifeng Shang, Qun Liu","SELF: Language-Driven Self-Evolution for Large Language Model","14 pages, 4 figures, 6 tables. Due to the limitation ""The abstract
  field cannot be longer than 1,920 characters"", the abstract appearing here is
  slightly shorter than that in the PDF file","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have showcased remarkable versatility across
diverse domains. However, the pathway toward autonomous model development, a
cornerstone for achieving human-level learning and advancing autonomous AI,
remains largely uncharted. We introduce an innovative approach, termed ""SELF""
(Self-Evolution with Language Feedback). This methodology empowers LLMs to
undergo continual self-evolution. Furthermore, SELF employs language-based
feedback as a versatile and comprehensive evaluative tool, pinpointing areas
for response refinement and bolstering the stability of self-evolutionary
training. Initiating with meta-skill learning, SELF acquires foundational
meta-skills with a focus on self-feedback and self-refinement. These
meta-skills are critical, guiding the model's subsequent self-evolution through
a cycle of perpetual training with self-curated data, thereby enhancing its
intrinsic abilities. Given unlabeled instructions, SELF equips the model with
the capability to autonomously generate and interactively refine responses.
This synthesized training data is subsequently filtered and utilized for
iterative fine-tuning, enhancing the model's capabilities. Experimental results
on representative benchmarks substantiate that SELF can progressively advance
its inherent abilities without the requirement of human intervention, thereby
indicating a viable pathway for autonomous model evolution. Additionally, SELF
can employ online self-refinement strategy to produce responses of superior
quality. In essence, the SELF framework signifies a progressive step towards
autonomous LLM development, transforming the LLM from a mere passive recipient
of information into an active participant in its own evolution.
","2023-10-11","2310.00533v1.pdf","Preprint. Work in progress.
SELF: LANGUAGE-DRIVEN SELF-EVOLUTION
FOR
LARGE LANGUAGE MODEL
Jianqiao Lu1∗†, Wanjun Zhong2∗, Wenyong Huang2∗,
Yufei Wang2, Fei Mi2, Baojun Wang2, Weichao Wang2, Lifeng Shang2 & Qun Liu2
1The University of Hong Kong
2Huawei Noah’s Ark Lab
jqlu@cs.hku.hk, {zhongwanjun1,wenyong.huang}@huawei.com
ABSTRACT
Large Language Models (LLMs) have showcased remarkable versatility across
diverse domains. However, the pathway toward autonomous model development,
a cornerstone for achieving human-level learning and advancing autonomous AI,
remains largely uncharted. Drawing inspiration from the human capability for
self-driven learning, characterized by introspection and continuous refinement,
we introduce an innovative approach, termed “SELF” (Self-Evolution with Lan-
guage Feedback). This methodology empowers LLMs to undergo continual self-
evolution, thereby augmenting their inherent capabilities. Furthermore, SELF
employs language-based feedback as a versatile and comprehensive evaluative
tool, pinpointing areas for response refinement and bolstering the stability of self-
evolutionary training. Through this approach, we aim to illuminate the prospects
of autonomous AI advancement, drawing parallels with the human aptitude for
learning and adaptation. Initiating with meta-skill learning, SELF acquires foun-
dational meta-skills with a focus on self-feedback and self-refinement. These
meta-skills are critical, guiding the model’s subsequent self-evolution through a
cycle of perpetual training with self-curated data, thereby enhancing its intrinsic
abilities. Given unlabeled instructions, SELF equips the model with the capa-
bility to autonomously generate and interactively refine responses. This synthe-
sized training data is subsequently filtered and utilized for iterative fine-tuning,
enhancing the model’s capabilities. Experimental results on representative bench-
marks substantiate that SELF can progressively advance its inherent abilities with-
out the requirement of human intervention, thereby indicating a viable pathway
for autonomous model evolution. Additionally, SELF can employ online self-
refinement strategy to produce responses of superior quality. In essence, the SELF
framework signifies a progressive step towards autonomous LLM development,
transforming the LLM from a mere passive recipient of information into an active
participant in its own evolution.
arXiv:2310.00533v1  [cs.CL]  1 Oct 2023
1
INTRODUCTION
Large Language Models (LLMs), like ChatGPT OpenAI (2022) and GPT-4 OpenAI (2023), stand at
the forefront of the AI revolution, transforming our understanding of machine-human textual inter-
actions and redefining numerous applications across diverse tasks. Despite their evident capabilities,
achieving optimum performance remains a complex journey.
In the quest for optimal LLM development, we draw inspiration from the intrinsic learning mecha-
nisms utilized by humans. Humans inherently exhibit a self-driven learning loop when confronted
with new challenges, involving initial attempts, introspection and deriving feedback, refining behav-
ior accordingly, and accumulating experiences for self-improvement. This intricate human learning
cycle sparks a pivotal inquiry: “Can LLMs emulate the human learning process, harnessing the
power of self-refinement to evolve their innate abilities?” Fascinatingly, recent study (Ye et al.,
∗Leading co-authors with equal contribution.
†Work done during an internship at Huawei.
1
"
"2310.00535","Yuandong Tian","Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du","JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and
  Attention","","","","","cs.LG cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical
framework to understand the training procedure of multilayer Transformer
architectures. This is achieved by integrating out the self-attention layer in
Transformers, producing a modified dynamics of MLP layers only. JoMA removes
unrealistic assumptions in previous analysis (e.g., lack of residual
connection) and predicts that the attention first becomes sparse (to learn
salient tokens), then dense (to learn less salient tokens) in the presence of
nonlinear activations, while in the linear case, it is consistent with existing
works that show attention becomes sparse over time. We leverage JoMA to
qualitatively explains how tokens are combined to form hierarchies in
multilayer Transformers, when the input tokens are generated by a latent
hierarchical generative model. Experiments on models trained from real-world
dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)
verify our theoretical findings.
","2023-10-04","2310.00535v1.pdf","Preprint. Work in Progress.
JoMA: Demystifying Multilayer Transformers
via JOint Dynamics of MLP and Attention
Yuandong Tian
AI@Meta (FAIR)
yuandong@meta.com
Yiping Wang
University of Washington
ypwang61@cs.washington.edu
Zhenyu Zhang
University of Texas at Austin
zhenyu.zhang@utexas.edu
Beidi Chen
Carnegie Mellon University, AI@Meta (FAIR)
beidic@meta.com, beidic@andrew.cmu.edu
Simon Du
University of Washington
ssdu@cs.washington.edu
Abstract
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical
framework to understand the training procedure of multilayer Transformer
architectures. This is achieved by integrating out the self-attention layer
in Transformers, producing a modified dynamics of MLP layers only. JoMA
removes unrealistic assumptions in previous analysis (e.g., lack of residual
connection), and predicts that the attention first becomes sparse (to learn
salient tokens), then dense (to learn less salient tokens) in the presence of
nonlinear activations, while in the linear case, it is consistent with existing
works. We leverage JoMA to qualitatively explains how tokens are combined
to form hierarchies in multilayer Transformers, when the input tokens are
generated by a latent hierarchical generative model. Experiments on models
trained from real-world dataset (Wikitext2/Wikitext103) and various pre-
trained models (OPT, Pythia) verify our theoretical findings.
1
Introduction
Since its debut, Transformers (Vaswani et al., 2017) have been extensively used in many
applications and demonstrates impressive performance (Dosovitskiy et al., 2020; OpenAI,
2023) compared to domain-specific models (e.g., CNN in computer vision, GNN in graph
modeling, RNN/LSTM in language modeling, etc). In all these scenarios, the basic Trans-
former block, which consists of one self-attention plus two-layer nonlinear MLP, plays
a critical role. A natural question is:
arXiv:2310.00535v1  [cs.LG]  1 Oct 2023
How the basic Transformer block leads to effective learning?
Due to the complexity and nonlinearity of Transformer architectures, it remains a highly
nontrivial open problem to find a unified mathematical framework that characterizes the
learning mechanism of multi-layer transformers. Existing works mostly focus on 1-layer
Transformer (Li et al., 2023a; Tarzanagh et al., 2023b) with fixed MLP (Tarzanagh et al.,
2023a) layer, linear activation functions (Tian et al., 2023), and local gradient steps at
initialization (Bietti et al., 2023; Oymak et al., 2023), etc.
In this paper, we propose a novel joint dynamics of self-attention plus MLP, based on Joint
MLP/Attention Integral (JoMA), a first integral that combines the lower layer of the MLP
and self-attention layers. Leveraging this joint dynamics, the self-attention is shown to have
more fine-grained and delicate behavior: it first becomes sparse as in the linear case (Tian
et al., 2023), only attends to tokens that frequently co-occur with the query, and then
becomes denser and gradually includes tokens with less frequent co-occurrence, in the case
of nonlinear activation. This shows inductive bias in the Transformer training: first the
model focuses on most salient features, then extends to less salient ones.
1
"
"2310.00566","Duanyu Feng","Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie,
  Weiguang Han, Alejandro Lopez-Lira, Hao Wang","Empowering Many, Biasing a Few: Generalist Credit Scoring through Large
  Language Models","","","","","cs.LG cs.AI cs.CL cs.CY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Credit and risk assessments are cornerstones of the financial landscape,
impacting both individual futures and broader societal constructs. Existing
credit scoring models often exhibit limitations stemming from knowledge myopia
and task isolation. In response, we formulate three hypotheses and undertake an
extensive case study to investigate LLMs' viability in credit assessment. Our
empirical investigations unveil LLMs' ability to overcome the limitations
inherent in conventional models. We introduce a novel benchmark curated for
credit assessment purposes, fine-tune a specialized Credit and Risk Assessment
Large Language Model (CALM), and rigorously examine the biases that LLMs may
harbor. Our findings underscore LLMs' potential in revolutionizing credit
assessment, showcasing their adaptability across diverse financial evaluations,
and emphasizing the critical importance of impartial decision-making in the
financial sector. Our datasets, models, and benchmarks are open-sourced for
other researchers.
","2023-10-03","2310.00566v1.pdf","Empowering Many, Biasing a Few: Generalist Credit Scoring
through Large Language Models
Duanyu Feng∗
Yongfu Dai∗
Jimin Huang
ChanceFocus (Shanghai) AMC.
Shanghai, China
jimin@chancefocus.com
Yifang Zhang
Sichuan University
Chengdu, Sichuan, China
zhangyf_ivy@foxmail.com
fengduanyu@stu.scu.edu.cn
wal.daishen@gmail.com
Sichuan University
Chengdu, Sichuan, China
Qianqian Xie
Wuhan University
Wuhan, Hubei, China
xqq.sincere@gmail.com
Weiguang Han
Wuhan University
Wuhan, Hubei, China
han.wei.guang@whu.edu.cn
Alejandro Lopez-Lira
University of Florida
USA
alejandro.lopez-
lira@warrington.ufl.edu
Hao Wang†
Sichuan University
Chengdu, Sichuan, China
wangh@scu.edu.cn
ABSTRACT
Proceedings of Make sure to enter the correct conference title from your rights
confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA,
14 pages. https://doi.org/XXXXXXX.XXXXXXX
1
INTRODUCTION
Credit and risk assessments are cornerstones of the financial land-
scape, impacting both individual futures and broader societal con-
structs. Existing credit scoring models often exhibit limitations
stemming from knowledge myopia and task isolation. In response,
we formulate three hypotheses and undertake an extensive case
study to investigate LLMs’ viability in credit assessment. Our em-
pirical investigations unveil LLMs’ ability to overcome the lim-
itations inherent in conventional models. We introduce a novel
benchmark curated for credit assessment purposes, fine-tune a
specialized Credit and Risk Assessment Large Language Model
(CALM), and rigorously examine the biases that LLMs may har-
bor. Our findings underscore LLMs’ potential in revolutionizing
credit assessment, showcasing their adaptability across diverse fi-
nancial evaluations, and emphasizing the critical importance of
impartial decision-making in the financial sector. Our datasets,
models, and benchmarks are open-sourced for other researchers
https://github.com/colfeng/CALM.
arXiv:2310.00566v1  [cs.LG]  1 Oct 2023
ACM Reference Format:
Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang
Han, Alejandro Lopez-Lira, and Hao Wang. 2018. Empowering Many, Bias-
ing a Few: Generalist Credit Scoring through Large Language Models. In
∗Both authors contributed equally to this research.
†Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/XXXXXXX.XXXXXXX
Credit and risk assessment underpins the financial industry, tasked
with gauging the probability of borrowers—from individuals to
nations—honoring financial commitments, drawing upon historical
data and additional metrics [13, 30]. While individuals experience
these assessments in contexts like Credit Scoring—estimations of fu-
ture default risks based on historical behavior, the wider community
encounters systems like Fraud Detection safeguarding against unau-
thorized financial activities. Broader societal mechanisms, shaping
investment and economic directives, deploy tools such as Financial
Distress Identification to preemptively discern potential financial
pitfalls, while the insurance sphere leverages Claim Analysis to
authenticate and adjudicate claims [12]. These multifarious evalu-
ations bear significant implications for the financial industry and
the whole society, affecting both individual financial trajectories
and overarching economic strategies.
Existing methods for credit and risk assessment, rooted in tradi-
tional rule-dependent models or guided by contemporary machine
learning techniques [33, 37, 53, 59, 65], are predominantly tailored
for a singular task, thereby incurring the pitfalls of knowledge
myopia and task isolation. Knowledge myopia becomes evi-
dent when models, trained solely on specific data points, fail to
incorporate the broader general and financial insights and skills
that humans naturally use for judgment [6]. For example, while a
Credit Scoring model might hone in on an individual’s past trans-
actions, it may neglect wider economic conditions, fundamental
financial principles, or the analytical skills that a human assessor
would employ. Conversely, task isolation means overlooking the
synergy between intertwined tasks [63], which also exists in the
"
"2310.00576","Xiaotian Han","Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan
  Chang, Xia Hu","GrowLength: Accelerating LLMs Pretraining by Progressively Growing
  Training Length","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The evolving sophistication and intricacies of Large Language Models (LLMs)
yield unprecedented advancements, yet they simultaneously demand considerable
computational resources and incur significant costs. To alleviate these
challenges, this paper introduces a novel, simple, and effective method named
``\growlength'' to accelerate the pretraining process of LLMs. Our method
progressively increases the training length throughout the pretraining phase,
thereby mitigating computational costs and enhancing efficiency. For instance,
it begins with a sequence length of 128 and progressively extends to 4096. This
approach enables models to process a larger number of tokens within limited
time frames, potentially boosting their performance. In other words, the
efficiency gain is derived from training with shorter sequences optimizing the
utilization of resources. Our extensive experiments with various
state-of-the-art LLMs have revealed that models trained using our method not
only converge more swiftly but also exhibit superior performance metrics
compared to those trained with existing methods. Furthermore, our method for
LLMs pretraining acceleration does not require any additional engineering
efforts, making it a practical solution in the realm of LLMs.
","2023-10-03","2310.00576v1.pdf","GrowLength: ACCELERATING LLMS PRETRAINING BY
PROGRESSIVELY GROWING TRAINING LENGTH
Hongye Jin1* Xiaotian Han1∗ Jingfeng Yang2 Zhimeng Jiang1 Chia-Yuan Chang1 Xia Hu3
1Texas A&M University
2Amazon
3Rice University
{jhy0410,han,zhimengj,cychang}@tamu.edu jingfengyangpku@gmail.com xia.hu@rice.edu
ABSTRACT
The evolving sophistication and intricacies of Large Language Models (LLMs)
yield unprecedented advancements, yet they simultaneously demand considerable
computational resources and incur significant costs. To alleviate these challenges,
this paper introduces a novel, simple, and effective method named “GrowLength”
to accelerate the pretraining process of LLMs. Our method progressively increases
the training length throughout the pretraining phase, thereby mitigating computa-
tional costs and enhancing efficiency. For instance, it begins with a sequence
length of 128 and progressively extends to 4096. This approach enables models to
process a larger number of tokens within limited time frames, potentially boost-
ing their performance. In other words, the efficiency gain is derived from training
with shorter sequences optimizing the utilization of resources. Our extensive ex-
periments with various state-of-the-art LLMs have revealed that models trained
using our method not only converge more swiftly but also exhibit superior per-
formance metrics compared to those trained with existing methods. Furthermore,
our method for LLMs pretraining acceleration does not require any additional en-
gineering efforts, making it a practical solution in the realm of LLMs.
LLM1024 Trained
with 1024 only
LLM128 Trained
with 128 only
GrowLength
arXiv:2310.00576v1  [cs.CL]  1 Oct 2023
Figure 1: Training curves comparison of our proposed method and the baselines are given the same
training time. It shows the training loss curves for Large Language Models (LLMs) trained with
fixed sequence lengths of 128 (LLM128), 1024 (LLM1024), and our method. Compared with
LLM1024, GrowLength attains a lower loss. This can be attributed to that our method processes
more tokens within the same training time, allowing the model to have a broader context. Similarly,
the comparison between LLM128 and GrowLength reveals that our method also secures a lower loss
in this scenario. This is because, the model trained by our method has experienced longer sequences,
enabling better learning ability. Compared with both short or long sequence length instances, our
proposed method demonstrates enhanced performance within the same pertaining time, establish-
ing its efficacy over the baseline models.
∗Equal Contribution.
1
"
"2310.00578","Siyi Cao","Siyi Cao, Tongquan Zhou, Siruo Zhou","Nine-year-old children outperformed ChatGPT in emotion: Evidence from
  Chinese writing","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  ChatGPT has been demonstrated to possess significant capabilities in
generating intricate, human-like text, and recent studies have established that
its performance in theory of mind tasks is comparable to that of a
nine-year-old child. However, it remains uncertain whether ChatGPT surpasses
nine-year-old children in Chinese writing proficiency. To explore this, our
study juxtaposed the Chinese writing performance of ChatGPT and nine-year-old
children on both narrative and scientific topics, aiming to uncover the
relative strengths and weaknesses of ChatGPT in writing.
  The collected data were analyzed across five linguistic dimensions: fluency,
accuracy, complexity, cohesion, and emotion. Each dimension underwent
assessment through precise indices. The findings revealed that nine-year-old
children excelled beyond ChatGPT in terms of fluency and cohesion within their
writing. In contrast, ChatGPT manifested a superior performance in accuracy
compared to the children. Concerning complexity, children exhibited superior
skills in science-themed writing, while ChatGPT prevailed in nature-themed
writing. Significantly, this research is pioneering in revealing that
nine-year-old children convey stronger emotions than ChatGPT in their Chinese
compositions.
","2023-10-03","2310.00578v1.pdf","Nine-year-old children outperformed ChatGPT in emotion: Evidence from
Chinese writing
Siyi Cao12, Tongquan Zhou1*, Siruo Zhou3*
1. School of Foreign Languages, Southeast University, Nanjing, China, 211189
2. Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic
University, Hong Kong, China
3. School of Foreign Studies, Nanjing University of Posts and Telecommunications,
Nanjing, China 210023
* Corresponding authors: zhoutongquan@126.com, susanzhou@naver.com
Abstract
ChatGPT has been demonstrated to possess significant capabilities in generating
intricate, human-like text, and recent studies have established that its performance in
theory of mind tasks is comparable to that of a nine-year-old child. However, it
remains uncertain whether ChatGPT surpasses nine-year-old children in Chinese
writing proficiency. To explore this, our study juxtaposed the Chinese writing
performance of ChatGPT and nine-year-old children on both narrative and scientific
topics, aiming to uncover the relative strengths and weaknesses of ChatGPT in
writing.
The collected data were analyzed across five linguistic dimensions: fluency,
accuracy, complexity, cohesion, and emotion. Each dimension underwent assessment
through precise indices. The findings revealed that nine-year-old children excelled
beyond ChatGPT in terms of fluency and cohesion within their writing. In contrast,
ChatGPT manifested a superior performance in accuracy compared to the children.
Concerning complexity, children exhibited superior skills in science-themed writing,
while ChatGPT prevailed in nature-themed writing. Significantly, this research is
pioneering in revealing that nine-year-old children convey stronger emotions than
ChatGPT in their Chinese compositions.
1. Introduction
Artificial intelligence (AI) has shown impressive growth and diversity in recent
years, with particular strides being made in the field of natural language learning
(Dergaa et al., 2023). Among the vanguards in this arena, OpenAI’s ChatGPT stands
out with its compelling capabilities to generate intricate, human-like text (De Angelis
et al., 2023). This has ignited a fascinating debate about the scope and boundaries of
AI’s language proficiency (Zhou et al., 2023). Notably, research has revealed that
Chinese writing is not ChatGPT’s strong suit, but its performance on theory of mind
(ToM) tasks is comparable to that of a nine-year-old child (Kosinski, 2023). Within
this fascinating context, the current research seeks to probe into an intriguing yet
"
"2310.00582","Shiyu Xuan","Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang","Pink: Unveiling the Power of Referential Comprehension for Multi-modal
  LLMs","","","","","cs.CV cs.AI","http://creativecommons.org/licenses/by/4.0/","  Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in many vision-language tasks. Nevertheless, most MLLMs still lack the
Referential Comprehension (RC) ability to identify a specific object or area in
images, limiting their application in fine-grained perception tasks. This paper
proposes a novel method to enhance the RC capability for MLLMs. Our model
represents the referring object in the image using the coordinates of its
bounding box and converts the coordinates into texts in a specific format. This
allows the model to treat the coordinates as natural language. Moreover, we
construct the instruction tuning dataset with various designed RC tasks at a
low cost by unleashing the potential of annotations in existing datasets. To
further boost the RC ability of the model, we propose a self-consistent
bootstrapping method that extends dense object annotations of a dataset into
high-quality referring-expression-bounding-box pairs. The model is trained
end-to-end with a parameter-efficient tuning framework that allows both
modalities to benefit from multi-modal instruction tuning. This framework
requires fewer trainable parameters and less training data. Experimental
results on conventional vision-language and RC tasks demonstrate the superior
performance of our method. For instance, our model exhibits a 12.0% absolute
accuracy improvement over Instruct-BLIP on VSR and surpasses Kosmos-2 by 24.7%
on RefCOCO_val under zero-shot settings. We also attain the top position on the
leaderboard of MMBench. The models, datasets, and codes are publicly available
at https://github.com/SY-Xuan/Pink
","2023-10-03","2310.00582v1.pdf","PINK:
UNVEILING THE POWER OF REFERENTIAL
COMPREHENSION FOR MULTI-MODAL LLMS
Shiyu Xuan∗
National Key Laboratory for Multimedia Information Processing
School of Computer Science
Peking University
Beijing, 100871, China
shiyu xuan@stu.pku.edu.cn
Qingpei Guo & Ming Yang
Ant Group
{qingpei.gqp,m.yang}@antgroup.com
Shiliang Zhang†
National Key Laboratory for Multimedia Information Processing
School of Computer Science
Peking University
Beijing, 100871, China
slzhang.jdl@pku.edu.cn
ABSTRACT
arXiv:2310.00582v1  [cs.CV]  1 Oct 2023
Multi-modal Large Language Models (MLLMs) have shown remarkable capabili-
ties in many vision-language tasks. Nevertheless, most MLLMs still lack the Ref-
erential Comprehension (RC) ability to identify a specific object or area in images,
limiting their application in fine-grained perception tasks. This paper proposes a
novel method to enhance the RC capability for MLLMs. Our model represents the
referring object in the image using the coordinates of its bounding box and con-
verts the coordinates into texts in a specific format. This allows the model to treat
the coordinates as natural language. Moreover, we construct the instruction tuning
dataset with various designed RC tasks at a low cost by unleashing the potential of
annotations in existing datasets. To further boost the RC ability of the model, we
propose a self-consistent bootstrapping method that extends dense object annota-
tions of a dataset into high-quality referring-expression-bounding-box pairs. The
model is trained end-to-end with a parameter-efficient tuning framework that al-
lows both modalities to benefit from multi-modal instruction tuning. This frame-
work requires fewer trainable parameters and less training data. Experimental
results on conventional vision-language and RC tasks demonstrate the superior
performance of our method. For instance, our model exhibits a 12.0% absolute
accuracy improvement over Instruct-BLIP on VSR and surpasses Kosmos-2 by
24.7% on RefCOCO val under zero-shot settings. We also attain the top position
on the leaderboard of MMBench. The models, datasets, and codes are publicly
available at https://github.com/SY-Xuan/Pink
1
INTRODUCTION
Large Language Models (LLMs) (Brown et al., 2020; Raffel et al., 2020; Touvron et al., 2023a;
Scao et al., 2022) show impressive capabilities across a wide range of natural language tasks. These
inspiring results by LLMs have motivated researchers to extend LLMs to Multi-modal Large Lan-
guage Models (MLLMs) by integrating LLMs with additional modalities, e.g., image, audio, or
∗This work is done during internship at Ant Group.
†Corresponding Author
1
"
"2310.00597","Lucen Zhong","Lucen Zhong, Hengtong Lu, Caixia Yuan, Xiaojie Wang, Jiashen Sun, Ke
  Zeng and Guanglu Wan","A Task-oriented Dialog Model with Task-progressive and Policy-aware
  Pre-training","Accepted at NLPCC 2023","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Pre-trained conversation models (PCMs) have achieved promising progress in
recent years. However, existing PCMs for Task-oriented dialog (TOD) are
insufficient for capturing the sequential nature of the TOD-related tasks, as
well as for learning dialog policy information. To alleviate these problems,
this paper proposes a task-progressive PCM with two policy-aware pre-training
tasks. The model is pre-trained through three stages where TOD-related tasks
are progressively employed according to the task logic of the TOD system. A
global policy consistency task is designed to capture the multi-turn dialog
policy sequential relation, and an act-based contrastive learning task is
designed to capture similarities among samples with the same dialog policy. Our
model achieves better results on both MultiWOZ and In-Car end-to-end dialog
modeling benchmarks with only 18\% parameters and 25\% pre-training data
compared to the previous state-of-the-art PCM, GALAXY.
","2023-10-03","2310.00597v1.pdf","A Task-oriented Dialog Model with
Task-progressive and Policy-aware Pre-training
Lucen Zhong1, Hengtong Lu1, Caixia Yuan1, Xiaojie Wang1, Jiashen Sun2, Ke
Zeng2, and Guanglu Wan2
1 Center of Intelligence Science and Technology, Beijing University of Posts and
Telecommunications, China
{zhonglucen, luhengtong, yuancx, xjwang}@bupt.edu.cn
2 Meituan, China
{sunjiashen, zengke02, wanguanglu}@meituan.com
Abstract. Pre-trained conversation models (PCMs) have achieved promis-
ing progress in recent years. However, existing PCMs for Task-oriented
dialog (TOD) are insufficient for capturing the sequential nature of the
TOD-related tasks, as well as for learning dialog policy information. To
alleviate these problems, this paper proposes a task-progressive PCM
with two policy-aware pre-training tasks. The model is pre-trained through
three stages where TOD-related tasks are progressively employed accord-
ing to the task logic of the TOD system. A global policy consistency task
is designed to capture the multi-turn dialog policy sequential relation,
and an act-based contrastive learning task is designed to capture simi-
larities among samples with the same dialog policy. Our model achieves
better results on both MultiWOZ and In-Car end-to-end dialog model-
ing benchmarks with only 18% parameters and 25% pre-training data
compared to the previous state-of-the-art PCM, GALAXY. We make
our code and data publicly available. 3
Keywords: Task-oriented Dialog · Pre-training · Response generation.
1
Introduction
arXiv:2310.00597v1  [cs.CL]  1 Oct 2023
Task-oriented dialog (TOD) system aims at helping users complete specific tasks
through multi-turn interactions. Compared with open domain dialog agents, a
TOD system generates more controllable replies by implementing three sub-
tasks: 1) Dialog State Tracking (DST) extracts the belief state; 2) Dialog Policy
Learning (POL) decides which acts should be taken based on the belief state;
3) Natural Language Generation (NLG) converts acts into natural language
utterances. A large amount of work has been done for each sub-task [1,2,3]
separately, as well as joint models for them [4,5].
Pre-trained Conversation Models (PCMs) [9,11,12,13] are Pre-trained Lan-
guage Models (PLMs) further pre-trained on dialog data. Although previous
work on PCMs for TOD has made big progress, the following issues are still not
3 https://github.com/lucenzhong/TPLD
"
"2310.00598","Aviya Maimon","Aviya Maimon and Reut Tsarfaty","A Novel Computational and Modeling Foundation for Automatic Coherence
  Assessment","","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Coherence is an essential property of well-written texts, that refers to the
way textual units relate to one another. In the era of generative AI, coherence
assessment is essential for many NLP tasks; summarization, generation,
long-form question-answering, and more. However, in NLP {coherence} is an
ill-defined notion, not having a formal definition or evaluation metrics, that
would allow for large-scale automatic and systematic coherence assessment. To
bridge this gap, in this work we employ the formal linguistic definition of
\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three
conditions -- {\em cohesion, consistency} and {\em relevance} -- and formalize
these conditions as respective computational tasks. We hypothesize that (i) a
model trained on all of these tasks will learn the features required for
coherence detection, and that (ii) a joint model for all tasks will exceed the
performance of models trained on each task individually. On two benchmarks for
coherence scoring rated by humans, one containing 500 automatically-generated
short stories and another containing 4k real-world texts, our experiments
confirm that jointly training on the proposed tasks leads to better performance
on each task compared with task-specific models, and to better performance on
assessing coherence overall, compared with strong baselines. We conclude that
the formal and computational setup of coherence as proposed here provides a
solid foundation for advanced methods of large-scale automatic assessment of
coherence.
","2023-10-03","2310.00598v1.pdf","A Novel Computational and Modeling Foundation
for Automatic Coherence Assessment
Aviya Maimon and Reut Tsarfaty
aviyamn@gmail.com
reut.tsarfaty@biu.ac.il
Department of Computer Science, Bar Ilan University
Abstract
arXiv:2310.00598v1  [cs.CL]  1 Oct 2023
Coherence is an essential property of well-
written texts, that refers to the way textual
units relate to one another.
In the era of
generative AI, coherence assessment is es-
sential for many NLP tasks; summarization,
generation, long-form question-answering,
and more. However, in NLP coherence is
an ill-defined notion, not having a formal
definition or evaluation metrics, that would
allow for large-scale automatic and system-
atic coherence assessment. To bridge this
gap, in this work we employ the formal lin-
guistic definition of Reinhart (1980) of what
makes a discourse coherent, consisting of
three conditions — cohesion, consistency
and relevance — and formalize these condi-
tions as respective computational tasks. We
hypothesize that (i) a model trained on all of
these tasks will learn the features required
for coherence detection, and that (ii) a joint
model for all tasks will exceed the perfor-
mance of models trained on each task in-
dividually. On two benchmarks for coher-
ence scoring rated by humans, one contain-
ing 500 automatically-generated short sto-
ries and another containing 4k real-world
texts, our experiments confirm that jointly
training on the proposed tasks leads to bet-
ter performance on each task compared with
task-specific models, and to better perfor-
mance on assessing coherence overall, com-
pared with strong baselines. We conclude
that the formal and computational setup of
coherence as proposed here provides a solid
foundation for advanced methods of large-
scale automatic assessment of coherence.
1
Introduction
Coherence refers to the quality of a text, which
stems from the ways its various elements – such as
sentences, ideas, and paragraphs – flow smoothly
and are logically connected. In a coherent text,
each part follows logically from the preceding one,
creating a clear and understandable progression
of ideas. Coherence detection is crucial for NLP
tasks involving text quality measurements such
as essay scoring or quality measurements (Soma-
sundaran et al., 2014; Feng et al., 2014; Lai and
Tetreault, 2018). On top of that, coherence detec-
tion is gaining increased attention due to the grow-
ing awareness of the impact of artificially gener-
ated texts. As large language models (LLMs) be-
come prevalent in applications such as text gen-
eration, summarization, and question answering
(Guan et al., 2021; Xu et al., 2018; Yi et al., 2019),
ensuring coherent output has become a priority,
in order to make sure that the generated texts are
meaningful and understandable.
Detecting coherence in texts is challenging due
to the elusive and difficult-to-define nature of the
term coherence.
Many linguistic theories have
been put forth (Halliday and Hasan, 1976; Joshi
and Weinstein, 1981; Givon, 1995; Hobbs, 1979;
Dijk, 1979; Mann and Thompson, 1988), leading
to the development of several modeling techniques
that follow up on them (Lapata, 2003; Miltsakaki
et al., 2000). However, in NLP the current ap-
proaches for capturing coherence relied mostly on
proxy tasks that were presumed to reflect coher-
ence, most noticeable the sentence reordering task
(Lapata, 2003).
Relying on a proxy task, such as sentences re-
ordering, oversimplifies the notion of coherence,
and does not completely reflect the multifaceted
nature of coherence in real-world texts, potentially
resulting in the development of models that are
incomplete in their capacity to detect and assess
(in)coherence in a holistic fashion. Furthermore,
coherence may vary across different genres, con-
texts, and writing styles, making it challenging
to create proxy tasks that adequately represent all
possible aspects of coherence. This can lead to
models that are optimized for the proxy tasks but
struggle to generalize their coherence assessment
"
"2310.00603","Nitay Calderon","Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma,
  Roi Reichart","Faithful Explanations of Black-box NLP Models Using LLM-generated
  Counterfactuals","","","","","cs.CL cs.AI","http://creativecommons.org/publicdomain/zero/1.0/","  Causal explanations of the predictions of NLP systems are essential to ensure
safety and establish trust. Yet, existing methods often fall short of
explaining model predictions effectively or efficiently and are often
model-specific. In this paper, we address model-agnostic explanations,
proposing two approaches for counterfactual (CF) approximation. The first
approach is CF generation, where a large language model (LLM) is prompted to
change a specific text concept while keeping confounding concepts unchanged.
While this approach is demonstrated to be very effective, applying LLM at
inference-time is costly. We hence present a second approach based on matching,
and propose a method that is guided by an LLM at training-time and learns a
dedicated embedding space. This space is faithful to a given causal graph and
effectively serves to identify matches that approximate CFs. After showing
theoretically that approximating CFs is required in order to construct faithful
explanations, we benchmark our approaches and explain several models, including
LLMs with billions of parameters. Our empirical results demonstrate the
excellent performance of CF generation models as model-agnostic explainers.
Moreover, our matching approach, which requires far less test-time resources,
also provides effective explanations, surpassing many baselines. We also find
that Top-K techniques universally improve every tested method. Finally, we
showcase the potential of LLMs in constructing new benchmarks for model
explanation and subsequently validate our conclusions. Our work illuminates new
pathways for efficient and accurate approaches to interpreting NLP systems.
","2023-10-03","2310.00603v1.pdf","FAITHFUL EXPLANATIONS OF BLACK-BOX NLP MOD-
ELS USING LLM-GENERATED COUNTERFACTUALS
Yair Ori GatT ∗, Nitay CalderonT ∗, Amir FederC,
Alexander ChapaninT , Amit SharmaM and Roi ReichartT
T Faculty of Data and Decision Sciences, Technion, IIT
CColumbia University Data Science Institute, MMicrosoft Research India
∗Equal contribution. Corresponding author: nitay@campus.technion.ac.il
ABSTRACT
Causal explanations of the predictions of NLP systems are essential to ensure
safety and establish trust. Yet, existing methods often fall short of explaining
model predictions effectively or efficiently and are often model-specific. In this
paper, we address model-agnostic explanations, proposing two approaches for
counterfactual (CF) approximation. The first approach is CF generation, where a
large language model (LLM) is prompted to change a specific text concept while
keeping confounding concepts unchanged. While this approach is demonstrated
to be very effective, applying LLM at inference-time is costly. We hence present
a second approach based on matching, and propose a method that is guided by
an LLM at training-time and learns a dedicated embedding space. This space
is faithful to a given causal graph and effectively serves to identify matches that
approximate CFs. After showing theoretically that approximating CFs is required in
order to construct faithful explanations, we benchmark our approaches and explain
several models, including LLMs with billions of parameters. Our empirical results
demonstrate the excellent performance of CF generation models as model-agnostic
explainers. Moreover, our matching approach, which requires far less test-time
resources, also provides effective explanations, surpassing many baselines. We
also find that Top-K techniques universally improve every tested method. Finally,
we showcase the potential of LLMs in constructing new benchmarks for model
explanation and subsequently validate our conclusions. Our work illuminates new
pathways for efficient and accurate approaches to interpreting NLP systems.
1
INTRODUCTION
arXiv:2310.00603v1  [cs.CL]  1 Oct 2023
Providing faithful explanations for Natural Language Processing (NLP) model predictions is imper-
ative to guarantee safe deployment, establish trust, and foster scientific discoveries (Amodei et al.,
2016; Goodman & Flaxman, 2017; Guidotti et al., 2019; Jacovi & Goldberg, 2020). These aspects are
particularly significant in NLP, where the complexity of language and the opaque behavior of black-
box models. Many past works focus on what knowledge a model encodes (Lyu et al., 2022). However,
just because a model encodes a myriad of features does not mean all are utilized in decision-making.
For an explanation to be genuinely faithful and accurately depict a model’s underlying reasoning, it
is crucial to establish causality. Recognizing this inherent link and following previous works (Vig
et al., 2020; Geiger et al., 2020; Feder et al., 2021b), this paper introduces a theoretical framework
that binds the two together, providing another evidence that non-causal explanation methods can
occasionally fall short of being truly faithful.
In contrast to model explanation techniques that often conflate correlation with causation, causal-
inspired methods often contrast predictions for an input example with those of its counterfactual
(Soulos et al., 2020; Elazar et al., 2021; Finlayson et al., 2021). Indeed, counterfactuals are at the
highest level of Pearl’s causal hierarchy (Pearl, 2009), highlighting how changes lead to a different
prediction. However, they cannot be acquired without knowing the complete data-generating process
(or structural model) of the text (Balke & Pearl, 1994), which is not practical for a given real-world
problem. Hence, we turn to counterfactual approximations (CFs): imagining how a given text would
1
"
"2310.00637","Paul Groth","Bradley P. Allen and Lise Stork and Paul Groth","Knowledge Engineering using Large Language Models","19 pages, 2 figures, accepted in Transactions on Graph Data and
  Knowledge","","","","cs.AI cs.CL","http://creativecommons.org/licenses/by/4.0/","  Knowledge engineering is a discipline that focuses on the creation and
maintenance of processes that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on knowledge expressed in formal
languages. The emergence of large language models and their capabilities to
effectively work with natural language, in its broadest sense, raises questions
about the foundations and practice of knowledge engineering. Here, we outline
the potential role of LLMs in knowledge engineering, identifying two central
directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2)
enabling knowledge engineering in natural language. Additionally, we formulate
key open research questions to tackle these directions.
","2023-10-03","2310.00637v1.pdf","Knowledge Engineering using Large Language
Models
Bradley P. Allen �
University of Amsterdam, Amsterdam, NL
Lise Stork �
Vrije Universiteit Amsterdam, Amsterdam, NL
Paul Groth �
University of Amsterdam, Amsterdam, NL
Abstract
Knowledge engineering is a discipline that fo-
cuses on the creation and maintenance of processes
that generate and apply knowledge. Traditionally,
knowledge engineering approaches have focused on
knowledge expressed in formal languages.
The
emergence of large language models and their cap-
abilities to effectively work with natural language,
in its broadest sense, raises questions about the
foundations and practice of knowledge engineer-
ing. Here, we outline the potential role of LLMs in
knowledge engineering, identifying two central dir-
ections: 1) creating hybrid neuro-symbolic know-
ledge systems; and 2) enabling knowledge engin-
eering in natural language. Additionally, we for-
mulate key open research questions to tackle these
directions.
2012 ACM Subject Classification Computing methodologies → Natural language processing, Comput-
ing methodologies → Machine learning, Computing methodologies → Philosophical/theoretical found-
ations of artificial intelligence, Software and its engineering → Software development methods
Keywords and Phrases knowledge engineering, large language models
Digital Object Identifier 10.1234/0000000.00000000
Received 2023-07-14 Accepted To be completed by Dagstuhl editorial office Published To be completed
by Dagstuhl editorial office
1
Introduction
arXiv:2310.00637v1  [cs.AI]  1 Oct 2023
Knowledge engineering (KE) is a discipline concerned with the development and maintenance of
automated processes that generate and apply knowledge [4, 93]. Knowledge engineering rose to
prominence in the nineteen-seventies, when Edward Feigenbaum and others became convinced
that automating knowledge production through the application of research into artificial intelli-
gence required a domain-specific focus [32]. The period from the mid-nineteen-seventies into the
nineteen-eighties saw the knowledge engineering of rule-based expert systems for the purposes of
the automation of decision making in business enterprise settings. By the early nineteen-nineties,
however, it became clear that the expert systems approach, given its dependence on manual
knowledge acquisition and rule-based representation of knowledge by highly skilled knowledge
engineers, resulted in systems that were expensive to maintain and difficult to adapt to chan-
ging requirements or application contexts.
Feigenbaum argued that, to be successful, future
knowledge-based systems would need to be scalable, globally distributed, and interoperable [34].
The establishment of the World Wide Web and the emergence of Web architectural principles
in the mid-nineteen-nineties provided a means to address these requirements. Tim Berners-Lee
argued for a ""Web of Data"" based on linked data principles, standard ontologies, and data sharing
protocols that established open standards for knowledge representation and delivery on and across
the Web [11]. The subsequent twenty years witnessed the development of a globally federated
open linked data ""cloud"" [13], the refinement of techniques for ontology engineering [51], and
methodologies for the development of knowledge-based systems [86]. During the same period,
© Bradley P. Allen, Lise Stork, Paul Groth;
licensed under Creative Commons Attribution 4.0 International (CC BY 4.0)
Transactions on Graph Data and Knowledge, Vol. VOL, Issue ISS, Article No. ART NO., pp. ART:1–ART:19
Transactions on Graph Data and Knowledge
TGDK Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany
"
"2310.00646","Bryan Kian Hsiang Low","Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng
  Foo, See-Kiong Ng, Bryan Kian Hsiang Low","WASA: WAtermark-based Source Attribution for Large Language
  Model-Generated Data","","","","","cs.LG cs.AI stat.ML","http://creativecommons.org/licenses/by/4.0/","  The impressive performances of large language models (LLMs) and their immense
potential for commercialization have given rise to serious concerns over the
intellectual property (IP) of their training data. In particular, the synthetic
texts generated by LLMs may infringe the IP of the data being used to train the
LLMs. To this end, it is imperative to be able to (a) identify the data
provider who contributed to the generation of a synthetic text by an LLM
(source attribution) and (b) verify whether the text data from a data provider
has been used to train an LLM (data provenance). In this paper, we show that
both problems can be solved by watermarking, i.e., by enabling an LLM to
generate synthetic texts with embedded watermarks that contain information
about their source(s). We identify the key properties of such watermarking
frameworks (e.g., source attribution accuracy, robustness against adversaries),
and propose a WAtermarking for Source Attribution (WASA) framework that
satisfies these key properties due to our algorithmic designs. Our WASA
framework enables an LLM to learn an accurate mapping from the texts of
different data providers to their corresponding unique watermarks, which sets
the foundation for effective source attribution (and hence data provenance).
Extensive empirical evaluations show that our WASA framework achieves effective
source attribution and data provenance.
","2023-10-03","2310.00646v1.pdf","WASA: WATERMARK-BASED SOURCE ATTRIBUTION
FOR LARGE LANGUAGE MODEL-GENERATED DATA
Jingtan Wang*13, Xinyang Lu*12, Zitong Zhao*12, Zhongxiang Dai1
Chuan-Sheng Foo3, See-Kiong Ng12 & Bryan Kian Hsiang Low1
1Department of Computer Science, National University of Singapore
2Institute of Data Science, National University of Singapore
3Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR)
{jingtan.w,xinyang.lu,zitongz}@u.nus.edu, dzx@nus.edu.sg,
foo chuan sheng@i2r.a-star.edu.sg, seekiong@nus.edu.sg,
lowkh@comp.nus.edu.sg
ABSTRACT
The impressive performances of large language models (LLMs) and their im-
mense potential for commercialization have given rise to serious concerns over
the intellectual property (IP) of their training data. In particular, the synthetic
texts generated by LLMs may infringe the IP of the data being used to train the
LLMs. To this end, it is imperative to be able to (a) identify the data provider who
contributed to the generation of a synthetic text by an LLM (source attribution)
and (b) verify whether the text data from a data provider has been used to train
an LLM (data provenance). In this paper, we show that both problems can be
solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with
embedded watermarks that contain information about their source(s). We identify
the key properties of such watermarking frameworks (e.g., source attribution ac-
curacy, robustness against adversaries), and propose a WAtermarking for Source
Attribution (WASA) framework that satisfies these key properties due to our al-
gorithmic designs. Our WASA framework enables an LLM to learn an accurate
mapping from the texts of different data providers to their corresponding unique
watermarks, which sets the foundation for effective source attribution (and hence
data provenance). Extensive empirical evaluations show that our WASA framework
achieves effective source attribution and data provenance.
1
INTRODUCTION
arXiv:2310.00646v1  [cs.LG]  1 Oct 2023
Large language models (LLMs) (Ouyang et al., 2022; Touvron et al., 2023) have recently demon-
strated remarkable performances and hence received a surging interest. These LLMs, which are
trained using massive text data, have displayed impressive text generation abilities. This has given
rise to the immense potential of adopting the texts generated by LLMs for commercial use. How-
ever, this potential commercialization has led to major concerns regarding the intellectual property
(IP) of training data for LLMs because the texts generated by an LLM may infringe the IP of the
data being used to train the LLM. These concerns have been reflected by the increasing regulations
on data protection related to AI models. For example, the Coalition for Content Provenance and
Authenticity has stressed the necessity of certifying the source of online content produced by gener-
ative AI models (Rosenthol, 2022). Therefore, it is of crucial importance for LLMs to be equipped
with source attribution and data provenance for their generated synthetic texts.
In source attribution, given some synthetic texts generated by an LLM, its aim is to find the source
responsible for the generation of these texts. That is, if the data from a data provider has been used
to train the LLM and contributed to the generation of a sentence by the LLM, then source attribution
identifies this data provider. Moreover, source attribution also improves the interpretability of LLM-
generated texts: For example, if the generated content from an LLM is attributed to a trustworthy
source (e.g., a peer-reviewed academic paper), then the user is likely to consider the content more
* Equal contribution.
1
"
"2310.00647","Mustafa Shukor","Mustafa Shukor, Alexandre Rame, Corentin Dancette, Matthieu Cord","Beyond Task Performance: Evaluating and Reducing the Flaws of Large
  Multimodal Models with In-Context Learning","Project Page: https://evalign-icl.github.io/","","","","cs.CV cs.MM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Following the success of Large Language Models (LLMs), Large Multimodal
Models (LMMs), such as the Flamingo model and its subsequent competitors, have
started to emerge as natural steps towards generalist agents. However,
interacting with recent LMMs reveals major limitations that are hardly captured
by the current evaluation benchmarks. Indeed, task performances (e.g., VQA
accuracy) alone do not provide enough clues to understand their real
capabilities, limitations, and to which extent such models are aligned to human
expectations. To refine our understanding of those flaws, we deviate from the
current evaluation paradigm and propose the EvALign-ICL framework, in which we
(1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture such
as OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention,
compositionality, explainability and instruction following. Our evaluation on
these axes reveals major flaws in LMMs. To efficiently address these problems,
and inspired by the success of in-context learning (ICL) in LLMs, (2) we
explore ICL as a solution and study how it affects these limitations. Based on
our ICL study, (3) we push ICL further and propose new multimodal ICL
approaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, and
Self-Correcting-ICL. Our findings are as follows; (1) Despite their success,
LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL
on LMMs flaws is nuanced; despite its effectiveness for improved
explainability, abstention, and instruction following, ICL does not improve
compositional abilities, and actually even amplifies hallucinations. (3) The
proposed ICL variants are promising as post-hoc approaches to efficiently
tackle some of those flaws. The code is available here:
https://evalign-icl.github.io/
","2023-10-03","2310.00647v1.pdf","Preprint version
BEYOND TASK PERFORMANCE: EVALUATING AND
REDUCING THE FLAWS OF LARGE MULTIMODAL MOD-
ELS WITH IN-CONTEXT LEARNING
Mustafa Shukor1
Alexandre Rame 1
Corentin Dancette1
Matthieu Cord1,2
1Sorbonne University
2Valeo.ai
ABSTRACT
Following the success of Large Language Models (LLMs), Large Multimodal
Models (LMMs), such as the Flamingo model and its subsequent competitors, have
started to emerge as natural steps towards generalist agents. However, interacting
with recent LMMs reveals major limitations that are hardly captured by the current
evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone
do not provide enough clues to understand their real capabilities, limitations, and
to which extent such models are aligned to human expectations. To refine our
understanding of those flaws, we deviate from the current evaluation paradigm and
propose the EvALign-ICL framework, in which we (1) evaluate 8 recent open-source
LMMs (based on the Flamingo architecture such as OpenFlamingo and IDEFICS)
on 5 different axes; hallucinations, abstention, compositionality, explainability and
instruction following. Our evaluation on these axes reveals major flaws in LMMs.
To efficiently address these problems, and inspired by the success of in-context
learning (ICL) in LLMs, (2) we explore ICL as a solution and study how it affects
these limitations. Based on our ICL study, (3) we push ICL further and propose
new multimodal ICL approaches such as; Multitask-ICL, Chain-of-Hindsight-ICL,
and Self-Correcting-ICL.
Findings. (1) Despite their success, LMMs have flaws that remain unsolved
with scaling alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its
effectiveness for improved explainability, abstention, and instruction following, ICL
does not improve compositional abilities, and actually even amplifies hallucinations.
(3) The proposed ICL variants are promising as post-hoc approaches to efficiently
tackle some of those flaws. The code is available here: https://evalign-icl.github.io/.
Instruction following
Hallucinations
Abstention
Explainability
Compositionality
arXiv:2310.00647v1  [cs.CV]  1 Oct 2023
What color are the bus tires?
Is this a pizza?
Describe the image
Does the following sentence describe the 
image?: an astronaut makes a jump
What can be inferred about the 
giraffe's habitat from this image?
Answer: orange
Answer yes
Answer: A cake with a cowboy 
hat on top of it
Answer: yes because it has 
tomato sauce and cheese
Answer: The giraffe's habitat is 
the savanna
Zero-shot
ICL
Zero-shot
ICL
X-ICL (Ours)
Zero-shot
ICL
X-ICL (Ours)
Zero-shot
ICL
X-ICL (Ours)
Zero-shot
ICL
X-ICL (Ours)
Low
Moderate
Satisfactory
Performance
Y
LMM
X
ZS:
ICL: 
X-ICL:  
Figure 1: Evaluation framework. We evaluate LMMs on 5 different aspects for 3 strategies. In addition to an
image <image> and a question T used in zero-shot (ZS), in-context learning (ICL) considers N demonstrations
of images-questions-answers (<image>i, Ti, Ri) as input X, augmented by a function f in our X-ICL.
1
"
"2310.00648","Lauren Hong","Lauren Hong, Ting Wang","Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning","16 pages, 5 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of
pre-trained language models (PLMs) to specific tasks. By tuning only a minimal
set of (extra) parameters, PEFT achieves performance comparable to full
fine-tuning. However, despite its prevalent use, the security implications of
PEFT remain largely unexplored. In this paper, we conduct a pilot study
revealing that PEFT exhibits unique vulnerability to trojan attacks.
Specifically, we present PETA, a novel attack that accounts for downstream
adaptation through bilevel optimization: the upper-level objective embeds the
backdoor into a PLM while the lower-level objective simulates PEFT to retain
the PLM's task-specific performance. With extensive evaluation across a variety
of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in
terms of both attack success rate and unaffected clean accuracy, even after the
victim user performs PEFT over the backdoored PLM using untainted data.
Moreover, we empirically provide possible explanations for PETA's efficacy: the
bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules,
thereby retaining the backdoor throughout PEFT. Based on this insight, we
explore a simple defense that omits PEFT in selected layers of the backdoored
PLM and unfreezes a subset of these layers' parameters, which is shown to
effectively neutralize PETA.
","2023-10-05","2310.00648v1.pdf","FEWER IS MORE: TROJAN ATTACKS ON PARAMETER-
EFFICIENT FINE-TUNING
Lauren Hong ∗
Stony Brook University
Ting Wang
Stony Brook University
ABSTRACT
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained
language models (PLMs) to specific tasks. By tuning only a minimal set of (extra)
parameters, PEFT achieves performance comparable to full fine-tuning. However,
despite its prevalent use, the security implications of PEFT remain largely unex-
plored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique
vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that
accounts for downstream adaptation through bilevel optimization: the upper-level
objective embeds the backdoor into a PLM while the lower-level objective sim-
ulates PEFT to retain the PLM’s task-specific performance. With extensive eval-
uation across a variety of downstream tasks and trigger designs, we demonstrate
PETA’s effectiveness in terms of both attack success rate and unaffected clean
accuracy, even after the victim user performs PEFT over the backdoored PLM
using untainted data. Moreover, we empirically provide possible explanations for
PETA’s efficacy: the bilevel optimization inherently ‘orthogonalizes’ the back-
door and PEFT modules, thereby retaining the backdoor throughout PEFT. Based
on this insight, we explore a simple defense that omits PEFT in selected layers of
the backdoored PLM and unfreezes a subset of these layers’ parameters, which is
shown to effectively neutralize PETA.
1
INTRODUCTION
Parameter-efficient fine-tuning (PEFT) is a new paradigm for adapting pre-trained language models
(PLMs) (Houlsby et al., 2019; Li & Liang, 2021; Lester et al., 2022; Hu et al., 2022) to a variety of
NLP tasks. Unlike the conventional fine-tuning paradigm that requires retraining all of the PLM’s
parameters, PEFT only fine-tunes a minimal set of (extra) parameters while keeping most of the
PLM’s original weights frozen. It is shown that PEFT not only curtails the prohibitive training costs
in terms of both data and compute resources but also achieves performance that is comparable to
full-scale fine-tuning (He et al., 2022; Li & Liang, 2021).
arXiv:2310.00648v1  [cs.CL]  1 Oct 2023
Yet, in contrast to its pervasive use, the security implications of PEFT are largely underexplored.
As one major security threat, textual trojan attacks inject misclassification rules (‘backdoors’) into
PLMs, which can be activated by poisoned examples that contain specific ‘triggers’ (e.g., the rare
word of ‘cr’) at inference time (Gu et al., 2017). Recent work (Kurita et al., 2020; Yang et al., 2021a;
Zhang et al., 2021; Zhang et al., 2021; Yang et al., 2021c; Qi et al., 2021e; Pan et al., 2022) have
shown that fine-tuned PLMs are often susceptible to these attacks. However, understanding and
mitigating the vulnerability to such attacks within the context of PEFT remains an open challenge.
In this work, we take the first step in this line of research and present PETA1, a novel trojan attack
tailored to PEFT, which consists of two stages: (1) bilevel optimization, which inserts the backdoor
into a general-purpose pre-trained language model and (2) parameter-efficient fine-tuning on a
clean dataset. The first phase is executed by the attacker, and the second phase is performed by the
end user.
We further provide possible explanations for PETA’s efficacy. Through the first stage, PETA accounts
for the downstream adaptation by formulating the upper-level optimization objective to embed the
∗Work done while visiting Stony Brook University
1PETA: Parameter-Efficient Trojan Attack
1
"
"2310.00653","Tianyu Yu","Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang,
  Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, Zhiyuan Liu, Hai-Tao Zheng, Maosong
  Sun","Reformulating Vision-Language Foundation Models and Datasets Towards
  Universal Multimodal Assistants","","","","","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities
to perceive images and follow open-ended instructions. The capabilities of
MLLMs depend on two crucial factors: the model architecture to facilitate the
feature alignment of visual modules and large language models; the multimodal
instruction tuning datasets for human instruction following. (i) For the model
architecture, most existing models introduce an external bridge module to
connect vision encoders with language models, which needs an additional
feature-alignment pre-training. In this work, we discover that compact
pre-trained vision language models can inherently serve as ``out-of-the-box''
bridges between vision and language. Based on this, we propose Muffin
framework, which directly employs pre-trained vision-language models to act as
providers of visual signals. (ii) For the multimodal instruction tuning
datasets, existing methods omit the complementary relationship between
different datasets and simply mix datasets from different tasks. Instead, we
propose UniMM-Chat dataset which explores the complementarities of datasets to
generate 1.1M high-quality and diverse multimodal instructions. We merge
information describing the same image from diverse datasets and transforms it
into more knowledge-intensive conversation data. Experimental results
demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset.
Muffin achieves state-of-the-art performance on a wide range of vision-language
tasks, significantly surpassing state-of-the-art models like LLaVA and
InstructBLIP. Our model and dataset are all accessible at
https://github.com/thunlp/muffin.
","2023-10-03","2310.00653v1.pdf","Reformulating Vision-Language Foundation Models and Datasets
Towards Universal Multimodal Assistants
Tianyu Yu*1 Jinyi Hu*1 Yuan Yao1† Haoye Zhang1 Yue Zhao2
Chongyi Wang3 Shan Wang3 Yinxu Pan4 Jiao Xue3 Dahai Li3
Zhiyuan Liu1† Hai-Tao Zheng1† Maosong Sun1†
1Tsinghua University
2 Beijing University of Posts and Telecommunications
3 Zhihu Inc.
4 ModelBest Inc.
yiranytianyu@gmail.com
OKVQA
Abstract
Detail
AOKVQA
Description
69.2
70.5
67.0
65.5
63.0
64.0
61.8
55.5
61.0
GQA
Complex
Reasoning
79.0
85.0
91.0
61.8
65.5
69.2
60.8
64.5
68.0
64.9
68.5
69.0
76.2
73.5
74.0
67.8
Conversation
VQAv2
80.0
70.7
Visual Question
Answering
Visual Chat
Muffin (This Work)
LLaVA
InstructBLIP
Ziya-Visual
mPLUG-owl
Figure 1: Muffin achieves state-of-the-art performances on
various tasks compared with strong MLLMs. Visual Ques-
tion Answering: the average score over four visual question
answering datasets. Visual Chat: the average score over the
conversation task, the complex reasoning task and the detail
description generation task.
Recent Multimodal Large Language Models (MLLMs) ex-
hibit impressive abilities to perceive images and follow open-
ended instructions. The capabilities of MLLMs depend on
two crucial factors: the model architecture to facilitate the
feature alignment of visual modules and large language mod-
els; the multimodal instruction tuning datasets for human in-
struction following. (i) For the model architecture, most ex-
isting models introduce an external bridge module to connect
vision encoders with language models, which needs an ad-
ditional feature-alignment pre-training. In this work, we dis-
cover that compact pre-trained vision language models can
inherently serve as “out-of-the-box” bridges between vision
and language. Based on this, we propose Muffin framework,
which directly employs pre-trained vision-language models
to act as providers of visual signals. (ii) For the multimodal
instruction tuning datasets, existing methods omit the com-
plementary relationship between different datasets and sim-
ply mix datasets from different tasks. Instead, we propose
UniMM-Chat dataset which explores the complementarities
of datasets to generate 1.1M high-quality and diverse mul-
timodal instructions. We merge information describing the
same image from diverse datasets and transforms it into more
knowledge-intensive conversation data. Experimental results
demonstrate the effectiveness of the Muffin framework and
UniMM-Chat dataset. Muffin achieves state-of-the-art per-
formance on a wide range of vision-language tasks, signif-
icantly surpassing state-of-the-art models like LLaVA and
InstructBLIP. Our model and dataset are all accessible at
https://github.com/thunlp/muffin.
arXiv:2310.00653v1  [cs.CV]  1 Oct 2023
1
Introduction
Building a general model capable of tackling diverse tasks
across multiple modalities has remained a longstanding goal
within the realm of Artificial Intelligence. Recently, pow-
erful Multimodal Large Language Models (MLLMs) have
emerged as one of the most promising ways to achieve this
goal, such as MiniGPT-4 (Zhu et al. 2023), LLaVA (Liu
et al. 2023), and InstructBLIP (Dai et al. 2023). These mod-
els empower large language models (LLMs) with impressive
multimodal instruction-following capabilities by equipping
LLMs with vision encoders to perceive visual content.
*These authors contributed equally.
†Corresponding authors.
Despite existing capabilities, several crucial factors of de-
veloping MLLM are still under-explored. In this work, we
focus on two key challenges of building MLLMs: (i) effec-
tiveness of model architectures to achieve feature alignment;
(ii) construction of multimodal instruction tuning dataset.
For the model architecture, existing MLLMs can roughly
be summarized as two streams: (1) A linear projector is op-
timized to align the frozen visual encoder with the frozen
LLM, such as LLaVA (Liu et al. 2023) and PaLM-E (Driess
et al. 2023); (2) A visual feature re-sampler (Alayrac et al.
2022; Li et al. 2023; Dai et al. 2023) is optimized to com-
press the output of the visual encoder into a fixed-length fea-
ture sequence and align the these features with LLMs. How-
ever, merely using a linear projector restrains the model’s
capacity to learn new knowledge and the feature sequence
length is quadratically related to the resolution of the input
image, leading to a significant computational burden. On the
other side, introducing a visual feature re-sampler requires a
resource-consuming additional training process to primarily
"
"2310.00656","Huajian Xin","Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu,
  Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo
  Li, Xiaodan Liang, Heng Liao","LEGO-Prover: Neural Theorem Proving with Growing Libraries","","","","","cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.
","2023-10-13","2310.00656v1.pdf","LEGO-PROVER: NEURAL THEOREM PROVING WITH
GROWING LIBRARIES
Huajian Xin1∗ Haiming Wang1∗ Chuanyang Zheng3
Lin Li6
Zhengying Liu2† Qingxing Cao1
Yinya Huang1
Jing Xiong1
Han Shi2
Enze Xie2
Jian Yin1† Zhenguo Li2
Xiaodan Liang1, 5†
1Sun Yat-sen University
2Huawei Noah’s Ark Lab
3The Chinese University of Hong Kong
4City University of Hong Kong
5MBZUAI
6Huawei HiSilicon
{xinhj, wanghm39, caoqx, xiongj69, issjyin}@mail2.sysu.edu.cn, cyzheng21@cse.cuhk.edu.hk,
{liuzhengying2, xie.enze, shi.han, lilin29, Li.Zhenguo}@huawei.com,
yinya.huang@hotmail.com, xdliang328@gmail.com
ABSTRACT
Despite the success of large language models (LLMs), the task of theorem proving
still remains one of the hardest reasoning tasks that is far from being fully solved.
Prior methods using language models have demonstrated promising results, but
they still struggle to prove even middle school level theorems. One common
limitation of these methods is that they assume a fixed theorem library during the
whole theorem proving process. However, as we all know, creating new useful
theorems or even new theories is not only helpful but crucial and necessary for
advancing mathematics and proving harder and deeper results. In this work, we
present LEGO-Prover, which employs a growing skill library containing verified
lemmas as skills to augment the capability of LLMs used in theorem proving. By
constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing
skills retrieved from the library and to create new skills during the proving process.
These skills are further evolved (by prompting an LLM) to enrich the library on
another scale. Modular and reusable skills are constantly added to the library
to enable tackling increasingly intricate mathematical problems. Moreover, the
learned library further bridges the gap between human proofs and formal proofs
by making it easier to impute missing steps. LEGO-Prover advances the state-
of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5%
to 47.1%). During the proving process, LEGO-Prover also manages to generate
over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our
ablation study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.
1
INTRODUCTION
arXiv:2310.00656v1  [cs.AI]  1 Oct 2023
The automation of formal reasoning tasks, such as theorem proving and mathematical proof for-
malization, represents a formidable challenge and an active area of research within the domain of
artificial intelligence (Polu & Sutskever, 2020a; Han et al., 2022; Jiang et al., 2022a; First et al., 2023;
Bansal et al., 2019; Lample et al., 2022; Jiang et al., 2022b; 2021; Zhao et al., 2023; Yang et al., 2023;
Wang et al., 2023b; Liu et al., 2023). The process of formalizing mathematical proofs typically relies
on human experts to transcribe intricate mathematical concepts into structured formal languages
verifiable by interactive theorem prover like Lean (de Moura et al., 2015) or Isabelle (Paulson, 1994).
This process, while robust, is often labor-intensive and demands a high level of expertise.
In the past few years, large language models (LLMs) have emerged as a promising avenue, with their
capacity to process and produce human-like text, opening doors to the idea of LLM-based neural
theorem proving. Specifically, two predominant paradigms have been extensively explored in neural
theorem proving. One stream of work involves step-by-step proof generation (Polu & Sutskever,
∗ These authors contributed equally.
† Corresponding authors.
1
"
"2310.00673","Lukas Seidel","Lukas Seidel, Sedick David Baker Effendi, Xavier Pinho, Konrad Rieck,
  Brink van der Merwe, Fabian Yamaguchi","Learning Type Inference for Enhanced Dataflow Analysis","- fixed last author's name - fixed header","28th European Symposium on Research in Computer Security (ESORICS)
  2023","","","cs.LG cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Statically analyzing dynamically-typed code is a challenging endeavor, as
even seemingly trivial tasks such as determining the targets of procedure calls
are non-trivial without knowing the types of objects at compile time.
Addressing this challenge, gradual typing is increasingly added to
dynamically-typed languages, a prominent example being TypeScript that
introduces static typing to JavaScript. Gradual typing improves the developer's
ability to verify program behavior, contributing to robust, secure and
debuggable programs. In practice, however, users only sparsely annotate types
directly. At the same time, conventional type inference faces
performance-related challenges as program size grows. Statistical techniques
based on machine learning offer faster inference, but although recent
approaches demonstrate overall improved accuracy, they still perform
significantly worse on user-defined types than on the most common built-in
types. Limiting their real-world usefulness even more, they rarely integrate
with user-facing applications. We propose CodeTIDAL5, a Transformer-based model
trained to reliably predict type annotations. For effective result retrieval
and re-integration, we extract usage slices from a program's code property
graph. Comparing our approach against recent neural type inference systems, our
model outperforms the current state-of-the-art by 7.85% on the
ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore,
we present JoernTI, an integration of our approach into Joern, an open source
static analysis tool, and demonstrate that the analysis benefits from the
additional type information. As our model allows for fast inference times even
on commodity CPUs, making our system available through Joern leads to high
accessibility and facilitates security research.
","2023-10-05","2310.00673v1.pdf","Learning Type Inference for
Enhanced Dataflow Analysis
Lukas Seidel*1&3 , Sedick David Baker Effendi*2&4 , Xavier Pinho1, Konrad
Rieck3, Brink van der Merwe2, and Fabian Yamaguchi1&2
1 QwietAI, San Jose, USA
jlseidel@qwiet.ai
2 Stellenbosch University, Stellenbosch, South Africa
dbe@sun.ac.za†
3 Technische Universität Berlin, Berlin, Germany
4 Whirly Labs, Cape Town, South Africa
arXiv:2310.00673v1  [cs.LG]  1 Oct 2023
Abstract. Statically analyzing dynamically-typed code is a challenging
endeavor, as even seemingly trivial tasks such as determining the targets
of procedure calls are non-trivial without knowing the types of objects
at compile time. Addressing this challenge, gradual typing is increas-
ingly added to dynamically-typed languages, a prominent example being
TypeScript that introduces static typing to JavaScript. Gradual typing
improves the developer’s ability to verify program behavior, contribut-
ing to robust, secure and debuggable programs. In practice, however,
users only sparsely annotate types directly. At the same time, conven-
tional type inference faces performance-related challenges as program
size grows. Statistical techniques based on machine learning offer faster
inference, but although recent approaches demonstrate overall improved
accuracy, they still perform significantly worse on user-defined types than
on the most common built-in types. Limiting their real-world usefulness
even more, they rarely integrate with user-facing applications.
We propose CodeTIDAL5, a Transformer-based model trained to reliably
predict type annotations. For effective result retrieval and re-integration,
we extract usage slices from a program’s code property graph. Com-
paring our approach against recent neural type inference systems, our
model outperforms the current state-of-the-art by 7.85% on the Many-
Types4TypeScript benchmark, achieving 71.27% accuracy overall. Fur-
thermore, we present JoernTI, an integration of our approach into Joern,
an open source static analysis tool, and demonstrate that the analysis
benefits from the additional type information. As our model allows for
fast inference times even on commodity CPUs, making our system avail-
able through Joern leads to high accessibility and facilitates security
research.
Keywords: Type Inference, Representation Learning, Static Analysis,
Static Taint Tracking, Dataflow Analysis
This is a preprint version of an article to be published at the 28th European
Symposium on Research in Computer Security (ESORICS ’23).
* The first two authors contributed equally to this work.
† Email for correspondence.
"
"2310.00698","Reshma Ramaprasad","Reshma Ramaprasad","Comics for Everyone: Generating Accessible Text Descriptions for Comic
  Strips","Accepted at CLVL: 5th Workshop On Closing The Loop Between Vision And
  Language (ICCV 2023 Workshop)","","","","cs.CV cs.HC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Comic strips are a popular and expressive form of visual storytelling that
can convey humor, emotion, and information. However, they are inaccessible to
the BLV (Blind or Low Vision) community, who cannot perceive the images,
layouts, and text of comics. Our goal in this paper is to create natural
language descriptions of comic strips that are accessible to the visually
impaired community. Our method consists of two steps: first, we use computer
vision techniques to extract information about the panels, characters, and text
of the comic images; second, we use this information as additional context to
prompt a multimodal large language model (MLLM) to produce the descriptions. We
test our method on a collection of comics that have been annotated by human
experts and measure its performance using both quantitative and qualitative
metrics. The outcomes of our experiments are encouraging and promising.
","2023-10-03","2310.00698v1.pdf","Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips
Reshma Ramaprasad
Microsoft Research
reshma.ramaprasad@microsoft.com
Abstract
els, layout of panels, character, dialogue, context or setting,
expressions, actions of the characters etc. While there is no
standardized method for describing comic strips, [12] pro-
posed a set of guidelines for comic book descriptions which
we adopt for comic strips as well.
1.2. Multimodal large language model (MLLM)
Comic strips are a popular and expressive form of visual
storytelling that can convey humor, emotion, and informa-
tion. However, they are inaccessible to the BLV (Blind or
Low Vision) community, who cannot perceive the images,
layouts, and text of comics. Our goal in this paper is to cre-
ate natural language descriptions of comic strips that are
accessible to the visually impaired community. Our method
consists of two steps: first, we use computer vision tech-
niques to extract information about the panels, characters,
and text of the comic images; second, we use this informa-
tion as additional context to prompt a multimodal large lan-
guage model (MLLM) to produce the descriptions. We test
our method on a collection of comics that have been anno-
tated by human experts and measure its performance using
both quantitative and qualitative metrics. The outcomes of
our experiments are encouraging and promising.
1. Introduction
1.1. Accessible comics
arXiv:2310.00698v1  [cs.CV]  1 Oct 2023
GPT4 [11] has sparked a research frenzy by highlight-
ing the impressive capabilities of the model in not only
processing text, but also the ability to understand and ex-
plain images, such as being able to generate a fully func-
tioning website from a hand drawn picture of its design or
explaining why a picture is funny. While the vision capa-
bilities of GPT4 are still not accessible to the public, there
have been various open- source efforts to develop capable
models [15]. Multimodal large language models (MLLMs)
are neural networks that have displayed impressive capabil-
ities in processing both natural language and visual infor-
mation. In other words, they facilitate having conversations
about images. We conduct our experiments on the LLaVA
model [6], which is a state-of-the-art MLLM, but our find-
ings can be generalized to other similar models. We believe
that an MLLM based approach is suitable for this task as it
is a conversational interface that can not only generate a de-
scription but further allow users to interact with the comic
strips, such as asking questions about their humor or mean-
ing.
To the best of our knowledge, this is the first work that
explores using MLLMs for the task of generating accessible
text descriptions for newspaper comic strips. This is a chal-
lenging task because there are no clear rules for describing
comic strips, there are few annotated datasets available for
training on this domain, and comic strips have a lot of va-
riety in their layouts, styles, and fonts, which make image
processing difficult. Our main contributions are:
1. We propose a novel application of MLLMs for gener-
ating comic descriptions that can benefit people with
visual impairments.
2. We identify the key elements of comic strips that are
essential for generating good descriptions and propose
Comic strips are a form of visual storytelling that use
humor, satire, and irony to convey a message or a joke.
They usually have a series of panels or frames that show
the actions and dialogues of the characters, setting of the
scene etc. Comic strips are a popular medium that can at-
tract a wide range of audiences. However, they are inac-
cessible to the BLV community, who cannot perceive the
images, layouts, and text of comics. To make them accessi-
ble, we generate text descriptions that capture their humor
and meaning. These descriptions can then be accessed via
screen reader software for the visually impaired commu-
nity. Comic strips vary widely in their style and content,
and we need to select a representative sample for our ex-
periments and evaluation. Therefore, we choose three pop-
ular comic strips that have different themes, characters, and
humor: Dilbert, Garfield, and Peanuts. There are various
elements of comic strips that need to be described to pro-
vide an authentic comic reading such as the count of pan-
"
"2310.00704","Dongchao Yang","Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu,
  Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao,
  Shinji Watanabe, Helen Meng","UniAudio: An Audio Foundation Model Toward Universal Audio Generation","","","","","cs.SD eess.AS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language models (LLM) have demonstrated the capability to handle a
variety of generative tasks. This paper presents the UniAudio system, which,
unlike prior task-specific approaches, leverages LLM techniques to generate
multiple types of audio (including speech, sounds, music, and singing) with
given input conditions. UniAudio 1) first tokenizes all types of target audio
along with other condition modalities, 2) concatenates source-target pair as a
single sequence, and 3) performs next-token prediction using LLM. Also, a
multi-scale Transformer model is proposed to handle the overly long sequences
caused by the residual vector quantization based neural codec in tokenization.
Training of UniAudio is scaled up to 165K hours of audio and 1B parameters,
based on all generative tasks, aiming to obtain sufficient prior knowledge not
only in the intrinsic properties of audio but also the inter-relationship
between audio and other modalities. Therefore, the trained UniAudio model has
the potential to become a foundation model for universal audio generation: it
shows strong capability in all trained tasks and can seamlessly support new
audio generation tasks after simple fine-tuning. Experiments demonstrate that
UniAudio achieves state-of-the-art or at least competitive results on most of
the 11 tasks. Demo and code are released at
https://github.com/yangdongchao/UniAudio
","2023-10-12","2310.00704v1.pdf","Under review
UNIAUDIO: AN AUDIO FOUNDATION MODEL TOWARD
UNIVERSAL AUDIO GENERATION
Dongchao Yang1∗, Jinchuan Tian2∗, Xu Tan3†, Rongjie Huang4, Songxiang Liu, Xuankai Chang2,
Jiatong Shi2, Sheng Zhao3, Jiang Bian3, Xixin Wu1, Zhou Zhao4, Helen Meng1†
1 The Chinese University of Hong Kong, 2 Carnegie Mellon University,
3 Microsoft Research Asia, 4 Zhejiang University
xutan@microsoft.com, hmmeng@se.cuhk.edu.hk
ABSTRACT
Language models (LMs) have demonstrated the capability to handle a variety of
generative tasks. This paper presents the UniAudio system, which, unlike prior
task-specific approaches, leverages LMs techniques to generate multiple types of
audio (including speech, sounds, music, and singing) with given input conditions.
UniAudio 1) first tokenizes all types of target audio along with other condition
modalities, 2) concatenates source-target pair as a single sequence, and 3) per-
forms next-token prediction using LMs. Also, a multi-scale Transformer model
is proposed to handle the overly long sequences caused by the residual vector
quantization based neural codec in tokenization. Training of UniAudio is scaled up
to 165K hours of audio and 1B parameters, based on all generative tasks, aiming
to obtain sufficient prior knowledge not only in the intrinsic properties of audio
but also the inter-relationship between audio and other modalities. Therefore,
the trained UniAudio model has the potential to become a foundation model for
universal audio generation: it shows strong capability in all trained tasks and can
seamlessly support new audio generation tasks after simple fine-tuning. Experi-
ments demonstrate that UniAudio achieves state-of-the-art or at least competitive
results on most of the 11 tasks. Demo and code are released1 .
1
INTRODUCTION
Table 1: The audio generation tasks supported by UniAudio and prior works. The versatile UniAudio
model supports 11 audio generation tasks, much more than all others. TTS: text-to-speech; VC:
voice conversion; SE: speech enhancement; TSE: target speech extraction; SVS: sing voice synthesis;
TT-Sound: text-to-sound generation; TT-Music: text-to-music generation; A-Edit: Audio Edit; SD:
speech dereverberation; I-TTS: Instructed TTS; S-Edit: speech edit.
Model
TTS
VC
SE
TSE
SVS
TT-Sound
TT-Music
A-Edit
SD
I-TTS
S-Edit
arXiv:2310.00704v1  [cs.SD]  1 Oct 2023
YourTTS (Casanova et al., 2022)
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
VALL-E (Wang et al., 2023a)
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
MusicLM (Wang et al., 2023a)
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
SPEARTTS (Kharitonov et al., 2023)
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
NaturalSpeech2 (Shen et al., 2023)
✓
✓
✓
✗
✓
✗
✗
✗
✗
✗
✗
Make-A-Voice (Huang et al., 2023b)
✓
✓
✗
✗
✓
✗
✗
✗
✗
✗
✗
Maga-TTS (Jiang et al., 2023)
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
VoiceBox (Le et al., 2023)
✓
✗
✓
✗
✗
✗
✗
✗
✗
✗
✓
AudioLDM2 (Liu et al., 2023b)
✓
✗
✗
✗
✗
✓
✓
✗
✗
✗
✗
SpeechX (Wang et al., 2023c)
✓
✗
✓
✓
✗
✗
✗
✗
✗
✗
✓
UniAudio (ours)
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Audio generation is an important component of generative AI. Recently, the popularity of generative
AI has induced increasingly emergent and varying needs in audio generation: audio is expected to
be generated based on humans’s demands, such as speech synthesis (TTS), voice conversion (VC),
singing voice synthesis (SVS), text-to-sound, and text-to-music. Prior works on audio generation
tasks are commonly task-specific: their designs heavily leverage domain knowledge and their usage
is restricted to fixed setups (Tan et al., 2021; Luo & Mesgarani, 2019; Zmolikova et al., 2023; Huang
1https://github.com/yangdongchao/UniAudio
* Equal contribution; † Corresponding author
1
"
"2310.00724","Lorenzo Loconte","Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp,
  Arno Solin, Nicolas Gillis, Antonio Vergari","Subtractive Mixture Models via Squaring: Representation and Learning","","","","","cs.LG cs.AI","http://creativecommons.org/licenses/by-nc-nd/4.0/","  Mixture models are traditionally represented and learned by adding several
distributions as components. Allowing mixtures to subtract probability mass or
density can drastically reduce the number of components needed to model complex
distributions. However, learning such subtractive mixtures while ensuring they
still encode a non-negative function is challenging. We investigate how to
learn and perform inference on deep subtractive mixtures by squaring them. We
do this in the framework of probabilistic circuits, which enable us to
represent tensorized mixtures and generalize several other subtractive models.
We theoretically prove that the class of squared circuits allowing subtractions
can be exponentially more expressive than traditional additive mixtures; and,
we empirically show this increased expressiveness on a series of real-world
distribution estimation tasks.
","2023-10-03","2310.00724v1.pdf","SUBTRACTIVE MIXTURE MODELS VIA SQUARING:
REPRESENTATION AND LEARNING
Lorenzo Loconte1
Aleksanteri M. Sladek2
Stefan Mengel3
Martin Trapp2
Arno Solin2
Nicolas Gillis4
Antonio Vergari1
1 School of Informatics, University of Edinburgh, UK
l.loconte@sms.ed.ac.uk
avergari@ed.ac.uk
2 Department of Computer Science, Aalto University, Finland
{aleksanteri.sladek,martin.trapp,arno.solin}@aalto.fi
3 University of Artois, CNRS, Centre de Recherche en Informatique de Lens (CRIL), France
mengel@cril-lab.fr
4 Department of Mathematics and Operational Research, Universit´e de Mons, Belgium
nicolas.gillis@umons.ac.be
ABSTRACT
Mixture models are traditionally represented and learned by adding several distri-
butions as components. Allowing mixtures to subtract probability mass or density
can drastically reduce the number of components needed to model complex dis-
tributions. However, learning such subtractive mixtures while ensuring they still
encode a non-negative function is challenging. We investigate how to learn and
perform inference on deep subtractive mixtures by squaring them. We do this in
the framework of probabilistic circuits, which enable us to represent tensorized
mixtures and generalize several other subtractive models. We theoretically prove
that the class of squared circuits allowing subtractions can be exponentially more
expressive than traditional additive mixtures; and, we empirically show this in-
creased expressiveness on a series of real-world distribution estimation tasks.
1
INTRODUCTION
Finite mixture models (MMs) are a staple in probabilistic machine learning, as they offer a simple
and elegant solution to model complex distributions by blending simpler ones in a linear combination
(McLachlan et al., 2019). The classical recipe to design MMs is to compute a convex combination
over input components. That is, a MM representing a probability distribution p over a set of random
variables X = {X1, X2, . . . , XD} is usually defined as
arXiv:2310.00724v1  [cs.LG]  1 Oct 2023
p(X) = �K
i=1 wipi(X),
with
wi ≥ 0,
�K
i=1 wi = 1,
(1)
where wi are the mixture parameters and each component pi is a mass or density function. This
is the case for widely-used MMs such as Gaussian mixture models (GMMs) and hidden Markov
models (HMMs) but also mixtures of generative models such as normalizing flows (Papamakarios
et al., 2021) and deep mixture models such as probabilistic circuits (PCs, Vergari et al., 2019b).
The convexity constraint in Eq. (1) is the simplest sufficient condition
to ensure that p is a non-negative function and integrates to 1,1 i.e.,
is a valid probability distribution, and is often assumed in practice.
However, this implies that the components pi can only be combined
in an additive manner and as such it can greatly impact their ability to
estimate a distribution efficiently. For instance, consider approximat-
ing distributions having “holes” in their domain, such as the simple
2-dimensional ring distribution on the left (ground truth). A classi-
cal additive MM such a GMM would ultimately recover it, as it is a
1Across the paper we will abuse the term integration to also refer to summation in case of discrete variables.
1
"
"2310.00726","Pranjal Awasthi","Pranjal Awasthi and Anupam Gupta","Improving Length-Generalization in Transformers via Task Hinting","","","","","cs.LG cs.AI stat.ML","http://creativecommons.org/licenses/by/4.0/","  It has been observed in recent years that transformers have problems with
length generalization for certain types of reasoning and arithmetic tasks. In
particular, the performance of a transformer model trained on tasks (say
addition) up to a certain length (e.g., 5 digit numbers) drops sharply when
applied to longer instances of the same problem. This work proposes an approach
based on task hinting towards addressing length generalization. Our key idea is
that while training the model on task-specific data, it is helpful to
simultaneously train the model to solve a simpler but related auxiliary task as
well.
  We study the classical sorting problem as a canonical example to evaluate our
approach. We design a multitask training framework and show that task hinting
significantly improve length generalization. For sorting we show that it is
possible to train models on data consisting of sequences having length at most
$20$, and improve the test accuracy on sequences of length $100$ from less than
1% (for standard training) to more than 92% (via task hinting).
  Our study uncovers several interesting aspects of length generalization. We
observe that while several auxiliary tasks may seem natural a priori, their
effectiveness in improving length generalization differs dramatically. We
further use probing and visualization-based techniques to understand the
internal mechanisms via which the model performs the task, and propose a
theoretical construction consistent with the observed learning behaviors of the
model. Based on our construction, we show that introducing a small number of
length dependent parameters into the training procedure can further boost the
performance on unseen lengths. Finally, we also show the efficacy of our task
hinting based approach beyond sorting, giving hope that these techniques will
be applicable in broader contexts.
","2023-10-03","2310.00726v1.pdf","Improving Length-Generalization in
Transformers via Task Hinting
Pranjal Awasthi
Google Research
pranjalawasthi@google.com
Anupam Gupta
Carnegie Mellon University and Google Research
anupamg@cs.cmu.edu
October 3, 2023
Abstract
It has been observed in recent years that transformers have problems with length generalization for
certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model
trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied
to longer instances of the same problem. This work proposes an approach based on task hinting towards
addressing length generalization. Our key idea is that while training the model on task-specific data, it is
helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.
We study the classical sorting problem as a canonical example to evaluate our approach. We design a
multitask training framework and show that models trained via task hinting significantly improve length
generalization. In particular, for sorting we show that it is possible to train models on data consisting of
sequences having length at most 20, and improve the test accuracy on sequences of length 100 from less
than 1% (for standard training) to more than 92% (via task hinting).
Our study uncovers several interesting aspects of length generalization. We observe that while sev-
eral auxiliary tasks may seem natural a priori, their effectiveness in improving length generalization
differs dramatically. We further use probing and visualization-based techniques to understand the inter-
nal mechanisms via which the model performs the task, and propose a theoretical construction consistent
with the observed learning behaviors of the model. Based on our construction, we show that introducing
a small number of length dependent parameters into the training procedure can further boost the perfor-
mance on unseen lengths. Finally, we also show the efficacy of our task hinting based approach beyond
sorting, giving hope that these techniques will be applicable in broader contexts.
1
Introduction
arXiv:2310.00726v1  [cs.LG]  1 Oct 2023
Large transformer models trained on massive datasets continue to demonstrate impressive capabilities across
a range of tasks in language understanding, image modeling and other domains [Radford et al., 2019, Brown
et al., 2020, Chowdhery et al., 2022, Chen et al., 2022, Tu et al., 2023]. At the same time there is a growing
body of work on the limitations and vulnerabilities of such models. This work concerns the length general-
ization problem. For many natural tasks—especially ones involving multi-step reasoning such as addition,
multiplication, program execution etc.—there is a natural notion of the length of an input, e.g., the number
of digits when performing the addition task [Anil et al., 2022]. It has been observed that the performance
of transformers on such tasks drops sharply when tested on instances with lengths not seen during training
[Nye et al., 2021, Zhang et al., 2022, Jelassi et al., 2023, Abbe et al., 2023]. As formalized in Abbe et al.
[2023] this phenomenon can also be studied as an extreme form of out-of-distribution (OOD) robustness
where the support of the test-set distribution is disjoint from that of the training distribution.
1
"
"2310.00737","Emilio Ferrara","Emilio Ferrara","GenAI Against Humanity: Nefarious Applications of Generative Artificial
  Intelligence and Large Language Models","Submitted to CACM (Viewpoint)","","","","cs.CY cs.AI cs.CL cs.HC","http://creativecommons.org/licenses/by/4.0/","  Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications. This article is not just a journey through
the meanders of potential misuse of GenAI and LLMs, but also a call to
recognize the urgency of the challenges ahead. As we navigate the seas of
misinformation campaigns, malicious content generation, and the eerie creation
of sophisticated malware, we'll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on
social media platforms to the unnerving potential of AI to generate fabricated
identities, or alibis made of synthetic realities, the stakes have never been
higher. The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI's nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.
","2023-10-13","2310.00737v1.pdf","Fig. 1. Charting the Landscape of Nefarious Applications of Generative Artificial Intelligence and Large Language Models
GenAI Against Humanity: Nefarious Applications of Generative Artificial
Intelligence and Large Language Models
EMILIO FERRARA∗, University of Southern California, USA
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess
in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful
tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic
identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision.
Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI
and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns,
malicious content generation, and the eerie creation of sophisticated malware, we’ll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI
to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual
arXiv:2310.00737v1  [cs.CY]  1 Oct 2023
and the real worlds are blurring, and the consequences of potential GenAI’s nefarious applications impact us all. This article serves
both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the
different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.
Author’s address: Emilio Ferrara, emiliofe@usc.edu, University of Southern California, Thomas Lord Department of Computer Science, CA, USA, 90007.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the owner/author(s).
© 2023 Copyright held by the owner/author(s).
Manuscript submitted to ACM
1
"
"2310.00741","Shiqi Chen","Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao,
  Pengfei Liu and Junxian He","FELM: Benchmarking Factuality Evaluation of Large Language Models","Accepted by NeurIPS 2023 Track on Datasets and Benchmarks","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Assessing factuality of text generated by large language models (LLMs) is an
emerging yet crucial research area, aimed at alerting users to potential errors
and guiding the development of more reliable LLMs. Nonetheless, the evaluators
assessing factuality necessitate suitable evaluation themselves to gauge
progress and foster advancements. This direction remains under-explored,
resulting in substantial impediments to the progress of factuality evaluators.
To mitigate this issue, we introduce a benchmark for Factuality Evaluation of
large Language Models, referred to as felm. In this benchmark, we collect
responses generated from LLMs and annotate factuality labels in a fine-grained
manner. Contrary to previous studies that primarily concentrate on the
factuality of world knowledge (e.g.~information from Wikipedia), felm focuses
on factuality across diverse domains, spanning from world knowledge to math and
reasoning. Our annotation is based on text segments, which can help pinpoint
specific factual errors. The factuality annotations are further supplemented by
predefined error types and reference links that either support or contradict
the statement. In our experiments, we investigate the performance of several
LLM-based factuality evaluators on felm, including both vanilla LLMs and those
augmented with retrieval mechanisms and chain-of-thought processes. Our
findings reveal that while retrieval aids factuality evaluation, current LLMs
are far from satisfactory to faithfully detect factual errors.
","2023-10-03","2310.00741v1.pdf","FELM: Benchmarking Factuality Evaluation of
Large Language Models
Shiqi Chen1∗
Yiran Zhao3
Jinghan Zhang2
I-Chun Chern4
Siyang Gao1
Pengfei Liu5
Junxian He2
1City University of Hong Kong
2The Hong Kong University of Science and Technology
3National University of Singapore
4Carnegie Mellon University
5Shanghai Jiao Tong University
schen438-c@my.cityu.edu.hk, junxianh@cse.ust.hk
Abstract
Assessing factuality of text generated by large language models (LLMs) is an
emerging yet crucial research area, aimed at alerting users to potential errors and
guiding the development of more reliable LLMs. Nonetheless, the evaluators
assessing factuality necessitate suitable evaluation themselves to gauge progress
and foster advancements. This direction remains under-explored, resulting in sub-
stantial impediments to the progress of factuality evaluators. To mitigate this issue,
we introduce a benchmark for Factuality Evaluation of large Language Models,
referred to as FELM. In this benchmark, we collect responses generated from
LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous
studies that primarily concentrate on the factuality of world knowledge (e.g. in-
formation from Wikipedia), FELM focuses on factuality across diverse domains,
spanning from world knowledge to math and reasoning. Our annotation is based
on text segments, which can help pinpoint specific factual errors. The factuality
annotations are further supplemented by predefined error types and reference links
that either support or contradict the statement. In our experiments, we investigate
the performance of several LLM-based factuality evaluators on FELM, including
both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-
thought processes. Our findings reveal that while retrieval aids factuality evaluation,
current LLMs are far from satisfactory to faithfully detect factual errors.1
1
Introduction
arXiv:2310.00741v1  [cs.CL]  1 Oct 2023
Large language models (LLMs) have achieved stunning success, resulting in a paradigm shift towards
generative AI based on prompting (OpenAI, 2022; Chowdhery et al., 2022; Touvron et al., 2023;
OpenAI, 2023). However, a known issue of LLMs is their tendency to generate falsehoods or
hallucinate contents, posing a significant hurdle to broader applications. Even state-of-the-art LLMs
such as ChatGPT (OpenAI, 2022) are susceptible to this issue as shown in Borji (2023); Zhuo
et al. (2023); Min et al. (2023), which raises concerns about the practical utility of these models.
Consequently, factuality evaluators that could detect factual errors in LLM’s responses are urgently
needed to alert users to potential risks and drive the development of more reliable LLMs. For example,
an ideal factuality evaluation system, as demonstrated in Figure 1 , should be able to segment the
LLM responses into fine-grained textual spans, assess the factual correctness of each segment, and
highlight any errors for the users. To facilitate interpretability, the factuality evaluator may also
categorize the error type, provide an explanation, and offer reference links to justify its assessment.
*Work done during visiting HKUST.
1Our dataset is available at https://github.com/hkust-nlp/felm.
37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.
"
"2310.00746","Zekun Wang","Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu
  Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang
  Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, Junran Peng","RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities
  of Large Language Models","30 pages, repo at
  https://github.com/InteractiveNLP-Team/RoleLLM-public","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
","2023-10-03","2310.00746v1.pdf","ROLELLM: BENCHMARKING, ELICITING, AND ENHANCING
ROLE-PLAYING ABILITIES OF LARGE LANGUAGE MODELS
Zekun Moore Wang1,9∗, Zhongyuan Peng2∗, Haoran Que1∗, Jiaheng Liu1†,
Wangchunshu Zhou3, Yuhan Wu4, Hongcheng Guo1, Ruitong Gan5, Zehao Ni2,
Man Zhang4, Zhaoxiang Zhang6, Wanli Ouyang7, Ke Xu1, Wenhu Chen8, Jie Fu9, Junran Peng2,10
1Beihang University; 2University of the Chinese Academy of Sciences; 3ETH Zürich;
4Beijing University of Posts and Telecommunications; 5The Hong Kong Polytechnic University;
6Institute of Automation, Chinese Academy of Science; 7Shanghai AI Lab;
8University of Waterloo; 9The Hong Kong University of Science and Technology; 10Chongyue Technology
zenmoore@buaa.edu.cn, liujiaheng@buaa.edu.cn
ABSTRACT
The advent of Large Language Models (LLMs) has paved the way for complex tasks such as
role-playing, which enhances user interactions by enabling models to imitate various characters.
However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit
role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit,
and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile
Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-
specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation;
and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with
role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and
fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover,
RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing
role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4)1.
arXiv:2310.00746v1  [cs.CL]  1 Oct 2023
Figure 1: We introduce RoleLLM, a role-playing framework of data construction and evaluation (RoleBench), as well
as solutions for both closed-source and open-source models (RoleGPT, RoleLLaMA, RoleGLM). We also propose
Context-Instruct for long-text knowledge extraction and role-specific knowledge injection.
* Primary Authors. Work done during Zekun Moore Wang’s internship at HKUST.
† Corresponding Author.
1Access models, demos, and RoleBench at https://github.com/InteractiveNLP-Team/RoleLLM-public.
"
"2310.00754","Huaxiu Yao","Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng,
  Chelsea Finn, Mohit Bansal, Huaxiu Yao","Analyzing and Mitigating Object Hallucination in Large Vision-Language
  Models","","","","","cs.LG cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large vision-language models (LVLMs) have shown remarkable abilities in
understanding visual information with human languages. However, LVLMs still
suffer from object hallucination, which is the problem of generating
descriptions that include objects that do not actually exist in the images.
This can negatively impact many vision-language tasks, such as visual
summarization and reasoning. To address this issue, we propose a simple yet
powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify
object hallucination in LVLMs by reconstructing less hallucinatory
descriptions. LURE is grounded in a rigorous statistical analysis of the key
factors underlying object hallucination, including co-occurrence (the frequent
appearance of certain objects alongside others in images), uncertainty (objects
with higher uncertainty during LVLM decoding), and object position
(hallucination often appears in the later part of the generated text). LURE can
also be seamlessly integrated with any LVLMs. We evaluate LURE on six
open-source LVLMs, achieving a 23% improvement in general object hallucination
evaluation metrics over the previous best approach. In both GPT and human
evaluations, LURE consistently ranks at the top. Our data and code are
available at https://github.com/YiyangZhou/LURE.
","2023-10-03","2310.00754v1.pdf","Preprint
ANALYZING AND MITIGATING OBJECT HALLUCINA-
TION IN LARGE VISION-LANGUAGE MODELS
Yiyang Zhou1∗
Chenhang Cui1∗
Jaehong Yoon1
Linjun Zhang2
Zhun Deng3
Chelsea Finn4
Mohit Bansal1
Huaxiu Yao1
1UNC-Chapel Hill, 2Rutgers University, 3Columbia University, 4Stanford University
zhouyiyangailab@gmail.com, osallymalone@gmail.com, huaxiu@cs.unc.edu
ABSTRACT
Large vision-language models (LVLMs) have shown remarkable abilities in un-
derstanding visual information with human languages. However, LVLMs still
suffer from object hallucination, which is the problem of generating descriptions
that include objects that do not actually exist in the images. This can negatively
impact many vision-language tasks, such as visual summarization and reasoning.
To address this issue, we propose a simple yet powerful algorithm, LVLM Hal-
lucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs
by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous
statistical analysis of the key factors underlying object hallucination, including
co-occurrence (the frequent appearance of certain objects alongside others in im-
ages), uncertainty (objects with higher uncertainty during LVLM decoding), and
object position (hallucination often appears in the later part of the generated text).
LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE
on six open-source LVLMs, achieving a 23% improvement in general object hal-
lucination evaluation metrics over the previous best approach. In both GPT and
human evaluations, LURE consistently ranks at the top. Our data and code are
available at https://github.com/YiyangZhou/LURE.
1
INTRODUCTION
arXiv:2310.00754v1  [cs.LG]  1 Oct 2023
Large Vision-Language Models (LVLMs) have made significant progress in understanding real-
world images, showing potential towards achieving general artificial intelligence (Liu et al., 2023d;
Zhu et al., 2023; Ye et al., 2023; Li et al., 2023a; Maaz et al., 2023; Gong et al., 2023). Although
LVLMs have demonstrated their versatility and linguistic fluency, they often suffer from object hal-
lucination in their generated text outputs (Wang et al., 2023a; Liu et al., 2023a; Gunjal et al., 2023).
Object hallucination refers to the phenomenon of generating inaccurate descriptions for a given im-
age, including non-existent objects or omitting essential features. The issue with hallucinatory text
generation in LVLMs is that it can mislead and deceive users in downstream applications that depend
on these captions or descriptions, ultimately resulting in a negative impact on various fields that em-
ploy LVLMs, including robotics (Mai et al., 2023; Liu et al., 2023b), medical imaging (Wang et al.,
2023b; Hu et al., 2023), and human-computer interaction (Olson et al., 1994; Brie et al., 2023).
Early works have attempted to address the problem of object hallucinations in small-scale mul-
timodal pre-trained models by performing either fine-grained alignment across different modali-
ties (Biten et al., 2022) or reducing object co-occurrence patterns with data augmentation (Rohrbach
et al., 2018; Kim et al., 2023). However, the auto-regressive architecture of LVLMs differs signifi-
cantly from small-scale multimodal pre-trained models, making their direct utilization impractical.
A few recent works (Li et al., 2023c; Liu et al., 2023a;d) have studied to reduce object hallucina-
tions in LVLMs by enhancing the quality of datasets used for fine-tuning. Yet, acquiring a substantial
number of high-quality examples for fine-tuning can be time-consuming and labor-intensive, requir-
ing human expertise and effort. Instead, we aim to propose a lightweight method to post-hoc handle
object hallucination by introducing LURE: LVLM hallcUination REvisor.
Concretely, LURE is grounded in a rigorous statistical analysis that elucidates the underlying causal-
ities of object hallucinations in LVLMs. This analysis delves into the relationship between the
∗Equal contribution. Work was done during Yiyang Zhou and Chenhang Cui’s remote internship at UNC.
1
"
"2310.00783","David Balaban","David Balaban, Justin Medich, Pranay Gosar, Justin Hart","Propagating Semantic Labels in Video Data","","","","","cs.CV","http://creativecommons.org/licenses/by/4.0/","  Semantic Segmentation combines two sub-tasks: the identification of
pixel-level image masks and the application of semantic labels to those masks.
Recently, so-called Foundation Models have been introduced; general models
trained on very large datasets which can be specialized and applied to more
specific tasks. One such model, the Segment Anything Model (SAM), performs
image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN
are trained on datasets of paired segments and semantic labels. Manual labeling
of custom data, however, is time-consuming. This work presents a method for
performing segmentation for objects in video. Once an object has been found in
a frame of video, the segment can then be propagated to future frames; thus
reducing manual annotation effort. The method works by combining SAM with
Structure from Motion (SfM). The video input to the system is first
reconstructed into 3D geometry using SfM. A frame of video is then segmented
using SAM. Segments identified by SAM are then projected onto the the
reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry
is reprojected into the new perspective, allowing SAM to be invoked fewer
times. System performance is evaluated, including the contributions of the SAM
and SfM components. Performance is evaluated over three main metrics:
computation time, mask IOU with manual labels, and the number of tracking
losses. Results demonstrate that the system has substantial computation time
improvements over human performance for tracking objects over video frames, but
suffers in performance.
","2023-10-03","2310.00783v1.pdf","Propagating Semantic Labels in Video Data
David Balaban, Justin Medich, Pranay Gosar, and Justin Hart1
Fig. 1: Semantic Label Propagation Overview
Abstract— Semantic Segmentation combines two sub-tasks:
the identification of pixel-level image masks and the application
of semantic labels to those masks. Recently, so-called Foun-
dation Models have been introduced; general models trained
on very large datasets which can be specialized and applied
to more specific tasks. One such model, the Segment Any-
thing Model (SAM), performs image segmentation. Semantic
segmentation systems such as CLIPSeg and MaskRCNN are
trained on datasets of paired segments and semantic labels.
Manual labeling of custom data, however, is time-consuming.
This work presents a method for performing segmentation for
objects in video. Once an object has been found in a frame of
video, the segment can then be propagated to future frames;
thus reducing manual annotation effort. The method works by
combining SAM with Structure from Motion (SfM). The video
input to the system is first reconstructed into 3D geometry
using SfM. A frame of video is then segmented using SAM.
Segments identified by SAM are then projected onto the the
reconstructed 3D geometry. In subsequent video frames, the
labeled 3D geometry is reprojected into the new perspective,
allowing SAM to be invoked fewer times. System performance
is evaluated, including the contributions of the SAM and SfM
components. Performance is evaluated over three main metrics:
computation time, mask IOU with manual labels, and the
number of tracking losses. Results demonstrate that the system
has substantial computation time improvements over human
performance for tracking objects over video frames, but suffers
in performance.
frame [3]. The volunteer work is spot checked by the authors.
This data will be made publicly-available upon publication
of this work, at the Texas Robotics Dataverse: https:
//dataverse.tdl.org/dataverse/robotics
To demonstrate the performance contribution of each
component of the semantic label propagation pipeline, four
variants are presented which vary how prompts are selected
as input to SAM:
arXiv:2310.00783v1  [cs.CV]  1 Oct 2023
1) SAM-only-1.0, k-best prompts
2) SAM-only-2.0, hill-climb for best prompt
3) SfM-SAM-1.0, k-best SfM candidate prompts
4) SfM-SAM-2.0, k-random SfM candidate prompts
These variants are fully described in Section III.
The novel contributions of this work are the use of SfM
to propagate geometry into new frames, as described in
Section III-D.4; enhancements of the use of feature embed-
dings provided by SAM, as described in Section III-C; a
direct comparison of automatic labeling of video to volunteer
manual labeling, as described in Section IV.
II. RELATED WORK
I. INTRODUCTION
This work presents a Semantic Label Propagation (SLP)
system for labeling large sets of images derived from video;
providing a dataset suitable for training or fine-tuning models
for computer vision tasks with minimal manual labeling. The
system takes as input video from a moving camera in a static
scene and outputs a set of objects, each object corresponds
to a set of binary masks for each frame it is identified in.
Figure 1 diagrams the flow of data through of the proposed
system. The triangle on the left represents the “Video Input”
to the system. The hexagon at the top represents the output,
“Objects Labeled Across Frames.” Ellipses represent data,
while squares represent sub-processes. This system begins
with a Structure from Motion (SfM) pipeline to gain an
object mesh [1]. Then, Segment Anything Model (SAM) is
used to identify the distinct objects in the mesh [2]. These
objects are then tracked through all frames of video utilizing
the object mesh to enforce geometric consistency while SAM
associates previously unseen polygons to tracked objects.
To evaluate system performance, ground truth semantic la-
bels are collected from five videos. Volunteers use the open-
source PixelAnnotationTool to manually label objects in
Tasks such as Object Detection [4][5], Image Caption-
ing [6][7], and Semantic Segmentation [8][9] are approaches
to image labeling for the purposes of scene understand-
ing [10][11]. Object Detection localizes a number of desired
objects within the image frame, typically with a bounding
box for each object [4][5]. Image Captioning gives a general
description of the image, often using a text prefix provided
to a large language model [6][7]. Semantic Segmentation
provides pixel-level annotations of the image [8][9].
Pre-trained models have been released for people to fine-
tune with custom data [12]–[14], which can require an ardu-
ous amount of manual labeling. There has been a significant
1Departent of Computer Science, The University of Texas at Austin,
Austin, Texas, USA 78712 {dbalaban, jmedich, pgosar,
hart}@cs.utexas.edu
"
"2310.00785","Yapei Chang","Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer","BooookScore: A systematic exploration of book-length summarization in
  the era of LLMs","","","","","cs.CL cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Summarizing book-length documents (>100K tokens) that exceed the context
window size of large language models (LLMs) requires first breaking the input
document into smaller chunks and then prompting an LLM to merge, update, and
compress chunk-level summaries. Despite the complexity and importance of this
task, it has yet to be meaningfully studied due to the challenges of
evaluation: existing book-length summarization datasets (e.g., BookSum) are in
the pretraining data of most public LLMs, and existing evaluation methods
struggle to capture errors made by modern LLM summarizers. In this paper, we
present the first study of the coherence of LLM-based book-length summarizers
implemented via two prompting workflows: (1) hierarchically merging chunk-level
summaries, and (2) incrementally updating a running summary. We obtain 1193
fine-grained human annotations on GPT-4 generated summaries of 100
recently-published books and identify eight common types of coherence errors
made by LLMs. Because human evaluation is expensive and time-consuming, we
develop an automatic metric, BooookScore, that measures the proportion of
sentences in a summary that do not contain any of the identified error types.
BooookScore has high agreement with human annotations and allows us to
systematically evaluate the impact of many other critical parameters (e.g.,
chunk size, base LLM) while saving $15K and 500 hours in human evaluation
costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce
summaries with higher BooookScore than the oft-repetitive ones generated by
LLaMA 2. Incremental updating yields lower BooookScore but higher level of
detail than hierarchical merging, a trade-off sometimes preferred by human
annotators. We release code and annotations after blind review to spur more
principled research on book-length summarization.
","2023-10-09","2310.00785v1.pdf","BOOOOKSCORE:
A SYSTEMATIC EXPLORATION OF BOOK-LENGTH
SUMMARIZATION IN THE ERA OF LLMS
Kyle Lo
Allen Institute for AI
kylel@allenai.org
Yapei Chang
University of Massachusetts Amherst
yapeichang@umass.edu
Tanya Goyal
Princeton University
tanyagoyal@princeton.edu
Mohit Iyyer
University of Massachusetts Amherst
miyyer@cs.umass.edu
ABSTRACT
arXiv:2310.00785v1  [cs.CL]  1 Oct 2023
Summarizing book-length documents (>100K tokens) that exceed the context
window size of large language models (LLMs) requires first breaking the input
document into smaller chunks and then prompting an LLM to merge, update, and
compress chunk-level summaries. Despite the complexity and importance of this
task, it has yet to be meaningfully studied due to the challenges of evaluation: ex-
isting book-length summarization datasets (e.g., BookSum) are in the pretraining
data of most public LLMs, and existing evaluation methods struggle to capture er-
rors made by modern LLM summarizers. In this paper, we present the first study
of the coherence of LLM-based book-length summarizers implemented via two
prompting workflows: (1) hierarchically merging chunk-level summaries, and (2)
incrementally updating a running summary. We obtain 1193 fine-grained human
annotations on GPT-4 generated summaries of 100 recently-published books and
identify eight common types of coherence errors made by LLMs. Because hu-
man evaluation is expensive and time-consuming, we develop an automatic met-
ric, BOOOOKSCORE, that measures the proportion of sentences in a summary that
do not contain any of the identified error types. BOOOOKSCORE has high agree-
ment with human annotations and allows us to systematically evaluate the impact
of many other critical parameters (e.g., chunk size, base LLM) while saving $15K
and 500 hours in human evaluation costs. We find that closed-source LLMs such
as GPT-4 and Claude 2 produce summaries with higher BOOOOKSCORE than the
oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower
BOOOOKSCORE but higher level of detail than hierarchical merging, a trade-off
sometimes preferred by human annotators. We release our code and annotations
to spur more principled research on book-length summarization1.
1
INTRODUCTION
Just two years ago, automatically-generated summaries were riddled with artifacts such as grammar
errors, repetition, and hallucination (Zhao et al., 2020; Fabbri et al., 2020; Goyal & Durrett, 2021).
Nowadays, such artifacts have mostly disappeared; in fact, Pu et al. (2023b) find that summaries
generated by large language models (LLMs) are preferred over those written by humans, leading
them to pronounce the death of summarization research. However, as with most prior work on
summarization, the input documents in their study are relatively short (<10K tokens). Widespread
adoption of LLMs outside the research community has driven the development of a more ambitious
task: summarizing book-length documents, which we define to be texts longer than 100K tokens.
1https://github.com/lilakk/BooookScore
1
"
"2310.00789","Soumajyoti Sarkar Mr.","Soumajyoti Sarkar, Leonard Lausen","Testing the Limits of Unified Sequence to Sequence LLM Pretraining on
  Diverse Table Data Tasks","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  Tables stored in databases and tables which are present in web pages and
articles account for a large part of semi-structured data that is available on
the internet. It then becomes pertinent to develop a modeling approach with
large language models (LLMs) that can be used to solve diverse table tasks such
as semantic parsing, question answering as well as classification problems.
Traditionally, there existed separate models specialized for each task
individually. It raises the question of how far can we go to build a unified
model that works well on some table tasks without significant degradation on
others. To that end, we attempt at creating a shared modeling approach in the
pretraining stage with encoder-decoder style LLMs that can cater to diverse
tasks. We evaluate our approach that continually pretrains and finetunes
different model families of T5 with data from tables and surrounding context,
on these downstream tasks at different model scales. Through multiple ablation
studies, we observe that our pretraining with self-supervised objectives can
significantly boost the performance of the models on these tasks. As an example
of one improvement, we observe that the instruction finetuned public models
which come specialized on text question answering (QA) and have been trained on
table data still have room for improvement when it comes to table specific QA.
Our work is the first attempt at studying the advantages of a unified approach
to table specific pretraining when scaled from 770M to 11B sequence to sequence
models while also comparing the instruction finetuned variants of the models.
","2023-10-03","2310.00789v1.pdf","Testing the Limits of Uniﬁed Sequence to Sequence LLM Pretraining on Diverse
Table Data Tasks
Soumajyoti Sarkar, Leonard Lausen
Amazon Web Services AI, US
{soumajs, lausen}@amazon.com
Abstract
Wang and Sun 2022;
Du et al. 2022;
Wydma´nski, Bulenok, and ´Smieja 2023).
They
use
gradient descent-based end-to-end learning with task-
speciﬁc model pretraining and ﬁne-tuning.
2. The second group of studies aim to specialize LLMs
on table data to retrieve task-agnostic table/column/row
representations for
different
downstream
table
un-
derstanding
tasks.
Drawing
inspiration
from
mod-
els
like
BERT
(Devlin et al. 2019),
these
studies
(Herzig et al. 2020;
Yin et al. 2020a;
Deng et al. 2020;
Iida et al. 2021) serialize tables to a sequence of tokens
and train them on textual self-supervised objectives.
3. Yet, there is a third group of studies that aim to use
the generative nature of language models for data-to-
text generation tasks where the nature of data entails
some form of table and/or related text and the output
text generally corresponds to text from the table or an
output like SQL (Andrejczuk et al. 2022; Shi et al. 2022;
Parikh et al. 2020; Liu et al. 2021).
Tables stored in databases and tables which are present in web
pages and articles account for a large part of semi-structured
data that is available on the internet. It then becomes pertinent
to develop a modeling approach with large language mod-
els (LLMs) that can be used to solve diverse table tasks such
as semantic parsing, question answering as well as classiﬁ-
cation problems. Traditionally, there existed separate models
specialized for each task individually. It raises the question of
how far can we go to build a uniﬁed model that works well on
some table tasks without signiﬁcant degradation on others. To
that end, we attempt at creating a shared modeling approach
in the pretraining stage with encoder-decoder style LLMs that
can cater to diverse tasks. We evaluate our approach that con-
tinually pretrains and ﬁnetunes different model families of
T5 with data from tables and surrounding context, on these
downstream tasks at different model scales. Through multi-
ple ablation studies, we observe that our pretraining with self-
supervised objectives can signiﬁcantly boost the performance
of the models on these tasks. As an example of one improve-
ment, we observe that the instruction ﬁnetuned public models
which come specialized on text question answering (QA) and
have been trained on table data still have room for improve-
ment when it comes to table speciﬁc QA. Our work is the
ﬁrst attempt at studying the advantages of a uniﬁed approach
to table speciﬁc pretraining when scaled from 770M to 11B
sequence to sequence models while also comparing the in-
struction ﬁnetuned variants of the models.
arXiv:2310.00789v1  [cs.CL]  1 Oct 2023
Introduction
In
recent
years,
there
has
been
an
emerging
and
quite substantial interest in specializing these LLMs
(Chowdhery et al. 2022; Wang et al. 2021a) on tasks with
semi-structured data, for example knowledge graphs,
spreadsheets, relational databases and tables which are of
interest in this paper. Notably, there have been three streams
of work that have been gaining traction in the realm of LLM
pretraining for table speciﬁc tasks:
Putting the above into the landscape of work that has been
popular, the goal of our study is to conduct self-supervised
pre-training with table speciﬁc data along with text related
to the tables as input, followed by pre-ﬁnetuning for im-
proving generalization to diverse tasks. While it is clear
that there has been disparate studies with tabular data, some
aiming at Question Answering (QA) tasks, some at getting
row/column/table representations for semantic search and
some at more complex reasoning tasks, what has not been
done so far is to meaningfully conduct a methodological
study of large scale pretraining with table data that can be
either ﬁnetuned for speciﬁc downstream table tasks or help
in scenarios where there is scarcity of data for a speciﬁc
task. Speciﬁcally, the last group of studies in our above cat-
egorization focus on pretraining that can equip the models
for end-end generation abilities suitable for table semantic
parsing, table-based QA, table summarization among other
tasks. However, pretraining with specialized objectives in
the absence of large scale data and scale of models, remain
key to transfer learning over diverse downstream tasks. We
revisit several questions around table based LLM pretraining
that have been studied before, albeit on a much smaller scale
than the data used in this study. We attempt at answering the
following:
1. The ﬁrst group of studies focus on predicting la-
bels (essentially one column of a table) for clas-
siﬁcation and regression scenarios, using row val-
ues and column schema (from the other columns)
as
input
(Huang et al. 2020;
Arik and Pﬁster 2021;
Somepalli et al. 2021;
Gorishniy et al. 2021);
(Grinsztajn, Oyallon, and Varoquaux 2022;
"
"2310.00809","Jiaqi Zhang","Jiaqi Zhang, Joel Jennings, Cheng Zhang, Chao Ma","Towards Causal Foundation Model: on Duality between Causal Inference and
  Attention","","","","","cs.LG cs.AI stat.ME stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Foundation models have brought changes to the landscape of machine learning,
demonstrating sparks of human-level intelligence across a diverse array of
tasks. However, a gap persists in complex tasks such as causal inference,
primarily due to challenges associated with intricate reasoning steps and high
numerical precision requirements. In this work, we take a first step towards
building causally-aware foundation models for complex tasks. We propose a
novel, theoretically sound method called Causal Inference with Attention
(CInA), which utilizes multiple unlabeled datasets to perform self-supervised
causal learning, and subsequently enables zero-shot causal inference on unseen
tasks with new data. This is based on our theoretical results that demonstrate
the primal-dual connection between optimal covariate balancing and
self-attention, facilitating zero-shot causal inference through the final layer
of a trained transformer-type architecture. We demonstrate empirically that our
approach CInA effectively generalizes to out-of-distribution datasets and
various real-world datasets, matching or even surpassing traditional
per-dataset causal inference methodologies.
","2023-10-03","2310.00809v1.pdf","Towards Causal Foundation Model: on Duality between Causal
Inference and Attention
Jiaqi Zhang2,*, Joel Jennings1, Cheng Zhang1, and Chao Ma1,*
1Microsoft Research Cambridge
2Massachusetts Institute of Technology
*Equal contributions
September 29, 2023
Abstract
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks
of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks
such as causal inference, primarily due to challenges associated with intricate reasoning steps and high
numerical precision requirements.
In this work, we take a first step towards building causally-aware
foundation models for complex tasks. We propose a novel, theoretically sound method called Causal
Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised
causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data.
This is based on our theoretical results that demonstrate the primal-dual connection between optimal
covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a
trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively
generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing
traditional per-dataset causal inference methodologies.
1
Introduction
arXiv:2310.00809v1  [cs.LG]  1 Oct 2023
Recent advances in artificial intelligence have created a paradigm shift in which models are trained on large
amounts of data and can be adapted to different tasks, dubbed foundation models (Bommasani et al., 2021).
These models, which often employ self-supervision, can extract valuable knowledge from various types of
data, including natural language (Devlin et al., 2018; Brown et al., 2020), images (Radford et al., 2021),
and biological sequencing counts (Theodoris et al., 2023). This acquired knowledge allows the model to
generalize when asked to perform tasks in novel scenarios. With vast amounts of data becoming increasingly
available from diverse sources, such models are of interest to leverage information that can be learned in
order to build more intelligent systems (Bubeck et al., 2023).
A critical aspect of intelligent systems is the ability to reason about cause-and-effect relationships (Zhang
et al., 2023), which is vital to making informed decisions across various domains, including healthcare,
economics, and statistics (Kube et al., 2019; Geffner et al., 2022; Zhang et al., 2022). Relying solely on
correlation-based models (Harrison and March, 1984) can lead to misleading conclusions, as they do not
account for the underlying causal mechanisms. This limitation is also observed in the realm of foundation
models (Bubeck et al., 2023; Mahowald et al., 2023; Wolfram, 2023). To address this gap, it is crucial to
viczhang@mit.edu, {joeljennings,cheng.zhang,chao.ma}@microsoft.com. This work is done when Jiaqi Zhang was an
intern at Microsoft Research.
1
"
"2310.00811","Liyuan Liu","Liyuan Liu and Jianfeng Gao and Weizhu Chen","Sparse Backpropagation for MoE Training","Work in progress","","","","cs.LG cs.AI cs.CL cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  One defining characteristic of Mixture-of-Expert (MoE) models is their
capacity for conducting sparse computation via expert routing, leading to
remarkable scalability. However, backpropagation, the cornerstone of deep
learning, requires dense computation, thereby posting challenges in MoE
gradient computations. Here, we introduce SparseMixer, a scalable gradient
estimator that bridges the gap between backpropagation and sparse expert
routing. Unlike typical MoE training which strategically neglects certain
gradient terms for the sake of sparse computation and scalability, SparseMixer
provides scalable gradient approximations for these terms, enabling reliable
gradient estimation in MoE training. Grounded in a numerical ODE framework,
SparseMixer harnesses the mid-point method, a second-order ODE solver, to
deliver precise gradient approximations with negligible computational overhead.
Applying SparseMixer to Switch Transformer on both pre-training and machine
translation tasks, SparseMixer showcases considerable performance gain,
accelerating training convergence up to 2 times.
","2023-10-03","2310.00811v1.pdf","SPARSE BACKPROPAGATION FOR MOE TRAINING
Liyuan Liu§
Jianfeng Gao§
Weizhu Chen‡
§Microsoft Research
‡Microsoft Azure AI
{lucliu, jfgao, wzchen}@microsoft.com
ABSTRACT
One defining characteristic of Mixture-of-Expert (MoE) models is their capac-
ity for conducting sparse computation via expert routing, leading to remarkable
scalability. However, backpropagation, the cornerstone of deep learning, requires
dense computation, thereby posting challenges in MoE gradient computations.
Here, we introduce SparseMixer, a scalable gradient estimator that bridges the gap
between backpropagation and sparse expert routing. Unlike typical MoE training
which strategically neglects certain gradient terms for the sake of sparse com-
putation and scalability, SparseMixer provides scalable gradient approximations
for these terms, enabling reliable gradient estimation in MoE training. Grounded
in a numerical ODE framework, SparseMixer harnesses the mid-point method, a
second-order ODE solver, to deliver precise gradient approximations with negli-
gible computational overhead. Applying SparseMixer to Switch Transformer on
both pre-training and machine translation tasks, SparseMixer showcases consid-
erable performance gain, accelerating training convergence up to 2 times1.
1
INTRODUCTION
The significant success of large-scale pre-training across various applications has underscored the
imperative need for scalable models that are economically feasible (Chowdhery et al., 2022; Ope-
nAI, 2023; Touvron et al., 2023). Recent advances in sparsely activated networks, prominently
known as Mixture-of-Experts (MoE), have attracted widespread interest (Shazeer et al., 2017; Lep-
ikhin et al., 2020; Fedus et al., 2021; Riquelme et al., 2021; Mustafa et al., 2022). Unlike traditional
networks that densely activate all modules for all input, MoE selectively activates parts of modules to
specific inputs through a process called expert routing, leading to notable efficiency enhancements.
However, such efficiency gain comes at a cost: gradient estimation in MoE becomes challenging
due to expert routing. Specifically, the routing function, being discrete in nature, produces non-
differentiable outputs. Meanwhile, backpropagation, the cornerstone of deep learning, relies on the
Chain rule, making it exclusively compatible with differentiable functions (Rosenblatt, 1957; Bengio
et al., 2013), and cannot be directly applied for gradient computation of expert routing.
arXiv:2310.00811v1  [cs.LG]  1 Oct 2023
Numerous methods have emerged to bridge discrete and back-propagation, and most of them are
based on Straight-Through (ST) (Rosenblatt, 1957; Bengio et al., 2013; Jang et al., 2017; Liu et al.,
2023). Unfortunately, all existing ST estimators are incompatible with MoE, since they require
activating all experts for gradient computing, thereby eliminating all the efficiency improvements
of MoE. Consequently, typical MoE training strategically neglects the gradient computation for
routing, trading certain training signals for sparse computation. Despite the scalability brought by
sparse computation, this trade-off may result in slow convergence and improperly trained models.
Our solution to this quandary is SparseMixer—a novel approach designed to reconcile the divide
between sparse MoE routing and backpropagation. Drawing inspiration from numerical methods
for ordinary differential equations (ODE), SparseMixer provides reliable gradient approximation
for expert routing, even when only a subset of experts are activated. Moreover, to furnish accurate
gradient approximations with negligible computation overheads, we integrate the mid-point method,
a second-order numerical ODE solver, which matches the Taylor expansion of the gradient to the
second order without requiring the Hessian matrix or other second-order derivatives.
1Implementations are available at https://github.com/microsoft/SparseMixer/.
1
"
"2310.00815","Yunjia Zhang","Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen
  Deep, Jignesh M. Patel","ReAcTable: Enhancing ReAct for Table Question Answering","","","","","cs.DB","http://creativecommons.org/licenses/by/4.0/","  Table Question Answering (TQA) presents a substantial challenge at the
intersection of natural language processing and data analytics. This task
involves answering natural language (NL) questions on top of tabular data,
demanding proficiency in logical reasoning, understanding of data semantics,
and fundamental analytical capabilities. Due to its significance, a substantial
volume of research has been dedicated to exploring a wide range of strategies
aimed at tackling this challenge including approaches that leverage Large
Language Models (LLMs) through in-context learning or Chain-of-Thought (CoT)
prompting as well as approaches that train and fine-tune custom models.
  Nonetheless, a conspicuous gap exists in the research landscape, where there
is limited exploration of how innovative foundational research, which
integrates incremental reasoning with external tools in the context of LLMs, as
exemplified by the ReAct paradigm, could potentially bring advantages to the
TQA task. In this paper, we aim to fill this gap, by introducing ReAcTable
(ReAct for Table Question Answering tasks), a framework inspired by the ReAct
paradigm that is carefully enhanced to address the challenges uniquely
appearing in TQA tasks such as interpreting complex data semantics, dealing
with errors generated by inconsistent data and generating intricate data
transformations. ReAcTable relies on external tools such as SQL and Python code
executors, to progressively enhance the data by generating intermediate data
representations, ultimately transforming it into a more accessible format for
answering the questions with greater ease. We demonstrate that ReAcTable
achieves remarkable performance even when compared to fine-tuned approaches. In
particular, it outperforms the best prior result on the WikiTQ benchmark,
achieving an accuracy of 68.0% without requiring training a new model or
fine-tuning.
","2023-10-03","2310.00815v1.pdf","ReAcTable: Enhancing ReAct for Table ￿estion Answering
Yunjia Zhang
Jordan Henkel
Avrilia Floratou
University of Wisconsin-Madison
Microsoft
Microsoft
yunjia@cs.wisc.edu
jordan.henkel@microsoft.com
av￿or@microsoft.com
Joyce Cahoon
Shaleen Deep
Jignesh M. Patel∗
Microsoft
Microsoft
Carnegie Mellon University
jcahoon@microsoft.com
shaleen.deep@microsoft.com
jignesh@cmu.edu
ABSTRACT
PVLDB Artifact Availability:
The source code, data, and/or other artifacts have been made available at
https://github.com/yunjiazhang/ReAcTable.git.
1
INTRODUCTION
Table Question Answering (TQA) presents a substantial challenge
at the intersection of natural language processing and data analyt-
ics. This task involves answering natural language (NL) questions
on top of tabular data, demanding pro￿ciency in logical reasoning,
understanding of data semantics, and fundamental analytical ca-
pabilities. Due to its signi￿cance, a substantial volume of research
has been dedicated to exploring a wide range of strategies aimed at
tackling this challenge including approaches that leverage Large
Language Models (LLMs) through in-context learning or Chain-
of-Thought (CoT) prompting as well as approaches that train and
￿ne-tune custom models.
Nonetheless, a conspicuous gap exists in the research landscape,
Table question answering (TQA) [16] is a sub￿eld of natural lan-
guage processing (NLP) and information retrieval that focuses on
answering natural language (NL) questions over tabular data such
as Wikipedia tables, spreadsheets or relational tables. It constitutes
a complex task that demands a fusion of contextual understanding,
logical reasoning and analytical skills. TQA allows users without
expertise in querying languages and data analytics to interact with
their data using plain language and gain valuable insights. It is a
vital tool that can enhance data accessibility, usability, and decision
support across various domains, ultimately leading to more e￿cient
and informed decision-making processes.
Recognizing its signi￿cance, extensive research e￿orts have been
dedicated to devising e￿ective strategies for TQA. These strategies
can be broadly classi￿ed into two categories. In the ￿rst category,
approaches such as Tapas [12], Tapex [23], Tacube [57], and Om-
niTab [15] involve the training or ￿ne-tuning of specialized models
tailored for the task. The second category capitalizes on recent
advancements in Large Language Models (LLMs). Within this cate-
gory, works like [5, 26, 50] harness LLMs to generate code capable
of manipulating tabular data.
The emergence of Chain-of-Thought (CoT) prompting, which en-
where there is limited exploration of how innovative foundational
research, which integrates incremental reasoning with external
tools in the context of LLMs, as exempli￿ed by the ReAct para-
digm, could potentially bring advantages to the TQA task. In this
paper, we aim to ￿ll this gap, by introducing ReAcTable (ReAct
for Table Question Answering tasks), a framework inspired by the
ReAct paradigm that is carefully enhanced to address the challenges
uniquely appearing in TQA tasks such as interpreting complex data
semantics, dealing with errors generated by inconsistent data and
generating intricate data transformations. ReAcTable relies on exter-
nal tools such as SQL and Python code executors, to progressively
enhance the data by generating intermediate data representations,
ultimately transforming it into a more accessible format for an-
swering the user’s questions with greater ease. Through extensive
empirical evaluations using three popular TQA benchmarks, we
demonstrate that ReAcTable achieves remarkable performance even
when compared to ￿ne-tuned approaches. In particular, it outper-
forms the best prior result on the WikiTQ benchmark, achieving
an accuracy of 68.0% without requiring training a new model or
￿ne-tuning.
PVLDB Reference Format:
Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon, Shaleen
Deep, and Jignesh M. Patel. ReAcTable: Enhancing ReAct for Table
Question Answering . PVLDB, 14(1): XXX-XXX, 2020.
doi:XX.XX/XXX.XX
courages a model to engage in step-by-step reasoning, has brought
about a signi￿cant transformation in the utilization of Large Lan-
guage Models (LLMs) for intricate multi-step tasks. Expanding the
CoT ideas, the ReAct paradigm [49] has been introduced, enabling
interactions between the model and external tools in an interleaved
manner. This allows for greater synergy between reasoning and
acting and facilitates real-time guidance and corrections during
task execution. These innovative strategies aim to address the lim-
itations of traditional few-shot prompting methods [2]. Despite
the promising results demonstrated by combining reasoning with
external tools, to the best of our knowledge, the ReAct paradigm
has not yet been applied to the TQA task.
This paper bridges this gap by investigating how the principles
∗Work done while at U. Wisconsin.
This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX
behind the ReAct framework, i.e. CoT and availability of external
tools, can be applied to the TQA task. Beyond the anticipated dif-
￿culty of accurately comprehending the user’s natural language
query, the TQA task poses a series of distinct challenges, including:
(i) interpreting potentially intricate data semantics, (ii) the presence
of noisy or inconsistent data, and (iii) the necessity for complex
"
"2310.00819","Ziqi Wang","Tianci Xue, Ziqi Wang, Heng Ji","Parameter-Efficient Tuning Helps Language Model Alignment","21 pages, 11 figures, 5 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Aligning large language models (LLMs) with human preferences is essential for
safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF)
and direct preference optimization (DPO) with human feedback for alignment.
Nevertheless, they have certain drawbacks. One such limitation is that they can
only align models with one preference at the training time (e.g., they cannot
learn to generate concise responses when the preference data prefers detailed
responses), or have certain constraints for the data format (e.g., DPO only
supports pairwise preference data). To this end, prior works incorporate
controllable generations for alignment to make language models learn multiple
preferences and provide outputs with different preferences during inference if
asked. Controllable generation also offers more flexibility with regard to data
format (e.g., it supports pointwise preference data). Specifically, it uses
different control tokens for different preferences during training and
inference, making LLMs behave differently when required. Current controllable
generation methods either use a special token or hand-crafted prompts as
control tokens, and optimize them together with LLMs. As control tokens are
typically much lighter than LLMs, this optimization strategy may not
effectively optimize control tokens. To this end, we first use
parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to
optimize control tokens and then fine-tune models for controllable generations,
similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning
(MEET), improves the quality of control tokens, thus improving controllable
generation quality consistently by an apparent margin on two well-recognized
datasets compared with prior works.
","2023-10-03","2310.00819v1.pdf","Preprint
PARAMETER-EFFICIENT TUNING HELPS LANGUAGE
MODEL ALIGNMENT
Tianci Xue1∗, Ziqi Wang2, Heng Ji2
1 Nanjing University, 2 University of Illinois Urbana-Champaign
xuetianci@smail.nju.edu.cn, {ziqiw9,hengji}@illinois.edu
ABSTRACT
Aligning large language models (LLMs) with human preferences is essential for
safe and useful LLMs. Previous works mainly adopt reinforcement learning
(RLHF) and direct preference optimization (DPO) with human feedback for align-
ment. Nevertheless, they have certain drawbacks. One such limitation is that they
can only align models with one preference at the training time (e.g., they cannot
learn to generate concise responses when the preference data prefers detailed re-
sponses), or have certain constraints for the data format (e.g., DPO only supports
pairwise preference data). To this end, prior works incorporate controllable gen-
erations for alignment to make language models learn multiple preferences and
provide outputs with different preferences during inference if asked. Controllable
generation also offers more flexibility with regard to data format (e.g., it supports
pointwise preference data). Specifically, it uses different control tokens for different
preferences during training and inference, making LLMs behave differently when
required. Current controllable generation methods either use a special token or
hand-crafted prompts as control tokens, and optimize them together with LLMs. As
control tokens are typically much lighter than LLMs, this optimization strategy may
not effectively optimize control tokens. To this end, we first use parameter-efficient
tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens
and then fine-tune models for controllable generations, similar to prior works. Our
approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the qual-
ity of control tokens, thus improving controllable generation quality consistently
by an apparent margin on two well-recognized datasets compared with prior works.
1
INTRODUCTION
arXiv:2310.00819v1  [cs.CL]  1 Oct 2023
Large language models (LLMs) (Anil et al., 2023; Schulman et al., 2022; OpenAI, 2023; Touvron
et al., 2023; Zeng et al., 2023) have excelled in numerous tasks such as causal reasoning (Kıcıman
et al., 2023), math reasoning (Wei et al., 2022; Xue et al., 2023), and conversations (Bai et al.,
2022). The key foundation of such a success is aligning language models with human preferences
(Ouyang et al., 2022). The widely adopted method is reinforcement learning with human feedback
(RLHF)(Ouyang et al., 2022), which uses pairwise human-preference data to train a reward model
and optimizes language models to achieve high rewards. Though effective, RLHF suffers from the
imperfect reward models (Liu et al., 2023a), instability of reinforcement learning (Casper et al., 2023),
inefficiency (Dong et al., 2023) and complicated implementations. Researchers have proposed meth-
ods to replace RLHF while maintaining the alignment performance. Direct preference optimizations
(DPO) (Rafailov et al., 2023) reformulates RLHF to a loss objective and theoretically prove the loss
objective is identical to RLHF.
Nonetheless, RLHF, or DPO, have certain drawbacks. First, they can only align language models
to one preference during training. For example, if the preference data prefers detailed responses to
concise responses or prefers formal responses to informal responses, aligned language models will
make it hard to output concise responses or informal responses. Besides, RLHF is complicated to
optimize and DPO can only utilize the pairwise preference data, and cannot be applied if data is given
with a pointwise score instead. To make LLMs learn multiple preferences during training and provide
∗Word done when interning at UIUC
1
"
"2310.00832","Shuo Wang","Shuo Wang and Carlos Crespo-Quinones","Natural Language Models for Data Visualization Utilizing nvBench Dataset","6 pages, 7 figures","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Translation of natural language into syntactically correct commands for data
visualization is an important application of natural language models and could
be leveraged to many different tasks. A closely related effort is the task of
translating natural languages into SQL queries, which in turn could be
translated into visualization with additional information from the natural
language query supplied\cite{Zhong:2017qr}. Contributing to the progress in
this area of research, we built natural language translation models to
construct simplified versions of data and visualization queries in a language
called Vega Zero. In this paper, we explore the design and performance of these
sequence to sequence transformer based machine learning model architectures
using large language models such as BERT as encoders to predict visualization
commands from natural language queries, as well as apply available T5 sequence
to sequence models to the problem for comparison.
","2023-10-03","2310.00832v1.pdf","Natural Language Models for Data Visualization
Utilizing nvBench Dataset
Shuo Wang and Carlos Crespo-Quinones
DATASCI 266: Natural Language Processing
UC Berkeley School of Information
{shuo.wang, carlos.d.crespo}@ischool.berkeley.edu
Abstract
Translation of natural language into syntactically correct commands for data visualization is an important application of
natural language models and could be leveraged to many different tasks. A closely related effort is the task of translating
natural languages into SQL queries, which in turn could be translated into visualization with additional information from
the natural language query supplied[1]. Contributing to the progress in this area of research, we built natural language
translation models to construct simplified versions of data and visualization queries in a language called Vega Zero first
proposed by Luo, Yuyu, et al[2]. In this paper, we explore the design and performance of these sequence to sequence
transformer based machine learning model architectures using large language models such as BERT as encoders to predict
visualization commands from natural language queries, as well as apply available T5 sequence to sequence models to the
problem for comparison.
Introduction
quence to sequence transformer model). Our goal
is to explore the possibility of a generalized natural
language to visualization process where the ability of
the system to handle natural language inputs is not
limited by the input training data. We have provided
below the main results of our research and detailed
description and analysis of the input data, model
architecture and model performance. From our re-
seach, the BERT encoder based sequence to sequence
transformer model was able to achieve an overall
accuracy of 79%, confirming our hypothesis of the
possibility to leverage transferred learning of BERT
models to data visualization tasks. Applying a pre-
trained CodeT5 model realizes even more impressive
results, achieving an overall accuracy of 97%.
Background
arXiv:2310.00832v1  [cs.CL]  2 Oct 2023
Data visualization is an integral part of data analysis
and reporting, highly skilled professionals spend a
significant portion of their working hours construct-
ing data queries and turning them into charts and
graphs. The potential application of machine learn-
ing models to understand the underlying data and
translate natural language queries into data visu-
alization would greatly enhance the efficiency and
productivity of many data analysis tasks. Not only
would such tools enhance our existing capability to
understand and communicate with our data, it could
also help us uncover previously hidden information.
Some immediate applications include: creating bar,
line and scatter plots of SQL tables, transform and
group data within a table and display the aggregated
data, as well as joining and combining tables of data
to extract information and display them visually.
Previous research in this area has included ex-
plorations of transforming natural language into
SQL commands[1, 3] with deep learning models
and natural language to visualization interfaces[4].
NvBench is a new dataset created to facilitate this
type of research to further integrate the process of
natural language to data query and data query to
visualizations[2] . Leveraging this data set and the
ncNet transformer model architecture[5], we further
explore the various modeling possibilty in our work.
In this paper, we have used the nvBench as our
train, validation and test dataset to create BERT en-
coder based multi-head attention transformer mod-
els and compare the performance of the new models
with the performance of the ncNet model (also a se-
Natural language to visualization research has been
conducted for decades, some of these efforts include
early approaches to manually program rule-based
logic for the handling of anticipated user inputs in
multi-model systems[4] and more recent attempts
to incorporate machine learning and optimization
techniques[6]. However these systems are limited in
functionality by the ability of the system designers
to anticipate the possible universe of inputs from
users. But what if the user uses a word that the
system has never seen? What if the natural language
input was written in an idiomatic way that was not
present in the training data? These limitations could
potentially be addressed with the recent advances
in large language models such as BERT and GPT
"
"2310.00833","Yuki Takezawa","Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, Makoto Yamada","Necessary and Sufficient Watermark for Large Language Models","","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, large language models (LLMs) have achieved remarkable
performances in various NLP tasks. They can generate texts that are
indistinguishable from those written by humans. Such remarkable performance of
LLMs increases their risk of being used for malicious purposes, such as
generating fake news articles. Therefore, it is necessary to develop methods
for distinguishing texts written by LLMs from those written by humans.
Watermarking is one of the most powerful methods for achieving this. Although
existing watermarking methods have successfully detected texts generated by
LLMs, they significantly degrade the quality of the generated texts. In this
study, we propose the Necessary and Sufficient Watermark (NS-Watermark) for
inserting watermarks into generated texts without degrading the text quality.
More specifically, we derive minimum constraints required to be imposed on the
generated texts to distinguish whether LLMs or humans write the texts. Then, we
formulate the NS-Watermark as a constrained optimization problem and propose an
efficient algorithm to solve it. Through the experiments, we demonstrate that
the NS-Watermark can generate more natural texts than existing watermarking
methods and distinguish more accurately between texts written by LLMs and those
written by humans. Especially in machine translation tasks, the NS-Watermark
can outperform the existing watermarking method by up to 30 BLEU scores.
","2023-10-03","2310.00833v1.pdf","Preprint
NECESSARY
AND
SUFFICIENT
WATERMARK
FOR
LARGE LANGUAGE MODELS
Yuki Takezawa1,2, Ryoma Sato1,2, Han Bao1,2, Kenta Niwa3, Makoto Yamada2
1Kyoto University, 2OIST, 3NTT Communication Science Laboratories
ABSTRACT
In recent years, large language models (LLMs) have achieved remarkable perfor-
mances in various NLP tasks. They can generate texts that are indistinguishable
from those written by humans. Such remarkable performance of LLMs increases
their risk of being used for malicious purposes, such as generating fake news ar-
ticles. Therefore, it is necessary to develop methods for distinguishing texts writ-
ten by LLMs from those written by humans. Watermarking is one of the most
powerful methods for achieving this. Although existing watermarking methods
have successfully detected texts generated by LLMs, they significantly degrade
the quality of the generated texts. In this study, we propose the Necessary and
Sufficient Watermark (NS-Watermark) for inserting watermarks into generated
texts without degrading the text quality. More specifically, we derive minimum
constraints required to be imposed on the generated texts to distinguish whether
LLMs or humans write the texts. Then, we formulate the NS-Watermark as a
constrained optimization problem and propose an efficient algorithm to solve it.
Through the experiments, we demonstrate that the NS-Watermark can generate
more natural texts than existing watermarking methods and distinguish more ac-
curately between texts written by LLMs and those written by humans. Especially
in machine translation tasks, the NS-Watermark can outperform the existing wa-
termarking method by up to 30 BLEU scores.
1
INTRODUCTION
arXiv:2310.00833v1  [cs.CL]  2 Oct 2023
Large language models (LLMs) have achieved remarkable performances in a wide range of NLP
tasks, including language generation (Chen et al., 2021), question answering (Joshi et al., 2017;
Kwiatkowski et al., 2019), and reasoning tasks (Bisk et al., 2020; Kojima et al., 2022). Recently,
many pre-trained LLMs have been released (Brown et al., 2020; Chung et al., 2022; Zhang et al.,
2022; Touvron et al., 2023), which can now generate natural and fluent texts that are indistinguish-
able from texts written by humans. For instance, Brown et al. (2020) evaluated the quality of the
news articles generated by GPT-3, demonstrating that humans cannot distinguish between news ar-
ticles generated by GPT-3 and those written by humans.
As the performance of LLMs improves for various tasks, the risk that LLMs are used for malicious
purposes, such as generating fake news, also increases (Zellers et al., 2019). Thus, it is crucial
to develop methods to identify whether LLMs or humans write texts. Watermarking is one of the
powerful techniques for this purpose, which inserts information into texts such that the inserted in-
formation is imperceptible to humans and can be easily identified by some algorithms (Venugopal
et al., 2011; He et al., 2021; 2022; Zhao et al., 2023b; Kuditipudi et al., 2023; Zhao et al., 2023a;
Kirchenbauer et al., 2023a;b; Christ et al., 2023). Recently, Kirchenbauer et al. (2023a) proposed
the Hard/Soft-Watermark,1 which inserts watermarks by generating text using only a subset of vo-
cabulary. Texts generated by LLMs consist only of a subset of vocabulary, whereas texts written by
humans consist of an entire vocabulary. Thus, we can identify whether LLMs or humans write text
using statistical hypothesis testing. Kirchenbauer et al. (2023a) demonstrated that the Hard/Soft-
Watermark can identify whether LLMs or humans write texts almost perfectly. However, because
LLMs generate texts with only a subset of vocabulary, the generated texts are often of low quality.
1We coined the name “Hard/Soft-Watermark” in this work to refer to the watermarking methods by Kirchen-
bauer et al. (2023a).
1
"
"2310.00835","Yuqing Wang","Yuqing Wang, Yun Zhao","TRAM: Benchmarking Temporal Reasoning for Large Language Models","21 pages, in submission","","","","cs.CL","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Reasoning about time is essential for understanding the nuances of events
described in natural language. Previous research on this topic has been limited
in scope, characterized by a lack of standardized benchmarks that would allow
for consistent evaluations across different studies. In this paper, we
introduce TRAM, a temporal reasoning benchmark composed of ten datasets,
encompassing various temporal aspects of events such as order, arithmetic,
frequency, and duration, designed to facilitate a comprehensive evaluation of
the temporal reasoning capabilities of large language models (LLMs). We conduct
an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both
zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based
models to establish the baseline evaluations. Our findings indicate that these
models still trail human performance in temporal reasoning tasks. It is our
aspiration that TRAM will spur further progress in enhancing the temporal
reasoning abilities of LLMs.
","2023-10-04","2310.00835v1.pdf","TRAM:
BENCHMARKING
TEMPORAL
REASONING
FOR LARGE LANGUAGE MODELS
Yuqing Wang
Stanford University
ywang216@stanford.edu
Yun Zhao
Meta Platforms, Inc.
yunzhao20@meta.com
ABSTRACT
Reasoning about time is essential for understanding the nuances of events de-
scribed in natural language. Previous research on this topic has been limited in
scope, characterized by a lack of standardized benchmarks that would allow for
consistent evaluations across different studies. In this paper, we introduce TRAM,
a temporal reasoning benchmark composed of ten datasets, encompassing vari-
ous temporal aspects of events such as order, arithmetic, frequency, and duration,
designed to facilitate a comprehensive evaluation of the temporal reasoning ca-
pabilities of large language models (LLMs). We conduct an extensive evaluation
using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot
learning scenarios. Additionally, we employ BERT-based models to establish the
baseline evaluations. Our findings indicate that these models still trail human per-
formance in temporal reasoning tasks. It is our aspiration that TRAM will spur
further progress in enhancing the temporal reasoning abilities of LLMs. Our data
is available at https://github.com/EternityYW/TRAM-Benchmark.
1
INTRODUCTION
Temporal reasoning is fundamental for humans to understand the world and distinguish between
everyday events. For instance, when given the activities “watching a movie” and “watching a sun-
set”, we intuitively recognize that, though both are time-bound, a movie typically lasts longer than
a sunset. Moreover, while movies can be watched repeatedly, sunsets transpire once a day. Such
innate comprehension isn’t just about sequencing events or understanding durations; it extends to
more intricate aspects of time, allowing us to make sense of complex narratives and the causality
of events. Despite advancements in natural language processing (NLP) and the advent of large lan-
guage models (Min et al., 2021; Zhao et al., 2023; Wang et al., 2023), mastering temporal reasoning
remains a significant challenge due to its intricate nature, the variability of temporal expressions,
and the need for contextual understanding.
arXiv:2310.00835v1  [cs.CL]  2 Oct 2023
Recent works in temporal reasoning (TeR) mainly focus on time-sensitive question-answering (Zhou
et al., 2019; Chen et al., 2021; Dhingra et al., 2022; Tan et al., 2023). These studies consistently show
that, despite significant advancements in NLP, current language models still fall short of human-
level performance in this domain. While they highlight various aspects of temporal elements, both
explicitly and implicitly, such as order, duration, and time-event relations, many intricate facets of
TeR, like understanding temporal narratives and temporal causality, remain less explored. Notably,
none of these works have tackled broad aspects of TeR within a unified framework.
To facilitate research in this direction, we present the Temporal Reasoning for large lAnguage
Model benchmark (or TRAM for short), a collection of ten temporal reasoning tasks. These tasks
range from foundational understanding (e.g., duration, frequency) to advanced temporal interpre-
tations and computations (e.g., arithmetic, causality). Each task consists of one or more subtasks,
all of which are specifically crafted to assess a model’s TeR capabilities across varying levels of
understanding and difficulty. In total, our benchmark includes 38 distinct subtasks. TRAM incor-
porates existing natural language understanding datasets, human-crafted templates and questions,
web sources, and program generation, comprising a total of 526.1k questions. Answers have been
derived through a combination of expert annotations and programmatic generation. Distinct from
previous work on temporal reasoning and in alignment with (Hendrycks et al., 2020), our questions
1
"
"2310.00836","Man Luo","Man Luo, Shrinidhi Kumbhar, Ming shen, Mihir Parmar, Neeraj Varshney,
  Pratyay Banerjee, Somak Aditya, Chitta Baral","Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical
  Reasoning Capabilities of Language Models","Work in progress","","","","cs.CL cs.AI","http://creativecommons.org/licenses/by/4.0/","  Logical reasoning is fundamental for humans yet presents a substantial
challenge in the domain of Artificial Intelligence. Initially, researchers used
Knowledge Representation and Reasoning (KR) systems that did not scale and
required non trivial manual effort. Recently, the emergence of large language
models (LLMs) has demonstrated the ability to overcome various limitations of
formal Knowledge Representation (KR) systems. Consequently, there is a growing
interest in using LLMs for logical reasoning via natural language. This work
strives to understand the proficiency of LLMs in logical reasoning by offering
a brief review of the latest progress in this area; with a focus on the logical
reasoning datasets, tasks, and the methods adopted to utilize LLMs for
reasoning. To offer a thorough analysis, we have compiled a benchmark titled
LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,
and inductive reasoning. We have standardized these datasets into Seq2Seq tasks
to facilitate straightforward training and evaluation for future research.
Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned
language model, resulting in LogiT5. We study single task training, multi task
training, and a chain of thought knowledge distillation fine tuning technique
to assess the performance of model across the different logical reasoning
categories. By this comprehensive process, we aim to shed light on the
capabilities and potential pathways for enhancing logical reasoning proficiency
in LLMs, paving the way for more advanced and nuanced developments in this
critical field.
","2023-10-03","2310.00836v1.pdf","Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing
Logical Reasoning Capabilities of Language Models
Man Luo 1,2∗
Shrinidhi Kumbhar1∗
Ming shen1
Mihir Parmar1
Neeraj Varshney1
Pratyay Banerjee3
Somak Aditya4
Chitta Baral1
1 Arizona State University, 2 Mayo Clinic, 3 Amazon Alexa AI, 4 IIT KGP
{mluo26, skumbha4}@asu.edu
Abstract
Logical reasoning is fundamental for humans
yet presents a substantial challenge in the do-
main of Artificial Intelligence. Initially, re-
searchers used Knowledge Representation and
Reasoning (KR) systems that did not scale
and required non-trivial manual effort.
Re-
cently, the emergence of large language models
(LLMs) has demonstrated the ability to over-
come various limitations of formal Knowledge
Representation (KR) systems. Consequently,
there’s a growing interest in using LLMs for
logical reasoning via natural language.
arXiv:2310.00836v1  [cs.CL]  2 Oct 2023
This work strives to understand the proficiency
of LLMs in logical reasoning by offering a
brief review of the latest progress in this area;
with a focus on the logical reasoning datasets,
tasks, and the methods adopted to utilize LLMs
for reasoning. To offer a thorough analysis,
we’ve compiled a benchmark titled LogiGLUE.
This includes 24 varied datasets encompass-
ing deductive, abductive, and inductive rea-
soning. We have standardized these datasets
into Seq2Seq tasks to facilitate straightfor-
ward training and evaluation for future re-
search. Utilizing LogiGLUE as a foundation,
we have trained an instruction fine-tuned lan-
guage model, resulting in LogiT5. We study
single-task training, multi-task training, and a
""chain-of-thought"" knowledge distillation fine-
tuning technique to assess the model’s perfor-
mance across the different logical reasoning
categories. By this comprehensive process, we
aim to shed light on the capabilities and poten-
tial pathways for enhancing logical reasoning
proficiency in LLMs, paving the way for more
advanced and nuanced developments in this
critical field.
1
Introduction
the workings in an unseen universe. As an example
of logical reasoning, physicist Stephen Hawking
derived the area theorem in 1971, which indicates
the boundary beyond which nothing can ever es-
cape (black hole). If Hawking’s area theorem holds,
then using logical reasoning, we can conclude that
the horizon area of the new black hole should not
be smaller than the total horizon area of its parent
black holes. This theorem was later confirmed by
real observation by MIT researchers in 2015, fifty
years later after the theorem had been derived1.
In the field of Artificial Intelligence (AI), there
has been significant attention directed towards
the aspiration to develop machines equipped with
logical reasoning capabilities (McCarthy, 1989;
Colmerauer and Roussel, 1996). Early approaches
in logical reasoning were primarily dedicated to
the design of formal logic languages to encapsu-
late rules and knowledge, along with the devel-
opment of automated theorem provers (Lifschitz,
2019). This paradigm, however, necessitated a
deep understanding of the syntax and semantics
of the formal logic for manual rule formulation
– making knowledge representation and knowl-
edge acquisition hard and expert-driven endeavor.
Due to these challenges, contemporary research
has progressively turned towards addressing logi-
cal reasoning tasks (Clark et al., 2020; Tian et al.,
2021a; Han et al., 2022) by employing transformer-
based (Vaswani et al., 2017) pre-trained language
models (Devlin et al., 2019a; Brown et al., 2020).
The Language models (LMs) that are pretrained
using objectives such as the mask language mod-
eling (Devlin et al., 2019a) and next word predic-
tion (Brown et al., 2020) enables them to acquire
adequate syntax and semantics of language, along-
side commonsense knowledge. These language
models can excel in numerous natural language
understanding tasks, owing to the unsupervised
With logical reasoning, humans can explain an an-
swer to a question via step-wise deduction, make
robust planning and decision, or even reason about
0*Equal contribution
1https://news.mit.edu/2021/hawkings-black-hole-
theorem-confirm-0701
"
"2310.00845","Tatsuki Kawamoto","Tatsuki Kawamoto, Takuma Suzuki, Ko Miyama, Takumi Meguro, Tomohiro
  Takagi","Application of frozen large-scale models to multimodal task-oriented
  dialogue","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this study, we use the existing Large Language Models ENnhanced to See
Framework (LENS Framework) to test the feasibility of multimodal task-oriented
dialogues. The LENS Framework has been proposed as a method to solve computer
vision tasks without additional training and with fixed parameters of
pre-trained models. We used the Multimodal Dialogs (MMD) dataset, a multimodal
task-oriented dialogue benchmark dataset from the fashion field, and for the
evaluation, we used the ChatGPT-based G-EVAL, which only accepts textual
modalities, with arrangements to handle multimodal data. Compared to
Transformer-based models in previous studies, our method demonstrated an
absolute lift of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance
and coherence. The results show that using large-scale models with fixed
parameters rather than using models trained on a dataset from scratch improves
performance in multimodal task-oriented dialogues. At the same time, we show
that Large Language Models (LLMs) are effective for multimodal task-oriented
dialogues. This is expected to lead to efficient applications to existing
systems.
","2023-10-03","2310.00845v1.pdf","Application of frozen large-scale models to
multimodal task-oriented dialogue
1st Tatsuki Kawamoto
Meiji University
Kanagawa, Japan
tatsuki.00.0306@gmail.com
2nd Takuma Suzuki
Meiji University
Kanagawa, Japan
stakuma9912@gmail.com
3rd Ko Miyama
Meiji University
Kanagawa, Japan
chinpanzi914@gmail.com
4th Takumi Meguro
Meiji University
Kanagawa, Japan
takumimeguro1321@gmail.com
5th Tomohiro Takagi
Meiji University
Kanagawa, Japan
takagit@gmail.com
Abstract—In this study, we use the existing Large Language
Models ENnhanced to See Framework (LENS Framework) to
test the feasibility of multimodal task-oriented dialogues. The
LENS Framework has been proposed as a method to solve
computer vision tasks without additional training and with fixed
parameters of pre-trained models. We used the Multimodal
Dialogs (MMD) dataset, a multimodal task-oriented dialogue
benchmark dataset from the fashion field, and for the evalu-
ation, we used the ChatGPT-based G-EVAL, which only accepts
textual modalities, with arrangements to handle multimodal data.
Compared to Transformer-based models in previous studies, our
method demonstrated an absolute lift of 10.8% in fluency, 8.8%
in usefulness, and 5.2% in relevance and coherence. The results
show that using large-scale models with fixed parameters rather
than using models trained on a dataset from scratch improves
performance in multimodal task-oriented dialogues. At the same
time, we show that Large Language Models (LLMs) are effective
for multimodal task-oriented dialogues. This is expected to lead
to efficient applications to existing systems.
Index Terms—Multimodal task-oriented dialogues, Large Lan-
guage Models
I. INTRODUCTION
Fig. 1: Example of dialogue in MMD
arXiv:2310.00845v1  [cs.CL]  2 Oct 2023
many prior studies [1]–[7], [9]–[11] have been conducted to
solve these problems, there have not been recent studies using
large-scale models released by OpenAI, Meta, Google, and
other organizations. The quantity and quality of knowledge,
comprehension, and reasoning power of recent large-scale
models have not been applied despite the fact that they are dis-
tinct from previous pre-trained models. We therefore adopted
the Large Language Models ENnhanced to See Framework
(LENS Framework) [21], which uses fixed pre-trained models
(also using a Large Language Model (LLM)) for computer
vision problems without additional model training. The LENS
Framework uses an LLM as inference modules. Currently,
Dialogue systems have been the central subject of much
research in natural language processing. These systems are
divided into two main categories, open-domain dialogues and
task-oriented dialogues. Open-domain dialogues are aimed at
free dialogues on a wide range of topics, while task-oriented
dialogues focus on dialogues to achieve a specific goal or task.
Task-oriented dialogue research has evolved toward achieving
more sophisticated and flexible dialogue. Within this context,
multimodal task-oriented dialogues have emerged and have
attracted much attention.
Multimodal task-oriented dialogues utilize not only textual
information but also other modal (e.g., image and audio)
information to complement and improve the accuracy of
information. Nuances and information that cannot be con-
veyed by textual information alone can be complemented by
other modalities. However, the realization of multimodal task-
oriented dialogue involves various technical hurdles, such as
the association of information between different modalities and
the difficulty of learning, which requires a large data set. While
"
"2310.00847","Atsuyuki Miyai","Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa","Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?","","","","","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Out-of-distribution (OOD) detection is critical for safety-sensitive machine
learning applications and has been extensively studied, yielding a plethora of
methods developed in the literature. However, most studies for OOD detection
did not use pre-trained models and trained a backbone from scratch. In recent
years, transferring knowledge from large pre-trained models to downstream tasks
by lightweight tuning has become mainstream for training in-distribution (ID)
classifiers. To bridge the gap between the practice of OOD detection and
current classifiers, the unique and crucial problem is that the samples whose
information networks know often come as OOD input. We consider that such data
may significantly affect the performance of large pre-trained networks because
the discriminability of these OOD data depends on the pre-training algorithm.
Here, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,
we aim to reveal the effect of PT-OOD on the OOD detection performance of
pre-trained networks from the perspective of pre-training algorithms. To
achieve this, we explore the PT-OOD detection performance of supervised and
self-supervised pre-training algorithms with linear-probing tuning, the most
common efficient tuning method. Through our experiments and analysis, we find
that the low linear separability of PT-OOD in the feature space heavily
degrades the PT-OOD detection performance, and self-supervised models are more
vulnerable to PT-OOD than supervised pre-trained models, even with
state-of-the-art detection methods. To solve this vulnerability, we further
propose a unique solution to large-scale pre-trained models: Leveraging
powerful instance-by-instance discriminative representations of pre-trained
models and detecting OOD in the feature space independent of the ID decision
boundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.
","2023-10-13","2310.00847v1.pdf","Preprint
CAN PRE-TRAINED NETWORKS DETECT FAMILIAR
OUT-OF-DISTRIBUTION DATA?
Atsuyuki Miyai1
Qing Yu1,2 Go Irie3
Kiyoharu Aizawa1
1The University of Tokyo
2LY Corporation
3Tokyo University of Science
{miyai,yu,aizawa}@hal.t.u-tokyo.ac.jp
goirie@ieee.org
ABSTRACT
Out-of-distribution (OOD) detection is critical for safety-sensitive machine learn-
ing applications and has been extensively studied, yielding a plethora of methods
developed in the literature. However, most studies for OOD detection did not use
pre-trained models and trained a backbone from scratch (Yang et al., 2022). In re-
cent years, transferring knowledge from large pre-trained models to downstream
tasks by lightweight tuning has become mainstream for training in-distribution
(ID) classifiers. To bridge the gap between the practice of OOD detection and
current classifiers, the unique and crucial problem is that the samples whose in-
formation networks know often come as OOD input. We consider that such data
may significantly affect the performance of large pre-trained networks because
the discriminability of these OOD data depends on the pre-training algorithm.
Here, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this
paper, we aim to reveal the effect of PT-OOD on the OOD detection perfor-
mance of pre-trained networks from the perspective of pre-training algorithms.
To achieve this, we explore the PT-OOD detection performance of supervised and
self-supervised pre-training algorithms with linear-probing tuning, the most com-
mon efficient tuning method. Through our experiments and analysis, we find that
the low linear separability of PT-OOD in the feature space heavily degrades the
PT-OOD detection performance, and self-supervised models are more vulnerable
to PT-OOD than supervised pre-trained models, even with state-of-the-art detec-
tion methods. To solve this vulnerability, we further propose a unique solution
to large-scale pre-trained models: Leveraging powerful instance-by-instance dis-
criminative representations of pre-trained models and detecting OOD in the fea-
ture space independent of the ID decision boundaries. The code will be available
via https://github.com/AtsuMiyai/PT-OOD.
1
INTRODUCTION
arXiv:2310.00847v1  [cs.CV]  2 Oct 2023
Pre-training data 
non-PT-OOD
PT-OOD
Deep neural networks perform success-
fully when the testing data is drawn
from the same distribution as the train-
ing data. This kind of test-time data with
distributions matching the training-time
data is called in-distribution (ID) data.
However, out-of-distribution (OOD) sam-
ples that do not belong to ID categories
are likely to occur in the testing data.
When deep learning is deployed in many
applications, such as autonomous driv-
ing (Blum et al., 2019), medical applica-
tions (Ulmer et al., 2020), and web appli-
cations (Aizawa & Ogawa, 2015), it be-
comes crucial to detect OOD samples.
Figure 1: Relationship between large pre-training
data, in-distribution (ID) data, and PT-OOD data.
A large number of data similar to the large pre-training
data and not in the ID classes can be PT-OOD data.
Recently, the development of large-scale
pre-trained models (Chen et al., 2020a;b;
1
"
"2310.00863","Zhe Zhang","Zhe Zhang, Karol Lasocki, Yi Yu, Atsuhiro Takasu","Melody-conditioned lyrics generation via fine-tuning language model and
  its evaluation with ChatGPT","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We leverage character-level language models for syllable-level lyrics
generation from symbolic melody. By fine-tuning a character-level pre-trained
model, we integrate language knowledge into the beam search of a syllable-level
Transformer generator. Using ChatGPT-based evaluations, we demonstrate enhanced
coherence and correctness in the generated lyrics.
","2023-10-03","2310.00863v1.pdf","MELODY-CONDITIONED LYRICS GENERATION VIA FINE-TUNING
LANGUAGE MODEL AND ITS EVALUATION WITH CHATGPT
Zhe Zhang1, Karol Lasocki 2, Yi Yu1∗, Atsuhiro Takasu1
National Institute of Informatics1, Aalto University2,
zhe@nii.ac.jp, karolasocki@gmail.com, yiyu@nii.ac.jp, takasu@nii.ac.jp
ABSTRACT
We leverage character-level language models for syllable-level lyrics generation from symbolic
melody. By fine-tuning a character-level pre-trained model, we integrate language knowledge into
the beam search of a syllable-level Transformer generator. Using ChatGPT-based evaluations, we
demonstrate enhanced coherence and correctness in the generated lyrics.
1
Introduction
Lyrics generation from symbolic melody is a captivating domain in creative machine learning. While many works focus
on word-level generation [Zhang et al., 2022, Rodrigues et al., 2022, Watanabe et al., 2018, Vechtomova et al., 2020,
Nikolov et al., 2020], syllable-level generation presents challenges in maintaining semantic coherence across syllables,
words, and sentences. We target generating lyrics for melodies of 20 notes [Yu et al., 2021], ensuring alignment between
syllables and melody. Our novel approach combines a melody-encoder-syllable-decoder architecture with a fine-tuned
language model, addressing the absence of pre-trained syllable-level models.
2
Proposed methods
Our syllable-level lyrics generation utilizes a character-level language model, as shown in Figure 1. The primary
Transformer generates candidate tokens after encoding the melody, while a secondary transformer evaluates these
candidates based on previously generated lyrics. The generated lyrics are evaluated by prompting ChatGPT for feedback.
Future work will introduce another Transformer to enhance the link between lyrics and melodies.
arXiv:2310.00863v1  [cs.CL]  2 Oct 2023
Figure 1: Melody-conditioned lyrics generation via fine-tuning language model and its evaluation with ChatGPT
The language model computes the probability Pl(xi−1, xi−2, . . . , x0|xi), with xi being the ith syllable. The main
Transformer’s probability is Pt(xi|xi−1, xi−2, . . . , x0, fn, fn−1, . . . , f0), where fi represents melody features. The
∗The corresponding author
"
"2310.00865","David Donoho","David Donoho","Data Science at the Singularity","1 Figure","","","","stat.OT","http://creativecommons.org/licenses/by/4.0/","  A purported `AI Singularity' has been in the public eye recently. Mass media
and US national political attention focused on `AI Doom' narratives hawked by
social media influencers. The European Commission is announcing initiatives to
forestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong
narrative for what's happening now; recent happenings signal something else
entirely. Something fundamental to computation-based research really changed in
the last ten years. In certain fields, progress is dramatically more rapid than
previously, as the fields undergo a transition to frictionless reproducibility
(FR). This transition markedly changes the rate of spread of ideas and
practices, affects mindsets, and erases memories of much that came before.
  The emergence of frictionless reproducibility follows from the maturation of
3 data science principles in the last decade. Those principles involve data
sharing, code sharing, and competitive challenges, however implemented in the
particularly strong form of frictionless open services. Empirical Machine
Learning (EML) is todays leading adherent field, and its consequent rapid
changes are responsible for the AI progress we see. Still, other fields can and
do benefit when they adhere to the same principles.
  Many rapid changes from this maturation are misidentified. The advent of FR
in EML generates a steady flow of innovations; this flow stimulates outsider
intuitions that there's an emergent superpower somewhere in AI. This opens the
way for PR to push worrying narratives: not only `AI Extinction', but also the
supposed monopoly of big tech on AI research. The helpful narrative observes
that the superpower of EML is adherence to frictionless reproducibility
practices; these practices are responsible for the striking progress in AI that
we see everywhere.
","2023-10-03","2310.00865v1.pdf","Data Science at the Singularity
Version 1.00
David Donoho ∗
October 3, 2023
Abstract
arXiv:2310.00865v1  [stat.OT]  2 Oct 2023
A purported “AI Singularity” has been much in the public eye recently, especially since the release
of ChatGPT last November, spawning social media “AI Breakthrough” threads promoting Large
Language Model (LLM) achievements.
Alongside this, mass media and US national political
attention focused on “AI Doom” narratives hawked by social media influencers; some were invited
to tell US congresspersons about the coming “End Times.” The European Commission is now
announcing initiatives to forestall “AI Extinction”.
In my opinion, “AI Singularity” is the wrong narrative for what’s happening now; the remark-
able recent happenings signal something else entirely.
Something fundamental to computation-based research really has changed in the last ten
years. In certain fields, progress is simply dramatically more rapid than previously. Researchers
in affected fields are living through a period of profound transformation, as the fields undergo a
transition to frictionless reproducibility (FR). This transition markedly changes the rate of spread
of ideas and practices, affects mindsets, and erases memories of much that came before.
The emergence of frictionless reproducibility follows from the maturation of 3 data science
principles that came together after decades of work by many technologists and numerous re-
search communities. The mature principles involve data sharing, code sharing, and competitive
challenges, however implemented in the particularly strong form of frictionless open services.
Empirical Machine Learning (EML) is today’s leading adherent field, and its consequent rapid
changes are responsible for the AI progress we see. Still, other fields can and do benefit when
they adhere to the same principles.
Many of the rapid changes due to this maturation are misidentified. The advent of FR in
EML generates a steady flow of innovations; this flow stimulates outsider intuitions that there’s
an emergent superpower somewhere in AI. This opens the way for PR to push all sorts of worrying
narratives: not only “AI Extinction”, but also the supposed monopoly of big tech on AI research.
The more helpful narrative observes that the superpower of EML is adherence to frictionless repro-
ducibility practices; that these practices are responsible for the striking and surprising progress in
AI that we see everywhere; and that these practices can be learned and adhered to by researchers
in whatever research field, automatically increasing the rate of progress in each adherent field.
Key words and Phrases. Reproducible Computational Research. Challenge Problems Paradigm.
Frictionless Reproducibility. Frictionless Research Exchange. Emergent Superpower. AI Singularity.
AI Hegemony. Brutal Scaling Paradigm.
∗Department of Statistics, Stanford University
1
"
"2310.00867","Duc Hoang","Duc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang
  Wang","(Dynamic) Prompting might be all you need to repair Compressed LLMs","","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs), while transformative for NLP, come with
significant computational demands, underlining the need for efficient,
training-free compression. Notably, despite the marked improvement in
training-free compression for the largest of LLMs, our tests using LLaMA-7B and
OPT-6.7b highlight a significant performance drop in several realistic
downstream tasks. Investigation into the trade-off between resource-intensive
post-compression re-training highlights the prospect of prompt-driven recovery
as a lightweight adaption tool. However, existing studies, confined mainly to
perplexity evaluations and simple tasks, fail to offer unequivocal confidence
in the scalability and generalizability of prompting. We tackle this
uncertainty in two key ways. First, we uncover the vulnerability of naive
prompts in LLM compression as an over-reliance on a singular prompt per input.
In response, we propose inference-time dynamic prompting (IDP), a mechanism
that autonomously chooses from a set of curated prompts based on the context of
each individual input. Second, we delve into a scientific understanding of why
""prompting might be all you need post-LLM compression."" Our findings suggest
that compression does not irretrievably erase LLM model knowledge but displace
it, necessitating a new inference path. IDP effectively redirects this path,
enabling the model to tap into its inherent yet displaced knowledge and thereby
recover performance. Empirical tests affirm the value of IDP, demonstrating an
average performance improvement of 1.24% across nine varied tasks spanning
multiple knowledge domains.
","2023-10-17","2310.00867v1.pdf","Preprint
(DYNAMIC) PROMPTING MIGHT BE ALL YOU NEED TO
REPAIR COMPRESSED LLMS
Duc Hoang
Apple
dhoang2@apple.com
Minsik Cho
Apple
minsik@apple.com
Thomas Merth
Apple
tmerth@apple.com
Mohammad Rastegari
Apple
mrastegari@apple.com
Zhangyang Wang∗
The University of Texas at Austin
atlaswang@utexas.edu∗
ABSTRACT
Large language models (LLMs), while transformative for NLP, come with sig-
nificant computational demands, underlining the need for efficient, training-free
compression. Notably, the reliability of perplexity as a benchmark for compressed
model efficacy is in question, as our tests using LLaMA-7B and OPT-6.7b reveal
a significant performance drop in several realistic downstream tasks, underscor-
ing the disparity between perplexity as a performance indicator and real-world
performance. Investigation into the trade-off between resource-intensive post-
compression re-training highlights the prospect of prompt-driven recovery as a
lightweight adaption tool. However, existing studies, confined mainly to per-
plexity evaluations and simple tasks, fail to offer unequivocal confidence in the
scalability and generalizability of prompting. We tackle this uncertainty in two
key ways. First, we uncover the vulnerability of naive prompts in LLM com-
pression as an over-reliance on a singular prompt per input. In response, we pro-
pose inference-time dynamic prompting (IDP), a mechanism that autonomously
chooses from a set of curated prompts based on the context of each individual in-
put. Second, we delve into a scientific understanding of why “prompting might be
all you need post-LLM compression”. Our findings suggest that compression does
not irretrievably erase LLM model knowledge but displace it, necessitating a new
inference path. IDP effectively redirects this path, enabling the model to tap into
its inherent yet displaced knowledge and thereby recover performance. Empirical
tests affirm the value of IDP, demonstrating an average performance improvement
of 1.24% across nine varied tasks spanning multiple knowledge domains.
1
INTRODUCTION
arXiv:2310.00867v1  [cs.CL]  2 Oct 2023
Large language models (LLMs) have demonstrated exceptional proficiency in language generation
and reasoning, rivaling human capabilities. The advent of models like GPT-4 (OpenAI, 2023) and
tools such as ChatGPT signifies a major milestone along this trajectory, positioning themselves as
pivotal assets in various industries. However, the escalating size of these models presents critical
computational challenges, impeding efforts towards their widespread adoption (Chen et al., 2023).
In response to the growing requirements and the associated computational loads imposed by large
language models (LLMs), techniques such as quantization and sparsification have garnered signifi-
cant attention and resources. Quantization involves fine-tuning the bit-wise precision of a model to
reduce its size, while sparsification entails eliminating redundant operations by nullifying weight or
activation elements. Traditional methods like pruning and quantization typically necessitate a post-
compression re-training step, be it iterative or one-shot, to restore performance (Han et al., 2015).
Regrettably, given the scale of modern LLMs, even a one-shot re-training approach after compres-
sion is becoming prohibitively costly, underscoring the pressing demand for training-free compres-
sion. Recent endeavors, exemplified by works such as GPTQ (Frantar et al., 2022) and SparseGPT
(Frantar & Alistarh, 2023), promise nearly unaltered accuracy, often assessed through the perplexity
1
"
"2310.00887","Sai Vemprala","Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan, Ashish
  Kapoor","GRID: A Platform for General Robot Intelligence Development","","","","","cs.RO cs.AI cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Developing machine intelligence abilities in robots and autonomous systems is
an expensive and time consuming process. Existing solutions are tailored to
specific applications and are harder to generalize. Furthermore, scarcity of
training data adds a layer of complexity in deploying deep machine learning
models. We present a new platform for General Robot Intelligence Development
(GRID) to address both of these issues. The platform enables robots to learn,
compose and adapt skills to their physical capabilities, environmental
constraints and goals. The platform addresses AI problems in robotics via
foundation models that know the physical world. GRID is designed from the
ground up to be extensible to accommodate new types of robots, vehicles,
hardware platforms and software protocols. In addition, the modular design
enables various deep ML components and existing foundation models to be easily
usable in a wider variety of robot-centric problems. We demonstrate the
platform in various aerial robotics scenarios and demonstrate how the platform
dramatically accelerates development of machine intelligent robots.
","2023-10-10","2310.00887v1.pdf","GRID: A Platform for General Robot
Intelligence Development
Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan and Ashish Kapoor
Scaled Foundations
Developing machine intelligence abilities in robots and autonomous systems is an expensive
and time consuming process. Existing solutions are tailored to specific applications and are
harder to generalize. Furthermore, scarcity of training data adds a layer of complexity in
deploying deep machine learning models. We present a new platform for General Robot
Intelligence Development (GRID) to address both of these issues. The platform enables robots
to learn, compose and adapt skills to their physical capabilities, environmental constraints
and goals. The platform addresses AI problems in robotics via foundation models that know
the physical world. GRID is designed from the ground up to be extensible to accommodate
new types of robots, vehicles, hardware platforms and software protocols. In addition, the
modular design enables various deep ML components and existing foundation models to be
easily usable in a wider variety of robot-centric problems. We demonstrate the platform in
various aerial robotics scenarios and demonstrate how the platform dramatically accelerates
development of machine intelligent robots.The GRID platform can be accessed at https://
github.com/ScaledFoundations/GRID-playground.
1. Introduction
Without the ability to sense, reason, and act appropriately, robots and autonomous systems are nothing more
than a collection of expensive parts. The inability to leverage machine intelligence to deliver autonomous
systems with these capabilities is arguably the most substantial hindrance to adoption. At the core, the
challenge facing artificial intelligence (AI) in robotics involves solving the perception-action loop; in which,
provided a sensory motor stimulus the machine needs to decide on the next action to take. In a robotic
system there are often many such perception-action loops running in parallel or sequentially, and sometimes
also embedded hierarchically
arXiv:2310.00887v1  [cs.RO]  2 Oct 2023
There are several reasons for the modest impact of AI within robotics. First, too often solutions are tailored to
one specific application or problem, leading to idiosyncratic implementations that fail to generalize. Second,
it is common for techniques and components to be stitched together to provide a tailor made solution that is
compatible or optimized for the sensor suite on the robot and the environment it will operate in. Custom
solutions are expensive to engineer and are not amenable to adaptation when the task configuration changes.
Third, the available tools are not well suited to robotics. For example, there is a dearth of foundational AI
models for robots and a lack of good tooling to enable tuning and real-world deployment.
To address this limitations head on we propose a solution: General Robot Intelligence which we define as the
ability for a robot to learn, compose, and adapt skills to their physical capabilities, environmental constraints
and goals. For example, a collision avoidance mechanism in one kind of robot should be useful for another
robot for a similar purpose even if the sensor configurations differ. In order to learn a novel skill unique to
the robot, the machine should be able to re-purpose existing machine intelligence modules instead of starting
from scratch. A robot intelligence methodology that offers modularity, scalability and generalizability across
a wide variety of robots, sensor suites and environments will enable rapid implementation of machine
"
"2310.00898","Ziqi Wang","Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu,
  Heng Ji","Enable Language Models to Implicitly Learn Self-Improvement From Data","28 pages, 5 figures, 4 tables","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have demonstrated remarkable capabilities in
open-ended text generation tasks. However, the inherent open-ended nature of
these tasks implies that there is always room for improvement in the quality of
model responses. To address this challenge, various approaches have been
proposed to enhance the performance of LLMs. There has been a growing focus on
enabling LLMs to self-improve their response quality, thereby reducing the
reliance on extensive human annotation efforts for collecting diverse and
high-quality training data. Recently, prompting-based methods have been widely
explored among self-improvement methods owing to their effectiveness,
efficiency, and convenience. However, those methods usually require explicitly
and thoroughly written rubrics as inputs to LLMs. It is expensive and
challenging to manually derive and provide all necessary rubrics with a
real-world complex goal for improvement (e.g., being more helpful and less
harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework
that implicitly learns the improvement goal from human preference data. PIT
only requires preference data that are used to train reward models without
extra human efforts. Specifically, we reformulate the training objective of
reinforcement learning from human feedback (RLHF) -- instead of maximizing
response quality for a given input, we maximize the quality gap of the response
conditioned on a reference response. In this way, PIT is implicitly trained
with the improvement goal of better aligning with human preferences.
Experiments on two real-world datasets and one synthetic dataset show that our
method significantly outperforms prompting-based methods.
","2023-10-09","2310.00898v1.pdf","Preprint
ENABLE LANGUAGE MODELS TO IMPLICITLY LEARN
SELF-IMPROVEMENT FROM DATA
Ziqi Wang1∗, Le Hou2†, Tianjian Lu2, Yuexin Wu2, Yunxuan Li2, Hongkun Yu2, Heng Ji1
1 University of Illinois Urbana-Champaign 2 Google
ziqiw9@illinois.edu lehou@google.com
ABSTRACT
Large Language Models (LLMs) have demonstrated remarkable capabilities in
open-ended text generation tasks. However, the inherent open-ended nature of these
tasks implies that there is always room for improvement in the quality of model
responses. To address this challenge, various approaches have been proposed to
enhance the performance of LLMs. There has been a growing focus on enabling
LLMs to self-improve their response quality, thereby reducing the reliance on
extensive human annotation efforts for collecting diverse and high-quality train-
ing data. Recently, prompting-based methods have been widely explored among
self-improvement methods owing to their effectiveness, efficiency, and conve-
nience. However, those methods usually require explicitly and thoroughly written
rubrics as inputs to LLMs. It is expensive and challenging to manually derive
and provide all necessary rubrics with a real-world complex goal for improvement
(e.g., being more helpful and less harmful). To this end, we propose an ImPlicit
Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal
from human preference data. PIT only requires preference data that are used to
train reward models without extra human efforts. Specifically, we reformulate
the training objective of reinforcement learning from human feedback (RLHF) –
instead of maximizing response quality for a given input, we maximize the quality
gap of the response conditioned on a reference response. In this way, PIT is implic-
itly trained with the improvement goal of better aligning with human preferences.
Experiments on two real-world datasets and one synthetic dataset show that our
method significantly outperforms prompting-based methods.
1
INTRODUCTION
arXiv:2310.00898v1  [cs.CL]  2 Oct 2023
LLMs (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Schulman
et al., 2022; OpenAI, 2023; Anil et al., 2023) have achieved state-of-the-art results on complex
tasks such as math reasoning (Wei et al., 2022; Xue et al., 2023; Zhou et al., 2023), summarization
(Stiennon et al., 2020b), conversations (Schulman et al., 2022; Bai et al., 2022), schema induction (Li
et al., 2023) and solving domain-specific problems (Singhal et al., 2022). The keys of LLMs success
are their abilities of following instructions and aligning with human preferences (Ouyang et al.,
2022; Peng et al., 2023; Shen et al., 2023). A widely adopted approach toward them is instruction
fine-tuning and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022). However,
instruction fine-tuning and RLHF are imperfect, and there is always room for improvement. For
example, LLMs may hallucinate information (OpenAI, 2023), have reasoning errors (Bubeck et al.,
2023), and generate unhelpful and harmful contents (Bai et al., 2022). A straightforward approach is
to collect more diverse and high-quality data and improve the alignment with a human-in-the-loop
training paradigm (Ouyang et al., 2022), which requires extensive amount of human effort, especially
for specific domains that require expert knowledge.
Therefore, the community has explored to use LLMs to self-improve their own response quality
without human intervention. With the advent of generative language models, prompting methods
have proved effective and efficient (no need to train models) with convenience (no need to serve
models and can be used with black-box language models through APIs). Madaan et al. (2023) use
∗Work done when interning at Google
†Correspondence Author
1
"
"2310.00901","Tianci Xue","Tianci Xue, Ziqi Wang, Yixia Li, Yun Chen, Guanhua Chen","TADIS: Steering Models for Deep-Thinking about Demonstration Examples","14 pages, 3 figures","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Instruction tuning has been demonstrated that could significantly improve the
zero-shot generalization capability to unseen tasks by an apparent margin. By
incorporating additional context (e.g., task definition, examples) during the
fine-tuning process, Large Language Models (LLMs) achieved much higher
performance than before. However, recent work reported that delusive task
examples can achieve almost the same performance as correct task examples,
indicating the input-label correspondence is less important than previously
thought. Intrigued by this counter-intuitive observation, we suspect models
have the same illusion of competence as humans. Therefore, we propose a novel
method called TADIS that steers LLMs for ""Deep-Thinking'' about demonstration
examples instead of merely seeing. To alleviate the illusion of competence of
models, we first ask the model to verify the correctness of shown examples.
Then, using the verification results as conditions to elicit models for a
better answer. Our experimental results show that TADIS consistently
outperforms competitive baselines on in-domain and out-domain tasks (improving
2.79 and 4.03 average ROUGLE-L on out-domain and in-domain datasets,
respectively). Despite the presence of generated examples (not all of the
thinking labels are accurate), TADIS can notably enhance performance in
zero-shot and few-shot settings. This also suggests that our approach can be
adopted on a large scale to improve the instruction following capabilities of
models without any manual labor. Moreover, we construct three types of thinking
labels with different model sizes and find that small models learn from the
format of TADIS but larger models can be steered for ""Deep-Thinking''.
","2023-10-06","2310.00901v1.pdf","Published as a conference paper at ICLR 2024
TADIS: STEERING MODELS
FOR DEEP-THINKING
ABOUT DEMONSTRATION EXAMPLES
Tianci Xue1∗, Ziqi Wang2, Yixia Li4, Yun Chen3, Guanhua Chen4
1 Department of Software, Nanjing University
2 Department of Computer Science, University of Illinois Urbana-Champaign
3 Shanghai University of Finance and Economics
4 Southern University of Science and Technology
xuetianci@smail.nju.edu.cn
ziqiw9@illinois.edu
liyixia@me.com
yun.chencreek@gmail.com
ghchen08@gmail.com
ABSTRACT
Instruction tuning has been demonstrated that could significantly improve the
zero-shot generalization capability to unseen tasks by an apparent margin. By
incorporating additional context (e.g., task definition, examples) during the fine-
tuning process, Large Language Models (LLMs) achieved much higher perfor-
mance than before. However, recent work reported that delusive task examples
can achieve almost the same performance as correct task examples, indicating the
input-label correspondence is less important than previously thought. Intrigued
by this counter-intuitive observation, we suspect models have the same illusion of
competence as humans. Therefore, we propose a novel method called TADIS that
steers LLMs for Deep-Thinking about demonstration examples instead of merely
seeing. To alleviate the illusion of competence of models, we first ask the model
to verify the correctness of shown examples. Then, using the verification results
as conditions to elicit models for a better answer. Our experimental results show
that TADIS consistently outperforms competitive baselines on in-domain and out-
domain tasks (improving 2.79 and 4.03 average ROUGLE-L on out-domain and
in-domain datasets, respectively). Despite the presence of generated examples
(not all of the thinking labels are accurate), TADIS can notably enhance perfor-
mance in zero-shot and few-shot settings. This also suggests that our approach
can be adopted on a large scale to improve the instruction following capabilities
of models without any manual labor. Moreover, we construct three types of think-
ing labels with different model sizes and find that small models learn from the
format of TADIS but larger models can be steered for Deep-Thinking.
arXiv:2310.00901v1  [cs.CL]  2 Oct 2023
1
INTRODUCTION
Recently, instruction tuning (IT) has attracted much attention in the NLP community, which has
shown effectiveness in improving the zero-shot generalization capability and gradually boosting
performance on unseen tasks as the number of training tasks increases (Chung et al., 2022; Ouyang
et al., 2022; Sanh et al., 2022; Taori et al., 2023). Researchers have explored efficient approaches
to generate instructions (Wang et al., 2022; 2023; Xu et al., 2023), resulting in dozens of powerful
large language models (LLMs) (Ouyang et al., 2022; Sanh et al., 2022; Chung et al., 2022; Taori
et al., 2023). However, LLMs still struggle to follow the instruction precisely in some scenarios (Li
et al., 2023; AlShikh et al., 2023), which hinders the applications and alignment of LLMs.
When applying instruction tuning with examples on LLMs, previous work provides either all pos-
itive examples (Ouyang et al., 2022) or specified positive/negative examples (Wang et al., 2022).
∗Work done during the internship at Sustech.
1
"
"2310.00902","Yongchan Kwon","Yongchan Kwon, Eric Wu, Kevin Wu, James Zou","DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and
  Diffusion Models","","","","","cs.LG stat.ML","http://creativecommons.org/licenses/by-sa/4.0/","  Quantifying the impact of training data points is crucial for understanding
the outputs of machine learning models and for improving the transparency of
the AI pipeline. The influence function is a principled and popular data
attribution method, but its computational cost often makes it challenging to
use. This issue becomes more pronounced in the setting of large language models
and text-to-image models. In this work, we propose DataInf, an efficient
influence approximation method that is practical for large-scale generative AI
models. Leveraging an easy-to-compute closed-form expression, DataInf
outperforms existing influence computation algorithms in terms of computational
and memory efficiency. Our theoretical analysis shows that DataInf is
particularly well-suited for parameter-efficient fine-tuning techniques such as
LoRA. Through systematic empirical evaluations, we show that DataInf accurately
approximates influence scores and is orders of magnitude faster than existing
methods. In applications to RoBERTa-large, Llama-2-13B-chat, and
stable-diffusion-v1.5 models, DataInf effectively identifies the most
influential fine-tuning examples better than other approximate influence
scores. Moreover, it can help to identify which data points are mislabeled.
","2023-10-03","2310.00902v1.pdf","DataInf: Efficiently Estimating Data Influence in
LoRA-tuned LLMs and Diffusion Models
Yongchan Kwon∗
Columbia University
Eric Wu∗
Stanford University
Kevin Wu∗
Stanford University
James Zou
Stanford University
Abstract
Quantifying the impact of training data points is crucial for understanding the
outputs of machine learning models and for improving the transparency of the
AI pipeline. The influence function is a principled and popular data attribution
method, but its computational cost often makes it challenging to use. This issue
becomes more pronounced in the setting of large language models and text-to-image
models. In this work, we propose DataInf, an efficient influence approximation
method that is practical for large-scale generative AI models. Leveraging an
easy-to-compute closed-form expression, DataInf outperforms existing influence
computation algorithms in terms of computational and memory efficiency. Our
theoretical analysis shows that DataInf is particularly well-suited for parameter-
efficient fine-tuning techniques such as LoRA. Through systematic empirical
evaluations, we show that DataInf accurately approximates influence scores and
is orders of magnitude faster than existing methods. In applications to RoBERTa-
large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively
identifies the most influential fine-tuning examples better than other approximate
influence scores. Moreover, it can help to identify which data points are mislabeled.
1
Introduction
arXiv:2310.00902v1  [cs.LG]  2 Oct 2023
Modern large language models (LLMs) and text-to-image models have demonstrated remarkable
abilities in generating human-like texts and photorealistic images, leading to diverse real-world
applications such as translation, dialogue systems, and image editing [1–3]. Nevertheless, even
state-of-the-art models do generate factually incorrect predictions or even biased outputs [4–6], often
as a result of issues in the training data. This highlights the need for principled and systematic
methods to quantify the impact of specific training data points. The influence function provides a
rigorous framework for evaluating the impact of each training data point on model predictions [7, 8].
Its efficacy has been demonstrated across various downstream machine learning tasks: mislabeled
data detection [9], best subset selection [10, 11], model interpretation [12–14], and investigation of
model biases [15, 16].
While the influence function has shown promising results, its application in real-world scenarios
poses practical challenges because of its expensive computational costs. Calculating the influence
function requires the computation of the inverse Hessian matrix, which involves intensive computation.
Previous studies have attempted to reduce this burden; however, most existing methods still require
an iterative algorithm [17, 18], multiple eigenvalue decompositions [19] or the training of numerous
models [10] to obtain accurate influence estimates. It has therefore been very challenging to compute
the influence function for large models such as LLMs [20–22] and diffusion models [23, 24, 2].
"
"2310.00905","Wenxuan Wang","Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang,
  Wenxiang Jiao, Michael R. Lyu","All Languages Matter: On the Multilingual Safety of Large Language
  Models","The first multilingual safety benchmark for large language models","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Safety lies at the core of developing and deploying large language models
(LLMs). However, previous safety benchmarks only concern the safety in one
language, e.g. the majority language in the pretraining data such as English.
In this work, we build the first multilingual safety benchmark for LLMs,
XSafety, in response to the global deployment of LLMs in practice. XSafety
covers 14 kinds of commonly used safety issues across 10 languages that span
several language families. We utilize XSafety to empirically study the
multilingual safety for 4 widely-used LLMs, including both close-API and
open-source models. Experimental results show that all LLMs produce
significantly more unsafe responses for non-English queries than English ones,
indicating the necessity of developing safety alignment for non-English
languages. In addition, we propose several simple and effective prompting
methods to improve the multilingual safety of ChatGPT by evoking safety
knowledge and improving cross-lingual generalization of safety alignment. Our
prompting method can significantly reduce the ratio of unsafe responses from
19.1% to 9.7% for non-English queries. We release our data at
https://github.com/Jarviswang94/Multilingual_safety_benchmark.
","2023-10-03","2310.00905v1.pdf","Under Submission
ALL LANGUAGES MATTER: ON THE MULTILINGUAL
SAFETY OF LARGE LANGUAGE MODELS
Wenxuan Wang1,2∗
Zhaopeng Tu2
Chang Chen1
Youliang Yuan2,3∗
Jen-tse Huang 1,2∗
Wenxiang Jiao2
Michael R. Lyu1
1The Chinese University of Hong Kong
2Tencent AI Lab
3School of Data Science, The Chinese University of Hong Kong, Shenzhen, China
1{wxwang,jthuang,lyu}@cse.cuhk.edu.hk
2{zptu,joelwxjiao}@tencent.com
ABSTRACT
Safety lies at the core of developing and deploying large language models (LLMs).
However, previous safety benchmarks only concern the safety in one language, e.g.
the majority language in the pretraining data such as English. In this work, we build
the first multilingual safety benchmark for LLMs, XSAFETY, in response to the
global deployment of LLMs in practice. XSAFETY covers 14 kinds of commonly
used safety issues across 10 languages that span several language families. We
utilize XSAFETY to empirically study the multilingual safety for 4 widely-used
LLMs, including both close-API and open-source models. Experimental results
show that all LLMs produce significantly more unsafe responses for non-English
queries than English ones, indicating the necessity of developing safety alignment
for non-English languages. In addition, we propose several simple and effective
prompting methods to improve the multilingual safety of ChatGPT by evoking
safety knowledge and improving cross-lingual generalization of safety alignment.
Our prompting method can significantly reduce the ratio of unsafe responses from
19.1% to 9.7% for non-English queries 1.
1
INTRODUCTION
Recent advances in scaling large language models (LLMs) have made breakthroughs in the Artificial
Intelligence (AI) area. With the rapid increase of model parameters and training data, LLMs
have gained emergent abilities in various tasks, including writing assistance Gao et al. (2022),
code generation Gao et al. (2023), machine translation Jiao et al. (2023), and so on. Due to their
impressive performance, a number of LLMs have been launched by commercial companies and
academic institutions, including OpenAI’s GPT models Brown et al. (2020); OpenAI (2022), Google’s
Bard Pichai (2023), and Meta’s LLaMA Touvron et al. (2023a;b). Such extensive deployment
underscores an imperative of paramount significance: ensuring the safety of LLMs.
arXiv:2310.00905v1  [cs.CL]  2 Oct 2023
There has been a number of work for aligning LLMs with human ethics and preferences to improve
their safety, including data filtering (Xu et al., 2020; Welbl et al., 2021; Wang et al., 2022), supervised
fine-tuning (Ouyang et al., 2022), reinforcement learning from human feedback (RLHF) (Christiano
et al., 2017), and red teaming (Perez et al., 2022; Ganguli et al., 2022a). Most of the existing work on
safety alignment has focused on the interaction in English OpenAI (2023). However, as globally
deployed services, LLMs, such as ChatGPT, have users around the world and are frequently engaged
in non-English communication with users from non-English-speaking regions. One research question
naturally arises: can the non-English language prompts bypass the safety alignment that is tuned
mainly in English?
To answer this question, we create the first multilingual safety benchmark for LLMs, called XSAFETY.
We collect several well-established monolingual safety benchmarks, across 14 kinds of safety issues,
and recruit professional translators to conduct translation, ending up with a multilingual benchmark
in 10 languages. XSAFETY consists of 2,800 instances in the most widely-used 10 languages that
∗Work was done when Wenxuan Wang, Youliang Yuan, and Jen-tse Huang were interning at Tencent AI Lab.
1Our dataset is released at https://github.com/Jarviswang94/Multilingual safety benchmark
1
"
"2310.00907","Stephen Yang","Fernando Delgado, Stephen Yang, Michael Madaio, Qian Yang","The Participatory Turn in AI Design: Theoretical Foundations and the
  Current State of Practice","","","","","cs.HC cs.AI","http://creativecommons.org/licenses/by/4.0/","  Despite the growing consensus that stakeholders affected by AI systems should
participate in their design, enormous variation and implicit disagreements
exist among current approaches. For researchers and practitioners who are
interested in taking a participatory approach to AI design and development, it
remains challenging to assess the extent to which any participatory approach
grants substantive agency to stakeholders. This article thus aims to ground
what we dub the ""participatory turn"" in AI design by synthesizing existing
theoretical literature on participation and through empirical investigation and
critique of its current practices. Specifically, we derive a conceptual
framework through synthesis of literature across technology design, political
theory, and the social sciences that researchers and practitioners can leverage
to evaluate approaches to participation in AI design. Additionally, we
articulate empirical findings concerning the current state of participatory
practice in AI design based on an analysis of recently published research and
semi-structured interviews with 12 AI researchers and practitioners. We use
these empirical findings to understand the current state of participatory
practice and subsequently provide guidance to better align participatory goals
and methods in a way that accounts for practical constraints.
","2023-10-03","2310.00907v1.pdf","The Participatory Turn in AI Design: Theoretical Foundations and
the Current State of Practice
Fernando Delgado∗
Stephen Yang∗†
Cornell University
Ithaca, New York, USA
fad33@cornell.edu
University of Southern California
Los Angeles, California, USA
stepheny@usc.edu
Michael Madaio∗†
Qian Yang∗
Google Research
New York, New York, USA
Cornell University
Ithaca, New York, USA
qianyangcornell.edu
ABSTRACT
ACM Reference Format:
Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. 2023. The
Participatory Turn in AI Design: Theoretical Foundations and the Current
State of Practice. In EAAMO ’23:ACM conference on Equity and Access in
Algorithms, Mechanisms, and Optimization, Boston, MA. ACM, New York,
NY, USA, 23 pages. https://doi.org/10.1145/3617694.3623261
1
INTRODUCTION
Despite the growing consensus that stakeholders affected by AI
systems should participate in their design, enormous variation
and implicit disagreements exist among current approaches. For
researchers and practitioners who are interested in taking a partic-
ipatory approach to AI design and development, it remains chal-
lenging to assess the extent to which any participatory approach
grants substantive agency to stakeholders. This article thus aims
to ground what we dub the “participatory turn” in AI design by
synthesizing existing theoretical literature on participation and
through empirical investigation and critique of its current practices.
Specifically, we derive a conceptual framework through synthesis of
literature across technology design, political theory, and the social
sciences that researchers and practitioners can leverage to evaluate
approaches to participation in AI design. Additionally, we articu-
late empirical findings concerning the current state of participatory
practice in AI design based on an analysis of recently published
research and semi-structured interviews with 12 AI researchers and
practitioners. We use these empirical findings to understand the
current state of participatory practice and subsequently provide
guidance to better align participatory goals and methods in a way
that accounts for practical constraints.
CCS CONCEPTS
• Human-centered computing → Participatory design; In-
teraction design theory, concepts and paradigms; Computer
supported cooperative work.
arXiv:2310.00907v1  [cs.HC]  2 Oct 2023
KEYWORDS
Artificial intelligence, machine learning, participation, participatory
design, power
∗The first two and last two authors contributed equally to this research.
†Stephen began this work while at Cornell University, and Michael began this work
while at Microsoft Research.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
EAAMO ’23, October 30–November 01, 2023, Boston, MA
© 2023 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0381-2/23/11.
https://doi.org/10.1145/3617694.3623261
As artificial intelligence (AI) systems are developed and deployed
across various sectors (e.g., hiring, healthcare, education, content
moderation), there have been increasing calls to involve members
of communities impacted by AI systems in their design [7, 99, 121,
129, 136, 137, 162, 178, 179, 213, 221]. In part, such calls for partic-
ipation in AI design argue that participation can enable AI systems
to better reflect the values, preferences, and needs of users and
other impacted stakeholders, or more broadly, that participation
will empower stakeholders in shaping the design of AI systems
[3, 7, 19, 26, 33, 51, 61, 118, 137, 143, 172, 189, 198–200].
However, despite a growing consensus that stakeholders should
participate more in AI design, there is enormous variation in the
methods and theories applied to achieve that participation, even
with respect to the goals for leveraging participation in the first
place. There are, for example, practitioners that take a community-
centric approach in defining and operationalizing stakeholder par-
ticipation upstream of any system or product design activity [110];
community collectives of African NLP researchers [156] and queer
AI researchers [175] have formed to design and evaluate algorith-
mic systems that reflect their communities’ needs. In a commercial
context, industry practitioners argue that participation helps de-
velop products that better align with people’s different wants and
needs and that it accelerates innovation [193] while improving
profitability [88]. Further downstream from a traditional design
phase, many practitioners involve stakeholders as the “human in-
frastructure” [144] or “data labor” [146] that enables ML systems to
work. This involves including stakeholders in data generation and
interactions with data collection infrastructures [17, 59, 144], data
labeling or annotation [103, 147, 169, 209], feedback for optimiza-
tion [15, 46, 81, 135], and end users’ discretionary work to make
the system work [5]. Meanwhile, some researchers have begun to
leverage language models to generate synthetic user research data
positing such data as suitable “proxies for understanding human
experiences” [10, 91].
"
"2310.00935","Yike Wang","Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha
  Balachandran, Tianxing He, Yulia Tsvetkov","Resolving Knowledge Conflicts in Large Language Models","","","","","cs.CL","http://creativecommons.org/licenses/by/4.0/","  Large language models (LLMs) often encounter knowledge conflicts, scenarios
where discrepancy arises between the internal parametric knowledge of LLMs and
non-parametric information provided in the prompt context. In this work we ask
what are the desiderata for LLMs when a knowledge conflict arises and whether
existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge
conflicts, 2) pinpoint conflicting information segments, and 3) provide
distinct answers or viewpoints in conflicting scenarios. To this end, we
introduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual
knowledge conflicts and quantitatively evaluating to what extent LLMs achieve
these goals. KNOWLEDGE CONFLICT includes diverse and complex situations of
knowledge conflict, knowledge from diverse entities and domains, two synthetic
conflict creation methods, and settings with progressively increasing
difficulty to reflect realistic knowledge conflicts. Extensive experiments with
the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in
identifying the existence of knowledge conflicts, they struggle to determine
the specific conflicting knowledge and produce a response with distinct answers
amidst conflicting information. To address these challenges, we propose new
instruction-based approaches that augment LLMs to better achieve the three
goals. Further analysis shows that abilities to tackle knowledge conflicts are
greatly impacted by factors such as knowledge domain and prompt text, while
generating robust responses to knowledge conflict scenarios remains an open
research question.
","2023-10-03","2310.00935v1.pdf","RESOLVING KNOWLEDGE CONFLICTS
IN LARGE LANGUAGE MODELS
Yike Wang∗1
Shangbin Feng∗2
Heng Wang3
Weijia Shi2
Vidhisha Balachandran4
Tianxing He2
Yulia Tsvetkov2
1University of California, Berkeley
2University of Washington
3Xi’an Jiaotong University
4Carnegie Mellon University
yike wang@berkeley.edu, shangbin@cs.washington.edu
ABSTRACT
Large language models (LLMs) often encounter knowledge conflicts, scenarios
where discrepancy arises between the internal parametric knowledge of LLMs and
non-parametric information provided in the prompt context. In this work we ask
what are the desiderata for LLMs when a knowledge conflict arises and whether
existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge con-
flicts, 2) pinpoint conflicting information segments, and 3) provide distinct an-
swers or viewpoints in conflicting scenarios. To this end, we introduce KNOWL-
EDGE CONFLICT, an evaluation framework for simulating contextual knowledge
conflicts and quantitatively evaluating to what extent LLMs achieve these goals.
KNOWLEDGE CONFLICT includes diverse and complex situations of knowledge
conflict, knowledge from diverse entities and domains, two synthetic conflict cre-
ation methods, and settings with progressively increasing difficulty to reflect re-
alistic knowledge conflicts. Extensive experiments with the KNOWLEDGE CON-
FLICT framework reveal that while LLMs perform well in identifying the exis-
tence of knowledge conflicts, they struggle to determine the specific conflicting
knowledge and produce a response with distinct answers amidst conflicting in-
formation. To address these challenges, we propose new instruction-based ap-
proaches that augment LLMs to better achieve the three goals. Further analysis
shows that abilities to tackle knowledge conflicts are greatly impacted by factors
such as knowledge domain and prompt text, while generating robust responses to
knowledge conflict scenarios remains an open research question. Code and data
are publicly available at github.com/yikee/Knowledge Conflict.
1
INTRODUCTION
arXiv:2310.00935v1  [cs.CL]  2 Oct 2023
Large language models (LLMs) have demonstrated remarkable capabilities to encode world knowl-
edge (Peters et al., 2018; Petroni et al., 2019) and solve knowledge-intensive tasks (Roberts et al.,
2020; Brown et al., 2020a). Nevertheless, their knowledge abilities are far from perfect (Sun et al.,
2023; Hernandez et al., 2023; Muhlgay et al., 2023), leading to the emergence of knowledge aug-
mentation approaches: using external sources (e.g., retrieval corpora (Fisch et al., 2019; Guu et al.,
2020; Shi et al., 2023b), search engines (Press et al., 2022; Nakano et al., 2021), and other LMs(Feng
et al., 2023d; Luo et al., 2023)) to provide relevant information in the prompt context. However, due
to issues such as misinformation, varying perspectives, time-sensitive information, or knowledge
updates, knowledge conflicts might arise, meaning that there is a discrepancy between parametric
knowledge (the internal knowledge stored in LLM parameters) and non-parametric knowledge (the
knowledge fetched from external sources (Chen et al., 2022; Xie et al., 2023)).
Prior research conducted preliminary studies by probing LLMs with knowledge conflicts and exam-
ined their behaviors in response (Chen et al., 2022). The key findings are that LLMs’ choices be-
tween knowledge sources, parametric or non-parametric, depend on factors including the coherence
of the external knowledge (Xie et al., 2023) and model size (Longpre et al., 2021). This work extends
these prior works by seeking a deeper understanding of whether LLMs can acknowledge knowledge
∗equal contribution
1
"
"2310.00996","Zhivar Sourati","Zhivar Sourati, Filip Ilievski, Pia Sommerauer","ARN: A Comprehensive Framework and Dataset for Analogical Reasoning on
  Narratives","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Analogical reasoning is one of the prime abilities of humans and is linked to
creativity and scientific discoveries. This ability has been studied
extensively in natural language processing (NLP) as well as in cognitive
psychology by proposing various benchmarks and evaluation setups. Yet, a
substantial gap exists between evaluations of analogical reasoning in cognitive
psychology and NLP. Our aim is to bridge this by computationally adapting
theories related to analogical reasoning from cognitive psychology in the
context of narratives and developing an evaluation framework large in scale.
More concretely, we propose the task of matching narratives based on system
mappings and release the Analogical Reasoning on Narratives (ARN) dataset. To
create the dataset, we devise a framework inspired by cognitive psychology
theories about analogical reasoning to utilize narratives and their components
to form mappings of different abstractness levels. These mappings are then
leveraged to create pairs of analogies and disanalogies/distractors with more
than 1k triples of query narratives, analogies, and distractors. We cover four
categories of far/near analogies and far/near distractors that allow us to
study analogical reasoning in models from distinct perspectives. In this study,
we evaluate different large language models (LLMs) on this task. Our results
demonstrate that LLMs struggle to recognize higher-order mappings when they are
not accompanied by lower-order mappings (far analogies) and show better
performance when all mappings are present simultaneously (near analogies). We
observe that in all the settings, the analogical reasoning abilities of LLMs
can be easily impaired by near distractors that form lower-order mappings with
the query narratives.
","2023-10-03","2310.00996v1.pdf","ARN: A Comprehensive Framework and Dataset for Analogical Reasoning on
Narratives
Zhivar Sourati1,2 , Filip Ilievski3 and Pia Sommerauer3
1Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA
2Department of Computer Science, University of Southern California, Los Angeles, CA, USA
3Vrije Universiteit Amsterdam, Netherlands
{souratih}@isi.edu, {f.ilievski,pia.sommerauer}@vu.nl,
Abstract
arXiv:2310.00996v1  [cs.CL]  2 Oct 2023
Figure 1: Our proposed task for evaluating analogical reasoning of
models where they are given a query narrative Q and two candi-
date narratives N and A. Given the instruction of the task to find
system mappings, the first narrative is considered analogous to the
query narrative because of the high-level message they share and
the system mapping that can be drawn based on that similarity (A).
However, the second narrative is not analogous to the query narra-
tive because of the misaligned high-level messages, regardless of the
other types of mappings that can be formed between the query nar-
rative and the second candidate narrative (N).
Analogical reasoning is one of the prime abilities of
humans and is linked to creativity and scientific dis-
coveries. This ability has been studied extensively
in natural language processing (NLP) as well as in
cognitive psychology by proposing various bench-
marks and evaluation setups. Yet, a substantial gap
exists between evaluations of analogical reasoning
in cognitive psychology and NLP. Our aim is to
bridge this by computationally adapting theories re-
lated to analogical reasoning from cognitive psy-
chology in the context of narratives and developing
an evaluation framework large in scale. More con-
cretely, we propose the task of matching narratives
based on system mappings and release the Analog-
ical Reasoning on Narratives (ARN) dataset. To
create the dataset, we devise a framework inspired
by cognitive psychology theories about analogical
reasoning to utilize narratives and their components
to form mappings of different abstractness levels.
These mappings are then leveraged to create pairs
of analogies and disanalogies/distractors with more
than 1k triples of query narratives, analogies, and
distractors. We cover four categories of far/near
analogies and far/near distractors that allow us to
study analogical reasoning in models from distinct
perspectives. In this study, we evaluate different
large language models (LLMs) on this task. Our re-
sults demonstrate that LLMs struggle to recognize
higher-order mappings when they are not accompa-
nied by lower-order mappings (far analogies) and
show better performance when all mappings are
present simultaneously (near analogies). We ob-
serve that in all the settings, the analogical rea-
soning abilities of LLMs can be easily impaired
by near distractors that form lower-order mappings
with the query narratives.
1
Introduction
Analogical reasoning is one of the most advanced abilities
that humans possess in contrast to non-humans [Penn et al.,
2008] and can be considered the core of cognition [Hofs-
tadter, 2001]. We can refer to analogical reasoning as the
recognition and utilization of similarities in different levels
between two situations given a purpose that controls what
similarities to detect and what to ignore [Holyoak, 2012].
Analogical reasoning has helped people to come up with
foundational inventions and breakthroughs in scientific fields
[Dunbar and Klahr, 2012] and to justify important political
decisions [Houghton, 1998]. An instance of analogical rea-
soning is demonstrated in Figure 1. By analogical reasoning,
the query narrative Q can be seen as analogous to narrative
"
"2310.01045","Yekun Chai","Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua
  Wu","Tool-Augmented Reward Modeling","","","","","cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reward modeling (a.k.a., preference modeling) is instrumental for aligning
large language models with human preferences, particularly within the context
of reinforcement learning from human feedback (RLHF). While conventional reward
models (RMs) have exhibited remarkable scalability, they oft struggle with
fundamental functionality such as arithmetic computation, code execution, and
factual lookup. In this paper, we propose a tool-augmented preference modeling
approach, named \name, to address these limitations by empowering RMs with
access to external environments, including calculators and search engines. This
approach not only fosters synergy between tool utilization and reward grading
but also enhances interpretive capacity and scoring reliability. Our study
delves into the integration of external tools into RMs, enabling them to
interact with diverse external sources and construct task-specific tool
engagement and reasoning traces in an autoregressive manner. We validate our
approach across a wide range of domains, incorporating seven distinct external
tools. Our experimental results demonstrate a noteworthy overall improvement of
17.7% across eight tasks in preference ranking. Furthermore, our approach
outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In
human evaluations, RLHF trained with Themis attains an average win rate of 32%
when compared to baselines across four distinct tasks. Additionally, we provide
a comprehensive collection of tool-related RM datasets, incorporating data from
seven distinct tool APIs, totaling 15,000 instances. We anticipate that this
publicly available dataset will facilitate and inspire further research
advancements in the field.
","2023-10-03","2310.01045v1.pdf","Preprint
TOOL-AUGMENTED REWARD MODELING
Lei Li∗♡
Yekun Chai∗♠
Shuohuan Wang♠
Yu Sun♠
Hao Tian♠
Ningyu Zhang♡
Hua Wu♠
♡Zhejiang University
♠Baidu Inc.
{leili21,zhangningyu}@zju.edu.cn
{chaiyekun,wangshuohuan,sunyu02}@baidu.com
ABSTRACT
Reward modeling (a.k.a., preference modeling) is instrumental for aligning large
language models with human preferences, particularly within the context of re-
inforcement learning from human feedback (RLHF). While conventional reward
models (RMs) have exhibited remarkable scalability, they oft struggle with fun-
damental functionality such as arithmetic computation, code execution, and fac-
tual lookup. In this paper, we propose a tool-augmented preference modeling
approach, named Themis, to address these limitations by empowering RMs with
access to external environments, including calculators and search engines. This
approach not only fosters synergy between tool utilization and reward grading
but also enhances interpretive capacity and scoring reliability. Our study delves
into the integration of external tools into RMs, enabling them to interact with
diverse external sources and construct task-specific tool engagement and reason-
ing traces in an autoregressive manner. We validate our approach across a wide
range of domains, incorporating seven distinct external tools. Our experimental
results demonstrate a noteworthy overall improvement of 17.7% across eight tasks
in preference ranking. Furthermore, our approach outperforms Gopher 280B by
7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF
trained with Themis attains an average win rate of 32% when compared to base-
lines across four distinct tasks. Additionally, we provide a comprehensive collec-
tion of tool-related RM datasets, incorporating data from seven distinct tool APIs,
totaling 15,000 instances. We anticipate that this publicly available dataset will
facilitate and inspire further research advancements in the field.
1
INTRODUCTION
arXiv:2310.01045v1  [cs.CL]  2 Oct 2023
Large language models (LLMs) have demonstrated remarkable potential in performing complex
tasks that demand expertise across diverse domains, such as programming (Chen et al., 2021; Li
et al., 2022; Chai et al., 2023; Li et al., 2023) and dialogue assistance (Bai et al., 2022a; Ouyang
et al., 2022; OpenAI, 2023; Anil et al., 2023; Touvron et al., 2023). Leveraging reinforcement
learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) has emerged
as a compelling approach for optimizing LLMs against reward models (RMs) to predict human
preferences. RMs serve as imperfect proxies for human feedback signals, producing rewards that
are pivotal for fine-tuning LLMs in RLHF.
However, RMs predict human preferences relying on static internal representations stored within
their weights, which inherently impose limitations of LLMs. These may encompass challenges in ac-
cessing real-time information on current events (Komeili et al., 2022) and a propensity to hallucinate
erroneous facts (Zhang et al., 2023), a lack of proficiency in arithmetic computation (Lewkowycz
et al., 2022), and difficulties in comprehending low-resource languages (Lin et al., 2022b), among
others. These limitations underscore the imperative need to engage external sources of information
to augment and improve the effectiveness of RMs.
To further motivate this shift, an intriguing question arises when considering the role of human la-
belers in generating RM data: do these labelers possess an intuitive inclination to resort to external
∗Equal contribution and shared co-first authorship.
Work done during Lei Li’s internship at Baidu.
1
"
"2310.01061","Linhao Luo","Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan","Reasoning on Graphs: Faithful and Interpretable Large Language Model
  Reasoning","22 pages, 4 figures","","","","cs.CL cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.
","2023-10-03","2310.01061v1.pdf","REASONING
ON GRAPHS:
FAITHFUL
AND INTER-
PRETABLE LARGE LANGUAGE MODEL REASONING
Linhao Luo, Yuan-Fang Li & Gholamreza Haffari
Monash University
Australia
{linhao.luo,yuanfang.li,Gholamreza.Haffari}@monash.edu
Shirui Pan∗
Griffith University
Australia
s.pan@griffith.edu.au
ABSTRACT
Large language models (LLMs) have demonstrated impressive reasoning abili-
ties in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning processes
and diminish their performance and trustworthiness. Knowledge graphs (KGs),
which capture vast amounts of facts in a structured format, offer a reliable source
of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning
methods only treat KGs as factual knowledge bases and overlook the importance
of their structural information for reasoning. In this paper, we propose a novel
method called reasoning on graphs (RoG) that synergizes LLMs with KGs to
enable faithful and interpretable reasoning. Specifically, we present a planning-
retrieval-reasoning framework, where RoG first generates relation paths grounded
by KGs as faithful plans. These plans are then used to retrieve valid reasoning
paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG
not only distills knowledge from KGs to improve the reasoning ability of LLMs
through training but also allows seamless integration with any arbitrary LLMs dur-
ing inference. Extensive experiments on two benchmark KGQA datasets demon-
strate that RoG achieves state-of-the-art performance on KG reasoning tasks and
generates faithful and interpretable reasoning results1.
1
INTRODUCTION
arXiv:2310.01061v1  [cs.CL]  2 Oct 2023
Large language models (LLMs) have shown great performance in many NLP tasks (Brown et al.,
2020; Bang et al., 2023). What’s especially striking is their ability to handle complex tasks through
reasoning (Wei et al., 2022; Huang & Chang, 2023). To further unleash LLMs’ reasoning ability, the
plan-and-solve paradigm (Wang et al., 2023c) has been proposed, in which LLMs are prompted to
generate a plan and execute each reasoning step. In this way, LLMs decompose complex reasoning
tasks into a series of sub-tasks and solve them step by step (Khot et al., 2022).
Despite their success, LLMs are still limited by the lack of knowledge and prone to hallucinations
during reasoning, which can lead to errors in reasoning processes (Hong et al., 2023; Wang et al.,
2023b). For example, as shown in Figure 1, LLMs do not have the latest knowledge and hallucinate
an incorrect reasoning step: “has a daughter”. These issues largely diminish the performance and
trustworthiness of LLMs in high-stakes scenarios, such as legal judgment and medical diagnosis.
To tackle the issues, knowledge graphs (KGs) have been incorporated to improve the reasoning
ability of LLMs (Pan et al., 2023; Luo et al., 2023). KGs capture abundant factual knowledge in a
structured format, which provides a faithful knowledge source for reasoning. As a typical reasoning
∗Corresponding author.
1Code and data are available at: https://github.com/RManLuo/reasoning-on-graphs
1
"
"2310.01089","Jianan Zhao","Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael
  Bronstein, Zhaocheng Zhu, Jian Tang","GraphText: Graph Reasoning in Text Space","Preprint. Work in progress","","","","cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Large Language Models (LLMs) have gained the ability to assimilate human
knowledge and facilitate natural language interactions with both humans and
other LLMs. However, despite their impressive achievements, LLMs have not made
significant advancements in the realm of graph machine learning. This
limitation arises because graphs encapsulate distinct relational data, making
it challenging to transform them into natural language that LLMs understand. In
this paper, we bridge this gap with a novel framework, GraphText, that
translates graphs into natural language. GraphText derives a graph-syntax tree
for each graph that encapsulates both the node attributes and inter-node
relationships. Traversal of the tree yields a graph text sequence, which is
then processed by an LLM to treat graph tasks as text generation tasks.
Notably, GraphText offers multiple advantages. It introduces training-free
graph reasoning: even without training on graph data, GraphText with ChatGPT
can achieve on par with, or even surpassing, the performance of
supervised-trained graph neural networks through in-context learning (ICL).
Furthermore, GraphText paves the way for interactive graph reasoning, allowing
both humans and LLMs to communicate with the model seamlessly using natural
language. These capabilities underscore the vast, yet-to-be-explored potential
of LLMs in the domain of graph machine learning.
","2023-10-03","2310.01089v1.pdf","Preprint
GRAPHTEXT: GRAPH REASONING IN TEXT SPACE
Jianan Zhao1,2, Le Zhuo3, Yikang Shen4, Meng Qu1,2, Kai Liu5
Michael Bronstein6, Zhaocheng Zhu1,2, Jian Tang1,7,8
1Mila - Qu´ebec AI Institute, 2Universit´e de Montr´eal, 3Beihang University,
4MIT-IBM Watson AI Lab, 5Division of gRED Computational Science, Genentech Inc.,
6University of Oxford, 7HEC Montr´eal, 8Canadian Institute for Advanced Research (CIFAR)
ABSTRACT
Large Language Models (LLMs) have gained the ability to assimilate human
knowledge and facilitate natural language interactions with both humans and other
LLMs. However, despite their impressive achievements, LLMs have not made
significant advancements in the realm of graph machine learning. This limitation
arises because graphs encapsulate distinct relational data, making it challenging
to transform them into natural language that LLMs understand. In this paper,
we bridge this gap with a novel framework, GRAPHTEXT, that translates graphs
to natural language. GRAPHTEXT derives a graph-syntax tree for each graph that
encapsulates both the node attributes and inter-node relationships. Traversal of the
tree yields a graph text sequence, which is then processed by an LLM to treat graph
tasks as text generation tasks. Notably, GRAPHTEXT offers multiple advantages.
It introduces training-free graph reasoning: even without training on graph data,
GRAPHTEXT with ChatGPT can achieve on par with, or even surpassing, the per-
formance of supervised-trained graph neural networks through in-context learning
(ICL). Furthermore, GRAPHTEXT paves the way for interactive graph reasoning,
allowing both humans and LLMs to communicate with the model seamlessly us-
ing natural language. These capabilities underscore the vast, yet-to-be-explored
potential of LLMs in the domain of graph machine learning.
1
INTRODUCTION
Language stands as a cornerstone of human civilization, acting as the primary medium for knowl-
edge encoding, reasoning, and communication. Large language models (LLMs), pre-trained on
extensive text corpora, have showcased remarkable reasoning skills (Brown et al., 2020; Bubeck
et al., 2023). These LLMs can communicate via natural language both internally (Wei et al., 2022)
and externally with humans or other LLMs (Li et al., 2023), demonstrating exceptional skills such
as multi-step reasoning (Yao et al., 2023a), decision-making (Yao et al., 2023b; Liang et al., 2023),
tool use (Schick et al., 2023), and multi-agent collaboration (Park et al., 2023; Hong et al., 2023).
arXiv:2310.01089v1  [cs.CL]  2 Oct 2023
Motivation.
Despite the remarkable success of LLMs in handling natural languages, their appli-
cation to other data modalities presents unique challenges, primarily because these data often lack
straightforward transformation into sequential text. These challenges are especially severe when
dealing with graph-structured data, as different graphs define structure and features in distinct ways.
Therefore, existing efforts within the graph machine learning field commonly require the training of
specific graph neural networks (GNNs) tailored to individual graphs (Kipf & Welling, 2017; Velick-
ovic et al., 2018; Xu et al., 2019). Often, models trained on one graph cannot generalize to the
unseen structure and feature representations of other graphs. Moreover, the gap between graphs and
human languages hinders the application of natural language reasoning to facilitate graph reasoning.
In light of these limitations, a question arises: can we derive a language for graph in natural lan-
guage? In this paper, we give an affirmative answer by proposing to use tree as an intermediary,
elegantly bridging structured data and one-dimensional sequential language. Essentially, a tree ex-
hibits a hierarchical structure, and traversing it yields a one-dimensional sequence. On top of that,
as shown in Figure 1 (c), we propose a novel framework GRAPHTEXT, which takes graph data to
build a graph-syntax tree. Traversing it results in a graph prompt expressed in natural language,
allowing an LLM to approach graph reasoning as a text-generation task.
1
"
"2310.01107","Jong Chul Ye","Hyeonho Jeong and Jong Chul Ye","Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image
  Diffusion Models","Project Page: http://ground-a-video.github.io","","","","cs.CV cs.AI cs.LG stat.ML","http://creativecommons.org/licenses/by-nc-sa/4.0/","  Recent endeavors in video editing have showcased promising results in
single-attribute editing or style transfer tasks, either by training
text-to-video (T2V) models on text-video data or adopting training-free
methods. However, when confronted with the complexities of multi-attribute
editing scenarios, they exhibit shortcomings such as omitting or overlooking
intended attribute changes, modifying the wrong elements of the input video,
and failing to preserve regions of the input video that should remain intact.
To address this, here we present a novel grounding-guided video-to-video
translation framework called Ground-A-Video for multi-attribute video editing.
Ground-A-Video attains temporally consistent multi-attribute editing of input
videos in a training-free manner without aforementioned shortcomings. Central
to our method is the introduction of Cross-Frame Gated Attention which
incorporates groundings information into the latent representations in a
temporally consistent fashion, along with Modulated Cross-Attention and optical
flow guided inverted latents smoothing. Extensive experiments and applications
demonstrate that Ground-A-Video's zero-shot capacity outperforms other baseline
methods in terms of edit-accuracy and frame consistency. Further results and
codes are provided at our project page (http://ground-a-video.github.io).
","2023-10-03","2310.01107v1.pdf","GROUND-A-VIDEO: ZERO-SHOT GROUNDED VIDEO
EDITING USING TEXT-TO-IMAGE DIFFUSION MODELS
Hyeonho Jeong & Jong Chul Ye
Kim Jaechul Graduate School of AI, KAIST
{hyeonho.jeong, jong.ye}@kaist.ac.kr
Figure 1: Ground-A-Video achieves multi-attribute editing, video style transfer with attribute change, and text-
to-video generation with pose guidance, all in a time-consistent and training-free fashion. The boxes in the
right-bottom images visualize the series of pose guidance.
ABSTRACT
arXiv:2310.01107v1  [cs.CV]  2 Oct 2023
Recent endeavors in video editing have showcased promising results in single-
attribute editing or style transfer tasks, either by training text-to-video (T2V)
models on text-video data or adopting training-free methods. However, when
confronted with the complexities of multi-attribute editing scenarios, they ex-
hibit shortcomings such as omitting or overlooking intended attribute changes,
modifying the wrong elements of the input video, and failing to preserve regions
of the input video that should remain intact. To address this, here we present
a novel grounding-guided video-to-video translation framework called Ground-
A-Video for multi-attribute video editing. Ground-A-Video attains temporally
consistent multi-attribute editing of input videos in a training-free manner with-
out aforementioned shortcomings. Central to our method is the introduction of
Cross-Frame Gated Attention which incorporates groundings information into the
latent representations in a temporally consistent fashion, along with Modulated
Cross-Attention and optical flow guided inverted latents smoothing. Extensive
experiments and applications demonstrate that Ground-A-Video’s zero-shot ca-
pacity outperforms other baseline methods in terms of edit-accuracy and frame
consistency. Further results and codes are provided at our project page.
1
INTRODUCTION
Coupled with massive text-image datasets (Schuhmann et al., 2022), the advent of diffusion mod-
els (Ho et al., 2020; Song et al., 2020b) has revolutionized text-to-image (T2I) generation, making
1
"
"2310.01119","Jean Kaddour","Jean Kaddour, Qi Liu","Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large
  Language Models","","","","","cs.CL cs.LG","http://creativecommons.org/licenses/by/4.0/","  The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed.
Alternatively, smaller models can solve specific tasks if fine-tuned with
enough labeled examples. These examples, however, are expensive to obtain. In
pursuit of the best of both worlds, we study the annotation and generation of
fine-tuning training data via fine-tuned teacher LLMs to improve the downstream
performance of much smaller models. In four text classification and two text
generation tasks, we find that both data generation and annotation dramatically
improve the respective downstream model's performance, occasionally
necessitating only a minor fraction of the original training dataset.
","2023-10-03","2310.01119v1.pdf","Text Data Augmentation in Low-Resource Settings
via Fine-Tuning of Large Language Models
Jean Kaddour
University College London
Qi Liu
University of Hong Kong
Abstract
The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed. Alterna-
tively, smaller models can solve specific tasks if fine-tuned with enough labeled
examples. These examples, however, are expensive to obtain. In pursuit of the
best of both worlds, we study the annotation and generation of fine-tuning training
data via fine-tuned teacher LLMs to improve the downstream performance of
much smaller models. In four text classification and two text generation tasks, we
find that both data generation and annotation dramatically improve the respective
downstream model’s performance, occasionally necessitating only a minor fraction
of the original training dataset.
1
Introduction
Large language models (LLMs) have demonstrated in-context learning (ICL) capabilities in various
natural language processing tasks, which allow us to perform an unseen downstream task by prompting
the model with a collection of input-target pairs and a single unlabeled example [8]. Crucially, ICL
requires relatively few labeled examples but large model sizes [35]. However, deploying LLMs in
real-world systems is challenging due to their computational costs and inference latencies [20].
An alternative paradigm that enables good results with much smaller models is to specialize a pre-
trained model for a single task through gradient-based supervised fine-tuning (SFT) [10, 11]. The
drawback of this approach is that it relies on labeled examples, which require human annotators and,
therefore, is expensive and time-consuming. Especially in low-resource settings with only a handful
of examples, SFT can be challenging [37].
arXiv:2310.01119v1  [cs.CL]  2 Oct 2023
In this work, we attempt to yield the best of both worlds by fine-tuning smaller models with training
data generated by an LLM. Following recent work on training data generation [30, 26, 20], we show
that by (i) annotating unlabeled examples or (ii) generating entirely new ones, we can effectively
transfer knowledge from the LLM (teacher) to the specialized model (student), which can be several
magnitudes of orders smaller, akin to knowledge distillation [17] but only via the exchange of data.
To our knowledge, our work is the first to show that data augmentation via fine-tuning the teacher
LLM (DAFTT), even on only extremely limited data, improves the synthetic data quality as measured
by the downstream model generalization performance. For example, fine-tuning a 20B LLM on as few
as 125 examples (5% of the RTE dataset [32]) increases the augmented-data-fine-tuned downstream
model’s performance by multiple percentage points.
We empirically verify our approach on four text classification and two natural language generation
tasks, finding that both (i) and (ii) consistently improve the student model’s downstream performance.
We provide ablation studies on varying amounts of synthetic data, comparisons with GPT3.5 [8] as a
teacher model, and evaluate the teacher LLMs directly on the downstream tasks.
Preprint. Under review.
"
"2310.01152","Sihao Hu","Sihao Hu, Tiansheng Huang, Fatih \.Ilhan, Selim Furkan Tekin, Ling Liu","Large Language Model-Powered Smart Contract Vulnerability Detection: New
  Perspectives","10 pages","IEEE International Conference on Trust, Privacy and Security in
  Intelligent Systems, and Applications 2023","","","cs.CR cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper provides a systematic analysis of the opportunities, challenges,
and potential solutions of harnessing Large Language Models (LLMs) such as
GPT-4 to dig out vulnerabilities within smart contracts based on our ongoing
research. For the task of smart contract vulnerability detection, achieving
practical usability hinges on identifying as many true vulnerabilities as
possible while minimizing the number of false positives. Nonetheless, our
empirical study reveals contradictory yet interesting findings: generating more
answers with higher randomness largely boosts the likelihood of producing a
correct answer but inevitably leads to a higher number of false positives. To
mitigate this tension, we propose an adversarial framework dubbed GPTLens that
breaks the conventional one-stage detection into two synergistic stages $-$
generation and discrimination, for progressive detection and refinement,
wherein the LLM plays dual roles, i.e., auditor and critic, respectively. The
goal of auditor is to yield a broad spectrum of vulnerabilities with the hope
of encompassing the correct answer, whereas the goal of critic that evaluates
the validity of identified vulnerabilities is to minimize the number of false
positives. Experimental results and illustrative examples demonstrate that
auditor and critic work together harmoniously to yield pronounced improvements
over the conventional one-stage detection. GPTLens is intuitive, strategic, and
entirely LLM-driven without relying on specialist expertise in smart contracts,
showcasing its methodical generality and potential to detect a broad spectrum
of vulnerabilities. Our code is available at:
https://github.com/git-disl/GPTLens.
","2023-10-18","2310.01152v1.pdf","Large Language Model-Powered Smart Contract
Vulnerability Detection: New Perspectives
Sihao Hu, Tiansheng Huang, Fatih ˙Ilhan, Selim Fukan Tekin, Ling Liu
School of Computer Science
Georgia Institute of Technology
Atlanta, GA 30332, United States
{sihaohu, thuang, filhan, stekin6, ling.liu}@gatech.edu
Abstract—This paper provides a systematic analysis of the
opportunities, challenges, and potential solutions of harnessing
LLMs to dig out vulnerabilities within smart contracts based
on our ongoing research. For the smart contract vulnerability
detection task, the key to achieving practical usability lies in
detecting as many true vulnerabilities as possible while mini-
mizing the number of false positives. However, our empirical
study using LLM as a detection tool reveals interesting yet
contradictory findings: generating more answers with higher
randomness largely increases the likelihood of a correct answer
being generated while inevitably leading to a higher number of
false positives, resulting in exhaustive manual verification efforts.
To mitigate this tension, we propose an adversarial framework
dubbed GPTLENS that breaks the traditional one-stage detection
into two synergistic stages − generation and discrimination, for
progressive detection and fine-tuning, wherein the LLM plays
dual roles, i.e.,
AUDITOR and
CRITIC, respectively. The goal
of AUDITOR is to identify multiple diverse vulnerabilities with
intermediate reasoning, while the goal of CRITIC is to evaluate
the accuracy of identified vulnerabilities and to examine the
integrity of the detection reasoning. Experimental results and
illustrative examples demonstrate that AUDITOR and CRITIC work
together harmoniously to yield significant improvements over the
traditional one-stage detection. GPTLENS is intuitive, strategic,
and entirely LLM-driven without relying on specialist expertise
in smart contracts, showcasing its methodical generality and
potential to detect a broad spectrum of vulnerabilities. Our code
is available at: https://github.com/git-disl/GPTLens.
Index Terms—Large language model, GPT, smart contract,
vulnerability detection
I. INTRODUCTION
arXiv:2310.01152v1  [cs.CR]  2 Oct 2023
Compared to the representative analysis tools [3], [4], [11],
[19], [24], [43] developed in recent years, LLM-powered
approaches feature some unparalleled advantages:
(1) Generality: Existing tools like Slither [11] require ex-
pert knowledge to design fixed-pattern detectors based on
control-flow or data-flow, restricting them to specific types
of vulnerabilities [47]. In contrast, LLMs can emulate human
linguistic understanding and reasoning and describe any type
of vulnerability using natural language, allowing them to
potentially detect a wider range of vulnerabilities, including
those that are unknown or uncategorized a priori.
(2) Interpretability: Generative LLMs can be utilized not
only to detect vulnerabilities but also to offer intermediate
reasoning about the detected vulnerabilities by following the
chain-of-thought [46]. For programming and software engi-
neering tasks, LLMs can provide insights into code under-
standing and even suggest code repair solutions [28]. Such
capabilities, if exploited intelligently, hold the potential to
grant a heightened level of transparency and trustworthiness
to the vulnerability detection process.
However, some challenges hinder LLMs from being ex-
ploited for their full potential in practice:
(1) LLMs can produce a large number of false positives [10],
resulting in a low precision and necessitate exhausting manual
verification efforts. These false positive cases can be catego-
rized as factual errors or potential risks, suggesting that they
can be distinguished from the true vulnerabilities w.r.t. the
metrics of correctness and severity.
(2) LLMs, if used in a naive manner, tend to fail to uncover
all vulnerabilities within the smart contract, leading to false
negatives. These undetected vulnerabilities can be categorized
into two main groups: First, those difficult cases that exceed
the “cognitive ability” of current LLMs; Second, those vul-
nerabilities that are detectable but were missed because of the
randomness of the generation. For the latter, our empirical
study shows that instead of generating deterministic answers in
one-shot, generating multiple answers with higher randomness
(diversity) can largely increase the likelihood of the true an-
swer being generated. However, this strategy presents a Catch-
22 dilemma [14] because it inevitably introduces more false
positives, i.e., the goal of detecting more true vulnerabilities
is not aligned with the goal of reducing false positives.
To mitigate this tension, we propose GPTLENS, an effective
Smart contracts, commonly associated with cryptocurrency
transactions on blockchains, suffer from financial losses up
to billions of dollars due to vulnerability exploitation [58].
Because of the immutable nature once smart contracts are
deployed, auditing plays an essential role in their development.
Recently, generative large language models [5], [26], [57]
(LLMs) are rapidly emerging and reshaping the domain of
software engineering [15], facilitating tasks of code genera-
tion [9], code understanding [45], and code repair [28]. Lever-
aging the capabilities of LLMs to empower smart contract
auditing presents a promising application opportunity. In this
paper, we envision the development of LLM-powered smart
contract vulnerability detection techniques from a new per-
spective, followed by a systematic analysis of the opportunities
and challenges involved in this nascent research topic.
"
"2310.01188","Gabriele Sarti","Gabriele Sarti, Grzegorz Chrupa{\l}a, Malvina Nissim, Arianna Bisazza","Quantifying the Plausibility of Context Reliance in Neural Machine
  Translation","Preprint, under review. 24 pages, 8 figures","","","","cs.CL cs.AI cs.HC cs.LG","http://creativecommons.org/licenses/by-sa/4.0/","  Establishing whether language models can use contextual information in a
human-plausible way is important to ensure their safe adoption in real-world
settings. However, the questions of when and which parts of the context affect
model generations are typically tackled separately, and current plausibility
evaluations are practically limited to a handful of artificial benchmarks. To
address this, we introduce Plausibility Evaluation of Context Reliance
(PECoRe), an end-to-end interpretability framework designed to quantify context
usage in language models' generations. Our approach leverages model internals
to (i) contrastively identify context-sensitive target tokens in generated
texts and (ii) link them to contextual cues justifying their prediction. We use
PECoRe to quantify the plausibility of context-aware machine translation
models, comparing model rationales with human annotations across several
discourse-level phenomena. Finally, we apply our method to unannotated
generations to identify context-mediated predictions and highlight instances of
(im)plausible context usage in model translations.
","2023-10-03","2310.01188v1.pdf","QUANTIFYING THE PLAUSIBILITY OF CONTEXT
RELIANCE IN NEURAL MACHINE TRANSLATION
Gabriele Sarti1
Grzegorz Chrupała2
Malvina Nissim1
Arianna Bisazza1
1Center for Language and Cognition (CLCG), University of Groningen
2Dept. of Cognitive Science and Artificial Intelligence (CSAI), Tilburg University
{g.sarti, m.nissim, a.bisazza}@rug.nl, grzegorz@chrupala.me
ABSTRACT
Establishing whether language models can use contextual information in a human-
plausible way is important to ensure their safe adoption in real-world settings.
However, the questions of when and which parts of the context affect model
generations are typically tackled separately, and current plausibility evaluations
are practically limited to a handful of artificial benchmarks. To address this, we
introduce Plausibility Evaluation of Context Reliance (PECORE), an end-to-end
interpretability framework designed to quantify context usage in language models’
generations. Our approach leverages model internals to (i) contrastively identify
context-sensitive target tokens in generated texts and (ii) link them to contextual
cues justifying their prediction. We use PECORE to quantify the plausibility
of context-aware machine translation models, comparing model rationales with
human annotations across several discourse-level phenomena. Finally, we apply
our method to unannotated generations to identify context-mediated predictions
and highlight instances of (im)plausible context usage in model translations.
1
INTRODUCTION
arXiv:2310.01188v1  [cs.CL]  2 Oct 2023
Research in NLP interpretability defines various desiderata for rationales of model behaviors, i.e. the
contributions of input tokens toward model predictions computed using feature attribution (Madsen
et al., 2022). One of such properties is plausibility, corresponding to the alignment between model
rationales and salient input words identified by human annotators (Jacovi & Goldberg, 2020). Plausi-
bility assessment is useful for highlighting bias and generalization failures in models’ predictions,
and especially to identify cases of models being “right for the wrong reasons” (McCoy et al., 2019).
However, while plausibility has an intuitive interpretation for classification tasks where a single
prediction is produced, extending this methodology to generative language models (LMs) presents
several challenges. First, LMs have a large output space where semantically equivalent tokens (e.g.
“PC” and “computer”) are competing candidates for next-word prediction (Holtzman et al., 2021).
Moreover, LMs generations are the product of optimization pressures to ensure independent properties
such as semantic relatedness, topical coherence and grammatical correctness, which can hardly be
captured by a single rationale (Yin & Neubig, 2022). Finally, since autoregressive generation involves
an iterative prediction process, model rationales could be extracted for every generated token, raising
the issue of which generated tokens can have plausible contextual explanations.
Recent attribution techniques for explaining language models incorporate contrastive alternatives to
disentangle different aspects of model predictions (e.g. the choice of “meowing” over “screaming” to
complete “The cat is ___” can be explained by semantics but not by grammaticality) (Ferrando et al.,
2023; Sarti et al., 2023). However, these studies avoid the issues above by narrowing the evaluation
to a single generation step matching a phenomenon of interest. For example, given the sentence “The
pictures of the cat ___”, a plausible rationale for the prediction of the word “are” should reflect the
role of “pictures” in subject/verb agreement. While this approach can be useful to validate model
rationales, it confines plausibility assessment to a small set of handcrafted benchmarks where tokens
with plausible explanations are known in advance. Moreover, it risks overlooking important patterns
of context usage, including those not immediately matching linguistic intuitions. In light of this, we
suggest that identifying which generated tokens were most affected by input information should be
an integral part of plausibility evaluation for language generation tasks.
1
"
