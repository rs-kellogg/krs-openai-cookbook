{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-text\n",
    "\n",
    "<font color='purple'>**Whisper**</font> is a speech-to-text service provided by **OpenAI**. It relies on **ASR** (Automatic Speech Recognition) technology, which is used to convert spoken language into written text.  The system is trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. English-language audio matched to English transcripts, non-English audio matched to English transcripts, and non-English audio matched to the corresponding transcript comprise 65%, 18%, and 17% of the data, respectively. Altogether, Whisper can translate audio from 99 different languages. It supports the following file types: _m4a_, _mp3_, _webm_, _mp4_, _mpga_, _wav_, and _mpeg_.\n",
    "\n",
    "In this tutorial, we'll show you how to use <font color='purple'>**Whisper**</font> with your OpenAI API credentials. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Example Use Cases:_\n",
    "\n",
    "Some example use cases for your work could include transcribing:\n",
    "\n",
    "- a Google or Zoom meeting or interview;\n",
    "- an in-person interview recorded on your computer (example today);\n",
    "- a presentation you make at a conference;\n",
    "- any other type of recorded text, like an earnings call audio clip (example provided). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Getting Started_\n",
    "\n",
    "You can install Whisper with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the following libraries in our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wave\n",
    "# pip install pyaudio\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyaudio\n",
    "import wave\n",
    "import openai\n",
    "import whisper\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to reference your API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: sk-2...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # pip install python-dotenv\n",
    "\n",
    "# load the .env file containing your API key\n",
    "load_dotenv()\n",
    "\n",
    "# display (obfuscated) API key\n",
    "print(f\"OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Example A: Toy Marketing Study for New Candy Reviews_\n",
    "\n",
    "Before we got started today, we offered everyone the option to try some candy they never had before. If this was a real marketing study we could record the audio using our computer's microphones with the code below.\n",
    "\n",
    "#### <font color='purple'>**AUDIO RECORDING**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio and save as a WAV file\n",
    "def record_audio(output_dir, participant_name, candy_name, duration=15):\n",
    "    # Use PyAudio to capture audio from the microphone\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "    rate = 44100\n",
    "    frames_per_buffer = 1024\n",
    "    audio_duration_seconds = duration \n",
    "\n",
    "    stream = audio.open(format=format,\n",
    "                        channels=channels,\n",
    "                        rate=rate,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=frames_per_buffer)\n",
    "\n",
    "    print(\"Recording audio...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(rate / frames_per_buffer * audio_duration_seconds)):\n",
    "        data = stream.read(frames_per_buffer)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Finished recording.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the captured audio as a WAV file\n",
    "    output_file = f\"{output_dir}/{participant_name}_{candy_name}.wav\"\n",
    "    audio_file_name = output_file\n",
    "\n",
    "    # Use wave module to write the frames to a WAV file\n",
    "    with wave.open(audio_file_name, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(audio.get_sample_size(format))\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(f\"Audio saved as {audio_file_name}\")\n",
    "\n",
    "    return audio_file_name\n",
    "\n",
    "# set output directory\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can record three reviews: (1) an honest review of something disgusting; (2) a polite review; and (3) a sarcastic review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# honest\n",
    "# audio = record_audio(output_dir, \"person\", \"gross\", duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polite\n",
    "# audio = record_audio(output_dir, \"person\", \"polite\", duration=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarcastic\n",
    "# audio = record_audio(output_dir, \"person\", \"sarcasm\", duration=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>**TRANSCRIPTION**</font>\n",
    "<font color='purple'>**Whisper**</font> makes it easy to transcribe the audio. Let's transcribe some of the candy reviews we recorded.\n",
    "\n",
    "Here's the honest review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, that was gross.\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "# transcribe the honest review\n",
    "audio = './person_gross.wav'\n",
    "audio_file = open(audio, \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file\n",
    "    )\n",
    "\n",
    "# transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "print(transcript.text)\n",
    "honest = transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the polite review: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um, that was, um, the wrapper, the wrapper was pretty, um, the taste was, um, interesting.\n"
     ]
    }
   ],
   "source": [
    "# transcribe the polite review\n",
    "audio = './person_polite.wav'\n",
    "audio_file = open(audio, \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file\n",
    "    )\n",
    "print(transcript.text)\n",
    "polite = transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here's the sarcastic one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, that was so good!\n"
     ]
    }
   ],
   "source": [
    "# record and transcribe the honest review\n",
    "audio = './person_sarcasm.wav'\n",
    "audio_file = open(audio, \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file\n",
    "    )\n",
    "print(transcript.text)\n",
    "sarcastic = transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>**SENTIMENT ANALYSIS**</font>\n",
    "\n",
    "We can also conduct a basic sentiment analysis on the generated audio output for the candy reviews. To analyze sentiments, we'll apply a pre-trained model called **`roberta-base-go-emotions'** from the Hugging Face Model Hub (https://huggingface.co/docs/hub/models-the-hub).\n",
    "\n",
    "<font color='blue'>_Note that the sentiment analysis code was summarized from this blog post: https://www.smashingmagazine.com/2023/09/generating-real-time-audio-sentiment-analysis-ai/.  Please see the original post for additional details._</font>\n",
    "\n",
    "First, we load the Whisper model for speech recognition.  Then, we initialize the sentiment analysis using a pre-trained model from Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Whisper and initialize the sentiment analysis using Roberta\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "sentiment_analysis = pipeline(\n",
    "  \"sentiment-analysis\",\n",
    "  framework=\"pt\",\n",
    "  model=\"SamLowe/roberta-base-go_emotions\",\n",
    "  top_k=3 # change this number to retrieve more than 3  sentiments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a function that extracts the top 3 emotions, scores, and corresponding emojis from the audio clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the sentiment and score from the Hugging Face model\n",
    "def analyze_sentiment(text):\n",
    "    results = sentiment_analysis(text)\n",
    "    results = results[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the corresponding emoji for the sentiment\n",
    "def get_sentiment_emoji(sentiment):\n",
    "  # Define the mapping of sentiments to emojis\n",
    "  emoji_mapping = {\n",
    "    \"disappointment\": \"üòû\",\n",
    "    \"sadness\": \"üò¢\",\n",
    "    \"annoyance\": \"üò†\",\n",
    "    \"neutral\": \"üòê\",\n",
    "    \"disapproval\": \"üëé\",\n",
    "    \"realization\": \"üòÆ\",\n",
    "    \"nervousness\": \"üò¨\",\n",
    "    \"approval\": \"üëç\",\n",
    "    \"joy\": \"üòÑ\",\n",
    "    \"anger\": \"üò°\",\n",
    "    \"embarrassment\": \"üò≥\",\n",
    "    \"caring\": \"ü§ó\",\n",
    "    \"remorse\": \"üòî\",\n",
    "    \"disgust\": \"ü§¢\",\n",
    "    \"grief\": \"üò•\",\n",
    "    \"confusion\": \"üòï\",\n",
    "    \"relief\": \"üòå\",\n",
    "    \"desire\": \"üòç\",\n",
    "    \"admiration\": \"üòå\",\n",
    "    \"optimism\": \"üòä\",\n",
    "    \"fear\": \"üò®\",\n",
    "    \"love\": \"‚ù§Ô∏è\",\n",
    "    \"excitement\": \"üéâ\",\n",
    "    \"curiosity\": \"ü§î\",\n",
    "    \"amusement\": \"üòÑ\",\n",
    "    \"surprise\": \"üò≤\",\n",
    "    \"gratitude\": \"üôè\",\n",
    "    \"pride\": \"ü¶Å\"\n",
    "  }\n",
    "  return emoji_mapping.get(sentiment, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it altogether\n",
    "def display_sentiment_results(text):\n",
    "    sentiment_results = analyze_sentiment(text)\n",
    "    for sentiment in sentiment_results:\n",
    "        label = sentiment['label']\n",
    "        emoji = get_sentiment_emoji(label)\n",
    "        result = f\"{label} {emoji}: {sentiment['score']}\"\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are sentiment results for the honest, polite, and sarcastic reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um, that was, um, the wrapper, the wrapper was pretty, um, the taste was, um, interesting.\n",
      "admiration üòå: 0.6466179490089417\n",
      "approval üëç: 0.15136606991291046\n",
      "excitement üéâ: 0.09306973963975906\n"
     ]
    }
   ],
   "source": [
    "print(polite)\n",
    "display_sentiment_results(polite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, that was gross.\n",
      "disgust ü§¢: 0.8205026984214783\n",
      "annoyance üò†: 0.10063128918409348\n",
      "neutral üòê: 0.06488493829965591\n"
     ]
    }
   ],
   "source": [
    "print(honest)\n",
    "display_sentiment_results(honest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, that was so good!\n",
      "admiration üòå: 0.9531673192977905\n",
      "approval üëç: 0.03131399303674698\n",
      "excitement üéâ: 0.01284793484956026\n"
     ]
    }
   ],
   "source": [
    "print(sarcastic)\n",
    "display_sentiment_results(sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Example B: Earnings Call Transcript for the `Chocolate Covered Stuff' company_\n",
    "\n",
    "<font color='blue'>_Note that this example and explanation builds off the one provided here: https://cookbook.openai.com/examples/whisper_prompting_guide. Please see the original post for additional details._</font>\n",
    "\n",
    "Let's take another toy example. I asked GPT to create a 20-second earnings call transcript for a Pakistani company called **Chocolate Covered Stuff**. I then recorded that transcript using the audio code provided earlier. You can do something similar and record it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio = record_audio(output_dir, \"choco_stuff\", \"earnings_call\", duration=45)\n",
    "# audio = record_audio(output_dir, \"urdu_hindi_line\", \"earnings_call\", duration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a transcription of this earnings call here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(text=\"As-salamu alaykum chocolate-covered-stuff family! Today's call is sweeter than the rumor of free chocolate bars. Our profits have surged faster than admiration for an Allama Iqbal poem. As we celebrate our success, it's with a heavy heart that we have to bid farewell to our esteemed CFO Mr. Choco Khan, who's been as sweet as our treat. His dedication has been integral to our growth, much like the chocolate covering our unique confections. And now, the moment that marks a new chapter for us, introducing our delightful line of chocolate-covered rickshaws with Kashmiri chai sprinkles. Hum duniya ko chocolate se muskarate hue banate hain. Let's keep shaping the world with a chocolate-covered smile.\")\n"
     ]
    }
   ],
   "source": [
    "earnings_audio = 'choco_stuff_earnings_call.wav'\n",
    "audio_file = open(earnings_audio, \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file\n",
    "    )\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to improve this transcription, we can use **prompts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>**PROMPTS**</font>\n",
    "\n",
    "<font color='purple'>**Whisper**</font> has an optional parameter called a **prompt**.  Prompts can be used for the following:\n",
    "\n",
    "- **Provide Context for Transcription** - stitch together multiple audio segments by providing the text of a previous one to give context to the next one. \n",
    "- **Make Spelling Corrections** - spell specific words and names mentioned in the audio clip, like Famke Janssen or Lupita Nyong'o.\n",
    "- **Specify which Language is Used**\n",
    "\n",
    "Unlike GPT prompting, these prompts cannot instruct the model to perform specific tasks. For instance \"Format listed items into Markdown format\" or \"Translate a French phrase to English\" will not work within the Whisper prompts.  \n",
    "\n",
    "The function below shows how to use prompts with Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a wrapper function for seeing how prompts affect transcriptions\n",
    "def transcribe(audio_filepath, prompt: str) -> str:\n",
    "    \"\"\"Given a prompt, transcribe the audio file.\"\"\"\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        file=open(audio_filepath, \"rb\"),\n",
    "        model=\"whisper-1\",\n",
    "        prompt=prompt,\n",
    "    )\n",
    "    return transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's transcribe the earnings call with a prompt.  Let's also conduct a sentiment analysis on the call text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(text=\"Assalamu alaikum chocolate covered stuff family! Today's call is sweeter than the rumor of free chocolate bars. Our profits have surged faster than admiration for an Allama Iqbal poem. As we celebrate our success, it's with a heavy heart that we have to bid farewell to our esteemed CFO, Mr. Chacko Khan, who's been as sweet as our treat. His dedication has been integral to our growth, much like the chocolate covering our unique confections. And now, the moment that marks a new chapter for us, introducing our delightful line of chocolate-covered rickshaws with Kashmiri chai sprinkles. Hum duniya ko chocolate se muskarate hue banate hain. Let's keep shaping the world with a chocolate-covered smile.\")\n",
      "joy üòÑ: 0.4196963310241699\n",
      "admiration üòå: 0.3140278458595276\n",
      "excitement üéâ: 0.09640142321586609\n"
     ]
    }
   ],
   "source": [
    "# earnings_call = transcribe(earnings_audio, prompt=\"Allama Iqbal was a great poet. Translate one line into Urdu text.\")\n",
    "earnings_call = client.audio.transcriptions.create(\n",
    "    prompt=\"Allama Iqbal was a great poet. Translate one line into Urdu text.\",\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file\n",
    "    )\n",
    "print(earnings_call)\n",
    "display_sentiment_results(earnings_call.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice, one line in the earnings call was in Urdu.  However, Whisper did not translate it or write it in _Nastaliq_ or Perso-Arabic script. Let's take that foreign-language Audio clip itself and see what Whisper does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'€ÅŸÖ ÿØŸÜ€åÿß ⁄©Ÿà ⁄Üÿß⁄©ŸÑŸπ ÿ≥€í ŸÖÿ≥⁄©ÿ±ÿßÿ™€í €ÅŸàÿ¶€í ÿ®ŸÜÿßÿ™€í €Å€å⁄∫€î'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's keep shaping the world with a chocolate-covered smile!); Hum duniya ko chocolate se muskuratay hue banatay hain\n",
    "urdu_hindi_audio = './urdu_hindi_earnings_call.wav'\n",
    "transcribe(urdu_hindi_audio, prompt=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without a prompt, it seemed to know it was Urdu.  However, we can use prompts to tell Whisper it is Hindi instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§π‡§Æ ‡§¶‡•Å‡§®‡•ç‡§Ø‡§æ ‡§ï‡•ã ‡§ö‡•å‡§ï‡§≤‡•á‡§ü ‡§∏‡•á ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§∞‡§æ‡§§‡•á ‡§π‡•Å‡§è ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe(urdu_hindi_audio, prompt=\"This is in Hindi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it wrote the words in _Devanagari_ script. Now what happens if we prompt Whisper to translate it to English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'€ÅŸÖ ÿØŸÜ€åÿß ⁄©Ÿà ⁄Üÿß⁄©ŸàŸÑ€åŸπ ÿ≥€í ŸÖÿ≥⁄©ÿ±ÿßÿ™€í €ÅŸàÿ¶€í ÿ®ŸÜÿßÿ™€í €Å€å⁄∫'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe(urdu_hindi_audio, prompt=\"Translate this Urdu line to English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this doesn't work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
